\section{背景知识}
\label{sec:related}

近年来，面向图结构数据的表示学习与图神经网络（Graph Neural Networks, GNN）得到了广泛关注。
本节从三个方面对相关工作进行简要综述：
首先回顾图结构数据与图表示学习的基础；讨论图神经网络模型的主要类别，及其鲁棒性与可扩展性；
最后结合课程涉及的若干应用领域，重点介绍自然语言处理、代码智能及其与大语言模型的结合。


\subsection{图结构数据与图表示学习}

\subsubsection{图及其典型任务}

图是一类用于刻画实体及其关系的通用数据结构，通常记为 $G = (V, E)$，其中 $V$ 为节点集合，$E$ 为边集合。
根据具体场景，可以得到带权图、有向图、异质图、多关系图和二分图等多种变体，广泛存在于社交网络、引文网络、知识图谱、生物分子结构以及程序代码等领域。

围绕图结构数据，常见学习任务大致包括三个层次：
节点级任务（如节点分类与节点属性预测）、边级任务（如链接预测和关系预测）以及以子图或整图为对象的任务（如图分类）。
这些任务的一个共同需求是将图中的节点、边或子图映射到低维向量空间，
以便采用标准的机器学习或深度学习模型进行处理，这一过程通常被称为图表示学习或图嵌入。

\subsubsection{谱图论与图信号处理}

在图上构建表示学习和神经网络模型时，一个核心问题是如何形式化"相邻节点应当有相似表示"这一直观要求。
谱图论提供了一种自然的刻画方式。
给定图的邻接矩阵 $A$ 和度矩阵 $D$，无向图的拉普拉斯矩阵通常定义为
\begin{equation}
    \label{eq:laplacian_simple}
    L = D - A.
\end{equation}
拉普拉斯二次型 $x^\top L x$ 可以被理解为对图信号平滑性的度量：
当相邻节点取值差异较大时，该值变大；当相邻节点取值相近时，该值较小，因此 $L$ 在一定意义上刻画了信号在图上的“粗糙程度”。

通过对拉普拉斯矩阵进行特征分解，可以得到一组“图上的正交基”，并据此定义图傅立叶变换，将图信号分解为不同“频率”的成分。
特征值较小的模式对应在图上变化缓慢的低频成分，特征值较大的模式对应变化剧烈的高频成分。
在这一框架下，对图信号进行低通滤波就对应于一种“平滑”操作，即在图结构约束下拉近相邻节点的特征。
许多谱方法的图卷积以及部分消息传递式 GNN，都可以在这一谱图论视角下解释为某种形式的图滤波。

\subsubsection{图嵌入方法}

在对图信号平滑性有了基本理解之后，一个自然的问题是如何据此构造具体的表示学习算法，
即如何将图中的节点嵌入到欧氏空间中的向量表示。
在图神经网络框架出现之前，相关研究主要沿着两条思路展开。

一条思路基于矩阵分解和谱嵌入，通过对拉普拉斯矩阵或相似度矩阵进行特征分解，
将节点嵌入到低维空间，并尽量保持图的局部邻接关系或整体结构，
拉普拉斯特征映射（Laplacian Eigenmaps）与谱聚类等方法属于这一类。
另一条思路基于随机游走和词向量技术。
在图上进行随机游走，将得到的节点序列类比为自然语言中的句子，
再借助word2vec风格的目标函数学习节点嵌入。
该方法认为在随机游走中经常共同出现的节点应具有相似的表示，
从而在嵌入空间中保持一阶、二阶邻接关系以及更高阶共现信息，典型代表包括DeepWalk、LINE等方法。

总体来看，这些方法解决了如何将图编码为向量的问题，
但通常采用"先嵌入、再建模"的两阶段流程，
嵌入学习与下游任务相互独立，无法端到端优化。
为实现表示学习与任务目标的联合训练，图神经网络应运而生。

\subsection{图神经网络模型及其扩展}

\subsubsection{核心思想与主要类别}

图神经网络将卷积神经网络的“局部聚合与参数共享”扩展到一般图结构，
通过消息传递让节点从邻居收集信息并非线性变换，叠加多层后逐步扩展感受野，
从而在统一框架下建模局部结构和长距离依赖。

按卷积定义方式，GNN 大致分为谱域方法和空间域方法。
谱域方法基于拉普拉斯谱和图傅立叶变换定义卷积，并通过多项式近似实现局部化与高效计算；
空间域方法（消息传递方法）直接在邻接结构上进行邻居聚合，是当前应用最广的一类，
可覆盖节点级（如节点分类、类型预测）、边级（如链接预测）和图级（如图分类）等任务。

\subsubsection{鲁棒性与可扩展性}

实际图数据往往含有噪声甚至对抗性扰动。
已有工作从攻击对象、攻击目标和攻击者知识等维度刻画图对抗攻击，
并通过对抗训练、学习“干净”图结构以及在聚合过程中加入结构正则等手段提升 GNN 的鲁棒性。

在大规模图上，标准 GCN 等模型面临邻域爆炸问题，计算和存储开销随层数迅速增长。
为此，研究者提出逐点采样、逐层采样以及基于图划分的子图／簇采样等策略，
在控制计算与显存成本的前提下，将 GNN 扩展到包含百万乃至上亿节点的图数据。

\subsection{图神经网络应用}

\subsubsection{图神经网络与代码智能}

在代码智能（code intelligence）场景中，程序语言具有严格语法和可执行语义，
适合构建抽象语法树（AST）、控制流图（CFG）、数据流图（DFG）和代码属性图（CPG）等程序图表示。
在这些图上应用 GNN，通过多跳消息传递可以同时利用局部上下文与远距离依赖，
从语法与语义两方面刻画代码结构。

基于程序图的 GNN 已用于多种代码分析任务：
图级任务如漏洞检测、恶意代码识别，需对函数或代码片段整体判定；
节点级任务如静态类型推断、变量误用检测、变量命名建议，需对变量或表达式节点精细预测；
序列到序列任务中也可结合程序图进行代码摘要生成或代码搜索。
其中，Typilus 所研究的类型推断就是在程序图上进行的节点级预测任务。

\subsubsection{其他应用}

除代码智能外，GNN 在生物信息学中用于分子性质预测、蛋白质结构建模及药物相关相互作用预测；
在自然语言处理领域，通过依存树、语义图或文档图提升语义角色标注、关系抽取和多跳问答等任务对长距离依赖的建模能力；
在知识图谱和多关系网络中，通过消息传递进行实体表示学习，在知识补全和多跳推理中取得良好效果。

近年来，GNN 与大语言模型（Large Language Models, LLM）的结合也受到关注：
一方面在图结构上引入 Transformer 或进行图预训练，并将图表示作为 LLM 的条件输入；
另一方面利用 LLM 为节点和边生成或补全语义信息。
二者的协同为自然语言处理与代码分析等场景带来新的建模空间。
