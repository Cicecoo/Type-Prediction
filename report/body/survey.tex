\section{相关工作与背景}
\label{sec:related}

近年来，面向图结构数据的表示学习与图神经网络（Graph Neural Networks, GNN）得到了广泛关注。
本节从三个方面对相关工作进行简要综述：
首先回顾图结构数据与图表示学习的基础；其次讨论图神经网络模型的主要类别及其在鲁棒性与可扩展性方面的扩展；
最后结合课程涉及的若干应用领域，重点介绍自然语言处理、代码智能及其与大语言模型的结合，为后续实验部分奠定背景。

\subsection{图结构数据与图表示学习}

\subsubsection{图及其典型任务}

图是一类用于刻画实体及其关系的通用数据结构，通常记为 $G = (V, E)$，其中 $V$ 为节点集合，$E$ 为边集合。
根据具体场景，可以得到带权图、有向图、异质图、多关系图和二分图等多种变体，广泛存在于社交网络、引文网络、知识图谱、生物分子结构以及程序代码等领域。

围绕图结构数据，常见学习任务大致包括三个层次：
节点级任务（如节点分类与节点属性预测）、边级任务（如链接预测和关系预测）以及以子图或整图为对象的任务（如社区发现与图分类）。
这些任务的一个共同需求是将图中的节点、边或子图映射到低维向量空间，
以便采用标准的机器学习或深度学习模型进行处理，这一过程通常被称为图表示学习或图嵌入。

\subsubsection{谱图论直觉与图信号处理}

在图上构建表示学习和神经网络模型时，一个核心问题是如何形式化“相邻节点应当有相似表示”这一直观要求。
谱图论提供了一种自然的刻画方式。
给定图的邻接矩阵 $A$ 和度矩阵 $D$，无向图的拉普拉斯矩阵通常定义为
\begin{equation}
    \label{eq:laplacian_simple}
    L = D - A.
\end{equation}
拉普拉斯二次型 $x^\top L x$ 可以被理解为对图信号平滑性的度量：
当相邻节点取值差异较大时，该值变大；当相邻节点取值相近时，该值较小，因此 $L$ 在一定意义上刻画了信号在图上的“粗糙程度”。

通过对拉普拉斯矩阵进行特征分解，可以得到一组“图上的正交基”，并据此定义图傅立叶变换，将图信号分解为不同“频率”的成分。
特征值较小的模式对应在图上变化缓慢的低频成分，特征值较大的模式对应变化剧烈的高频成分。
在这一框架下，对图信号进行低通滤波就对应于一种“平滑”操作，即在图结构约束下拉近相邻节点的特征。
许多谱方法的图卷积以及部分消息传递式 GNN，都可以在这一谱图论视角下解释为某种形式的图滤波。

\subsubsection{早期图嵌入方法}

在对图信号平滑性有了基本理解之后，一个自然的问题是如何据此构造具体的表示学习算法，即如何将图中的节点嵌入到欧氏空间中的向量表示。
在图神经网络框架出现之前，相关研究主要沿着两条思路展开。

一条思路基于矩阵分解和谱嵌入，通过对拉普拉斯矩阵或相似度矩阵进行特征分解，
将节点嵌入到低维空间，并尽量保持图的局部邻接关系或整体结构，Laplacian Eigenmaps 与谱聚类等方法属于这一类。
另一条思路基于随机游走和词向量技术，在图上进行随机游走，将得到的节点序列视为“句子”，
再借助 word2vec 风格的目标函数学习节点嵌入，
从而在嵌入空间中保持一阶、二阶邻接关系以及更高阶共现信息，典型代表包括 DeepWalk、LINE 等方法。

总体来看，这些早期方法解决了如何将图编码为向量的问题，
但通常采用“先嵌入、再建模”的两阶段管线，与下游预测任务之间缺乏严格的端到端耦合。
这一局限为在深度学习框架下构建可端到端训练的图神经网络模型提供了动力。

\subsection{图神经网络模型及其扩展}

\subsubsection{图神经网络的主要类别}

在欧式数据（如图像和文本）上，卷积神经网络通过局部感受野、参数共享和池化等机制取得了显著成功，
可以被视为在规则网格上进行的局部加权聚合与多层特征提取。
图神经网络可以被看作将这一“局部聚合加参数共享”的思想推广到一般图结构上的结果，
在此基础上形成了若干具有代表性的模型类别。

从卷积定义的角度，现有工作通常将图神经网络划分为谱域方法和空间域（消息传递）方法。
谱域方法基于拉普拉斯谱和图傅立叶变换，将卷积定义为谱域中的逐点乘积，并通过多项式近似等技术实现高效计算；
空间域方法则直接在图的邻接结构上进行邻居聚合，在每一层中，
节点从其邻居收集消息并进行加权组合与非线性变换，是当前应用最广泛的一类 GNN。

从任务粒度的角度，可以区分节点级、边级和图级图神经网络。
节点级模型以单个节点为基本预测单元，常用于节点分类与节点属性预测；
边级模型关注边或关系的预测与打分，例如链接预测与交互预测；
图级模型通过全局或层次池化将节点表示汇聚为整图表示，用于图分类或图回归任务。
在异质图和多关系图等更复杂的场景中，还需要显式区分不同类型的节点与边，形成异质图神经网络；
在时序图和动态图中，则进一步引入时间维度，形成动态图神经网络。

在一个较为统一的视角下，许多 GNN 都可以用“图滤波、非线性变换、层叠和读出”的框架来描述。
给定输入特征和图结构，模型在局部邻域内聚合信息并进行非线性变换，
堆叠多层之后，通过合适的读出算子得到节点、边或整图的表示。
这一统一框架为分析不同 GNN 的先验假设和滤波特性提供了基础，
也为后续关于鲁棒性和可扩展性的研究提供了出发点。

\subsubsection{图神经网络的鲁棒性}

在许多实际应用中，图结构和节点特征往往包含噪声，甚至可能受到有意的对抗性攻击，
因此鲁棒性问题逐渐成为图神经网络研究中的一个重要方向。
已有工作从攻击对象、攻击目标和攻击知识等多维度对图上的对抗攻击进行建模：
攻击可以针对节点特征、图结构或二者同时进行；
可以是针对特定节点或子图的有针对性攻击，也可以是整体性能退化的无差别攻击；
攻击者对模型和数据掌握的信息多少，又决定了白盒、灰盒和黑盒等不同设置。

在这些扰动下，图神经网络的预测性能往往会显著下降。
为提高鲁棒性，已有研究提出了在图结构或特征层面进行对抗训练、
在训练过程中显式学习“干净”图结构并同时优化预测模型、
以及在图滤波与邻居聚合过程中引入结构正则化等多种策略，
在一定程度上提升了 GNN 在噪声和攻击环境下的稳定性。

\subsubsection{图神经网络的可扩展性}

随着图规模的增大，标准 GCN 等模型面临邻域爆炸问题：
在多层传播之后，一个节点的感受野迅速扩展，
需要访问的节点数可能呈指数级增长，从而带来巨大的计算和存储开销。
为在大规模图上高效训练和推理，研究者提出了多种采样式图神经网络方法。

逐点采样方法在每一层为每个节点随机采样固定数量的邻居，以控制每层的计算复杂度；
逐层采样方法在每一层整体采样一批节点或边作为下一层的输入，从全局角度控制采样方差；
子图或簇采样方法则通过图划分或子图采样，将训练划分为在多个较小子图上的 mini-batch 学习。
这些方法从不同角度缓解了大规模图上的可扩展性问题，使得图神经网络能够处理包含百万乃至上亿节点的图数据。

\subsection{图神经网络应用}

从前文的讨论可以看到，图神经网络在模型结构、鲁棒性与可扩展性方面已经形成了较为系统的理论与方法体系。
在此基础上，GNN 被广泛应用于处理具有显式或隐式图结构的数据，包括程序代码、自然语言、生物分子、知识图谱等多个领域。
本小节重点介绍与本报告实验密切相关的代码智能方向，并在此基础上简要概述其他典型应用场景和前沿方向。

\subsubsection{图神经网络与代码智能}

在代码智能（code intelligence）场景中，程序语言具有严格的语法规则和可执行语义，
适合构建多种程序图表示，例如抽象语法树（Abstract Syntax Tree, AST）、
控制流图（Control Flow Graph, CFG）、数据流图（Data Flow Graph, DFG）
以及融合多种静态分析信息的代码属性图（Code Property Graph, CPG）等。
在这些程序图上应用图神经网络，通过多跳消息传递可以同时聚合局部上下文与远距离依赖，
从语法与语义两个层面刻画代码的结构化特征。

基于程序图的图神经网络已被用于多种代码分析任务。
在图级或子图级任务中，典型问题包括代码漏洞或缺陷检测、恶意代码识别等，
需要对一个函数、方法或更大代码片段给出整体判定；
在节点级任务中，常见问题包括静态类型推断、变量误用检测、变量命名建议等，
需要对变量或表达式节点进行精细化预测；
在序列到序列类任务中，还可以结合程序图表示生成代码摘要或支持代码搜索。
% 本报告所涉及的两个实验可以视为这一研究方向的具体代表：
% 基于代码属性图和图神经网络的漏洞检测对应于图级分类任务，
而基于 Typilus 的类型推断则对应于在程序图上进行节点级预测，
模型需要综合语法结构与上下文信息，以推断变量和表达式的类型。

\subsubsection{其他应用}

除代码智能外，图神经网络在多个领域得到了系统应用。
在生物信息学中，分子可视为图（原子为节点、化学键为边），蛋白质结构也可以建模为图，
GNN 被用于分子性质预测、蛋白质界面预测以及药物--靶点、药物--蛋白质和药物--疾病等多种相互作用预测任务。
在自然语言处理领域，句子可解析为依存句法树或语义图，文档之间也可以通过共享实体构成文档图，
图神经网络通过在这些结构上传播信息，提升了语义角色标注、关系抽取和多跳问答等任务中对长距离依赖和全局语义的建模能力。
在知识图谱和多关系网络中，实体与关系构成异质图或多关系图，GNN 在知识补全与多跳推理等任务中表现出良好效果。

% 近年来，图神经网络与大语言模型（Large Language Models, LLM）的结合逐渐成为前沿方向。
% 一方面，可以在图结构上引入 Transformer 或进行图预训练，学习通用的图表示；
% 另一方面，可以利用图结构或图神经网络作为辅助模块，
% 将知识图谱、文档图或代码图等结构信息编码为大模型的输入，从而增强其推理和理解能力。
% 反过来，大语言模型还可以用于丰富图数据本身，例如为节点和边生成自然语言描述，
% 将文本嵌入作为节点特征，形成“文本 + 图结构”联合建模的框架。
% 在自然语言处理与代码分析等场景中，如何在统一框架下有效融合图结构建模与大语言模型的强表达能力，是当前和未来的重要研究方向之一。

近年来，图神经网络与大语言模型（Large Language Models, LLM）的结合逐渐成为前沿方向：
一方面在图结构上引入 Transformer 或进行图预训练，
并将知识图谱、文档图或代码图编码为向量或文本作为大模型的条件输入，以提升推理与理解能力；
另一方面利用大语言模型为节点和边生成自然语言描述或补全缺失信息，为图提供更丰富的语义特征。
二者的结合在自然语言处理与代码分析等场景中展现出显著潜力，
也为基于程序图的类型推断等任务提供了进一步扩展的空间。

