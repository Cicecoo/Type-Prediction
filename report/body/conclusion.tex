\section{总结与展望}
\label{sec:conclusion}

本报告基于《图神经网络导论》课程所学知识,
在NaturalCC框架下复现了Typilus方法,
并进行了初步的对比实验和分析。

\subsection{工作总结}

本报告的主要工作包括:

\textbf{1. Typilus模型复现}:
在NaturalCC框架下复现了Typilus方法,
验证了其在Python类型推断任务上的有效性。
复现过程中遇到并解决了显存溢出、词典格式不兼容、特殊token不一致等多个技术问题,
积累了图神经网络模型训练的实践经验。

\textbf{2. 对比实验}:
实现了基于Transformer的类型预测模型作为对比,
初步验证了图结构建模相对序列建模的优势。
同时通过超参数消融实验,
探索了不同配置对模型性能的影响。

\textbf{3. 工程实践}:
搭建了实验环境,处理了数据集,
构建了模型训练和评估流程,
为后续研究提供了基础。

\subsection{主要发现}

通过实验,得到以下主要发现:

\textbf{1. 图结构建模的价值}:
显式建模代码的数据流、控制流等图结构,
相比纯序列建模能够更有效地捕获类型推断所需的依赖关系。
图神经网络提供的结构归纳偏置有助于模型学习,
且消息传递路径具有一定的可解释性。

\textbf{2. 技术挑战}:
复现过程中的主要挑战包括显存管理、数据格式兼容性、特殊token处理等。
这些问题的解决需要对深度学习框架和图神经网络有深入理解。

\textbf{3. 模型局限}:
当前模型在罕见类型、泛型参数、跨模块依赖等方面存在不足,
反映了类型推断任务的复杂性。

\subsection{不足与局限}

本报告的工作存在以下不足:

\textbf{1. 实验范围有限}:
受限于时间和资源,
实验的广度和深度仍有提升空间。
超参数调优、模型架构探索、评估维度等方面可以进一步完善。

\textbf{2. 数据处理}:
数据集的构建和预处理流程可以进一步优化,
以提升数据质量和模型性能。

\textbf{3. 工程实现}:
在推理效率、显存优化等方面仍有改进空间。

\subsection{未来工作}

基于当前工作,可能的改进方向包括:

\textbf{1. 模型架构}:
探索结合图神经网络和Transformer的混合模型,
或尝试其他图神经网络变体(如GAT、GraphSAINT)。

\textbf{2. 预训练}:
在大规模代码语料上预训练,
学习通用的代码表示,再在类型推断上微调。

\textbf{3. 细粒度类型}:
扩展模型以支持泛型参数推断(如List[int]),
需要建模类型的层次结构。

\textbf{4. 跨模块建模}:
引入外部库的类型信息,
提升对复杂项目的建模能力。

\textbf{5. 实际应用}:
在真实IDE环境中集成类型推断功能,
收集用户反馈并持续改进。

\subsection{研究意义}

本报告的工作具有以下意义:

\textbf{学术价值}:
验证了Typilus方法的有效性,
为图神经网络在代码智能领域的应用提供了实证支持。

\textbf{工程价值}:
总结的技术问题和解决方案可以帮助研究者避免相同陷阱,
加速研究进程。

\textbf{教育价值}:
完整的复现过程展示了从理论到实践的完整链路,
可作为图神经网络课程的实践案例。

\textbf{应用前景}:
类型推断技术可集成到开发工具中,
提升代码质量和开发效率。
