\section{总结与展望}
\label{sec:conclusion}

本报告基于《图神经网络导论》课程所学知识，
在 NaturalCC 框架下复现了 Typilus 方法，
并进行了初步的对比实验和分析。
此外尝试跑通了 Transformer 方法。

本次实验工作主要包括三个方面：一是结合课堂内容和文献，对图神经网络基础、代码属性图，
以及 Typilus 的整体思路和模型结构进行了整理与梳理；
二是完成 Typilus 的复现，包括环境配置、本地化数据构建，
以及处理一系列工程问题，完成 Typilus 的训练和评估；
三是围绕学习率等关键超参数进行了初步实验，观察不同设置下模型的收敛情况和测试表现。

复现过程中遇到并解决了显存溢出、词典格式不兼容、特殊token不一致等多个技术问题,
积累了图神经网络模型训练的实践经验。
在复现 Typilus 时，通过检查数据预处理流程，
定位并修复了 \texttt{subtoken} 特征缺失和 triplet 损失中距离矩阵显存占用过高的问题；
在尝试 Transformer 序列模型时，逐步理解了 Typilus 图数据与序列格式之间的对应关系，
并解决了词典初始化和特殊 token 处理不当导致的 CUDA 索引错误。
这些问题的解决也加深了对代码智能任务中数据表示和模型实现两个层面的理解。

本次实验仍存在不少不足之处。当前对 Typilus 的评估还比较粗略，主要集中在整体的 Top-$k$ 准确率和少量学习率设置上，
对不同类型类别、不同图规模以及典型错误案例的系统分析还不够充分，超参数实验也较为初步，还有很多不清楚原因的地方。
Transformer 方向目前只完成了数据转换和训练流程的打通，还没有形成可以与 Typilus 正式对比的实验结果。

未来如果有更多时间，可以继续完成 Transformer 模型的实验，
帮助更直观地理解“图结构建模”和“序列建模”在代码类型推断任务中的差异和优势。
把 Typilus 这一具体实例与课堂上学到的图神经网络理论更好地结合起来，也为今后进一步学习和研究相关方向打下基础。


