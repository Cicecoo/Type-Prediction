\section{Typilus模型复现}
\label{sec:reproduction}

% 本章记录Typilus模型在NaturalCC框架下的复现过程，
% 包括复现实验设置、训练过程与结果，以及复现过程中遇到的主要技术问题和对应的解决办法。

本章记录 Typilus 模型在 NaturalCC 框架下的复现过程，
包括复现实验的基本配置、实现细节，以及过程中遇到的主要问题和解决方法。
% 基于 NaturalCC 框架实现基于 Transformer 的类型推断，
% 在后文的对比实验中，将以本章复现得到的 Typilus 结果，
% 与基于 Transformer 的序列模型进行简单比较。

% \subsection{复现实验设置}

% 完成环境配置和数据准备后，使用 NaturalCC 仓库中内置的 Typilus 任务配置作为起点。
% 为避免引入额外变量，复现实验尽量沿用原实现的设置，仅在显存等资源限制下对 batch size 等少数参数做小幅调整。

% Typilus 模型的主要超参数包括嵌入维度、GGNN 层数、隐藏维度、学习率和批量大小等。
% 本实验采用 NaturalCC 提供的默认配置：嵌入维度 512、GGNN 层数 8、triplet margin 1.0，优化器和学习率调度保持默认。
% 训练、验证和测试均基于前文构建的 Typilus 数据集划分。

% 训练过程中在验证集上监控 Top-$k$ 准确率和 MRR 指标，并保留若干性能较好的检查点，用于后续对比和误差分析。

\subsection{复现实验设置}

完成环境配置和数据准备后，复现实验使用 NaturalCC 仓库中内置的 Typilus 任务配置作为起点。
为避免引入额外变量，主要沿用原实现的设置，仅在显存等资源限制下对 batch size 等少数参数做了适当下调。

Typilus 模型的主要超参数包括嵌入维度、GGNN 层数、隐藏维度、学习率和批量大小等。
本实验采用 NaturalCC 提供的默认配置：嵌入维度 512、GGNN 层数 8、triplet margin 1.0，
优化器与学习率调度保持默认，训练、验证和测试均基于前文构建的 Typilus 数据集划分进行。
此外对基于 Transformer 的类型推断模型进行了尝试。

% 此外实现了基于 Transformer 的类型推断模型，
% 使用同一 Typilus 数据集，但将代码表示从程序图转换为 token 序列，
% 基于 NaturalCC 中的代码实现。

% 训练过程中在验证集上监控 Top-$k$ 准确率和 MRR 等指标，
% 并保留若干性能较好的检查点，作为后续误差分析和对比实验的参考。


% \subsection{训练过程与结果}

% 在解决环境与数据预处理相关问题之后，Typilus模型可以在GPU上正常训练。
% 从损失变化来看，前几个epoch中训练损失较快下降，
% 随后在相对稳定的区间内震荡，验证集Top-1和Top-$k$准确率随之提升并逐渐趋于平稳。

% 受数据规模、预处理细节以及实现差异等因素影响，
% 本实验复现的绝对指标与原论文报告存在一定差距，
% 但整体趋势与预期一致：相较于随机预测，Typilus在验证集和测试集上显著优于简单基线，
% 能够利用程序图结构和上下文信息，对常见类型给出较为可靠的预测。
% 在后续章节中，Typilus复现模型作为图神经网络基线，与基于Transformer的序列模型进行对比分析。

% \subsection{关键技术问题与解决}

% 复现过程中遇到的主要技术问题集中在图数据格式、显存占用以及词典和特殊token的处理等方面。

\subsection{技术问题与解决}

% 复现 Typilus 并在此基础上加入 Transformer 序列基线的过程中，主要遇到几类问题：
复现 Typilus 的过程中主要遇到几类问题：
图数据字段不一致、距离计算带来的显存压力，以及在构造 Transformer 训练数据时
代码序列与类型标签的对齐和特殊 token 处理问题。
本节按类别对这些问题及其解决方法作简要说明。

\subsubsection{图数据与节点特征不一致}

在最初的 Typilus 训练尝试中，模型在前向传播阶段出现 \texttt{KeyError: 'subtoken'} 错误，
提示 DGL 图的节点特征中缺少 \texttt{subtoken} 字段。
排查发现，NaturalCC 某些版本对节点特征字段名有所调整，
而预处理脚本仍按照旧字段名输出，导致模型在访问节点特征时找不到对应键。

为解决这一问题，在数据预处理和模型入口两侧统一了字段命名：
一方面更新预处理脚本，保证图数据中包含模型期望的 \texttt{subtoken} 或等价字段；
另一方面检查 NaturalCC 中 Typilus 模型的输入接口，避免重复删除或覆盖同一字段。
修正后，图数据能够顺利加载，编码器可以正常读取节点的 token 表示。

\subsubsection{距离计算与显存占用}

在训练和验证阶段，Typilus 需要根据节点表示和类型标签计算相似度，
用于 triplet 损失和 Top-$k$ 指标的统计。
原始实现中，数据集在 \texttt{collate} 阶段为 batch 内所有目标节点构造了一个
大小为 $B \times B$ 的稠密邻接矩阵，其中 $B$ 为该 batch 中参与监督的节点个数，
邻接矩阵元素指示两节点类型是否相同。
在此基础上，triplet 损失一次性构造所有节点对的距离矩阵（同为 $B \times B$），
当 $B$ 较大时会占用大量显存，实际运行中触发 CUDA OOM 错误。

为降低显存开销，一方面在数据集构建阶段不再显式构造稠密邻接矩阵，
而是仅传递长度为 $B$ 的类型标签向量；
另一方面在 \texttt{TripletCriterion} 中实现了基于标签的“内存友好”计算路径：
当输入为一维标签向量时，将 batch 内的节点表示按固定大小划分为若干小块，记每块大小为 $K$。
对于每一块，将该块中的节点表示作为 anchor，与整个 batch 的表示计算
一个 $K \times B$ 的距离矩阵，根据标签现场生成正负样本掩码，并计算对应的 triplet 损失分量，
再在各个块之间累加得到完整损失。
这一改动不改变 triplet 损失的定义和数值形式，
但将原本一次性构造的 $B \times B$ 距离矩阵拆分为多次 $K \times B$ 的小矩阵计算，
显著降低了前向和验证阶段的显存峰值，使 Typilus 模型能够在单卡显存有限的条件下稳定完成训练和评估。

\subsubsection{Transformer 序列数据与标签对齐}

在构造基于 Transformer 的序列模型时，需要将 Typilus 的图数据转换为
一行一条样本的 \texttt{.code}/\texttt{.type} 文本格式。
最初直接使用 \texttt{nodes} 数组或随意添加起止标记，加载数据时频繁触发
\texttt{len(tokens) != len(labels)} 的断言。

问题主要在于：一方面 \texttt{token-sequence} 与 \texttt{nodes} 的长度并不总是一致，
真正的代码序列应以 \texttt{token-sequence} 中的 id 为准；
另一方面个别样本在生成类型标签时越界或为空，写入文本后再用 \texttt{split()} 解析时，
token 内部的空白字符还会导致长度进一步错位。

为保证严格对齐，转换过程中采用 \texttt{token-sequence} + 词典反查得到代码 token，
类型标签由 \texttt{supernodes} 注解生成，并要求每条样本满足
\texttt{len(token\_ids) == len(type\_labels)}，否则直接丢弃；
写文件前对 token 做简单清洗，去除或替换内部空白，避免读取阶段被拆分成多个“伪 token”。

\subsubsection{CUDA 索引错误与特殊 token 处理}

在 Transformer 训练阶段多次出现 \texttt{CUDA error: device-side assert triggered}，
定位到 embedding 查表时输入了非法索引。
排查发现并非 “id 超出词表大小”，而是存在大量 \texttt{-1} 被当作 token id 使用。

原因与特殊 token 初始化有关：Dictionary 在某些加载路径下没有正确添加
\texttt{<pad>}、\texttt{<unk>}、\texttt{<s>}、\texttt{</s>}，
导致 \texttt{pad\_idx} 和 \texttt{unk\_idx} 仍为 \texttt{-1}，
OOV token 和 padding 最终都被映射为 \texttt{-1}，从而触发设备端断言。

为避免这一问题，在构造和加载词典时统一采用 Dictionary 的默认初始化，
由其自动创建上述四个特殊符号，并在数据转换脚本中只使用这一套约定，
对 Typilus 中的 \texttt{[PAD]}、\texttt{[UNK]} 等写法显式映射到对应符号。
同时增加简单的覆盖检查，确认 \texttt{.code}/\texttt{.type} 中的 token 均能在词典中找到，
调整后 CUDA 索引错误不再出现，序列模型可以在 Typilus 数据上稳定训练。```

% \subsubsection{其他技术细节}

% 复现过程中还遇到若干次要问题，例如：
% \begin{itemize}
%     \item 不同版本 PyTorch 在 checkpoint 序列化格式上的差异，需要通过指定 \texttt{strict = False} 等方式兼容加载；
%     \item DGL 批处理接口在版本更新后行为略有变化，需要根据最新文档调整图合并与拆分方式；
%     \item 部分算子在低精度下数值不稳定，因此训练中统一采用单精度浮点（float32）。
% \end{itemize}
% 这些问题通过查阅官方文档、阅读错误堆栈并逐步修改代码得到解决。

\subsection{复现小结}

在前述环境配置、数据准备以及关键问题修正之后，Typilus 模型和 Transformer 模型可以在 NaturalCC 框架下正常训练和验证。
其中，Typilus 模型能够在给定数据集上收敛；Transformer 模型受限于时间问题，仅跑通了训练流程。