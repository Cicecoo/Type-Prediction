\section{相关工作与背景}
\label{sec:related}

近年来，面向图结构数据的表示学习与图神经网络（Graph Neural Networks, GNN）得到了广泛关注。
本节从三个方面对相关工作进行简要综述：首先回顾图结构数据与图表示学习的基础；其次讨论图神经网络模型及其在鲁棒性与可扩展性方面的研究进展；最后概述若干典型应用场景，重点突出与代码智能相关的方向，为后续实验部分奠定背景。

\subsection{图结构数据与图表示学习}

\subsubsection{图及其典型任务}

图是一类用于刻画实体及其关系的通用数据结构，通常记为 $G = (V, E)$，其中 $V$ 为节点集合，$E$ 为边集合。
根据具体需求，可以进一步得到带权图、有向图、异质图、多关系图和二分图等多种变体。
这类结构在社交网络、引文网络、知识图谱、交通网络、生物分子结构以及程序代码等领域中普遍存在。

围绕图结构数据，常见的学习任务可以大致分为三个层次。
第一个层次是节点级任务，例如节点分类、节点聚类和节点属性预测等；第二个层次是边级任务，如链接预测和关系预测；第三个层次则是以子图或整图为对象的任务，例如社区发现、子图匹配和图分类。
这些任务有一个共同需求，即希望将图中的节点、边或子图映射到低维向量空间，使得后续可以采用标准的机器学习或深度学习模型进行处理。
这一过程通常被称为图表示学习（graph representation learning）或图嵌入（graph embedding）。

\subsubsection{谱图论基础与图信号处理}

在图上构建神经网络时，一个核心问题是如何刻画“相邻节点应当有相似表示”这一直观要求。
谱图论提供了一套方便的工具，将这一要求形式化为“图信号的平滑性”和“频率”概念，从而为图卷积和图滤波提供理论基础。

给定图的邻接矩阵 $A$ 和度矩阵 $D$，无向图的拉普拉斯矩阵定义为
\begin{equation}
    \label{eq:laplacian_simple}
    L = D - A.
\end{equation}
直观上，如果两个节点之间存在边，则它们在 $L$ 中会产生一个“惩罚项”：当相邻节点的取值差异很大时，
相应的二次型 $x^\top L x$ 会变大；反之，如果相邻节点的取值接近，则这一值会较小。
因此，拉普拉斯矩阵可以被看作是一种度量“图信号是否平滑”的算子。

从线性代数的角度，拉普拉斯矩阵可以被对角化，其特征值和特征向量构成了一套“图上的正交基”。
与传统傅立叶分析中“正弦/余弦基”对应不同频率类似，拉普拉斯的特征向量也可以按照特征值大小进行排序：
特征值较小的特征向量对应在图上变化较为缓慢的模式，可以理解为低频成分；特征值较大的特征向量对应在图上变化剧烈的模式，可以理解为高频成分。

在这一框架下，定义在节点上的特征向量可以被视作图信号。
将图信号在上述特征向量基上展开，就得到了所谓的图傅立叶变换；在这一“频域”中对不同频率成分进行加权，即构成了图上的滤波操作。
如果滤波器主要保留低频成分、抑制高频成分，那么在原图上就表现为一种“平滑”的运算：相邻节点的特征会被拉近。
反之，如果强调高频成分，则更强调局部的突变或边界。

许多基于谱的图卷积方法可以被理解为对图信号施加某种形式的低通滤波，即在图结构约束下进行平滑；
而大量基于消息传递的图神经网络，也可以在这一谱图论视角下解释为近似的图滤波操作。

\subsubsection{早期图嵌入方法}

在图神经网络提出之前，图表示学习主要依赖于两类方法。
一类方法基于矩阵分解和谱嵌入，通过对拉普拉斯矩阵或相似度矩阵进行特征分解，将节点嵌入到低维空间，并尽量保持图的局部邻接关系或整体结构。
典型代表包括 Laplacian Eigenmaps、谱聚类等。
这类方法在中小规模图上效果良好，但在大规模图上往往面临计算成本较高、难以充分利用非线性表达能力等局限。

另一类方法基于随机游走和词向量技术，通过在图上进行随机游走，将得到的节点序列视为“句子”，再借助 word2vec 风格的目标函数学习节点嵌入，从而在嵌入空间中保持一阶、二阶邻接关系以及更高阶的共现信息。
DeepWalk、LINE 等方法是这一类工作的代表。
与基于谱分解的方法相比，它们在大规模网络上具有更好的可扩展性。

总体来看，这些早期方法共同解决了如何将图编码为向量的问题，但通常采用“先嵌入、再建模”的两阶段管线，与下游预测任务之间缺乏严格的端到端耦合。
这一局限为后续在深度学习框架下构建可端到端训练的图神经网络模型提供了动力。

\subsection{图神经网络模型及其扩展}

\subsubsection{从深度学习到图神经网络}

在欧式数据（如图像、语音和文本）上，卷积神经网络（Convolutional Neural Networks, CNN）通过局部感受野、参数共享和池化等机制取得了显著成功。
卷积可以被视为在规则网格上进行局部加权聚合，而池化则在更大尺度上对信息进行压缩与聚合。
在一般图结构上，节点邻居的排列不再服从规则网格，传统卷积核无法直接应用。

图神经网络可以被视作将“局部聚合加参数共享”的思想推广到非欧式空间的一类模型族。
从卷积定义的角度来看，现有工作通常将 GNN 分为谱域方法和空间域（消息传递）方法。
谱域方法基于图傅立叶变换和拉普拉斯谱，将卷积定义为谱域的逐点乘积，并借助特征分解或多项式近似实现相应运算；
空间域或消息传递方法则直接在图的邻接结构上进行信息传播，在每一层中，节点从其邻居收集消息并进行加权组合与非线性变换。

从任务类型的角度，可以区分节点级 GNN、边级 GNN 和图级 GNN。
节点级模型以节点为基本预测单元；边级模型关注边关系的预测或打分；图级模型通常通过全局池化或层次池化获得整图表示，用于图分类或回归等任务。
在异质图和多关系图等更复杂的场景下，还需要在聚合函数、注意力机制或参数共享方式上针对不同类型的节点和边进行扩展。

从算子层面来看，许多 GNN 可以用“图滤波、非线性变换、层叠和池化”的统一框架来描述。
给定输入特征 $X$ 和图结构，首先通过图滤波或消息传递在局部邻域内聚合信息，然后施加非线性激活，堆叠多层之后再通过池化得到节点级或图级表示。
统一图神经网络（Unified GNN, UGNN）等工作进一步将不同 GNN 写成特定的图趋势滤波器，在统一视角下分析它们的先验假设与滤波特性。

\subsubsection{图神经网络的鲁棒性}

在许多实际应用中，图结构和节点特征往往包含噪声，甚至可能受到有意的对抗性攻击。
因此，鲁棒性问题逐渐成为图神经网络研究中的一个重要方向。
已有工作从多个维度对图上的对抗攻击进行刻画：在攻击对象上，可以针对节点特征、图结构，或同时修改二者；在攻击目标上，可以针对单个或少量关键节点实施有针对性的攻击，也可以通过无差别攻击使整体性能退化；在攻击知识方面，根据攻击者对模型细节和训练数据的掌握程度，可以区分白盒、灰盒和黑盒攻击。

从数学角度来看，这类攻击可以抽象为对邻接矩阵 $A$ 和特征矩阵 $X$ 施加扰动，从而得到被攻击的图 $\tilde{A} = A + \Delta A$ 和特征 $\tilde{X} = X + \Delta X$。
在这些扰动下，图神经网络的预测性能往往会显著下降，甚至产生严重误判。
为了提高鲁棒性，已有研究提出了多种防御策略，例如在图结构或特征层面进行对抗训练，使模型在扰动数据上保持稳定；在训练过程中显式学习“干净”的图结构并同时优化预测模型；或者在图滤波与邻居聚合过程中引入结构正则化，以抑制异常边和异常节点的影响。
这些方法为在安全敏感场景下部署 GNN 提供了基础。

\subsubsection{图神经网络的可扩展性}

随着图规模的增大，标准 GCN 等模型面临邻域爆炸（neighborhood explosion）问题：在 $L$ 层传播之后，一个节点的感受野扩展到 $L$ 跳邻域，需要访问的节点数量可能呈指数级增长，从而带来巨大的计算和存储开销。
为了在大规模图上高效训练和推理，研究者提出了多种采样式图神经网络方法。

一类方法以 GraphSAGE 为代表，在每一层为每个节点随机采样固定数量的邻居，将每层的复杂度控制在常数级别，并通过调整采样数在效率和性能之间进行权衡。
另一类方法以 FastGCN、LADIES 等为代表，不再从每个节点的邻居中独立采样，而是在每一层整体采样一批节点作为下一层输入，以控制采样方差并提高训练效率。
还有一类方法则基于子图或簇采样，例如 Cluster-GCN、GraphSAINT 等，先对原图进行划分或子图采样，再在每个子图上进行 mini-batch 训练，在保留局部结构信息的同时显著降低计算开销。

这些方法从不同角度缓解了大规模图上的可扩展性问题，使得图神经网络能够处理包含百万乃至上亿节点的图数据。
在大规模社交网络、生物网络以及“大代码”（Big Code）等场景中，可扩展的 GNN 设计尤为关键。

\subsection{典型应用与前沿方向}

\subsubsection{生物信息学中的图神经网络}

在生物信息学领域，图结构自然地出现在分子结构、蛋白质结构以及各类生物关联网络中。
将分子视为图（原子为节点、化学键为边），可以利用图神经网络学习分子级的向量表示，用于预测溶解度、毒性、活性等理化性质。
类似地，将蛋白质结构建模为图，节点对应氨基酸残基或原子，边表示空间接触或相互作用，可以用于预测蛋白质界面和结合位点。

在更宏观的层面，各类药物--靶点、药物--药物、蛋白质--蛋白质以及药物--疾病关系可以构成生物网络或知识图谱。
在此类网络上应用图神经网络，有助于完成药物重定位、相互作用预测以及多药联用副作用预测等任务。
这些工作表明，图神经网络能够有效建模复杂生物系统，在药物发现和疾病机理研究中具有重要应用前景。

\subsubsection{自然语言处理中的图神经网络}

在自然语言处理领域，除了序列信息之外，句法和语义结构也可以通过图进行建模。
常见的图结构包括依存句法树、抽象语义表示（AMR），以及跨句或跨文档的实体关系图和任务图等。
在这些图上应用图神经网络，可以更好地捕捉长距离依赖和非局部语义关系。

以语义角色标注为例，将句子解析为依存树之后，可以在该树上构建图神经网络，对节点和边进行信息传播，从而综合利用局部上下文和全局结构信息。
类似的思想还被用于关系抽取和多跳问答等任务。
自然语言中的这些结构化表示与程序语言中的抽象语法树（AST）、控制流图和数据流图等有着天然的对应关系，也说明了在代码分析中引入图建模的合理性。

\subsubsection{代码智能中的图神经网络}

代码智能（code intelligence）是图神经网络的重要应用方向之一。
与自然语言相比，程序语言具有严格的语法规则和可执行语义，允许构建多种程序图表示，例如抽象语法树、控制流图、数据流图、程序依赖图以及融合多种信息的代码属性图（Code Property Graph）。
借助这些图结构，可以从语法和语义两个层面刻画代码的结构化特征。

基于程序图的表示学习方法大致可以分为若干方向。
一种方向采用循环神经网络或 Tree-LSTM 等模型对语法树或标记序列进行编码；
另一种方向则采用图神经网络在程序图上进行消息传递与聚合；
近年来还出现了基于 Transformer 的大规模预训练代码模型。
围绕这些表示，研究者提出了代码漏洞或缺陷检测、静态类型推断、变量误用检测、代码摘要生成、代码搜索、代码补全以及二进制代码相似度分析等多种下游任务。

在这些任务中，图神经网络通过在程序图上聚合局部上下文信息与远距离依赖，能够有效捕捉代码的结构化模式，为后续预测提供更具表达力的表示。
与本报告实验相关的类型推断问题，可以视作在程序图上进行节点级预测的一个具体实例。

\subsubsection{图神经网络与大语言模型的结合}

随着 Transformer 及大语言模型（Large Language Models, LLM）的发展，图神经网络与预训练语言模型之间的结合逐渐成为研究热点。
已有工作从多个角度探索二者的协同作用。
一方面，可以在图结构上引入 Transformer 或进行图预训练，学习通用的图表示，以提升在多任务、多领域上的迁移能力；
另一方面，可以利用图结构或图神经网络作为辅助模块，将知识图谱、分子图、代码图等结构信息编码为文本或向量，作为大模型的额外输入，从而增强其推理和理解能力。
此外，还可以反向利用大语言模型对图数据进行增强，例如为节点和边生成自然语言描述，将文本嵌入作为节点特征，甚至直接在自然语言空间完成部分图推理与决策。

在代码场景中，预训练代码模型擅长刻画序列和局部语义信息，而图神经网络更擅长利用显式的程序结构。
如何在统一框架下融合图结构建模与大语言模型的强表达能力，是当前和未来研究中的一个重要方向。

综上所述，图表示学习和图神经网络已经形成了从理论基础、模型方法到典型应用和前沿方向的相对完整的研究体系。
本报告后续实验将立足于这一体系，在程序代码的图结构表示基础上，具体考察图神经网络在静态分析与类型推断等任务中的应用效果。
