
# 图神经网络课程 PPT 内容总结

> 本文档为课程 11 章理论课 + 实验部分的纯文本总结，供无法阅读 PPT 的 agent 使用。  
> 内容围绕“图神经网络（Graph Neural Networks, GNN）及其应用”展开。

---

## 一、课程整体结构

课程主线可以概括为：

> 图是什么 → 神经网络基础 → 图嵌入 → 图神经网络基本原理 → 鲁棒性与可扩展性 → 在生物 / NLP / 代码中的应用 → GNN 与大语言模型（LLM）结合 → 实验（Devign / Typilus）

对应 PPT 章节：

1. 绪论  
2. 图论基础  
3. 神经网络基础  
4. 图嵌入  
5. 图神经网络  
6. 图神经网络的鲁棒性  
7. 图神经网络的可扩展性  
8. 生物信息中的图神经网络  
9. 自然语言处理中的图神经网络  
10. 代码智能中的图神经网络  
11. 图神经网络与大语言模型  
12. 实验：图神经网络实验（Devign、Typilus）

---

## 第二章起的详细内容（按 PPT 顺序）

> 下面按章节逐一总结：每章包括“主题 + 主要内容 + 作用”。

---

## 第 1 章 绪论：图与图神经网络的动机

**主题：** 为什么要研究图和图神经网络？

**主要内容：**

- 图在现实世界中的广泛存在：
  - 社交网络（用户、关注关系）
  - 推荐系统（用户–物品交互）
  - 知识图谱（实体–关系）
  - 生物分子（原子–化学键）
  - 程序代码结构（语法树、控制流、数据流等）
- 图的基本类型：
  - 有向图 / 无向图
  - 带权图 / 无权图
  - 同质图 / 异质图（不同类型节点与边）
  - 静态图 / 动态图（随时间演化）
- 图上的典型计算任务：
  - 节点级任务：节点分类、节点聚类、节点表示学习
  - 边级任务：链路预测、关系预测
  - 图级任务：图分类、图回归（例如分子性质预测）
- 图表示学习的需求：
  - 传统图算法（PageRank、随机游走等）难以端到端适配复杂任务
  - 希望把图/节点映射到连续向量空间（embedding），再用深度学习模型解决下游任务
- 课程内容概览：
  - 图论基础 → 深度学习基础 → 图嵌入 → 图神经网络 → 鲁棒性/可扩展性 → 应用（生物 / NLP / 代码）→ GNN + LLM → 实验

**这章的作用：**

- 提供整个课程的大背景与动机：为什么要学习 GNN，以及后续各章节的大致位置。

---

## 第 2 章 图论基础：图的数学与图信号处理

**主题：** 图的数学表示与图信号处理（为谱图神经网络打基础）

**主要内容：**

- 图的矩阵表示：
  - 邻接矩阵 \(A\)
  - 度矩阵 \(D\)
  - 图拉普拉斯矩阵 \(L = D - A\)
  - 归一化拉普拉斯矩阵等
- 图的一些常见性质：
  - 连通性、路径、圈、直径
  - 聚类系数、度分布等
  - 图结构对信息传播与算法性质的影响
- 图信号处理（Graph Signal Processing, GSP）：
  - 把节点上的特征理解为“图信号”
  - 利用拉普拉斯特征分解获得“图傅里叶基”
  - 图傅里叶变换：在频域分析信号平滑性与滤波
  - 图滤波的思想：在谱域上进行放大/抑制

**这章的作用：**

- 为谱图卷积、ChebNet、GCN 等模型提供数学基础，解释“卷积”如何在图上被定义。

---

## 第 3 章 神经网络基础：从 MLP 到 Transformer

**主题：** 深度学习基本模型与训练方法，为 GNN 中的“NN”部分打基础。

**主要内容：**

- 深度学习简史：
  - 感知机 → 多层感知机（MLP）
  - 卷积神经网络（CNN）
  - 循环神经网络（RNN/LSTM/GRU）
  - 注意力机制与 Transformer
- 主要模型结构：
  - **MLP：** 多层线性变换 + 非线性激活
  - **CNN：** 局部感受野、权值共享、平移不变性
  - **RNN/LSTM：** 用于序列建模，考虑时间依赖
- Transformer 与自注意力：
  - Self-Attention 的 Q/K/V 机制
  - 多头注意力、残差连接、LayerNorm
  - 在 NLP 预训练模型（BERT、GPT 等）中的使用
- 自编码器（Autoencoder）：
  - 编码–解码结构
  - 压缩表示、去噪、无监督表示学习
- 深度网络训练技巧：
  - 损失函数与反向传播
  - 优化算法（SGD, Adam 等）
  - 正则化（L2、Dropout）、归一化（BatchNorm）
- 预训练与迁移学习：
  - 自监督任务（掩码预测、下一句预测等）
  - 在大规模数据上预训练，在下游任务中微调

**这章的作用：**

- 为后续 GNN、图 Transformer、GNN + LLM 等内容提供基础的神经网络、注意力与预训练知识。

---

## 第 4 章 图嵌入：从词向量到图表示

**主题：** 不依赖 GNN 的图表示学习方法（前 GNN 时代的主线）。

**主要内容：**

1. 从词嵌入到图嵌入：
   - word2vec（Skip-gram / CBOW）思想
   - 把节点类比为“单词”，随机游走产生“节点序列”类比“句子”
2. 简单图嵌入方法（不同目标）：
   - **保留邻域信息：**
     - DeepWalk：随机游走 + word2vec
     - node2vec：通过参数控制 BFS/DFS 倾向，折中结构与社区
   - **保留结构角色：**
     - 强调“节点扮演的角色”（如 hub/桥接点），而不是具体邻居是谁
   - **保留节点状态：**
     - 考虑节点特征和状态演化
   - **保留社区结构（选学）：**
     - 使 embedding 反映社区划分等高层结构
3. 复杂图嵌入方法：
   - 异质图嵌入：多种节点/边类型
   - 二分图嵌入：如用户–物品
   - 多维/多视图图嵌入
   - 符号图嵌入、超图嵌入
   - 动态图嵌入：处理随时间变化的图结构

**这章的作用：**

- 展示传统图嵌入方法，构成 GNN 出现前的图表示学习路线，也是 GNN 的重要对比基线。

---

## 第 5 章 图神经网络：图上的深度学习框架

**主题：** 图神经网络的统一框架、图卷积及图池化。

**主要内容：**

1. 图神经网络简介：
   - 目标：在图结构数据上做端到端学习
   - 输入：图结构（邻接关系） + 节点特征（可选边特征）
   - 输出：节点级/边级/图级表示，用于分类、回归、预测等
2. 谱图论与图上的信号处理（承接第 2 章）：
   - 利用拉普拉斯谱定义图傅里叶变换
   - 谱卷积：在频域上定义卷积（本质是逐点乘法）
   - 直接使用谱卷积计算代价高，需要近似
3. 图卷积与滤波方法：
   - ChebNet：用 Chebyshev 多项式近似谱滤波，避免显式特征分解
   - GCN（Kipf & Welling）的一阶近似：
     - 通过添加自环、度归一化得到 \(\tilde{A} = D^{-\frac12}(A+I)D^{-\frac12}\)
     - 每一层更新：\(H^{(l+1)} = \sigma(\tilde{A} H^{(l)} W^{(l)})\)
   - 一般化的 Message Passing GNN 形式：
     - 消息计算：  
       \(m_{v}^{(l+1)} = \text{AGG}\_{u \in N(v)} \phi(h_v^{(l)}, h_u^{(l)}, e_{uv})\)
     - 状态更新：  
       \(h_v^{(l+1)} = \psi(h_v^{(l)}, m_v^{(l+1)})\)
4. 图池化（Graph Pooling）：
   - 全局池化：mean/sum/max，得到图级表示
   - 层次池化：如 DiffPool、Top-K Pool 等，学习子结构/层次结构

**这章的作用：**

- 是整门课的技术核心：给出 GNN 的统一抽象框架和典型实例（GCN），为后续所有应用章节提供基础。

---

## 第 6 章 图神经网络的鲁棒性：对抗攻击与防御

**主题：** 图上的对抗样本、攻击方式与防御策略。

**主要内容：**

1. 深度模型的鲁棒性问题：
   - 图像模型中的对抗样本示例：加入细微扰动即可误导 CNN
   - 引出类似问题：GNN 在图数据上也容易被恶意扰动欺骗
2. 图对抗攻击（Graph Adversarial Attack）：
   - 扰动形式：
     - 图结构扰动 \(\Delta A\)：增删边、改变连接结构
     - 节点特征扰动 \(\Delta F\)：修改特征值
   - 目标：在保持扰动“不可察觉”的前提下，使模型输出错误预测
3. 攻击场景与知识水平：
   - 白盒攻击：攻击者完全了解模型结构与参数，可用梯度构造对抗样本
   - 灰盒攻击：攻击者只了解部分信息（如训练数据或模型结构）
   - 黑盒攻击：只能查询模型输出，通过试探或替代模型实现攻击
4. 图对抗防御（Graph Adversarial Defense）：
   - 对抗训练：在训练时加入对抗样本，使模型学会应对扰动
   - 图净化（Graph Purification）：在推理前修正图结构，删除可疑边/节点
   - 图结构学习：联合学习更鲁棒的邻接关系，减弱攻击对下游任务的影响

**这章的作用：**

- 从安全和稳定性角度，讨论 GNN 在实际系统中的风险与防御方法。

---

## 第 7 章 图神经网络的可扩展性：大规模图上的效率问题

**主题：** 如何在大规模图上高效训练 GNN。

**主要内容：**

1. 问题背景：
   - 真实场景图规模巨大的例子：社交网络、推荐系统、知识图谱等
   - 标准 GCN 的全图训练要多次计算 \(\tilde{A}H\)，在大图上计算和内存开销巨大
   - 邻居爆炸、采样偏差等问题
2. 三类主流采样方法：
   - **逐点采样（Node-wise / Point-wise）**
     - 对每个目标节点只采样少量邻居
     - 典型方法：GraphSAGE 的邻居采样
     - 优：计算局部化；缺：方差可能较大
   - **逐层采样（Layer-wise）**
     - 每一层为一批节点采样邻居，形成层级子图
     - 示例：FastGCN、LADIES 等
     - 尝试从理论上控制采样偏差与方差
   - **子图采样（Subgraph / Cluster-based）**
     - 对原图进行簇划分或随机子图采样，每个 mini-batch 在子图上训练
     - 示例：Cluster-GCN、GraphSAINT 等
     - 优点：局部密集计算、缓存友好，更适合 GPU 训练
3. 小结：
   - 可扩展 GNN 的核心思想：在保证近似正确性的前提下，局部地看一部分图进行训练
   - 采样策略直接影响效率与精度，是大规模 GNN 应用中的关键设计点

**这章的作用：**

- 让 GNN 真正能跑在工业级大图上，是从“理论模型”走向“大规模部署”的重要一环。

---

## 第 8 章 生物信息中的图神经网络：分子与生物网络建模

**主题：** GNN 在生物信息学中的典型应用。

**主要内容：**

1. 生物信息中的图结构数据：
   - 分子图：节点是原子，边是化学键
   - 蛋白质图：节点是氨基酸残基或原子，边为空间接触/化学作用
   - 生物网络：药物–靶点网络、蛋白质–蛋白质网络、疾病–基因网络等
2. 分子表示学习：
   - 在分子图上使用 GNN 学习分子 embedding，用于性质预测（溶解度、毒性、活性等）
   - 在蛋白质结构图上使用 GNN 预测功能或结构
   - 在分子关联网络上利用 GNN 预测节点/边的属性或相互作用
3. 分子/生物关联预测任务：
   - 药物–靶点相互作用预测（DTI）
   - 药物–药物相互作用预测（DDI）
   - 蛋白质–蛋白质相互作用预测（PPI）
   - 药物–疾病关联预测等

**这章的作用：**

- 展示 GNN 在生命科学与药物发现领域的实际影响，说明图结构在科学计算中的重要性。

---

## 第 9 章 自然语言处理中的图神经网络

**主题：** NLP 中隐藏的图结构及 GNN 的用法。

**主要内容：**

1. 语言中的图结构：
   - 句法树（dependency tree / constituency tree）
   - 语义图（语义角色标注 SRL 图、AMR 图等）
   - 文档级实体图：实体、句子、段落之间的关系
   - 知识图谱（实体–关系–实体）
2. GNN 在典型 NLP 任务中的应用：
   - 语义角色标注（SRL）：将谓词–论元关系表示为图，通过 GNN 建模短语间关系
   - 神经机器翻译（NMT）：在 encoder 中引入依存图或语义图增强句子表示
   - 关系抽取：构造实体及上下文图，用 GNN 沿图路径传播信息预测实体对关系
   - 多跳问答（multi-hop QA）：把句子、实体构造成图，用多层 GNN 模拟多跳推理
   - 知识图谱上的神经网络：在 KG 上做链接预测、实体分类、知识问答等

**这章的作用：**

- 说明 NLP 并不只是序列建模，很多任务本质上是“在图上做推理”，GNN 是对 RNN/Transformer 的有力补充。

---

## 第 10 章 代码智能中的图神经网络

**主题：** 在程序代码上使用图表示与 GNN，构建“代码智能”系统。  
**（与你的实验 Typilus 强相关，是实验背景重点。）**

**主要内容：**

1. 代码智能背景：
   - “Big Code”：GitHub 等平台上存在海量开源代码
   - 目标：从大规模代码数据中挖掘知识，实现自动补全、错误检测等智能功能
   - 对比自然语言：
     - 程序语言语法更严格、结构更复杂、可执行
2. 代码的图表示：
   - Token 序列（类似自然语言）
   - 抽象语法树（AST）
   - 控制流图（CFG）、数据流图（DFG）
   - 代码属性图（Code Property Graph, CPG）：融合 AST + 控制流 + 数据流等多种图信息
3. GNN 在代码任务中的应用场景：
   - 变量错误使用检测（Variable Misuse）
   - 类型推断（Type Inference，Typilus）
   - 代码缺陷检测 / 漏洞检测（如 Devign）
   - 代码摘要生成（Code Summarization）
   - 代码搜索（Code Search）
   - 二进制代码相似度检测与恶意代码检测
4. Typilus 所在的任务类型（简要）：
   - 面向动态/可选类型语言（如 Python）的类型缺失问题
   - 利用图结构（AST、数据流等） + 神经网络预测变量/表达式类型
   - 结合子词/类型嵌入，实现细粒度类型预测

**这章的作用：**

- 直接为两个实验（Devign 漏洞检测、Typilus 类型推断）提供问题背景和技术路线。

---

## 第 11 章 图神经网络与大语言模型（LLM）

**主题：** 图神经网络与大语言模型的结合方式、图预训练与未来趋势。

**主要内容：**

1. Transformer 与大语言模型回顾：
   - Self-Attention 与 Transformer 架构
   - LLM（如 GPT 系列、代码大模型等）通过大规模预训练掌握语言/代码知识
2. 图结构与 Transformer 的结合方式：
   - 将图结构编码成序列（路径、邻居列表等）供 Transformer 使用
   - 在 Transformer 内部显式引入图结构信息（图注意力、结构化位置编码等）
3. 图预训练：
   - 在大规模图数据上设计自监督任务：如节点/边掩码预测、子图预测等
   - 获得通用图表示，迁移到下游任务
4. GNN 与 LLM 的几种关系：
   - LLM 辅助 GNN：
     - LLM 用于生成或理解节点/边的自然语言描述，为图建模提供丰富的语义特征
   - GNN 辅助 LLM：
     - GNN 用于建模结构化知识（知识图谱、代码结构、分子结构），LLM 负责自然语言部分
   - 联合建模或协同训练：
     - 图与文本/代码共同输入混合架构
     - 通过蒸馏或对齐，让图模型与 LLM 协同工作
5. 图的文本/代码表示：
   - 使用自然语言或 DSL 描述图结构，使 LLM 能“读懂”图
   - 为基于 LLM 的图推理提供输入桥梁
6. 开放问题与趋势：
   - 如何在保持可解释性的前提下结合符号结构（图、逻辑）与 LLM
   - GNN 与 LLM 的互补性与潜在融合方向

**这章的作用：**

- 把课程衔接到当前研究前沿：图学习与大模型的融合，展示未来可能的研究/应用方向。

---

## 实验：图神经网络实验（Devign & Typilus）

**主题：** 通过两个代码相关任务，加深对 GNN 的理解与实践。

---

### 实验一：基于图神经网络的代码漏洞检测（Devign）

**任务描述：**

- 输入：函数级代码片段（通常来自 C/C++ 项目）
- 输出：该函数是否包含安全漏洞（二分类）
- 数据结构：基于代码属性图（CPG），综合 AST + 控制流 + 数据流等信息

**模型思路（Devign）：**

- 使用 GNN（如 Gated Graph Recurrent Network, GGRN）在 CPG 上进行消息传递
- 学习函数级别的图表示，用于漏洞预测

**实验要求（典型）：**

- 阅读 Devign 原始论文
- 理解并复现论文中的模型实现
- 尝试改进 GNN 结构或特征设计，提高漏洞检测性能

---

### 实验二：基于图神经网络的代码类型推断（Typilus）

**任务背景：**

- 动态/可选类型语言（如 Python）中大量缺失类型标注
- 传统静态分析工具通常只能给出较粗类别，且保守

**Typilus 思路概述：**

- 从 GitHub 等平台收集有类型标注的 Python 项目
- 将代码转换为图结构（结合 AST、类型信息、数据流、控制流等）
- 为变量/表达式建立上下文表示，使用神经网络（包括图结构建模和类型嵌入）预测类型
- 目标：输出准确且细粒度的类型建议（Neural Type Hints）

**实验要求（典型）：**

- 阅读 Typilus 论文，理解问题定义、模型架构与训练方法
- 了解数据构建流程：爬取代码仓库、静态分析、图构建、数据划分等
- 在给定环境中跑通 Typilus 或其简化版本
- 思考并尝试改进：如调整图结构、引入新的特征或模型模块

---

## 评分与报告结构（概念性）

> 具体评分细则以课程要求为准，这里只总结常见结构，方便 agent 了解报告预期。

- 相关工作（约 30%）：
  - 需要将课堂所学内容（图论、图嵌入、GNN、应用等）整合为一个迷你 survey
  - 并与实验任务（Devign/Typilus）关联，比如说明“代码图是怎样的一类图”“为何使用 GNN”
- 实验完成度（约 50%）：
  - 是否成功复现/运行给定模型
  - 是否做了有效的改进或实验对比
- 实验报告质量（约 20%）：
  - 报告结构清晰（概述、背景、问题、模型、实验、总结等）
  - 分析到位，有对失败尝试或误差来源的讨论

---

## 对其他 Agent 的使用建议

如果你要把这份 MD 提供给另一个无法读 PPT 的 agent：

- 这份文件已经是一个**结构化的课程总结**，涵盖：
  - 11 章理论课重点
  - 实验课背景与要求
- 对于“Typilus 类型推断”相关任务：
  - 特别关注：第 4 章（图嵌入）、第 5 章（GNN）、第 7 章（可扩展性）、第 10 章（代码智能）、第 11 章（GNN + LLM）以及实验部分。
- 如需进一步专门面向 Typilus 的背景综述，可在此基础上再提炼一份“Typilus 专用小结”。

