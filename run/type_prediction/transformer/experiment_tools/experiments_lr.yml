# Transformer实验配置 - 学习率搜索
# 与Typilus实验保持一致的实验设计

base_config: config_base.yml

experiments:
  - name: transformer_baseline
    description: "Transformer基线 - 使用token-sequence"
    changes:
      optimization:
        lrs: [1e-4]
      checkpoint:
        save_dir: '~/workspace/type_pred/naturalcc/run/type_prediction/transformer/checkpoints/baseline'
        patience: 3
      model:
        d_model: 256
        n_encoder_layers: 4
        d_ff: 1024
  
  - name: transformer_lr_5e5
    description: "学习率 5e-5"
    changes:
      optimization:
        lrs: [5e-5]
      checkpoint:
        save_dir: '~/workspace/type_pred/naturalcc/run/type_prediction/transformer/checkpoints/lr_5e5'
        patience: 3
  
  - name: transformer_lr_7p5e5
    description: "学习率 7.5e-5"
    changes:
      optimization:
        lrs: [7.5e-5]
      checkpoint:
        save_dir: '~/workspace/type_pred/naturalcc/run/type_prediction/transformer/checkpoints/lr_7p5e5'
        patience: 3
  
  - name: transformer_lr_1e4
    description: "学习率 1e-4 (baseline重复验证)"
    changes:
      optimization:
        lrs: [1e-4]
      checkpoint:
        save_dir: '~/workspace/type_pred/naturalcc/run/type_prediction/transformer/checkpoints/lr_1e4'
        patience: 3
  
  - name: transformer_lr_2e4
    description: "学习率 2e-4"
    changes:
      optimization:
        lrs: [2e-4]
      checkpoint:
        save_dir: '~/workspace/type_pred/naturalcc/run/type_prediction/transformer/checkpoints/lr_2e4'
        patience: 3
  
  - name: transformer_deep
    description: "更深的Transformer (6层)"
    changes:
      optimization:
        lrs: [1e-4]
      checkpoint:
        save_dir: '~/workspace/type_pred/naturalcc/run/type_prediction/transformer/checkpoints/deep'
        patience: 3
      model:
        n_encoder_layers: 6
        d_model: 256
  
  - name: transformer_wide
    description: "更宽的Transformer (d_model=512)"
    changes:
      optimization:
        lrs: [1e-4]
      checkpoint:
        save_dir: '~/workspace/type_pred/naturalcc/run/type_prediction/transformer/checkpoints/wide'
        patience: 3
      model:
        d_model: 512
        d_ff: 2048
        n_encoder_layers: 4
