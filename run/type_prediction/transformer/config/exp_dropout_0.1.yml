criterion: type_predicition_cross_entropy
optimizer: fairseq_adam
lr_scheduler: polynomial_decay
tokenizer: null
bpe: null
common:
  no_progress_bar: 0
  log_interval: 100
  log_format: simple
  tensorboard_logdir: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/logs/tensorboard
  seed: 1
  cpu: 0
  fp16: 0
  bf16: 0
  memory_efficient_fp16: 0
  memory_efficient_bf16: 0
  fp16_no_flatten_grads: 0
  fp16_init_scale: 128
  fp16_scale_window: null
  fp16_scale_tolerance: 0.0
  min_loss_scale: 0.0001
  threshold_loss_scale: null
  user_dir: null
  empty_cache_freq: 0
  all_gather_list_size: 16384
  task: type_prediction
dataset:
  num_workers: 0
  skip_invalid_size_inputs_valid_test: 1
  max_tokens: null
  max_sentences: 32
  required_batch_size_multiple: 8
  dataset_impl: raw
  train_subset: train
  valid_subset: valid
  validate_interval: 1
  fixed_validation_seed: null
  disable_validation: 0
  max_tokens_valid: null
  max_sentences_valid: 32
  curriculum: 100
  test_subset: test
  num_shards: 1
  shard_id: 0
  joined_dictionary: 1
  srcdict: /mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt
  tgtdict: /mnt/data1/zhaojunzhang/typilus-data/transformer/dict.txt
distributed_training:
  distributed_world_size: 1
  distributed_rank: 0
  distributed_backend: nccl
  distributed_init_method: null
  distributed_port: -1
  device_id: 0
  distributed_no_spawn: 0
  ddp_backend: c10d
  bucket_cap_mb: 25
  fix_batches_to_gpus: null
  find_unused_parameters: 0
  fast_stat_sync: 0
  broadcast_buffers: 0
  global_sync_iter: 50
  warmup_iterations: 500
task:
  data: /mnt/data1/zhaojunzhang/typilus-data/transformer
  sample_break_mode: complete
  tokens_per_sample: 1024
  mask_prob: 0.15
  leave_unmasked_prob: 0.1
  random_token_prob: 0.1
  freq_weighted_replacement: 0
  mask_whole_words: 0
  pooler_activation_fn: tanh
  source_lang: code
  target_lang: type
  load_alignments: 0
  left_pad_source: 1
  left_pad_target: 0
  max_source_positions: 2048
  max_target_positions: 2048
  upsample_primary: 1
  truncate_source: 1
  eval_accuracy: 1
model:
  arch: typetransformer
  pooler_dropout: 0.0
  activation_fn: gelu
  dropout: 0.1
  attention_dropout: 0.1
  activation_dropout: 0.0
  relu_dropout: 0.0
  encoder_type: lstm
  encoder_positional_embeddings: 1
  encoder_embed_path: 0
  encoder_embed_dim: 512
  encoder_ffn_embed_dim: 2048
  encoder_layers: 6
  encoder_attention_heads: 8
  encoder_normalize_before: 0
  encoder_learned_pos: 0
  decoder_embed_path: ''
  decoder_embed_dim: 0
  decoder_ffn_embed_dim: 0
  decoder_layers: 0
  decoder_attention_heads: 0
  decoder_learned_pos: 0
  decoder_normalize_before: 0
  share_decoder_input_output_embed: 0
  share_all_embeddings: 0
  no_token_positional_embeddings: 0
  adaptive_softmax_cutoff: 0
  adaptive_softmax_dropout: 0.0
  no_cross_attention: 0
  cross_self_attention: 0
  layer_wise_attention: 0
  encoder_layerdrop: 0.0
  decoder_layerdrop: 0.0
  encoder_layers_to_keep: null
  decoder_layers_to_keep: null
  layernorm_embedding: 0
  no_scale_embedding: 0
  encoder_max_relative_len: 0
  max_source_positions: 2048
  max_target_positions: 2048
optimization:
  max_epoch: 50
  max_update: 0
  clip_norm: 25
  sentence_avg: null
  update_freq:
  - 1
  lr:
  - 0.0001
  lrs:
  - 0.0001
  min_lr: -1
  use_bmuf: 0
  force_anneal: null
  warmup_updates: 1000
  end_learning_rate: 0.0
  power: 1.0
  total_num_update: 50000
  adam:
    adam_betas: (0.9, 0.999)
    adam_eps: 1.0e-06
    weight_decay: 0.01
    use_old_adam: 1
  adagrad:
    weight_decay: 0.0
  binary_cross_entropy:
    infonce: 0
    loss-weights: ''
    log-keys: ''
checkpoint:
  save_dir: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/checkpoints
  restore_file: checkpoint_last.pt
  reset_dataloader: null
  reset_lr_scheduler: null
  reset_meters: null
  reset_optimizer: null
  optimizer_overrides: '{}'
  save_interval: 1
  save_interval_updates: 0
  keep_interval_updates: 0
  keep_last_epochs: 5
  keep_best_checkpoints: 3
  no_save: 0
  no_epoch_checkpoints: 0
  no_last_checkpoints: 0
  no_save_optimizer_state: null
  best_checkpoint_metric: loss
  maximize_best_checkpoint_metric: 0
  patience: 10
eval:
  path: /mnt/data1/zhaojunzhang/experiments/transformer_series/exp_dropout_0.1/checkpoints/checkpoint_best.pt
  remove_bpe: null
  quiet: 0
  results_path: null
  model_overrides: '{}'
