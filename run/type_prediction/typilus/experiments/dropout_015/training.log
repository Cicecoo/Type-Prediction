[32m[2025-11-21 15:36:53]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/config.yml (train_enhanced.py:666, cli_main())[0m
[32m[2025-11-21 15:36:53]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:694, cli_main())[0m
[32m[2025-11-21 15:36:53]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs (train_enhanced.py:561, single_main())[0m
[32m[2025-11-21 15:36:53]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 15:36:53]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 15:36:53]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-21 15:37:03]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 64, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.15, inplace=False)
      (1): Linear(in_features=64, out_features=64, bias=False)
      (2): Dropout(p=0.15, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(64, 64)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(128, 64)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=False)
      (1): Dropout(p=0.15, inplace=False)
      (2): Linear(in_features=64, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:568, single_main())[0m
[32m[2025-11-21 15:37:03]    INFO >> æ¨¡åž‹: typilus, æŸå¤±å‡½æ•°: TypilusCriterion (train_enhanced.py:569, single_main())[0m
[32m[2025-11-21 15:37:03]    INFO >> æ¨¡åž‹å‚æ•°: 847843 (å¯è®­ç»ƒ: 847843) (train_enhanced.py:570, single_main())[0m
[32m[2025-11-21 15:37:03]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 15:37:03]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80579 MB ; used memory = 1340 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 15:37:03]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 15:37:03]    INFO >> ä½¿ç”¨ 1 ä¸ªGPUè®­ç»ƒ (train_enhanced.py:576, single_main())[0m
[32m[2025-11-21 15:37:03]    INFO >> no existing checkpoint found /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-21 15:37:03]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-21 15:38:12]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-21 15:38:13] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 15:38:20]    INFO >> epoch 001:     50 / 1539 loss=5.513, wps=5039.9, ups=6.97, wpb=720, bsz=720, num_updates=50, lr=0.0004, gnorm=7.179, clip=0, train_wall=7, gb_free=74.2, wall=75 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:38:23] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 73.64 GiB is allocated by PyTorch, and 4.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:38:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:38:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:38:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75347 MiB |  75407 MiB |   1756 GiB |   1682 GiB |
|       from large pool |  74991 MiB |  75051 MiB |   1743 GiB |   1670 GiB |
|       from small pool |    356 MiB |    357 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75347 MiB |  75407 MiB |   1756 GiB |   1682 GiB |
|       from large pool |  74991 MiB |  75051 MiB |   1743 GiB |   1670 GiB |
|       from small pool |    356 MiB |    357 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75195 MiB |  75254 MiB |   1750 GiB |   1676 GiB |
|       from large pool |  74841 MiB |  74900 MiB |   1737 GiB |   1664 GiB |
|       from small pool |    354 MiB |    355 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80498 MiB |  91994 MiB |  11532 MiB |
|       from large pool |  80070 MiB |  80136 MiB |  91590 MiB |  11520 MiB |
|       from small pool |    392 MiB |    394 MiB |    404 MiB |     12 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5054 MiB |   5960 MiB |    875 GiB |    870 GiB |
|       from large pool |   5018 MiB |   5948 MiB |    859 GiB |    855 GiB |
|       from small pool |     35 MiB |     37 MiB |     15 GiB |     14 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6579    |    6582    |  136626    |  130047    |
|       from large pool |     868    |     869    |   57719    |   56851    |
|       from small pool |    5711    |    5714    |   78907    |   73196    |
|---------------------------------------------------------------------------|
| Active allocs         |    6579    |    6582    |  136626    |  130047    |
|       from large pool |     868    |     869    |   57719    |   56851    |
|       from small pool |    5711    |    5714    |   78907    |   73196    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     758    |     793    |    1006    |     248    |
|       from large pool |     562    |     612    |     804    |     242    |
|       from small pool |     196    |     197    |     202    |       6    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     654    |     656    |   86039    |   85385    |
|       from large pool |     306    |     306    |   44256    |   43950    |
|       from small pool |     348    |     350    |   41783    |   41435    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:38:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:38:23] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:38:29]    INFO >> epoch 001:    101 / 1539 loss=5.849, wps=4382, ups=7.18, wpb=610.7, bsz=610.7, num_updates=100, lr=0.0004, gnorm=7.784, clip=0, train_wall=6, gb_free=75.6, wall=82 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:38:36]    INFO >> epoch 001:    151 / 1539 loss=6.018, wps=5442.4, ups=6.66, wpb=816.7, bsz=816.7, num_updates=150, lr=0.0004, gnorm=7.445, clip=0, train_wall=7, gb_free=74.2, wall=90 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:38:43]    INFO >> epoch 001:    201 / 1539 loss=5.995, wps=4841.1, ups=7.54, wpb=641.7, bsz=641.7, num_updates=200, lr=0.0004, gnorm=8.323, clip=0, train_wall=6, gb_free=74.8, wall=96 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:38:50]    INFO >> epoch 001:    251 / 1539 loss=6.021, wps=4636.4, ups=7.27, wpb=637.9, bsz=637.9, num_updates=250, lr=0.0004, gnorm=7.717, clip=0, train_wall=6, gb_free=71.5, wall=103 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:38:59]    INFO >> epoch 001:    301 / 1539 loss=6.016, wps=5059, ups=6.43, wpb=786.5, bsz=786.5, num_updates=300, lr=0.0004, gnorm=7.47, clip=0, train_wall=7, gb_free=73.8, wall=111 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:39:06]    INFO >> epoch 001:    351 / 1539 loss=6.015, wps=4853.9, ups=7.23, wpb=671.6, bsz=671.6, num_updates=350, lr=0.0004, gnorm=6.708, clip=0, train_wall=6, gb_free=72.4, wall=118 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:39:13]    INFO >> epoch 001:    401 / 1539 loss=5.662, wps=5485.8, ups=6.44, wpb=851.7, bsz=851.7, num_updates=400, lr=0.0004, gnorm=8.185, clip=0, train_wall=7, gb_free=73.2, wall=125 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:39:23] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.81 GiB is allocated by PyTorch, and 823.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:39:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:39:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:39:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79614 MiB |  79674 MiB |  12419 GiB |  12342 GiB |
|       from large pool |  79520 MiB |  79580 MiB |  12346 GiB |  12268 GiB |
|       from small pool |     94 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79614 MiB |  79674 MiB |  12419 GiB |  12342 GiB |
|       from large pool |  79520 MiB |  79580 MiB |  12346 GiB |  12268 GiB |
|       from small pool |     94 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79544 MiB |  79604 MiB |  12379 GiB |  12302 GiB |
|       from large pool |  79450 MiB |  79510 MiB |  12306 GiB |  12228 GiB |
|       from small pool |     93 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 166722 MiB |  86224 MiB |
|       from large pool |  80400 MiB |  80400 MiB | 166242 MiB |  85842 MiB |
|       from small pool |     98 MiB |    392 MiB |    480 MiB |    382 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    823 MiB |   3119 MiB |   6143 GiB |   6143 GiB |
|       from large pool |    819 MiB |   3112 MiB |   6057 GiB |   6057 GiB |
|       from small pool |      3 MiB |     23 MiB |     85 GiB |     85 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2036    |    2039    |     869 K  |     867 K  |
|       from large pool |     477    |     478    |     421 K  |     421 K  |
|       from small pool |    1559    |    1562    |     447 K  |     445 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2036    |    2039    |     869 K  |     867 K  |
|       from large pool |     477    |     478    |     421 K  |     421 K  |
|       from small pool |    1559    |    1562    |     447 K  |     445 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     251    |     757    |    1201    |     950    |
|       from large pool |     202    |     561    |     961    |     759    |
|       from small pool |      49    |     196    |     240    |     191    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     132    |     133    |  540020    |  539888    |
|       from large pool |      79    |      81    |  324754    |  324675    |
|       from small pool |      53    |      55    |  215266    |  215213    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:39:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:39:23] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:39:27]    INFO >> epoch 001:    452 / 1539 loss=5.956, wps=2314.8, ups=3.66, wpb=631.6, bsz=631.6, num_updates=450, lr=0.0004, gnorm=6.63, clip=0, train_wall=6, gb_free=71.8, wall=139 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:39:38]    INFO >> epoch 001:    502 / 1539 loss=5.701, wps=3723.7, ups=5.01, wpb=743.1, bsz=743.1, num_updates=500, lr=0.0004, gnorm=7.33, clip=0, train_wall=9, gb_free=72.6, wall=149 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:39:46]    INFO >> epoch 001:    552 / 1539 loss=5.817, wps=4489.5, ups=6.83, wpb=657, bsz=657, num_updates=550, lr=0.0004, gnorm=6.856, clip=0, train_wall=7, gb_free=65.5, wall=156 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:39:53]    INFO >> epoch 001:    602 / 1539 loss=5.763, wps=4490.1, ups=6.72, wpb=668.6, bsz=668.6, num_updates=600, lr=0.0004, gnorm=7.85, clip=0, train_wall=7, gb_free=73.2, wall=164 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:40:01]    INFO >> epoch 001:    652 / 1539 loss=5.706, wps=4481, ups=6.29, wpb=712.2, bsz=712.2, num_updates=650, lr=0.0004, gnorm=7.235, clip=0, train_wall=7, gb_free=73.5, wall=172 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:40:11]    INFO >> epoch 001:    702 / 1539 loss=5.759, wps=3887.3, ups=5.78, wpb=672.5, bsz=672.5, num_updates=700, lr=0.0004, gnorm=6.868, clip=0, train_wall=8, gb_free=74.1, wall=180 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:40:19]    INFO >> epoch 001:    752 / 1539 loss=5.633, wps=4723.8, ups=6.24, wpb=757.4, bsz=757.4, num_updates=750, lr=0.0004, gnorm=7.373, clip=0, train_wall=7, gb_free=73.7, wall=188 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:40:27]    INFO >> epoch 001:    802 / 1539 loss=5.631, wps=4799.7, ups=6.62, wpb=725.2, bsz=725.2, num_updates=800, lr=0.0004, gnorm=7.154, clip=0, train_wall=7, gb_free=73.4, wall=196 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:40:34]    INFO >> epoch 001:    852 / 1539 loss=5.564, wps=4118.9, ups=6.42, wpb=641.1, bsz=641.1, num_updates=850, lr=0.0004, gnorm=7.27, clip=0, train_wall=7, gb_free=71.8, wall=204 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:40:43]    INFO >> epoch 001:    902 / 1539 loss=5.568, wps=4526.2, ups=6.86, wpb=660.1, bsz=660.1, num_updates=900, lr=0.0004, gnorm=6.743, clip=0, train_wall=7, gb_free=72.1, wall=211 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:40:51]    INFO >> epoch 001:    952 / 1539 loss=5.562, wps=4641.8, ups=6.51, wpb=713.2, bsz=713.2, num_updates=950, lr=0.0004, gnorm=6.943, clip=0, train_wall=7, gb_free=71.8, wall=219 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:40:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 901.25 MiB is free. Including non-PyTorch memory, this process has 78.24 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:40:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:40:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:40:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76322 MiB |  78597 MiB |  28415 GiB |  28340 GiB |
|       from large pool |  76303 MiB |  78579 MiB |  28261 GiB |  28187 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76322 MiB |  78597 MiB |  28415 GiB |  28340 GiB |
|       from large pool |  76303 MiB |  78579 MiB |  28261 GiB |  28187 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB |  28342 GiB |  28268 GiB |
|       from large pool |  76285 MiB |  78562 MiB |  28189 GiB |  28114 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79604 MiB |  79604 MiB | 305536 MiB | 225932 MiB |
|       from large pool |  79580 MiB |  79580 MiB | 304982 MiB | 225402 MiB |
|       from small pool |     24 MiB |     98 MiB |    554 MiB |    530 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3281 MiB |   4789 MiB |  24278 GiB |  24274 GiB |
|       from large pool |   3276 MiB |   4783 MiB |  24100 GiB |  24097 GiB |
|       from small pool |      5 MiB |     27 MiB |    177 GiB |    177 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     638    |     644    |    1881 K  |    1880 K  |
|       from large pool |     340    |     346    |     949 K  |     948 K  |
|       from small pool |     298    |     354    |     931 K  |     931 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     638    |     644    |    1881 K  |    1880 K  |
|       from large pool |     340    |     346    |     949 K  |     948 K  |
|       from small pool |     298    |     354    |     931 K  |     931 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     123    |    1327    |    1231    |
|       from large pool |      84    |      84    |    1050    |     966    |
|       from small pool |      12    |      49    |     277    |     265    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     115    |     117    |    1087 K  |    1087 K  |
|       from large pool |      93    |      95    |     661 K  |     661 K  |
|       from small pool |      22    |      56    |     426 K  |     426 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:40:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:40:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:41:00]    INFO >> epoch 001:   1003 / 1539 loss=5.542, wps=3359.9, ups=5.17, wpb=649.4, bsz=649.4, num_updates=1000, lr=0.0004, gnorm=6.768, clip=0, train_wall=7, gb_free=72.1, wall=228 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:41:09]    INFO >> epoch 001:   1053 / 1539 loss=5.449, wps=5025, ups=6.11, wpb=822.6, bsz=822.6, num_updates=1050, lr=0.0004, gnorm=8.009, clip=0, train_wall=7, gb_free=68, wall=237 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:41:18] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 197.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:41:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:41:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:41:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 9         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76955 MiB |  77477 MiB |  32758 GiB |  32683 GiB |
|       from large pool |  76942 MiB |  77464 MiB |  32579 GiB |  32504 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76955 MiB |  77477 MiB |  32758 GiB |  32683 GiB |
|       from large pool |  76942 MiB |  77464 MiB |  32579 GiB |  32504 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB |  32677 GiB |  32602 GiB |
|       from large pool |  76923 MiB |  77445 MiB |  32498 GiB |  32423 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80308 MiB |  80308 MiB | 310972 MiB | 230664 MiB |
|       from large pool |  80284 MiB |  80284 MiB | 310232 MiB | 229948 MiB |
|       from small pool |     24 MiB |    210 MiB |    740 MiB |    716 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3352 MiB |   9143 MiB |  29235 GiB |  29232 GiB |
|       from large pool |   3341 MiB |   9131 MiB |  29029 GiB |  29026 GiB |
|       from small pool |     11 MiB |     23 MiB |    206 GiB |    206 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     604    |     611    |    2166 K  |    2165 K  |
|       from large pool |     315    |     322    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1080 K  |    1079 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     604    |     611    |    2166 K  |    2165 K  |
|       from large pool |     315    |     322    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1080 K  |    1079 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     189    |    1425    |    1337    |
|       from large pool |      76    |      84    |    1055    |     979    |
|       from small pool |      12    |     105    |     370    |     358    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      84    |      84    |    1245 K  |    1245 K  |
|       from large pool |      60    |      60    |     746 K  |     746 K  |
|       from small pool |      24    |      53    |     498 K  |     498 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:41:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:41:18] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:41:20]    INFO >> epoch 001:   1104 / 1539 loss=5.48, wps=4841.1, ups=5.21, wpb=929.6, bsz=929.6, num_updates=1100, lr=0.0004, gnorm=7.487, clip=2, train_wall=8, gb_free=72.8, wall=246 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:41:27]    INFO >> epoch 001:   1154 / 1539 loss=5.447, wps=4546.8, ups=6.56, wpb=692.6, bsz=692.6, num_updates=1150, lr=0.0004, gnorm=7.052, clip=0, train_wall=7, gb_free=73.1, wall=254 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:41:35]    INFO >> epoch 001:   1204 / 1539 loss=5.385, wps=4428.5, ups=6.53, wpb=678.4, bsz=678.4, num_updates=1200, lr=0.0004, gnorm=7.251, clip=0, train_wall=7, gb_free=71.2, wall=262 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:41:43]    INFO >> epoch 001:   1254 / 1539 loss=5.369, wps=4709.5, ups=6.44, wpb=730.9, bsz=730.9, num_updates=1250, lr=0.0004, gnorm=7.134, clip=0, train_wall=7, gb_free=71.3, wall=269 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:41:52]    INFO >> epoch 001:   1304 / 1539 loss=5.185, wps=4524.4, ups=6.15, wpb=735.4, bsz=735.4, num_updates=1300, lr=0.0004, gnorm=6.455, clip=0, train_wall=7, gb_free=74.3, wall=277 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:42:00]    INFO >> epoch 001:   1354 / 1539 loss=5.283, wps=4274, ups=6.5, wpb=657.2, bsz=657.2, num_updates=1350, lr=0.0004, gnorm=6.78, clip=2, train_wall=7, gb_free=73.4, wall=285 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:42:08]    INFO >> epoch 001:   1404 / 1539 loss=5.095, wps=4434.7, ups=6.22, wpb=712.5, bsz=712.5, num_updates=1400, lr=0.0004, gnorm=7.247, clip=0, train_wall=7, gb_free=73.3, wall=293 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:42:16]    INFO >> epoch 001:   1454 / 1539 loss=5.094, wps=4534, ups=6.46, wpb=702.2, bsz=702.2, num_updates=1450, lr=0.0004, gnorm=7.271, clip=2, train_wall=7, gb_free=71.6, wall=301 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:42:25]    INFO >> epoch 001:   1504 / 1539 loss=4.935, wps=4548.6, ups=6.54, wpb=695.9, bsz=695.9, num_updates=1500, lr=0.0004, gnorm=6.525, clip=0, train_wall=7, gb_free=70.8, wall=309 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:42:30]    INFO >> epoch 001 | loss 5.577 | wps 4446.1 | ups 6.24 | wpb 712.7 | bsz 712.7 | num_updates 1535 | lr 0.0004 | gnorm 7.228 | clip 0.2 | train_wall 214 | gb_free 76.6 | wall 314 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:42:30] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:42:45]    INFO >> epoch 001 | valid on 'valid' subset | loss 4.689 | wps 9884 | wpb 5412.5 | bsz 5412.5 | num_updates 1535 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-21 15:42:46]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:42:46]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_best.pt (epoch 1 @ 1535 updates, score 4.689) (writing took 0.016955 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:42:46] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 15:42:48]    INFO >> epoch 002:     15 / 1539 loss=4.811, wps=1558.6, ups=2.13, wpb=731.3, bsz=731.3, num_updates=1550, lr=0.0004, gnorm=6.499, clip=0, train_wall=7, gb_free=74.4, wall=332 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:42:56]    INFO >> epoch 002:     65 / 1539 loss=4.602, wps=4687.5, ups=7.12, wpb=658.8, bsz=658.8, num_updates=1600, lr=0.0004, gnorm=7.509, clip=0, train_wall=7, gb_free=73.4, wall=339 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:43:04]    INFO >> epoch 002:    115 / 1539 loss=4.7, wps=4653.3, ups=6.51, wpb=714.6, bsz=714.6, num_updates=1650, lr=0.0004, gnorm=6.391, clip=0, train_wall=7, gb_free=65.7, wall=347 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:43:12]    INFO >> epoch 002:    165 / 1539 loss=4.367, wps=4787.8, ups=6.48, wpb=738.8, bsz=738.8, num_updates=1700, lr=0.0004, gnorm=7.008, clip=0, train_wall=7, gb_free=73.6, wall=354 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:43:19]    INFO >> epoch 002:    215 / 1539 loss=4.696, wps=4739.3, ups=6.73, wpb=704.2, bsz=704.2, num_updates=1750, lr=0.0004, gnorm=7.718, clip=0, train_wall=7, gb_free=71.2, wall=362 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:43:29]    INFO >> epoch 002:    265 / 1539 loss=4.442, wps=5193.6, ups=5.95, wpb=873.5, bsz=873.5, num_updates=1800, lr=0.0004, gnorm=7.621, clip=2, train_wall=8, gb_free=74.7, wall=370 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:43:36]    INFO >> epoch 002:    315 / 1539 loss=4.472, wps=4359, ups=6.76, wpb=645.1, bsz=645.1, num_updates=1850, lr=0.0004, gnorm=7.762, clip=0, train_wall=7, gb_free=71.9, wall=378 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:43:44]    INFO >> epoch 002:    365 / 1539 loss=4.453, wps=4502.9, ups=6.87, wpb=655.8, bsz=655.8, num_updates=1900, lr=0.0004, gnorm=7.416, clip=0, train_wall=7, gb_free=74, wall=385 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:43:51]    INFO >> epoch 002:    415 / 1539 loss=4.328, wps=4545.5, ups=6.37, wpb=713.2, bsz=713.2, num_updates=1950, lr=0.0004, gnorm=6.957, clip=0, train_wall=7, gb_free=76.2, wall=393 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:43:52] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 197.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 3.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:43:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:43:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:43:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76327 MiB |  78599 MiB |  61960 GiB |  61886 GiB |
|       from large pool |  76308 MiB |  78581 MiB |  61611 GiB |  61537 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76327 MiB |  78599 MiB |  61960 GiB |  61886 GiB |
|       from large pool |  76308 MiB |  78581 MiB |  61611 GiB |  61537 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB |  61824 GiB |  61750 GiB |
|       from large pool |  76285 MiB |  78562 MiB |  61476 GiB |  61402 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80308 MiB |  80494 MiB | 311158 MiB | 230850 MiB |
|       from large pool |  80284 MiB |  80284 MiB | 310232 MiB | 229948 MiB |
|       from small pool |     24 MiB |    210 MiB |    926 MiB |    902 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3980 MiB |   5490 MiB |  61031 GiB |  61027 GiB |
|       from large pool |   3975 MiB |   5484 MiB |  60635 GiB |  60631 GiB |
|       from small pool |      5 MiB |     27 MiB |    395 GiB |    395 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |    4090 K  |    4090 K  |
|       from large pool |     340    |     346    |    1969 K  |    1969 K  |
|       from small pool |     300    |     355    |    2121 K  |    2120 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |    4090 K  |    4090 K  |
|       from large pool |     340    |     346    |    1969 K  |    1969 K  |
|       from small pool |     300    |     355    |    2121 K  |    2120 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     181    |    1518    |    1430    |
|       from large pool |      76    |      76    |    1055    |     979    |
|       from small pool |      12    |     105    |     463    |     451    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      93    |      99    |    2309 K  |    2309 K  |
|       from large pool |      70    |      76    |    1305 K  |    1305 K  |
|       from small pool |      23    |      51    |    1004 K  |    1004 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:43:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:43:52] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:44:01]    INFO >> epoch 002:    466 / 1539 loss=4.479, wps=4366.2, ups=5.99, wpb=729.4, bsz=729.4, num_updates=2000, lr=0.0004, gnorm=6.885, clip=0, train_wall=7, gb_free=71.6, wall=401 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:44:09]    INFO >> epoch 002:    516 / 1539 loss=4.476, wps=4671.7, ups=6.59, wpb=708.4, bsz=708.4, num_updates=2050, lr=0.0004, gnorm=8.204, clip=0, train_wall=7, gb_free=71.2, wall=409 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:44:16]    INFO >> epoch 002:    566 / 1539 loss=4.424, wps=4377.4, ups=7.03, wpb=622.3, bsz=622.3, num_updates=2100, lr=0.0004, gnorm=6.444, clip=0, train_wall=7, gb_free=74, wall=416 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:44:24]    INFO >> epoch 002:    616 / 1539 loss=4.251, wps=5394.7, ups=6.19, wpb=871.1, bsz=871.1, num_updates=2150, lr=0.0004, gnorm=7.916, clip=2, train_wall=8, gb_free=68.6, wall=424 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:44:33]    INFO >> epoch 002:    666 / 1539 loss=4.272, wps=4842.7, ups=6.27, wpb=772.9, bsz=772.9, num_updates=2200, lr=0.0004, gnorm=8.213, clip=2, train_wall=7, gb_free=68.4, wall=432 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:44:41]    INFO >> epoch 002:    716 / 1539 loss=4.352, wps=4763.6, ups=6.72, wpb=708.6, bsz=708.6, num_updates=2250, lr=0.0004, gnorm=6.903, clip=0, train_wall=7, gb_free=73.1, wall=439 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:44:48]    INFO >> epoch 002:    766 / 1539 loss=4.367, wps=4291.4, ups=6.45, wpb=665.2, bsz=665.2, num_updates=2300, lr=0.0004, gnorm=5.941, clip=0, train_wall=7, gb_free=75.7, wall=447 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:44:56]    INFO >> epoch 002:    816 / 1539 loss=4.41, wps=4300.1, ups=6.56, wpb=656, bsz=656, num_updates=2350, lr=0.0004, gnorm=6.871, clip=0, train_wall=7, gb_free=72.2, wall=455 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:45:04]    INFO >> epoch 002:    866 / 1539 loss=4.302, wps=4514.8, ups=6.27, wpb=719.8, bsz=719.8, num_updates=2400, lr=0.0004, gnorm=7.573, clip=0, train_wall=8, gb_free=75.7, wall=463 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:45:13]    INFO >> epoch 002:    916 / 1539 loss=4.343, wps=4342.7, ups=6.47, wpb=671.5, bsz=671.5, num_updates=2450, lr=0.0004, gnorm=5.952, clip=0, train_wall=7, gb_free=73.1, wall=470 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:45:21]    INFO >> epoch 002:    966 / 1539 loss=4.177, wps=4147.4, ups=6.5, wpb=637.9, bsz=637.9, num_updates=2500, lr=0.0004, gnorm=7.008, clip=0, train_wall=7, gb_free=69.2, wall=478 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:45:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 37.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.54 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:45:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:45:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:45:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79337 MiB |  79397 MiB |  78153 GiB |  78075 GiB |
|       from large pool |  78941 MiB |  79001 MiB |  77714 GiB |  77637 GiB |
|       from small pool |    395 MiB |    396 MiB |    438 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79337 MiB |  79397 MiB |  78153 GiB |  78075 GiB |
|       from large pool |  78941 MiB |  79001 MiB |  77714 GiB |  77637 GiB |
|       from small pool |    395 MiB |    396 MiB |    438 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79098 MiB |  79157 MiB |  77983 GiB |  77905 GiB |
|       from large pool |  78705 MiB |  78764 MiB |  77545 GiB |  77468 GiB |
|       from small pool |    393 MiB |    394 MiB |    437 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80468 MiB |  80470 MiB | 335872 MiB | 255404 MiB |
|       from large pool |  80032 MiB |  80032 MiB | 334532 MiB | 254500 MiB |
|       from small pool |    436 MiB |    438 MiB |   1340 MiB |    904 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1070 MiB |   4610 MiB |  79127 GiB |  79126 GiB |
|       from large pool |   1030 MiB |   4603 MiB |  78628 GiB |  78627 GiB |
|       from small pool |     40 MiB |     41 MiB |    498 GiB |    498 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7296    |    7299    |    5179 K  |    5172 K  |
|       from large pool |     933    |     934    |    2516 K  |    2515 K  |
|       from small pool |    6363    |    6366    |    2663 K  |    2656 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7296    |    7299    |    5179 K  |    5172 K  |
|       from large pool |     933    |     934    |    2516 K  |    2515 K  |
|       from small pool |    6363    |    6366    |    2663 K  |    2656 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     690    |     691    |    2130    |    1440    |
|       from large pool |     472    |     472    |    1460    |     988    |
|       from small pool |     218    |     219    |     670    |     452    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     454    |     454    |    2903 K  |    2902 K  |
|       from large pool |      63    |      63    |    1651 K  |    1651 K  |
|       from small pool |     391    |     391    |    1251 K  |    1251 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:45:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:45:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 15:45:27] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.95 GiB is allocated by PyTorch, and 1.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:45:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:45:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:45:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 15        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78678 MiB |  78798 MiB |  79117 GiB |  79040 GiB |
|       from large pool |  78593 MiB |  78713 MiB |  78673 GiB |  78596 GiB |
|       from small pool |     85 MiB |     86 MiB |    443 GiB |    443 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78678 MiB |  78798 MiB |  79117 GiB |  79040 GiB |
|       from large pool |  78593 MiB |  78713 MiB |  78673 GiB |  78596 GiB |
|       from small pool |     85 MiB |     86 MiB |    443 GiB |    443 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78642 MiB |  78762 MiB |  78944 GiB |  78868 GiB |
|       from large pool |  78558 MiB |  78677 MiB |  78501 GiB |  78425 GiB |
|       from small pool |     84 MiB |     85 MiB |    443 GiB |    442 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB | 356340 MiB | 275836 MiB |
|       from large pool |  80416 MiB |  80416 MiB | 354934 MiB | 274518 MiB |
|       from small pool |     88 MiB |    436 MiB |   1406 MiB |   1318 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1705 MiB |   8357 MiB |  80006 GiB |  80005 GiB |
|       from large pool |   1702 MiB |   8349 MiB |  79501 GiB |  79499 GiB |
|       from small pool |      2 MiB |     29 MiB |    505 GiB |    505 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1873    |    1876    |    5242 K  |    5240 K  |
|       from large pool |     462    |     464    |    2546 K  |    2545 K  |
|       from small pool |    1411    |    1414    |    2696 K  |    2694 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1873    |    1876    |    5242 K  |    5240 K  |
|       from large pool |     462    |     464    |    2546 K  |    2545 K  |
|       from small pool |    1411    |    1414    |    2696 K  |    2694 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     221    |     689    |    2198    |    1977    |
|       from large pool |     177    |     471    |    1495    |    1318    |
|       from small pool |      44    |     218    |     703    |     659    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     167    |     168    |    2940 K  |    2940 K  |
|       from large pool |     120    |     125    |    1671 K  |    1671 K  |
|       from small pool |      47    |      59    |    1269 K  |    1269 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:45:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:45:27] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:45:30]    INFO >> epoch 002:   1018 / 1539 loss=4.356, wps=3879.4, ups=5.64, wpb=687.9, bsz=687.9, num_updates=2550, lr=0.0004, gnorm=7.042, clip=0, train_wall=7, gb_free=72.9, wall=487 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:45:38]    INFO >> epoch 002:   1068 / 1539 loss=4.269, wps=4805.9, ups=6.29, wpb=763.7, bsz=763.7, num_updates=2600, lr=0.0004, gnorm=6.337, clip=0, train_wall=7, gb_free=73.5, wall=495 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:45:47]    INFO >> epoch 002:   1118 / 1539 loss=4.01, wps=4824.8, ups=6.09, wpb=791.9, bsz=791.9, num_updates=2650, lr=0.0004, gnorm=8.164, clip=0, train_wall=8, gb_free=69.2, wall=503 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:45:55]    INFO >> epoch 002:   1168 / 1539 loss=4.323, wps=4915.4, ups=6.38, wpb=770.1, bsz=770.1, num_updates=2700, lr=0.0004, gnorm=6.3, clip=0, train_wall=7, gb_free=72.3, wall=511 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:46:03]    INFO >> epoch 002:   1218 / 1539 loss=4.087, wps=4510.5, ups=6.35, wpb=710, bsz=710, num_updates=2750, lr=0.0004, gnorm=7.287, clip=2, train_wall=7, gb_free=70.9, wall=519 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:46:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 121.25 MiB is free. Including non-PyTorch memory, this process has 79.00 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 5.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:46:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:46:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:46:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72821 MiB |  75425 MiB |  86354 GiB |  86283 GiB |
|       from large pool |  72808 MiB |  75412 MiB |  85870 GiB |  85799 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72821 MiB |  75425 MiB |  86354 GiB |  86283 GiB |
|       from large pool |  72808 MiB |  75412 MiB |  85870 GiB |  85799 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB |  86167 GiB |  86096 GiB |
|       from large pool |  72788 MiB |  75391 MiB |  85684 GiB |  85613 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80384 MiB |  80384 MiB | 387082 MiB | 306698 MiB |
|       from large pool |  80360 MiB |  80360 MiB | 385538 MiB | 305178 MiB |
|       from small pool |     24 MiB |    226 MiB |   1544 MiB |   1520 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5660 MiB |  11732 MiB |  87442 GiB |  87437 GiB |
|       from large pool |   5649 MiB |  11720 MiB |  86890 GiB |  86885 GiB |
|       from small pool |     11 MiB |     25 MiB |    551 GiB |    551 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |    5719 K  |    5718 K  |
|       from large pool |     312    |     320    |    2781 K  |    2781 K  |
|       from small pool |     291    |     356    |    2937 K  |    2937 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |    5719 K  |    5718 K  |
|       from large pool |     312    |     320    |    2781 K  |    2781 K  |
|       from small pool |     291    |     356    |    2937 K  |    2937 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     120    |     229    |    2295    |    2175    |
|       from large pool |     108    |     116    |    1523    |    1415    |
|       from small pool |      12    |     113    |     772    |     760    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     118    |     118    |    3207 K  |    3206 K  |
|       from large pool |      91    |      91    |    1823 K  |    1823 K  |
|       from small pool |      27    |      54    |    1383 K  |    1383 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:46:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:46:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:46:11]    INFO >> epoch 002:   1269 / 1539 loss=4.179, wps=4513.8, ups=5.9, wpb=765.3, bsz=765.3, num_updates=2800, lr=0.0004, gnorm=7.296, clip=0, train_wall=7, gb_free=70.5, wall=527 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:46:19]    INFO >> epoch 002:   1319 / 1539 loss=4.166, wps=4491.5, ups=6.79, wpb=661.5, bsz=661.5, num_updates=2850, lr=0.0004, gnorm=6.222, clip=0, train_wall=7, gb_free=75, wall=535 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:46:26]    INFO >> epoch 002:   1369 / 1539 loss=4.02, wps=4777.8, ups=6.56, wpb=728.5, bsz=728.5, num_updates=2900, lr=0.0004, gnorm=7.953, clip=0, train_wall=7, gb_free=70.5, wall=542 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:46:34]    INFO >> epoch 002:   1419 / 1539 loss=4.163, wps=4228.6, ups=6.47, wpb=653.6, bsz=653.6, num_updates=2950, lr=0.0004, gnorm=6.939, clip=0, train_wall=7, gb_free=64.8, wall=550 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:46:44]    INFO >> epoch 002:   1469 / 1539 loss=4.124, wps=3539.4, ups=5.04, wpb=702.5, bsz=702.5, num_updates=3000, lr=0.0004, gnorm=7.452, clip=2, train_wall=9, gb_free=70.3, wall=560 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:46:55]    INFO >> epoch 002:   1519 / 1539 loss=4.157, wps=4287.7, ups=6.37, wpb=673.3, bsz=673.3, num_updates=3050, lr=0.0004, gnorm=6.92, clip=2, train_wall=7, gb_free=74.2, wall=568 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:46:58]    INFO >> epoch 002 | loss 4.323 | wps 4255.8 | ups 5.97 | wpb 712.7 | bsz 712.7 | num_updates 3070 | lr 0.0004 | gnorm 7.129 | clip 0.4 | train_wall 224 | gb_free 72.4 | wall 571 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:46:58] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:47:11]    INFO >> epoch 002 | valid on 'valid' subset | loss 3.997 | wps 11132.6 | wpb 5412.5 | bsz 5412.5 | num_updates 3070 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:47:12]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:47:12]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 2 @ 3070 updates, score 3.997) (writing took 0.017663 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:47:12] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:47:16]    INFO >> epoch 003:     30 / 1539 loss=4.054, wps=1571.2, ups=2.31, wpb=681.1, bsz=681.1, num_updates=3100, lr=0.000392, gnorm=6.801, clip=0, train_wall=7, gb_free=70.7, wall=589 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:47:25]    INFO >> epoch 003:     80 / 1539 loss=3.993, wps=4966.6, ups=6.43, wpb=772.8, bsz=772.8, num_updates=3150, lr=0.000392, gnorm=7.652, clip=2, train_wall=7, gb_free=73.4, wall=597 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:47:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 78.04 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 2.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:47:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:47:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:47:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76326 MiB |  78600 MiB | 102451 GiB | 102377 GiB |
|       from large pool |  76307 MiB |  78583 MiB | 101871 GiB | 101796 GiB |
|       from small pool |     18 MiB |     19 MiB |    580 GiB |    580 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76326 MiB |  78600 MiB | 102451 GiB | 102377 GiB |
|       from large pool |  76307 MiB |  78583 MiB | 101871 GiB | 101796 GiB |
|       from small pool |     18 MiB |     19 MiB |    580 GiB |    580 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 102235 GiB | 102160 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 101655 GiB | 101581 GiB |
|       from small pool |     18 MiB |     19 MiB |    579 GiB |    579 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79400 MiB |  79400 MiB | 483464 MiB | 404064 MiB |
|       from large pool |  79376 MiB |  79376 MiB | 481770 MiB | 402394 MiB |
|       from small pool |     24 MiB |     74 MiB |   1694 MiB |   1670 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3073 MiB |   4962 MiB | 103111 GiB | 103108 GiB |
|       from large pool |   3068 MiB |   4948 MiB | 102454 GiB | 102451 GiB |
|       from small pool |      5 MiB |     27 MiB |    657 GiB |    657 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |    6746 K  |    6745 K  |
|       from large pool |     340    |     346    |    3209 K  |    3208 K  |
|       from small pool |     300    |     356    |    3536 K  |    3536 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |    6746 K  |    6745 K  |
|       from large pool |     340    |     346    |    3209 K  |    3208 K  |
|       from small pool |     300    |     356    |    3536 K  |    3536 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     106    |     118    |    2420    |    2314    |
|       from large pool |      94    |      94    |    1573    |    1479    |
|       from small pool |      12    |      37    |     847    |     835    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     110    |     112    |    3792 K  |    3792 K  |
|       from large pool |      85    |      87    |    2095 K  |    2095 K  |
|       from small pool |      25    |      57    |    1696 K  |    1696 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:47:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:47:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:47:35]    INFO >> epoch 003:    131 / 1539 loss=4.133, wps=4457.5, ups=5.34, wpb=835, bsz=835, num_updates=3200, lr=0.000392, gnorm=7.202, clip=0, train_wall=7, gb_free=71.9, wall=607 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:47:42]    INFO >> epoch 003:    181 / 1539 loss=4.229, wps=4479.6, ups=6.82, wpb=656.8, bsz=656.8, num_updates=3250, lr=0.000392, gnorm=6.987, clip=0, train_wall=7, gb_free=73, wall=614 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:47:47] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 153.25 MiB is free. Including non-PyTorch memory, this process has 78.97 GiB memory in use. Of the allocated memory 75.67 GiB is allocated by PyTorch, and 2.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:47:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:47:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:47:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 23        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76958 MiB |  77481 MiB | 105988 GiB | 105912 GiB |
|       from large pool |  76945 MiB |  77468 MiB | 105386 GiB | 105311 GiB |
|       from small pool |     12 MiB |     21 MiB |    601 GiB |    601 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76958 MiB |  77481 MiB | 105988 GiB | 105912 GiB |
|       from large pool |  76945 MiB |  77468 MiB | 105386 GiB | 105311 GiB |
|       from small pool |     12 MiB |     21 MiB |    601 GiB |    601 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 105764 GiB | 105689 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 105164 GiB | 105089 GiB |
|       from small pool |     12 MiB |     21 MiB |    600 GiB |    600 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80352 MiB |  80352 MiB | 489864 MiB | 409512 MiB |
|       from large pool |  80326 MiB |  80326 MiB | 487984 MiB | 407658 MiB |
|       from small pool |     26 MiB |    210 MiB |   1880 MiB |   1854 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3393 MiB |   8163 MiB | 107065 GiB | 107061 GiB |
|       from large pool |   3380 MiB |   8148 MiB | 106384 GiB | 106380 GiB |
|       from small pool |     13 MiB |     27 MiB |    681 GiB |    681 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |    6985 K  |    6984 K  |
|       from large pool |     315    |     322    |    3323 K  |    3323 K  |
|       from small pool |     291    |     356    |    3662 K  |    3661 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |    6985 K  |    6984 K  |
|       from large pool |     315    |     322    |    3323 K  |    3323 K  |
|       from small pool |     291    |     356    |    3662 K  |    3661 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     100    |     199    |    2520    |    2420    |
|       from large pool |      87    |      94    |    1580    |    1493    |
|       from small pool |      13    |     105    |     940    |     927    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      95    |      95    |    3925 K  |    3925 K  |
|       from large pool |      69    |      69    |    2166 K  |    2166 K  |
|       from small pool |      26    |      58    |    1758 K  |    1758 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:47:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:47:47] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:47:51]    INFO >> epoch 003:    232 / 1539 loss=4.088, wps=4389.9, ups=5.72, wpb=767.2, bsz=767.2, num_updates=3300, lr=0.000392, gnorm=7.422, clip=4, train_wall=8, gb_free=74, wall=623 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:48:00]    INFO >> epoch 003:    282 / 1539 loss=4.066, wps=4912, ups=6.53, wpb=751.9, bsz=751.9, num_updates=3350, lr=0.000392, gnorm=6.398, clip=0, train_wall=7, gb_free=70.6, wall=630 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:48:08]    INFO >> epoch 003:    332 / 1539 loss=4.078, wps=4802.6, ups=6.08, wpb=790.4, bsz=790.4, num_updates=3400, lr=0.000392, gnorm=7.224, clip=0, train_wall=8, gb_free=73.8, wall=639 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:48:16]    INFO >> epoch 003:    382 / 1539 loss=4.014, wps=4146.3, ups=6.02, wpb=688.4, bsz=688.4, num_updates=3450, lr=0.000392, gnorm=7.242, clip=0, train_wall=8, gb_free=72.5, wall=647 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:48:24]    INFO >> epoch 003:    432 / 1539 loss=4.117, wps=4311.9, ups=6.56, wpb=657.8, bsz=657.8, num_updates=3500, lr=0.000392, gnorm=6.676, clip=2, train_wall=7, gb_free=66.2, wall=654 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:48:33]    INFO >> epoch 003:    482 / 1539 loss=4.01, wps=4735.9, ups=6.28, wpb=754, bsz=754, num_updates=3550, lr=0.000392, gnorm=6.534, clip=0, train_wall=7, gb_free=73.1, wall=662 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:48:41]    INFO >> epoch 003:    532 / 1539 loss=3.978, wps=4978.9, ups=6.46, wpb=770.5, bsz=770.5, num_updates=3600, lr=0.000392, gnorm=6.887, clip=4, train_wall=7, gb_free=73.8, wall=670 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:48:49]    INFO >> epoch 003:    582 / 1539 loss=3.928, wps=4487.2, ups=6.23, wpb=720.1, bsz=720.1, num_updates=3650, lr=0.000392, gnorm=6.831, clip=0, train_wall=8, gb_free=71.6, wall=678 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:48:56]    INFO >> epoch 003:    632 / 1539 loss=3.941, wps=4693.7, ups=6.96, wpb=674.5, bsz=674.5, num_updates=3700, lr=0.000392, gnorm=6.76, clip=0, train_wall=7, gb_free=66.5, wall=685 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:49:05]    INFO >> epoch 003:    682 / 1539 loss=4.051, wps=4909.2, ups=6.49, wpb=756.5, bsz=756.5, num_updates=3750, lr=0.000392, gnorm=6.579, clip=0, train_wall=7, gb_free=75.1, wall=693 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:49:14]    INFO >> epoch 003:    732 / 1539 loss=3.834, wps=4638.4, ups=5.72, wpb=810.3, bsz=810.3, num_updates=3800, lr=0.000392, gnorm=6.808, clip=2, train_wall=8, gb_free=73.9, wall=702 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:49:22]    INFO >> epoch 003:    782 / 1539 loss=3.863, wps=4994.2, ups=6.28, wpb=795, bsz=795, num_updates=3850, lr=0.000392, gnorm=6.411, clip=2, train_wall=7, gb_free=73.7, wall=710 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:49:23] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 29.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.96 GiB is allocated by PyTorch, and 1.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:49:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:49:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:49:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78744 MiB |  78804 MiB | 123508 GiB | 123431 GiB |
|       from large pool |  78658 MiB |  78718 MiB | 122809 GiB | 122733 GiB |
|       from small pool |     85 MiB |     86 MiB |    698 GiB |    697 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78744 MiB |  78804 MiB | 123508 GiB | 123431 GiB |
|       from large pool |  78658 MiB |  78718 MiB | 122809 GiB | 122733 GiB |
|       from small pool |     85 MiB |     86 MiB |    698 GiB |    697 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78703 MiB |  78762 MiB | 123247 GiB | 123170 GiB |
|       from large pool |  78617 MiB |  78677 MiB | 122550 GiB | 122473 GiB |
|       from small pool |     85 MiB |     86 MiB |    697 GiB |    696 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80476 MiB |  80476 MiB | 517426 MiB | 436950 MiB |
|       from large pool |  80386 MiB |  80386 MiB | 515324 MiB | 434938 MiB |
|       from small pool |     90 MiB |    246 MiB |   2102 MiB |   2012 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1671 MiB |   7867 MiB | 124788 GiB | 124786 GiB |
|       from large pool |   1667 MiB |   7857 MiB | 123995 GiB | 123994 GiB |
|       from small pool |      4 MiB |     27 MiB |    792 GiB |    792 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1884    |    1887    |    8145 K  |    8143 K  |
|       from large pool |     463    |     464    |    3900 K  |    3900 K  |
|       from small pool |    1421    |    1424    |    4244 K  |    4243 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1884    |    1887    |    8145 K  |    8143 K  |
|       from large pool |     463    |     464    |    3900 K  |    3900 K  |
|       from small pool |    1421    |    1424    |    4244 K  |    4243 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     225    |     300    |    2734    |    2509    |
|       from large pool |     180    |     180    |    1683    |    1503    |
|       from small pool |      45    |     123    |    1051    |    1006    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     169    |     169    |    4572 K  |    4572 K  |
|       from large pool |     121    |     124    |    2540 K  |    2540 K  |
|       from small pool |      48    |      51    |    2031 K  |    2031 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:49:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:49:23] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:49:30]    INFO >> epoch 003:    833 / 1539 loss=3.89, wps=4466.4, ups=6.07, wpb=736, bsz=736, num_updates=3900, lr=0.000392, gnorm=7.727, clip=0, train_wall=7, gb_free=73, wall=718 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:49:39]    INFO >> epoch 003:    883 / 1539 loss=4.05, wps=4535, ups=6.33, wpb=717, bsz=717, num_updates=3950, lr=0.000392, gnorm=6.294, clip=0, train_wall=7, gb_free=72.5, wall=726 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:49:47]    INFO >> epoch 003:    933 / 1539 loss=3.868, wps=4634.5, ups=6.35, wpb=729.5, bsz=729.5, num_updates=4000, lr=0.000392, gnorm=6.845, clip=0, train_wall=7, gb_free=67.3, wall=734 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:49:56]    INFO >> epoch 003:    983 / 1539 loss=4.045, wps=3804.6, ups=5.56, wpb=684.3, bsz=684.3, num_updates=4050, lr=0.000392, gnorm=7.237, clip=0, train_wall=8, gb_free=71.5, wall=743 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:50:04]    INFO >> epoch 003:   1033 / 1539 loss=4.035, wps=4155.6, ups=6.53, wpb=636.3, bsz=636.3, num_updates=4100, lr=0.000392, gnorm=6.209, clip=0, train_wall=7, gb_free=68.3, wall=750 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:50:13]    INFO >> epoch 003:   1083 / 1539 loss=3.9, wps=4630.2, ups=6.87, wpb=673.9, bsz=673.9, num_updates=4150, lr=0.000392, gnorm=6.889, clip=0, train_wall=7, gb_free=72.9, wall=758 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:50:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 39.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.52 GiB is allocated by PyTorch, and 2.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:50:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:50:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:50:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78301 MiB |  78361 MiB | 133746 GiB | 133670 GiB |
|       from large pool |  77916 MiB |  77976 MiB | 132993 GiB | 132917 GiB |
|       from small pool |    385 MiB |    386 MiB |    753 GiB |    752 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78301 MiB |  78361 MiB | 133746 GiB | 133670 GiB |
|       from large pool |  77916 MiB |  77976 MiB | 132993 GiB | 132917 GiB |
|       from small pool |    385 MiB |    386 MiB |    753 GiB |    752 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78077 MiB |  78137 MiB | 133463 GiB | 133387 GiB |
|       from large pool |  77694 MiB |  77753 MiB | 132711 GiB | 132635 GiB |
|       from small pool |    383 MiB |    384 MiB |    752 GiB |    751 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80466 MiB |  80504 MiB | 539302 MiB | 458836 MiB |
|       from large pool |  80042 MiB |  80326 MiB | 536864 MiB | 456822 MiB |
|       from small pool |    424 MiB |    426 MiB |   2438 MiB |   2014 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2104 MiB |   6644 MiB | 134066 GiB | 134064 GiB |
|       from large pool |   2065 MiB |   6637 MiB | 133209 GiB | 133207 GiB |
|       from small pool |     38 MiB |     40 MiB |    856 GiB |    856 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7109    |    7112    |    8817 K  |    8810 K  |
|       from large pool |     916    |     917    |    4239 K  |    4238 K  |
|       from small pool |    6193    |    6196    |    4578 K  |    4572 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7109    |    7112    |    8817 K  |    8810 K  |
|       from large pool |     916    |     917    |    4239 K  |    4238 K  |
|       from small pool |    6193    |    6196    |    4578 K  |    4572 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     742    |     743    |    3261    |    2519    |
|       from large pool |     530    |     530    |    2042    |    1512    |
|       from small pool |     212    |     213    |    1219    |    1007    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     510    |     511    |    4950 K  |    4949 K  |
|       from large pool |     132    |     132    |    2765 K  |    2764 K  |
|       from small pool |     378    |     379    |    2185 K  |    2184 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:50:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:50:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:50:21]    INFO >> epoch 003:   1134 / 1539 loss=3.94, wps=4090.2, ups=5.97, wpb=684.8, bsz=684.8, num_updates=4200, lr=0.000392, gnorm=6.733, clip=0, train_wall=7, gb_free=73, wall=766 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:50:29]    INFO >> epoch 003:   1184 / 1539 loss=3.948, wps=4260.9, ups=6.33, wpb=673.6, bsz=673.6, num_updates=4250, lr=0.000392, gnorm=6.037, clip=0, train_wall=7, gb_free=73.5, wall=774 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:50:37]    INFO >> epoch 003:   1234 / 1539 loss=3.876, wps=4437.7, ups=6.18, wpb=718.1, bsz=718.1, num_updates=4300, lr=0.000392, gnorm=6.441, clip=2, train_wall=8, gb_free=70.5, wall=782 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:50:46]    INFO >> epoch 003:   1284 / 1539 loss=3.873, wps=4192.7, ups=6.77, wpb=619.7, bsz=619.7, num_updates=4350, lr=0.000392, gnorm=5.925, clip=2, train_wall=7, gb_free=73.5, wall=789 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:50:53]    INFO >> epoch 003:   1334 / 1539 loss=3.872, wps=4701.5, ups=6.64, wpb=708, bsz=708, num_updates=4400, lr=0.000392, gnorm=6.81, clip=4, train_wall=7, gb_free=72.1, wall=797 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:51:01]    INFO >> epoch 003:   1384 / 1539 loss=3.793, wps=4572.1, ups=6.6, wpb=692.7, bsz=692.7, num_updates=4450, lr=0.000392, gnorm=7.498, clip=0, train_wall=7, gb_free=74.2, wall=805 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:51:08]    INFO >> epoch 003:   1434 / 1539 loss=3.951, wps=4357.4, ups=6.69, wpb=651.7, bsz=651.7, num_updates=4500, lr=0.000392, gnorm=6.036, clip=0, train_wall=7, gb_free=72.1, wall=812 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:51:16]    INFO >> epoch 003:   1484 / 1539 loss=3.889, wps=4198.1, ups=6.37, wpb=658.9, bsz=658.9, num_updates=4550, lr=0.000392, gnorm=7.016, clip=0, train_wall=7, gb_free=72.4, wall=820 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:51:24]    INFO >> epoch 003:   1534 / 1539 loss=3.938, wps=4399.5, ups=6.64, wpb=662.9, bsz=662.9, num_updates=4600, lr=0.000392, gnorm=8.05, clip=0, train_wall=7, gb_free=72, wall=827 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:51:24]    INFO >> epoch 003 | loss 3.974 | wps 4253.1 | ups 5.97 | wpb 712.7 | bsz 712.7 | num_updates 4605 | lr 0.000392 | gnorm 6.848 | clip 0.8 | train_wall 225 | gb_free 74.4 | wall 828 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:51:24] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:51:39]    INFO >> epoch 003 | valid on 'valid' subset | loss 3.902 | wps 10378.5 | wpb 5412.5 | bsz 5412.5 | num_updates 4605 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:51:39]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:51:39]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 3 @ 4605 updates, score 3.902) (writing took 0.014377 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:51:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:51:47]    INFO >> epoch 004:     45 / 1539 loss=3.882, wps=1624.9, ups=2.16, wpb=751.7, bsz=751.7, num_updates=4650, lr=0.000376, gnorm=6.998, clip=0, train_wall=8, gb_free=72.7, wall=851 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:51:57]    INFO >> epoch 004:     95 / 1539 loss=3.895, wps=4711.9, ups=6.23, wpb=756, bsz=756, num_updates=4700, lr=0.000376, gnorm=6.493, clip=0, train_wall=8, gb_free=67.8, wall=859 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:52:08]    INFO >> epoch 004:    145 / 1539 loss=3.459, wps=5169.6, ups=4.93, wpb=1048.6, bsz=1048.6, num_updates=4750, lr=0.000376, gnorm=7.774, clip=2, train_wall=10, gb_free=75.1, wall=869 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:52:15]    INFO >> epoch 004:    195 / 1539 loss=3.919, wps=4676.6, ups=6.67, wpb=701.3, bsz=701.3, num_updates=4800, lr=0.000376, gnorm=5.99, clip=0, train_wall=7, gb_free=71.9, wall=876 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:52:24]    INFO >> epoch 004:    245 / 1539 loss=3.818, wps=4797.9, ups=6.45, wpb=743.6, bsz=743.6, num_updates=4850, lr=0.000376, gnorm=8.303, clip=4, train_wall=7, gb_free=73.5, wall=884 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:52:32]    INFO >> epoch 004:    295 / 1539 loss=3.995, wps=4369.3, ups=6.75, wpb=647.4, bsz=647.4, num_updates=4900, lr=0.000376, gnorm=5.975, clip=0, train_wall=7, gb_free=74.6, wall=891 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:52:39]    INFO >> epoch 004:    345 / 1539 loss=3.903, wps=4649.2, ups=6.76, wpb=687.4, bsz=687.4, num_updates=4950, lr=0.000376, gnorm=5.471, clip=0, train_wall=7, gb_free=69.9, wall=899 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:52:47]    INFO >> epoch 004:    395 / 1539 loss=3.618, wps=4913.3, ups=6.46, wpb=761.1, bsz=761.1, num_updates=5000, lr=0.000376, gnorm=6.774, clip=0, train_wall=7, gb_free=67.8, wall=907 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:52:55]    INFO >> epoch 004:    445 / 1539 loss=3.969, wps=3729.3, ups=5.91, wpb=630.5, bsz=630.5, num_updates=5050, lr=0.000376, gnorm=6.225, clip=0, train_wall=8, gb_free=71.4, wall=915 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:53:05]    INFO >> epoch 004:    495 / 1539 loss=3.888, wps=4514.5, ups=6.27, wpb=720.1, bsz=720.1, num_updates=5100, lr=0.000376, gnorm=6.131, clip=0, train_wall=7, gb_free=74, wall=923 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:53:12]    INFO >> epoch 004:    545 / 1539 loss=3.875, wps=4464.2, ups=6.36, wpb=701.6, bsz=701.6, num_updates=5150, lr=0.000376, gnorm=6.642, clip=0, train_wall=7, gb_free=73.5, wall=931 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:53:20]    INFO >> epoch 004:    595 / 1539 loss=3.883, wps=4042.2, ups=6.42, wpb=629.8, bsz=629.8, num_updates=5200, lr=0.000376, gnorm=6.043, clip=0, train_wall=7, gb_free=71, wall=939 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:53:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 13.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 1.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:53:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:53:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:53:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79173 MiB |  79233 MiB | 167559 GiB | 167482 GiB |
|       from large pool |  79083 MiB |  79143 MiB | 166611 GiB | 166534 GiB |
|       from small pool |     89 MiB |     91 MiB |    948 GiB |    948 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79173 MiB |  79233 MiB | 167559 GiB | 167482 GiB |
|       from large pool |  79083 MiB |  79143 MiB | 166611 GiB | 166534 GiB |
|       from small pool |     89 MiB |     91 MiB |    948 GiB |    948 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79123 MiB |  79183 MiB | 167207 GiB | 167130 GiB |
|       from large pool |  79034 MiB |  79093 MiB | 166260 GiB | 166183 GiB |
|       from small pool |     89 MiB |     90 MiB |    946 GiB |    946 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80492 MiB |  80492 MiB | 564646 MiB | 484154 MiB |
|       from large pool |  80398 MiB |  80398 MiB | 562140 MiB | 481742 MiB |
|       from small pool |     94 MiB |     94 MiB |   2506 MiB |   2412 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1258 MiB |   7162 MiB | 164240 GiB | 164239 GiB |
|       from large pool |   1254 MiB |   7152 MiB | 163165 GiB | 163164 GiB |
|       from small pool |      4 MiB |     27 MiB |   1074 GiB |   1074 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1961    |    1964    |   11044 K  |   11042 K  |
|       from large pool |     470    |     471    |    5271 K  |    5271 K  |
|       from small pool |    1491    |    1494    |    5772 K  |    5770 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1961    |    1964    |   11044 K  |   11042 K  |
|       from large pool |     470    |     471    |    5271 K  |    5271 K  |
|       from small pool |    1491    |    1494    |    5772 K  |    5770 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     223    |     223    |    3348    |    3125    |
|       from large pool |     176    |     176    |    2095    |    1919    |
|       from small pool |      47    |      47    |    1253    |    1206    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     140    |     140    |    6216 K  |    6216 K  |
|       from large pool |      92    |      96    |    3444 K  |    3444 K  |
|       from small pool |      48    |      58    |    2772 K  |    2772 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:53:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:53:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:53:28]    INFO >> epoch 004:    646 / 1539 loss=3.763, wps=4198.3, ups=6.26, wpb=671.1, bsz=671.1, num_updates=5250, lr=0.000376, gnorm=7.285, clip=2, train_wall=7, gb_free=68.7, wall=947 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:53:38] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 983.25 MiB is free. Including non-PyTorch memory, this process has 78.16 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:53:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:53:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:53:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76323 MiB |  78596 MiB | 170178 GiB | 170104 GiB |
|       from large pool |  76304 MiB |  78579 MiB | 169217 GiB | 169142 GiB |
|       from small pool |     18 MiB |     24 MiB |    961 GiB |    961 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76323 MiB |  78596 MiB | 170178 GiB | 170104 GiB |
|       from large pool |  76304 MiB |  78579 MiB | 169217 GiB | 169142 GiB |
|       from small pool |     18 MiB |     24 MiB |    961 GiB |    961 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 169821 GiB | 169746 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 168861 GiB | 168786 GiB |
|       from small pool |     18 MiB |     24 MiB |    960 GiB |    959 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79522 MiB |  80432 MiB | 637044 MiB | 557522 MiB |
|       from large pool |  79496 MiB |  80338 MiB | 634538 MiB | 555042 MiB |
|       from small pool |     26 MiB |     94 MiB |   2506 MiB |   2480 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3198 MiB |   4143 MiB | 166736 GiB | 166733 GiB |
|       from large pool |   3191 MiB |   4135 MiB | 165646 GiB | 165643 GiB |
|       from small pool |      7 MiB |     23 MiB |   1089 GiB |   1089 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   11213 K  |   11212 K  |
|       from large pool |     340    |     346    |    5361 K  |    5361 K  |
|       from small pool |     300    |     356    |    5852 K  |    5851 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   11213 K  |   11212 K  |
|       from large pool |     340    |     346    |    5361 K  |    5361 K  |
|       from small pool |     300    |     356    |    5852 K  |    5851 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     107    |     222    |    3403    |    3296    |
|       from large pool |      94    |     175    |    2150    |    2056    |
|       from small pool |      13    |      47    |    1253    |    1240    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     126    |     128    |    6309 K  |    6309 K  |
|       from large pool |      98    |     100    |    3503 K  |    3503 K  |
|       from small pool |      28    |      51    |    2805 K  |    2805 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:53:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:53:38] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:53:39]    INFO >> epoch 004:    697 / 1539 loss=3.843, wps=3267, ups=5.13, wpb=636.6, bsz=636.6, num_updates=5300, lr=0.000376, gnorm=5.877, clip=0, train_wall=7, gb_free=67.8, wall=956 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:53:48]    INFO >> epoch 004:    747 / 1539 loss=3.932, wps=3802.9, ups=5.65, wpb=673.1, bsz=673.1, num_updates=5350, lr=0.000376, gnorm=6.006, clip=0, train_wall=8, gb_free=72.9, wall=965 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:53:56]    INFO >> epoch 004:    797 / 1539 loss=3.778, wps=4629.2, ups=6.52, wpb=710.1, bsz=710.1, num_updates=5400, lr=0.000376, gnorm=5.676, clip=0, train_wall=7, gb_free=72.3, wall=973 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:54:03]    INFO >> epoch 004:    847 / 1539 loss=3.815, wps=4866.2, ups=6.51, wpb=747.5, bsz=747.5, num_updates=5450, lr=0.000376, gnorm=6.151, clip=0, train_wall=7, gb_free=63.6, wall=981 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:54:12]    INFO >> epoch 004:    897 / 1539 loss=3.83, wps=4676.2, ups=6.01, wpb=778.3, bsz=778.3, num_updates=5500, lr=0.000376, gnorm=7.275, clip=0, train_wall=8, gb_free=70.2, wall=989 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:54:19]    INFO >> epoch 004:    947 / 1539 loss=3.847, wps=4275.9, ups=6.61, wpb=646.7, bsz=646.7, num_updates=5550, lr=0.000376, gnorm=5.443, clip=0, train_wall=7, gb_free=67.5, wall=996 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:54:28]    INFO >> epoch 004:    997 / 1539 loss=3.777, wps=4651.1, ups=6.08, wpb=764.9, bsz=764.9, num_updates=5600, lr=0.000376, gnorm=6.099, clip=0, train_wall=8, gb_free=75.5, wall=1005 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:54:35]    INFO >> epoch 004:   1047 / 1539 loss=3.92, wps=4251.7, ups=6.76, wpb=628.6, bsz=628.6, num_updates=5650, lr=0.000376, gnorm=5.957, clip=0, train_wall=7, gb_free=72.5, wall=1012 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:54:45]    INFO >> epoch 004:   1097 / 1539 loss=3.793, wps=4669.4, ups=6.4, wpb=729.6, bsz=729.6, num_updates=5700, lr=0.000376, gnorm=5.892, clip=0, train_wall=7, gb_free=71.1, wall=1020 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:54:53]    INFO >> epoch 004:   1147 / 1539 loss=3.857, wps=4392.8, ups=6.49, wpb=676.4, bsz=676.4, num_updates=5750, lr=0.000376, gnorm=5.802, clip=0, train_wall=7, gb_free=56.5, wall=1028 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:55:01]    INFO >> epoch 004:   1197 / 1539 loss=3.772, wps=4723.7, ups=6.26, wpb=754.7, bsz=754.7, num_updates=5800, lr=0.000376, gnorm=6.18, clip=0, train_wall=7, gb_free=75.6, wall=1036 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:55:08]    INFO >> epoch 004:   1247 / 1539 loss=3.774, wps=4402.9, ups=7.01, wpb=627.8, bsz=627.8, num_updates=5850, lr=0.000376, gnorm=5.229, clip=0, train_wall=7, gb_free=75.6, wall=1043 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:55:18]    INFO >> epoch 004:   1297 / 1539 loss=3.825, wps=4005.8, ups=6.21, wpb=644.6, bsz=644.6, num_updates=5900, lr=0.000376, gnorm=5.988, clip=2, train_wall=8, gb_free=74.6, wall=1051 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:55:26]    INFO >> epoch 004:   1347 / 1539 loss=3.674, wps=4984.1, ups=6.22, wpb=801.1, bsz=801.1, num_updates=5950, lr=0.000376, gnorm=6.455, clip=0, train_wall=8, gb_free=66.8, wall=1059 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:55:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 2.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:55:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:55:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:55:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 34        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75053 MiB |  75996 MiB | 190873 GiB | 190800 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 189802 GiB | 189729 GiB |
|       from small pool |     12 MiB |     13 MiB |   1070 GiB |   1070 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75053 MiB |  75996 MiB | 190873 GiB | 190800 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 189802 GiB | 189729 GiB |
|       from small pool |     12 MiB |     13 MiB |   1070 GiB |   1070 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 190472 GiB | 190399 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 189403 GiB | 189330 GiB |
|       from small pool |     12 MiB |     13 MiB |   1069 GiB |   1069 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78634 MiB |  79742 MiB | 645400 MiB | 566766 MiB |
|       from large pool |  78608 MiB |  79496 MiB | 642674 MiB | 564066 MiB |
|       from small pool |     26 MiB |    246 MiB |   2726 MiB |   2700 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3580 MiB |   7454 MiB | 189472 GiB | 189468 GiB |
|       from large pool |   3567 MiB |   7440 MiB | 188257 GiB | 188254 GiB |
|       from small pool |     13 MiB |     21 MiB |   1214 GiB |   1214 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   12564 K  |   12563 K  |
|       from large pool |     314    |     322    |    6052 K  |    6052 K  |
|       from small pool |     291    |     336    |    6511 K  |    6510 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   12564 K  |   12563 K  |
|       from large pool |     314    |     322    |    6052 K  |    6052 K  |
|       from small pool |     291    |     336    |    6511 K  |    6510 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     217    |    3521    |    3430    |
|       from large pool |      78    |      94    |    2158    |    2080    |
|       from small pool |      13    |     123    |    1363    |    1350    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      90    |    7038 K  |    7038 K  |
|       from large pool |      60    |      61    |    3937 K  |    3937 K  |
|       from small pool |      29    |      51    |    3100 K  |    3100 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:55:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:55:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:55:35]    INFO >> epoch 004:   1398 / 1539 loss=3.741, wps=4440.7, ups=5.61, wpb=791.5, bsz=791.5, num_updates=6000, lr=0.000376, gnorm=6.285, clip=2, train_wall=8, gb_free=71.6, wall=1068 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:55:42]    INFO >> epoch 004:   1448 / 1539 loss=3.815, wps=4645.4, ups=6.53, wpb=711.7, bsz=711.7, num_updates=6050, lr=0.000376, gnorm=6.582, clip=0, train_wall=7, gb_free=73.5, wall=1075 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:55:51]    INFO >> epoch 004:   1498 / 1539 loss=3.766, wps=4359.6, ups=6.39, wpb=682, bsz=682, num_updates=6100, lr=0.000376, gnorm=6.744, clip=0, train_wall=7, gb_free=74.2, wall=1083 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:55:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 25.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.10 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:55:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:55:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:55:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 35        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78888 MiB |  78948 MiB | 194338 GiB | 194261 GiB |
|       from large pool |  78495 MiB |  78555 MiB | 193245 GiB | 193169 GiB |
|       from small pool |    393 MiB |    394 MiB |   1092 GiB |   1092 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78888 MiB |  78948 MiB | 194338 GiB | 194261 GiB |
|       from large pool |  78495 MiB |  78555 MiB | 193245 GiB | 193169 GiB |
|       from small pool |    393 MiB |    394 MiB |   1092 GiB |   1092 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78858 MiB |  78917 MiB | 193930 GiB | 193853 GiB |
|       from large pool |  78467 MiB |  78526 MiB | 192839 GiB | 192762 GiB |
|       from small pool |    390 MiB |    392 MiB |   1090 GiB |   1090 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80480 MiB |  80482 MiB | 647248 MiB | 566768 MiB |
|       from large pool |  80048 MiB |  80048 MiB | 644114 MiB | 564066 MiB |
|       from small pool |    432 MiB |    434 MiB |   3134 MiB |   2702 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1531 MiB |   4145 MiB | 193502 GiB | 193500 GiB |
|       from large pool |   1492 MiB |   4108 MiB | 192262 GiB | 192260 GiB |
|       from small pool |     38 MiB |     41 MiB |   1239 GiB |   1239 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7252    |    7255    |   12813 K  |   12806 K  |
|       from large pool |     929    |     930    |    6169 K  |    6168 K  |
|       from small pool |    6323    |    6326    |    6643 K  |    6637 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7252    |    7255    |   12813 K  |   12806 K  |
|       from large pool |     929    |     930    |    6169 K  |    6168 K  |
|       from small pool |    6323    |    6326    |    6643 K  |    6637 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     318    |     319    |    3749    |    3431    |
|       from large pool |     102    |     102    |    2182    |    2080    |
|       from small pool |     216    |     217    |    1567    |    1351    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     462    |     464    |    7175 K  |    7174 K  |
|       from large pool |      77    |      77    |    4009 K  |    4009 K  |
|       from small pool |     385    |     387    |    3165 K  |    3165 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:55:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:55:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:55:58]    INFO >> epoch 004 | loss 3.814 | wps 4184 | ups 5.87 | wpb 712.7 | bsz 712.7 | num_updates 6140 | lr 0.000376 | gnorm 6.31 | clip 0.4 | train_wall 227 | gb_free 70.1 | wall 1090 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:55:58] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:56:13]    INFO >> epoch 004 | valid on 'valid' subset | loss 3.846 | wps 9969.1 | wpb 5412.5 | bsz 5412.5 | num_updates 6140 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:56:13]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:56:13]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 4 @ 6140 updates, score 3.846) (writing took 0.021527 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:56:13] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:56:15]    INFO >> epoch 005:     10 / 1539 loss=3.853, wps=1370.7, ups=2.13, wpb=643.2, bsz=643.2, num_updates=6150, lr=0.000354, gnorm=6.845, clip=0, train_wall=7, gb_free=72.1, wall=1107 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:56:25]    INFO >> epoch 005:     60 / 1539 loss=3.929, wps=4207.8, ups=5.97, wpb=704.5, bsz=704.5, num_updates=6200, lr=0.000354, gnorm=5.878, clip=0, train_wall=8, gb_free=73.4, wall=1115 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:56:32]    INFO >> epoch 005:    110 / 1539 loss=3.838, wps=4661, ups=6.61, wpb=705.4, bsz=705.4, num_updates=6250, lr=0.000354, gnorm=6.679, clip=0, train_wall=7, gb_free=71.5, wall=1123 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:56:40]    INFO >> epoch 005:    160 / 1539 loss=3.857, wps=4344.3, ups=6.79, wpb=639.6, bsz=639.6, num_updates=6300, lr=0.000354, gnorm=5.968, clip=0, train_wall=7, gb_free=74, wall=1130 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:56:47]    INFO >> epoch 005:    210 / 1539 loss=3.775, wps=4364.8, ups=6.5, wpb=671.6, bsz=671.6, num_updates=6350, lr=0.000354, gnorm=5.302, clip=0, train_wall=7, gb_free=71, wall=1138 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:56:57]    INFO >> epoch 005:    260 / 1539 loss=3.75, wps=5167.9, ups=5.77, wpb=895.2, bsz=895.2, num_updates=6400, lr=0.000354, gnorm=6.123, clip=0, train_wall=8, gb_free=67.8, wall=1146 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:57:05]    INFO >> epoch 005:    310 / 1539 loss=3.706, wps=4533, ups=6.43, wpb=704.8, bsz=704.8, num_updates=6450, lr=0.000354, gnorm=6.131, clip=0, train_wall=7, gb_free=71.8, wall=1154 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:57:13]    INFO >> epoch 005:    360 / 1539 loss=3.565, wps=4879.6, ups=6.31, wpb=772.8, bsz=772.8, num_updates=6500, lr=0.000354, gnorm=6.614, clip=2, train_wall=7, gb_free=69.5, wall=1162 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:57:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 27.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.85 GiB is allocated by PyTorch, and 1.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:57:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:57:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:57:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 36        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78634 MiB |  78694 MiB | 211161 GiB | 211084 GiB |
|       from large pool |  78243 MiB |  78303 MiB | 209958 GiB | 209881 GiB |
|       from small pool |    390 MiB |    391 MiB |   1202 GiB |   1202 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78634 MiB |  78694 MiB | 211161 GiB | 211084 GiB |
|       from large pool |  78243 MiB |  78303 MiB | 209958 GiB | 209881 GiB |
|       from small pool |    390 MiB |    391 MiB |   1202 GiB |   1202 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78618 MiB |  78677 MiB | 210720 GiB | 210643 GiB |
|       from large pool |  78229 MiB |  78288 MiB | 209518 GiB | 209442 GiB |
|       from small pool |    388 MiB |    389 MiB |   1201 GiB |   1200 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80478 MiB |  80480 MiB | 647308 MiB | 566830 MiB |
|       from large pool |  80048 MiB |  80048 MiB | 644174 MiB | 564126 MiB |
|       from small pool |    430 MiB |    432 MiB |   3134 MiB |   2704 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1783 MiB |   5192 MiB | 209174 GiB | 209172 GiB |
|       from large pool |   1744 MiB |   5187 MiB | 207812 GiB | 207810 GiB |
|       from small pool |     39 MiB |     41 MiB |   1361 GiB |   1361 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7208    |    7211    |   13967 K  |   13960 K  |
|       from large pool |     925    |     926    |    6636 K  |    6635 K  |
|       from small pool |    6283    |    6286    |    7330 K  |    7324 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7208    |    7211    |   13967 K  |   13960 K  |
|       from large pool |     925    |     926    |    6636 K  |    6635 K  |
|       from small pool |    6283    |    6286    |    7330 K  |    7324 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     317    |     318    |    3750    |    3433    |
|       from large pool |     102    |     102    |    2183    |    2081    |
|       from small pool |     215    |     216    |    1567    |    1352    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     476    |     477    |    7847 K  |    7846 K  |
|       from large pool |      90    |      90    |    4312 K  |    4312 K  |
|       from small pool |     386    |     387    |    3534 K  |    3534 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:57:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:57:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:57:21]    INFO >> epoch 005:    411 / 1539 loss=3.829, wps=4264.8, ups=6.26, wpb=681.6, bsz=681.6, num_updates=6550, lr=0.000354, gnorm=6.156, clip=0, train_wall=7, gb_free=73.9, wall=1170 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:57:28]    INFO >> epoch 005:    461 / 1539 loss=3.818, wps=4084.7, ups=6.72, wpb=608, bsz=608, num_updates=6600, lr=0.000354, gnorm=5.59, clip=0, train_wall=7, gb_free=74.7, wall=1177 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:57:41]    INFO >> epoch 005:    511 / 1539 loss=3.602, wps=2931.6, ups=4.01, wpb=730.5, bsz=730.5, num_updates=6650, lr=0.000354, gnorm=5.923, clip=0, train_wall=12, gb_free=69.8, wall=1190 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:57:48]    INFO >> epoch 005:    561 / 1539 loss=3.915, wps=4104.8, ups=6.61, wpb=620.7, bsz=620.7, num_updates=6700, lr=0.000354, gnorm=5.655, clip=0, train_wall=7, gb_free=72.5, wall=1197 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:57:53] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.48 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:57:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:57:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:57:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 38        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79276 MiB |  79336 MiB | 216573 GiB | 216496 GiB |
|       from large pool |  79185 MiB |  79245 MiB | 215343 GiB | 215265 GiB |
|       from small pool |     91 MiB |     92 MiB |   1230 GiB |   1230 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79276 MiB |  79336 MiB | 216573 GiB | 216496 GiB |
|       from large pool |  79185 MiB |  79245 MiB | 215343 GiB | 215265 GiB |
|       from small pool |     91 MiB |     92 MiB |   1230 GiB |   1230 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79244 MiB |  79303 MiB | 216121 GiB | 216043 GiB |
|       from large pool |  79153 MiB |  79212 MiB | 214892 GiB | 214815 GiB |
|       from small pool |     90 MiB |     92 MiB |   1228 GiB |   1228 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB | 647732 MiB | 567230 MiB |
|       from large pool |  80408 MiB |  80408 MiB | 644594 MiB | 564186 MiB |
|       from small pool |     94 MiB |    430 MiB |   3138 MiB |   3044 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1165 MiB |   6539 MiB | 214884 GiB | 214883 GiB |
|       from large pool |   1162 MiB |   6529 MiB | 213490 GiB | 213489 GiB |
|       from small pool |      2 MiB |     25 MiB |   1393 GiB |   1393 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1983    |    1986    |   14316 K  |   14314 K  |
|       from large pool |     472    |     473    |    6818 K  |    6818 K  |
|       from small pool |    1511    |    1514    |    7497 K  |    7496 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1983    |    1986    |   14316 K  |   14314 K  |
|       from large pool |     472    |     473    |    6818 K  |    6818 K  |
|       from small pool |    1511    |    1514    |    7497 K  |    7496 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     155    |     317    |    3759    |    3604    |
|       from large pool |     108    |     108    |    2190    |    2082    |
|       from small pool |      47    |     215    |    1569    |    1522    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     138    |     140    |    8036 K  |    8036 K  |
|       from large pool |      88    |      91    |    4428 K  |    4428 K  |
|       from small pool |      50    |      58    |    3608 K  |    3608 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:57:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:57:53] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:57:57]    INFO >> epoch 005:    612 / 1539 loss=3.81, wps=4402.1, ups=5.82, wpb=756, bsz=756, num_updates=6750, lr=0.000354, gnorm=6.454, clip=0, train_wall=7, gb_free=70.1, wall=1206 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:58:08]    INFO >> epoch 005:    662 / 1539 loss=3.778, wps=4652.1, ups=6.31, wpb=736.8, bsz=736.8, num_updates=6800, lr=0.000354, gnorm=5.874, clip=0, train_wall=7, gb_free=74.2, wall=1214 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:58:16]    INFO >> epoch 005:    712 / 1539 loss=3.854, wps=4587.8, ups=6.29, wpb=729, bsz=729, num_updates=6850, lr=0.000354, gnorm=6.724, clip=0, train_wall=7, gb_free=71.4, wall=1222 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:58:23]    INFO >> epoch 005:    762 / 1539 loss=3.733, wps=4743.2, ups=6.61, wpb=717.1, bsz=717.1, num_updates=6900, lr=0.000354, gnorm=5.735, clip=0, train_wall=7, gb_free=72.9, wall=1229 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:58:32]    INFO >> epoch 005:    812 / 1539 loss=3.702, wps=3897.2, ups=5.79, wpb=673.7, bsz=673.7, num_updates=6950, lr=0.000354, gnorm=5.913, clip=0, train_wall=8, gb_free=69.7, wall=1238 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:58:41]    INFO >> epoch 005:    862 / 1539 loss=3.648, wps=4238.7, ups=6.32, wpb=670.9, bsz=670.9, num_updates=7000, lr=0.000354, gnorm=5.964, clip=0, train_wall=7, gb_free=66.5, wall=1246 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:58:49]    INFO >> epoch 005:    912 / 1539 loss=3.743, wps=4351.1, ups=6.52, wpb=667, bsz=667, num_updates=7050, lr=0.000354, gnorm=5.898, clip=0, train_wall=7, gb_free=71.7, wall=1254 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:58:55] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 131.25 MiB is free. Including non-PyTorch memory, this process has 78.99 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 3.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:58:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:58:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:58:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 39        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76325 MiB |  78599 MiB | 227233 GiB | 227158 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 225947 GiB | 225873 GiB |
|       from small pool |     18 MiB |     24 MiB |   1285 GiB |   1285 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76325 MiB |  78599 MiB | 227233 GiB | 227158 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 225947 GiB | 225873 GiB |
|       from small pool |     18 MiB |     24 MiB |   1285 GiB |   1285 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 226757 GiB | 226683 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 225474 GiB | 225399 GiB |
|       from small pool |     18 MiB |     24 MiB |   1283 GiB |   1283 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80374 MiB |  80442 MiB | 647732 MiB | 567358 MiB |
|       from large pool |  80348 MiB |  80348 MiB | 644594 MiB | 564246 MiB |
|       from small pool |     26 MiB |     94 MiB |   3138 MiB |   3112 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4048 MiB |   5924 MiB | 225933 GiB | 225929 GiB |
|       from large pool |   4041 MiB |   5910 MiB | 224477 GiB | 224474 GiB |
|       from small pool |      7 MiB |     23 MiB |   1455 GiB |   1455 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   14999 K  |   14999 K  |
|       from large pool |     340    |     346    |    7173 K  |    7173 K  |
|       from small pool |     300    |     356    |    7826 K  |    7825 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   14999 K  |   14999 K  |
|       from large pool |     340    |     346    |    7173 K  |    7173 K  |
|       from small pool |     300    |     356    |    7826 K  |    7825 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     120    |     154    |    3759    |    3639    |
|       from large pool |     107    |     107    |    2190    |    2083    |
|       from small pool |      13    |      47    |    1569    |    1556    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     111    |     113    |    8408 K  |    8408 K  |
|       from large pool |      84    |      86    |    4655 K  |    4655 K  |
|       from small pool |      27    |      48    |    3752 K  |    3752 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:58:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:58:55] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:58:57]    INFO >> epoch 005:    963 / 1539 loss=3.743, wps=4006.7, ups=5.92, wpb=676.6, bsz=676.6, num_updates=7100, lr=0.000354, gnorm=5.954, clip=0, train_wall=7, gb_free=70.4, wall=1262 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:59:06]    INFO >> epoch 005:   1013 / 1539 loss=3.406, wps=5245.9, ups=5.5, wpb=954.2, bsz=954.2, num_updates=7150, lr=0.000354, gnorm=6.741, clip=2, train_wall=9, gb_free=70.3, wall=1271 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:59:16]    INFO >> epoch 005:   1063 / 1539 loss=3.583, wps=4641.2, ups=5.87, wpb=791.1, bsz=791.1, num_updates=7200, lr=0.000354, gnorm=5.801, clip=0, train_wall=8, gb_free=64.7, wall=1280 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:59:24]    INFO >> epoch 005:   1113 / 1539 loss=3.662, wps=4071.1, ups=6.5, wpb=626.1, bsz=626.1, num_updates=7250, lr=0.000354, gnorm=6.185, clip=0, train_wall=7, gb_free=74.2, wall=1287 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:59:26] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 199.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 4.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:59:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:59:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:59:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 43        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72823 MiB |  75427 MiB | 232756 GiB | 232685 GiB |
|       from large pool |  72810 MiB |  75414 MiB | 231439 GiB | 231368 GiB |
|       from small pool |     12 MiB |     21 MiB |   1316 GiB |   1316 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72823 MiB |  75427 MiB | 232756 GiB | 232685 GiB |
|       from large pool |  72810 MiB |  75414 MiB | 231439 GiB | 231368 GiB |
|       from small pool |     12 MiB |     21 MiB |   1316 GiB |   1316 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 232269 GiB | 232198 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 230954 GiB | 230883 GiB |
|       from small pool |     12 MiB |     21 MiB |   1314 GiB |   1314 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80306 MiB |  80306 MiB | 688436 MiB | 608130 MiB |
|       from large pool |  80280 MiB |  80280 MiB | 685098 MiB | 604818 MiB |
|       from small pool |     26 MiB |    226 MiB |   3338 MiB |   3312 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5580 MiB |   9572 MiB | 230915 GiB | 230909 GiB |
|       from large pool |   5567 MiB |   9557 MiB | 229423 GiB | 229418 GiB |
|       from small pool |     13 MiB |     31 MiB |   1491 GiB |   1491 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   15362 K  |   15361 K  |
|       from large pool |     312    |     320    |    7349 K  |    7348 K  |
|       from small pool |     291    |     356    |    8012 K  |    8012 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   15362 K  |   15361 K  |
|       from large pool |     312    |     320    |    7349 K  |    7348 K  |
|       from small pool |     291    |     356    |    8012 K  |    8012 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     170    |     326    |    3990    |    3820    |
|       from large pool |     157    |     213    |    2321    |    2164    |
|       from small pool |      13    |     113    |    1669    |    1656    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     159    |     161    |    8616 K  |    8616 K  |
|       from large pool |     133    |     135    |    4773 K  |    4773 K  |
|       from small pool |      26    |      58    |    3842 K  |    3842 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:59:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:59:26] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:59:33]    INFO >> epoch 005:   1164 / 1539 loss=3.7, wps=4356.8, ups=5.58, wpb=780.4, bsz=780.4, num_updates=7300, lr=0.000354, gnorm=6.197, clip=2, train_wall=8, gb_free=69.6, wall=1296 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:59:42]    INFO >> epoch 005:   1214 / 1539 loss=3.809, wps=4061.1, ups=6.17, wpb=657.7, bsz=657.7, num_updates=7350, lr=0.000354, gnorm=5.535, clip=0, train_wall=8, gb_free=68.7, wall=1305 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:59:49]    INFO >> epoch 005:   1264 / 1539 loss=3.69, wps=4428.6, ups=6.97, wpb=635.1, bsz=635.1, num_updates=7400, lr=0.000354, gnorm=5.463, clip=0, train_wall=7, gb_free=73.5, wall=1312 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:59:57]    INFO >> epoch 005:   1314 / 1539 loss=3.695, wps=4112.6, ups=6.31, wpb=651.5, bsz=651.5, num_updates=7450, lr=0.000354, gnorm=5.382, clip=0, train_wall=7, gb_free=70, wall=1320 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:00:05]    INFO >> epoch 005:   1364 / 1539 loss=3.697, wps=4542.1, ups=6.26, wpb=725.6, bsz=725.6, num_updates=7500, lr=0.000354, gnorm=6.003, clip=0, train_wall=7, gb_free=74.7, wall=1328 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:00:13]    INFO >> epoch 005:   1414 / 1539 loss=3.86, wps=4168, ups=6.62, wpb=630.1, bsz=630.1, num_updates=7550, lr=0.000354, gnorm=5.507, clip=0, train_wall=7, gb_free=74.2, wall=1335 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:00:22]    INFO >> epoch 005:   1464 / 1539 loss=3.652, wps=4701.5, ups=6.16, wpb=763.7, bsz=763.7, num_updates=7600, lr=0.000354, gnorm=6.112, clip=0, train_wall=8, gb_free=48.5, wall=1343 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:00:31]    INFO >> epoch 005:   1514 / 1539 loss=3.7, wps=4664.4, ups=5.81, wpb=802.7, bsz=802.7, num_updates=7650, lr=0.000354, gnorm=6.281, clip=2, train_wall=8, gb_free=54.7, wall=1352 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:00:35]    INFO >> epoch 005 | loss 3.731 | wps 4107 | ups 5.76 | wpb 712.7 | bsz 712.7 | num_updates 7675 | lr 0.000354 | gnorm 6.008 | clip 0.3 | train_wall 233 | gb_free 63.5 | wall 1356 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:00:35] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:00:52]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.779 | wps 9821 | wpb 5412.5 | bsz 5412.5 | num_updates 7675 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:00:52]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:00:52]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 5 @ 7675 updates, score 3.779) (writing took 0.018395 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:00:52] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:00:56]    INFO >> epoch 006:     25 / 1539 loss=3.76, wps=1503.4, ups=2.12, wpb=708.8, bsz=708.8, num_updates=7700, lr=0.000327, gnorm=5.68, clip=0, train_wall=7, gb_free=74.9, wall=1375 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:01:04]    INFO >> epoch 006:     75 / 1539 loss=3.754, wps=5092, ups=5.9, wpb=863.7, bsz=863.7, num_updates=7750, lr=0.000327, gnorm=5.87, clip=0, train_wall=8, gb_free=75.2, wall=1384 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:01:12]    INFO >> epoch 006:    125 / 1539 loss=3.671, wps=4425.5, ups=6.55, wpb=675.3, bsz=675.3, num_updates=7800, lr=0.000327, gnorm=5.908, clip=0, train_wall=7, gb_free=70.3, wall=1392 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:01:20]    INFO >> epoch 006:    175 / 1539 loss=3.688, wps=4375, ups=6.35, wpb=689, bsz=689, num_updates=7850, lr=0.000327, gnorm=6.062, clip=0, train_wall=7, gb_free=74.3, wall=1399 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:01:23] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 535.25 MiB is free. Including non-PyTorch memory, this process has 78.59 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 4.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:01:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:01:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:01:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 44        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75055 MiB |  75998 MiB | 254753 GiB | 254679 GiB |
|       from large pool |  75043 MiB |  75985 MiB | 253307 GiB | 253233 GiB |
|       from small pool |     12 MiB |     15 MiB |   1446 GiB |   1446 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75055 MiB |  75998 MiB | 254753 GiB | 254679 GiB |
|       from large pool |  75043 MiB |  75985 MiB | 253307 GiB | 253233 GiB |
|       from small pool |     12 MiB |     15 MiB |   1446 GiB |   1446 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 254221 GiB | 254148 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 252777 GiB | 252704 GiB |
|       from small pool |     12 MiB |     15 MiB |   1443 GiB |   1443 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79970 MiB |  80406 MiB | 690438 MiB | 610468 MiB |
|       from large pool |  79944 MiB |  80306 MiB | 687026 MiB | 607082 MiB |
|       from small pool |     26 MiB |    100 MiB |   3412 MiB |   3386 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4914 MiB |   8989 MiB | 249563 GiB | 249559 GiB |
|       from large pool |   4900 MiB |   8976 MiB | 247929 GiB | 247925 GiB |
|       from small pool |     13 MiB |     25 MiB |   1633 GiB |   1633 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   16790 K  |   16790 K  |
|       from large pool |     314    |     322    |    7980 K  |    7979 K  |
|       from small pool |     291    |     356    |    8810 K  |    8810 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   16790 K  |   16790 K  |
|       from large pool |     314    |     322    |    7980 K  |    7979 K  |
|       from small pool |     291    |     356    |    8810 K  |    8810 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     210    |    4031    |    3859    |
|       from large pool |     159    |     160    |    2325    |    2166    |
|       from small pool |      13    |      50    |    1706    |    1693    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     162    |     162    |    9442 K  |    9442 K  |
|       from large pool |     134    |     134    |    5197 K  |    5197 K  |
|       from small pool |      28    |      57    |    4245 K  |    4245 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:01:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:01:23] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 16:01:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 535.25 MiB is free. Including non-PyTorch memory, this process has 78.59 GiB memory in use. Of the allocated memory 72.94 GiB is allocated by PyTorch, and 5.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:01:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:01:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:01:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 45        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69875 MiB |  74872 MiB | 255914 GiB | 255845 GiB |
|       from large pool |  69858 MiB |  74855 MiB | 254463 GiB | 254394 GiB |
|       from small pool |     17 MiB |     17 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69875 MiB |  74872 MiB | 255914 GiB | 255845 GiB |
|       from large pool |  69858 MiB |  74855 MiB | 254463 GiB | 254394 GiB |
|       from small pool |     17 MiB |     17 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69846 MiB |  74842 MiB | 255380 GiB | 255311 GiB |
|       from large pool |  69829 MiB |  74825 MiB | 253930 GiB | 253862 GiB |
|       from small pool |     17 MiB |     17 MiB |   1449 GiB |   1449 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79970 MiB |  80006 MiB | 690474 MiB | 610504 MiB |
|       from large pool |  79944 MiB |  79944 MiB | 687026 MiB | 607082 MiB |
|       from small pool |     26 MiB |     62 MiB |   3448 MiB |   3422 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  10094 MiB |  11472 MiB | 250619 GiB | 250609 GiB |
|       from large pool |  10085 MiB |  11463 MiB | 248979 GiB | 248969 GiB |
|       from small pool |      8 MiB |     27 MiB |   1639 GiB |   1639 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     643    |     661    |   16854 K  |   16853 K  |
|       from large pool |     344    |     362    |    8013 K  |    8013 K  |
|       from small pool |     299    |     356    |    8840 K  |    8840 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     643    |     661    |   16854 K  |   16853 K  |
|       from large pool |     344    |     362    |    8013 K  |    8013 K  |
|       from small pool |     299    |     356    |    8840 K  |    8840 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     190    |    4049    |    3877    |
|       from large pool |     159    |     159    |    2325    |    2166    |
|       from small pool |      13    |      31    |    1724    |    1711    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     151    |     153    |    9478 K  |    9477 K  |
|       from large pool |     121    |     123    |    5219 K  |    5219 K  |
|       from small pool |      30    |      62    |    4258 K  |    4258 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:01:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:01:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:01:31]    INFO >> epoch 006:    227 / 1539 loss=3.747, wps=3784.7, ups=5.26, wpb=719.2, bsz=719.2, num_updates=7900, lr=0.000327, gnorm=5.515, clip=0, train_wall=8, gb_free=71.4, wall=1409 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:01:38]    INFO >> epoch 006:    277 / 1539 loss=3.731, wps=4142.8, ups=6.62, wpb=625.7, bsz=625.7, num_updates=7950, lr=0.000327, gnorm=4.791, clip=0, train_wall=7, gb_free=74.5, wall=1416 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:01:46]    INFO >> epoch 006:    327 / 1539 loss=3.49, wps=5055.2, ups=6.09, wpb=830.5, bsz=830.5, num_updates=8000, lr=0.000327, gnorm=6.444, clip=0, train_wall=8, gb_free=70.3, wall=1425 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:01:56]    INFO >> epoch 006:    377 / 1539 loss=3.795, wps=4044.3, ups=6.15, wpb=657.6, bsz=657.6, num_updates=8050, lr=0.000327, gnorm=5.116, clip=0, train_wall=8, gb_free=71.1, wall=1433 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:02:04]    INFO >> epoch 006:    427 / 1539 loss=3.595, wps=4886.3, ups=6.33, wpb=772.1, bsz=772.1, num_updates=8100, lr=0.000327, gnorm=5.414, clip=0, train_wall=7, gb_free=75, wall=1441 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:02:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.39 GiB is free. Including non-PyTorch memory, this process has 76.72 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 5.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:02:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:02:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:02:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 47        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72059 MiB |  72460 MiB | 262338 GiB | 262268 GiB |
|       from large pool |  72042 MiB |  72442 MiB | 260852 GiB | 260782 GiB |
|       from small pool |     17 MiB |     21 MiB |   1486 GiB |   1486 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72059 MiB |  72460 MiB | 262338 GiB | 262268 GiB |
|       from large pool |  72042 MiB |  72442 MiB | 260852 GiB | 260782 GiB |
|       from small pool |     17 MiB |     21 MiB |   1486 GiB |   1486 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 261790 GiB | 261719 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 260305 GiB | 260235 GiB |
|       from small pool |     17 MiB |     21 MiB |   1484 GiB |   1484 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78056 MiB |  80154 MiB | 696490 MiB | 618434 MiB |
|       from large pool |  78026 MiB |  79944 MiB | 692858 MiB | 614832 MiB |
|       from small pool |     30 MiB |    210 MiB |   3632 MiB |   3602 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5996 MiB |   8377 MiB | 256685 GiB | 256680 GiB |
|       from large pool |   5983 MiB |   8364 MiB | 255005 GiB | 255000 GiB |
|       from small pool |     12 MiB |     31 MiB |   1680 GiB |   1680 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   17287 K  |   17286 K  |
|       from large pool |     308    |     315    |    8230 K  |    8230 K  |
|       from small pool |     298    |     356    |    9056 K  |    9056 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   17287 K  |   17286 K  |
|       from large pool |     308    |     315    |    8230 K  |    8230 K  |
|       from small pool |     298    |     356    |    9056 K  |    9056 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     264    |    4144    |    3972    |
|       from large pool |     157    |     159    |    2328    |    2171    |
|       from small pool |      15    |     105    |    1816    |    1801    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     169    |     169    |    9721 K  |    9721 K  |
|       from large pool |     137    |     137    |    5362 K  |    5362 K  |
|       from small pool |      32    |      55    |    4358 K  |    4358 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:02:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:02:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:02:12]    INFO >> epoch 006:    478 / 1539 loss=3.864, wps=4170.6, ups=5.91, wpb=705.8, bsz=705.8, num_updates=8150, lr=0.000327, gnorm=5.285, clip=0, train_wall=7, gb_free=65, wall=1449 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:02:21]    INFO >> epoch 006:    528 / 1539 loss=3.689, wps=4157.1, ups=5.87, wpb=707.6, bsz=707.6, num_updates=8200, lr=0.000327, gnorm=5.52, clip=0, train_wall=8, gb_free=72, wall=1458 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:02:30]    INFO >> epoch 006:    578 / 1539 loss=3.745, wps=4935.2, ups=6.19, wpb=797.6, bsz=797.6, num_updates=8250, lr=0.000327, gnorm=5.679, clip=0, train_wall=8, gb_free=74.1, wall=1466 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:02:37]    INFO >> epoch 006:    628 / 1539 loss=3.698, wps=4425.2, ups=6.86, wpb=644.9, bsz=644.9, num_updates=8300, lr=0.000327, gnorm=5.699, clip=0, train_wall=7, gb_free=70.5, wall=1473 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:02:46]    INFO >> epoch 006:    678 / 1539 loss=3.67, wps=4445.1, ups=6.09, wpb=730.2, bsz=730.2, num_updates=8350, lr=0.000327, gnorm=5.896, clip=0, train_wall=8, gb_free=73.6, wall=1481 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:02:53]    INFO >> epoch 006:    728 / 1539 loss=3.762, wps=4088.9, ups=6.64, wpb=616.1, bsz=616.1, num_updates=8400, lr=0.000327, gnorm=4.652, clip=0, train_wall=7, gb_free=62, wall=1489 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:02:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.16 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:02:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:02:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:02:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 48        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77932 MiB |  77992 MiB | 270810 GiB | 270734 GiB |
|       from large pool |  77549 MiB |  77609 MiB | 269273 GiB | 269198 GiB |
|       from small pool |    383 MiB |    384 MiB |   1536 GiB |   1536 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77932 MiB |  77992 MiB | 270810 GiB | 270734 GiB |
|       from large pool |  77549 MiB |  77609 MiB | 269273 GiB | 269198 GiB |
|       from small pool |    383 MiB |    384 MiB |   1536 GiB |   1536 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77897 MiB |  77956 MiB | 270242 GiB | 270166 GiB |
|       from large pool |  77516 MiB |  77575 MiB | 268708 GiB | 268632 GiB |
|       from small pool |    381 MiB |    382 MiB |   1534 GiB |   1533 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 698924 MiB | 618436 MiB |
|       from large pool |  80066 MiB |  80066 MiB | 694898 MiB | 614832 MiB |
|       from small pool |    422 MiB |    424 MiB |   4026 MiB |   3604 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2495 MiB |   6667 MiB | 264813 GiB | 264811 GiB |
|       from large pool |   2456 MiB |   6661 MiB | 263075 GiB | 263073 GiB |
|       from small pool |     38 MiB |     40 MiB |   1737 GiB |   1737 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7076    |    7079    |   17882 K  |   17874 K  |
|       from large pool |     913    |     914    |    8520 K  |    8519 K  |
|       from small pool |    6163    |    6166    |    9361 K  |    9355 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7076    |    7079    |   17882 K  |   17874 K  |
|       from large pool |     913    |     914    |    8520 K  |    8519 K  |
|       from small pool |    6163    |    6166    |    9361 K  |    9355 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     402    |     403    |    4375    |    3973    |
|       from large pool |     191    |     191    |    2362    |    2171    |
|       from small pool |     211    |     212    |    2013    |    1802    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     527    |     528    |   10056 K  |   10055 K  |
|       from large pool |     150    |     150    |    5552 K  |    5552 K  |
|       from small pool |     377    |     378    |    4503 K  |    4503 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:02:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:02:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:03:01]    INFO >> epoch 006:    779 / 1539 loss=3.611, wps=4040.7, ups=6.29, wpb=642.5, bsz=642.5, num_updates=8450, lr=0.000327, gnorm=5.29, clip=0, train_wall=7, gb_free=74.1, wall=1497 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:03:10]    INFO >> epoch 006:    829 / 1539 loss=3.695, wps=4292.7, ups=6.54, wpb=656.8, bsz=656.8, num_updates=8500, lr=0.000327, gnorm=5.355, clip=0, train_wall=7, gb_free=66.6, wall=1504 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:03:18]    INFO >> epoch 006:    879 / 1539 loss=3.575, wps=4671.4, ups=6.4, wpb=729.6, bsz=729.6, num_updates=8550, lr=0.000327, gnorm=5.82, clip=0, train_wall=7, gb_free=70.2, wall=1512 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:03:27]    INFO >> epoch 006:    929 / 1539 loss=3.655, wps=4754.3, ups=5.75, wpb=826.1, bsz=826.1, num_updates=8600, lr=0.000327, gnorm=5.78, clip=0, train_wall=8, gb_free=73.1, wall=1521 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:03:35]    INFO >> epoch 006:    979 / 1539 loss=3.597, wps=4317, ups=6.16, wpb=701.1, bsz=701.1, num_updates=8650, lr=0.000327, gnorm=5.317, clip=0, train_wall=8, gb_free=75.1, wall=1529 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:03:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.57 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.27 GiB is free. Including non-PyTorch memory, this process has 76.84 GiB memory in use. Of the allocated memory 66.03 GiB is allocated by PyTorch, and 10.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:03:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:03:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:03:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 49        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63499 MiB |  70130 MiB | 279630 GiB | 279567 GiB |
|       from large pool |  63479 MiB |  70110 MiB | 278045 GiB | 277983 GiB |
|       from small pool |     20 MiB |     60 MiB |   1584 GiB |   1584 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63499 MiB |  70130 MiB | 279630 GiB | 279567 GiB |
|       from large pool |  63479 MiB |  70110 MiB | 278045 GiB | 277983 GiB |
|       from small pool |     20 MiB |     60 MiB |   1584 GiB |   1584 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63481 MiB |  70111 MiB | 279042 GiB | 278980 GiB |
|       from large pool |  63461 MiB |  70091 MiB | 277460 GiB | 277398 GiB |
|       from small pool |     20 MiB |     60 MiB |   1582 GiB |   1582 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78178 MiB |  80428 MiB | 698924 MiB | 620746 MiB |
|       from large pool |  78146 MiB |  80006 MiB | 694898 MiB | 616752 MiB |
|       from small pool |     32 MiB |    422 MiB |   4026 MiB |   3994 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7300 MiB |  10790 MiB | 272795 GiB | 272788 GiB |
|       from large pool |   7288 MiB |  10778 MiB | 271002 GiB | 270994 GiB |
|       from small pool |     11 MiB |     27 MiB |   1793 GiB |   1793 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |    1326    |   18465 K  |   18465 K  |
|       from large pool |     330    |     403    |    8814 K  |    8814 K  |
|       from small pool |     316    |     924    |    9651 K  |    9650 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |    1326    |   18465 K  |   18465 K  |
|       from large pool |     330    |     403    |    8814 K  |    8814 K  |
|       from small pool |     316    |     924    |    9651 K  |    9650 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     176    |     401    |    4375    |    4199    |
|       from large pool |     160    |     190    |    2362    |    2202    |
|       from small pool |      16    |     211    |    2013    |    1997    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     164    |     213    |   10388 K  |   10388 K  |
|       from large pool |     129    |     160    |    5749 K  |    5749 K  |
|       from small pool |      35    |      54    |    4638 K  |    4638 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:03:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:03:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:03:45]    INFO >> epoch 006:   1030 / 1539 loss=3.723, wps=4064.6, ups=5.57, wpb=730.1, bsz=730.1, num_updates=8700, lr=0.000327, gnorm=4.684, clip=0, train_wall=8, gb_free=11.5, wall=1538 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:03:53]    INFO >> epoch 006:   1080 / 1539 loss=3.654, wps=4457.4, ups=6.16, wpb=723.5, bsz=723.5, num_updates=8750, lr=0.000327, gnorm=5.81, clip=2, train_wall=8, gb_free=72.4, wall=1546 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:04:02]    INFO >> epoch 006:   1130 / 1539 loss=3.625, wps=4458.7, ups=5.89, wpb=757.6, bsz=757.6, num_updates=8800, lr=0.000327, gnorm=4.935, clip=0, train_wall=8, gb_free=72.1, wall=1555 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:04:11]    INFO >> epoch 006:   1180 / 1539 loss=3.701, wps=4723.9, ups=6.38, wpb=740.5, bsz=740.5, num_updates=8850, lr=0.000327, gnorm=5.818, clip=0, train_wall=7, gb_free=69, wall=1562 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:04:19]    INFO >> epoch 006:   1230 / 1539 loss=3.711, wps=4130.6, ups=6.26, wpb=659.3, bsz=659.3, num_updates=8900, lr=0.000327, gnorm=5.5, clip=0, train_wall=7, gb_free=73.8, wall=1570 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:04:26]    INFO >> epoch 006:   1280 / 1539 loss=3.756, wps=4321.9, ups=6.64, wpb=650.6, bsz=650.6, num_updates=8950, lr=0.000327, gnorm=5.694, clip=0, train_wall=7, gb_free=75.3, wall=1578 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:04:34]    INFO >> epoch 006:   1330 / 1539 loss=3.647, wps=4730.7, ups=6.38, wpb=741, bsz=741, num_updates=9000, lr=0.000327, gnorm=5.273, clip=0, train_wall=7, gb_free=71.3, wall=1586 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:04:42]    INFO >> epoch 006:   1380 / 1539 loss=3.711, wps=4189.3, ups=6.88, wpb=608.9, bsz=608.9, num_updates=9050, lr=0.000327, gnorm=5.434, clip=0, train_wall=7, gb_free=68.9, wall=1593 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:04:49]    INFO >> epoch 006:   1430 / 1539 loss=3.642, wps=4769.1, ups=6.34, wpb=751.8, bsz=751.8, num_updates=9100, lr=0.000327, gnorm=5.67, clip=0, train_wall=7, gb_free=70.5, wall=1601 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:04:58]    INFO >> epoch 006:   1480 / 1539 loss=3.5, wps=4419.1, ups=6.16, wpb=717, bsz=717, num_updates=9150, lr=0.000327, gnorm=5.691, clip=2, train_wall=8, gb_free=69.4, wall=1609 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:05:05]    INFO >> epoch 006:   1530 / 1539 loss=3.68, wps=4585.3, ups=6.8, wpb=674.1, bsz=674.1, num_updates=9200, lr=0.000327, gnorm=4.577, clip=0, train_wall=7, gb_free=74.6, wall=1616 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:05:06]    INFO >> epoch 006 | loss 3.678 | wps 4165.3 | ups 5.86 | wpb 711.1 | bsz 711.1 | num_updates 9209 | lr 0.000327 | gnorm 5.49 | clip 0.1 | train_wall 228 | gb_free 72.3 | wall 1618 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:05:06] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:05:24]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.804 | wps 10147 | wpb 5412.5 | bsz 5412.5 | num_updates 9209 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:05:24]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:05:24]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 6 @ 9209 updates, score 3.804) (writing took 0.013676 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:05:24] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:05:32]    INFO >> epoch 007:     41 / 1539 loss=3.728, wps=1468.8, ups=2.09, wpb=704.3, bsz=704.3, num_updates=9250, lr=0.000295, gnorm=6.028, clip=0, train_wall=8, gb_free=71.6, wall=1640 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:05:33] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.82 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 70.77 GiB is allocated by PyTorch, and 6.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:05:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:05:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:05:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 51        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72064 MiB |  72463 MiB | 300570 GiB | 300499 GiB |
|       from large pool |  72047 MiB |  72446 MiB | 298863 GiB | 298793 GiB |
|       from small pool |     17 MiB |     25 MiB |   1706 GiB |   1706 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72064 MiB |  72463 MiB | 300570 GiB | 300499 GiB |
|       from large pool |  72047 MiB |  72446 MiB | 298863 GiB | 298793 GiB |
|       from small pool |     17 MiB |     25 MiB |   1706 GiB |   1706 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 299940 GiB | 299870 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 298236 GiB | 298165 GiB |
|       from small pool |     17 MiB |     25 MiB |   1704 GiB |   1704 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78638 MiB |  78794 MiB | 712374 MiB | 633736 MiB |
|       from large pool |  78610 MiB |  78670 MiB | 708256 MiB | 629646 MiB |
|       from small pool |     28 MiB |    124 MiB |   4118 MiB |   4090 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6573 MiB |   8375 MiB | 290276 GiB | 290269 GiB |
|       from large pool |   6562 MiB |   8363 MiB | 288349 GiB | 288342 GiB |
|       from small pool |     10 MiB |     35 MiB |   1927 GiB |   1927 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   19804 K  |   19803 K  |
|       from large pool |     308    |     315    |    9401 K  |    9401 K  |
|       from small pool |     298    |     356    |   10402 K  |   10402 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   19804 K  |   19803 K  |
|       from large pool |     308    |     315    |    9401 K  |    9401 K  |
|       from small pool |     298    |     356    |   10402 K  |   10402 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     170    |     222    |    4426    |    4256    |
|       from large pool |     156    |     160    |    2367    |    2211    |
|       from small pool |      14    |      62    |    2059    |    2045    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     165    |     166    |   11170 K  |   11169 K  |
|       from large pool |     137    |     138    |    6143 K  |    6143 K  |
|       from small pool |      28    |      60    |    5026 K  |    5026 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:05:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:05:33] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:05:40]    INFO >> epoch 007:     92 / 1539 loss=3.603, wps=4209.4, ups=6.24, wpb=674.7, bsz=674.7, num_updates=9300, lr=0.000295, gnorm=6.011, clip=0, train_wall=7, gb_free=74.6, wall=1648 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:05:47]    INFO >> epoch 007:    142 / 1539 loss=3.713, wps=4306.9, ups=6.49, wpb=663.1, bsz=663.1, num_updates=9350, lr=0.000295, gnorm=5.742, clip=0, train_wall=7, gb_free=73.4, wall=1656 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:05:55]    INFO >> epoch 007:    192 / 1539 loss=3.429, wps=5449.9, ups=6.16, wpb=885.1, bsz=885.1, num_updates=9400, lr=0.000295, gnorm=5.25, clip=2, train_wall=8, gb_free=67.7, wall=1664 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:06:04]    INFO >> epoch 007:    242 / 1539 loss=3.704, wps=4745.7, ups=5.97, wpb=794.3, bsz=794.3, num_updates=9450, lr=0.000295, gnorm=5.355, clip=0, train_wall=8, gb_free=70.7, wall=1673 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:06:12]    INFO >> epoch 007:    292 / 1539 loss=3.58, wps=4475.2, ups=6.4, wpb=699.6, bsz=699.6, num_updates=9500, lr=0.000295, gnorm=5.959, clip=0, train_wall=7, gb_free=71.4, wall=1680 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:06:17] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 6.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:06:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:06:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:06:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 52        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69006 MiB |  71763 MiB | 308769 GiB | 308702 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 307018 GiB | 306951 GiB |
|       from small pool |     16 MiB |     18 MiB |   1751 GiB |   1751 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69006 MiB |  71763 MiB | 308769 GiB | 308702 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 307018 GiB | 306951 GiB |
|       from small pool |     16 MiB |     18 MiB |   1751 GiB |   1751 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 308120 GiB | 308053 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 306372 GiB | 306304 GiB |
|       from small pool |     16 MiB |     17 MiB |   1748 GiB |   1748 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78634 MiB |  78836 MiB | 712572 MiB | 633938 MiB |
|       from large pool |  78610 MiB |  78610 MiB | 708256 MiB | 629646 MiB |
|       from small pool |     24 MiB |    226 MiB |   4316 MiB |   4292 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6899 MiB |  10616 MiB | 298160 GiB | 298153 GiB |
|       from large pool |   6892 MiB |  10608 MiB | 296182 GiB | 296175 GiB |
|       from small pool |      7 MiB |     23 MiB |   1978 GiB |   1978 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   20344 K  |   20344 K  |
|       from large pool |     343    |     351    |    9674 K  |    9674 K  |
|       from small pool |     298    |     342    |   10670 K  |   10669 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   20344 K  |   20344 K  |
|       from large pool |     343    |     351    |    9674 K  |    9674 K  |
|       from small pool |     298    |     342    |   10670 K  |   10669 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     168    |     269    |    4525    |    4357    |
|       from large pool |     156    |     156    |    2367    |    2211    |
|       from small pool |      12    |     113    |    2158    |    2146    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     150    |     154    |   11473 K  |   11473 K  |
|       from large pool |     124    |     128    |    6324 K  |    6323 K  |
|       from small pool |      26    |      52    |    5149 K  |    5149 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:06:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:06:17] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:06:21]    INFO >> epoch 007:    343 / 1539 loss=3.67, wps=3681, ups=5.36, wpb=687.1, bsz=687.1, num_updates=9550, lr=0.000295, gnorm=5.189, clip=0, train_wall=8, gb_free=69.5, wall=1690 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:06:31]    INFO >> epoch 007:    393 / 1539 loss=3.76, wps=4089, ups=6.84, wpb=598.1, bsz=598.1, num_updates=9600, lr=0.000295, gnorm=5.011, clip=0, train_wall=7, gb_free=62, wall=1697 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:06:39]    INFO >> epoch 007:    443 / 1539 loss=3.616, wps=4329.8, ups=6.39, wpb=677.8, bsz=677.8, num_updates=9650, lr=0.000295, gnorm=4.998, clip=0, train_wall=7, gb_free=74.5, wall=1705 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:06:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 123.25 MiB is free. Including non-PyTorch memory, this process has 79.00 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:06:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:06:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:06:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 54        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76958 MiB |  77480 MiB | 313195 GiB | 313120 GiB |
|       from large pool |  76945 MiB |  77467 MiB | 311423 GiB | 311348 GiB |
|       from small pool |     12 MiB |     18 MiB |   1772 GiB |   1772 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76958 MiB |  77480 MiB | 313195 GiB | 313120 GiB |
|       from large pool |  76945 MiB |  77467 MiB | 311423 GiB | 311348 GiB |
|       from small pool |     12 MiB |     18 MiB |   1772 GiB |   1772 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 312537 GiB | 312462 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 310767 GiB | 310692 GiB |
|       from small pool |     12 MiB |     18 MiB |   1769 GiB |   1769 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80382 MiB |  80382 MiB | 719316 MiB | 638934 MiB |
|       from large pool |  80358 MiB |  80358 MiB | 714952 MiB | 634594 MiB |
|       from small pool |     24 MiB |     72 MiB |   4364 MiB |   4340 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3423 MiB |   7648 MiB | 302334 GiB | 302331 GiB |
|       from large pool |   3412 MiB |   7637 MiB | 300332 GiB | 300329 GiB |
|       from small pool |     11 MiB |     23 MiB |   2002 GiB |   2002 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   20614 K  |   20613 K  |
|       from large pool |     315    |     322    |    9816 K  |    9816 K  |
|       from small pool |     291    |     356    |   10797 K  |   10797 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   20614 K  |   20613 K  |
|       from large pool |     315    |     322    |    9816 K  |    9816 K  |
|       from small pool |     291    |     356    |   10797 K  |   10797 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     138    |     197    |    4557    |    4419    |
|       from large pool |     126    |     161    |    2375    |    2249    |
|       from small pool |      12    |      36    |    2182    |    2170    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     132    |     132    |   11621 K  |   11621 K  |
|       from large pool |     106    |     106    |    6417 K  |    6417 K  |
|       from small pool |      26    |      51    |    5203 K  |    5203 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:06:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:06:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:06:47]    INFO >> epoch 007:    494 / 1539 loss=3.623, wps=4285, ups=5.77, wpb=742.8, bsz=742.8, num_updates=9700, lr=0.000295, gnorm=5.774, clip=0, train_wall=8, gb_free=72.2, wall=1714 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:06:55]    INFO >> epoch 007:    544 / 1539 loss=3.798, wps=4357.8, ups=6.9, wpb=631.2, bsz=631.2, num_updates=9750, lr=0.000295, gnorm=4.31, clip=0, train_wall=7, gb_free=74.3, wall=1721 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:07:02]    INFO >> epoch 007:    594 / 1539 loss=3.782, wps=4549.2, ups=6.65, wpb=684.3, bsz=684.3, num_updates=9800, lr=0.000295, gnorm=4.912, clip=0, train_wall=7, gb_free=75.1, wall=1728 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:07:10]    INFO >> epoch 007:    644 / 1539 loss=3.458, wps=4454.7, ups=6.38, wpb=697.8, bsz=697.8, num_updates=9850, lr=0.000295, gnorm=7.247, clip=2, train_wall=7, gb_free=70.6, wall=1736 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:07:18]    INFO >> epoch 007:    694 / 1539 loss=3.473, wps=5167.8, ups=6.18, wpb=835.9, bsz=835.9, num_updates=9900, lr=0.000295, gnorm=6.491, clip=0, train_wall=8, gb_free=73.9, wall=1744 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:07:26]    INFO >> epoch 007:    744 / 1539 loss=3.714, wps=4596.9, ups=6.46, wpb=712, bsz=712, num_updates=9950, lr=0.000295, gnorm=5.479, clip=0, train_wall=7, gb_free=72.8, wall=1752 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:07:36]    INFO >> epoch 007:    794 / 1539 loss=3.666, wps=4315.2, ups=6.59, wpb=654.6, bsz=654.6, num_updates=10000, lr=0.000295, gnorm=4.94, clip=0, train_wall=7, gb_free=72.1, wall=1760 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:07:44]    INFO >> epoch 007:    844 / 1539 loss=3.667, wps=4620.4, ups=6.39, wpb=722.7, bsz=722.7, num_updates=10050, lr=0.000295, gnorm=4.986, clip=0, train_wall=7, gb_free=70.6, wall=1767 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:07:52]    INFO >> epoch 007:    894 / 1539 loss=3.661, wps=4436.2, ups=6.45, wpb=688.1, bsz=688.1, num_updates=10100, lr=0.000295, gnorm=5.349, clip=0, train_wall=7, gb_free=73.8, wall=1775 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:07:59]    INFO >> epoch 007:    944 / 1539 loss=3.676, wps=4578.9, ups=6.55, wpb=699, bsz=699, num_updates=10150, lr=0.000295, gnorm=5.245, clip=0, train_wall=7, gb_free=71.9, wall=1783 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:08:09]    INFO >> epoch 007:    994 / 1539 loss=3.668, wps=4695.5, ups=6.38, wpb=736.1, bsz=736.1, num_updates=10200, lr=0.000295, gnorm=5.531, clip=0, train_wall=7, gb_free=70.3, wall=1791 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:08:16]    INFO >> epoch 007:   1044 / 1539 loss=3.666, wps=4673.3, ups=6.62, wpb=705.7, bsz=705.7, num_updates=10250, lr=0.000295, gnorm=4.949, clip=0, train_wall=7, gb_free=73, wall=1798 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:08:24]    INFO >> epoch 007:   1094 / 1539 loss=3.626, wps=4416.3, ups=6.37, wpb=693.4, bsz=693.4, num_updates=10300, lr=0.000295, gnorm=5.006, clip=0, train_wall=7, gb_free=71.6, wall=1806 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:08:32]    INFO >> epoch 007:   1144 / 1539 loss=3.584, wps=4642.3, ups=5.91, wpb=784.9, bsz=784.9, num_updates=10350, lr=0.000295, gnorm=5.196, clip=0, train_wall=8, gb_free=74, wall=1815 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:08:42]    INFO >> epoch 007:   1194 / 1539 loss=3.541, wps=4937.5, ups=6.08, wpb=811.5, bsz=811.5, num_updates=10400, lr=0.000295, gnorm=6.255, clip=2, train_wall=8, gb_free=72.8, wall=1823 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:08:49]    INFO >> epoch 007:   1244 / 1539 loss=3.656, wps=4623.3, ups=6.74, wpb=686.2, bsz=686.2, num_updates=10450, lr=0.000295, gnorm=5.285, clip=0, train_wall=7, gb_free=68.3, wall=1830 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:08:57]    INFO >> epoch 007:   1294 / 1539 loss=3.623, wps=4258.7, ups=6.65, wpb=640.4, bsz=640.4, num_updates=10500, lr=0.000295, gnorm=5.491, clip=0, train_wall=7, gb_free=70.7, wall=1838 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:09:04]    INFO >> epoch 007:   1344 / 1539 loss=3.72, wps=4695.9, ups=6.73, wpb=697.4, bsz=697.4, num_updates=10550, lr=0.000295, gnorm=5.714, clip=0, train_wall=7, gb_free=70.4, wall=1845 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:09:13]    INFO >> epoch 007:   1394 / 1539 loss=3.529, wps=4472.7, ups=6.4, wpb=699.1, bsz=699.1, num_updates=10600, lr=0.000295, gnorm=5.621, clip=0, train_wall=7, gb_free=67.1, wall=1853 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:09:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 51.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.76 GiB is allocated by PyTorch, and 1.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:09:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:09:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:09:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 56        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78538 MiB |  78598 MiB | 341065 GiB | 340988 GiB |
|       from large pool |  78151 MiB |  78211 MiB | 339139 GiB | 339063 GiB |
|       from small pool |    386 MiB |    388 MiB |   1925 GiB |   1925 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78538 MiB |  78598 MiB | 341065 GiB | 340988 GiB |
|       from large pool |  78151 MiB |  78211 MiB | 339139 GiB | 339063 GiB |
|       from small pool |    386 MiB |    388 MiB |   1925 GiB |   1925 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78257 MiB |  78317 MiB | 340344 GiB | 340268 GiB |
|       from large pool |  77872 MiB |  77932 MiB | 338421 GiB | 338345 GiB |
|       from small pool |    384 MiB |    386 MiB |   1922 GiB |   1922 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80454 MiB |  80456 MiB | 750380 MiB | 669926 MiB |
|       from large pool |  80028 MiB |  80028 MiB | 745612 MiB | 665584 MiB |
|       from small pool |    426 MiB |    428 MiB |   4768 MiB |   4342 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1855 MiB |   6794 MiB | 329930 GiB | 329928 GiB |
|       from large pool |   1816 MiB |   6788 MiB | 327751 GiB | 327749 GiB |
|       from small pool |     39 MiB |     40 MiB |   2178 GiB |   2178 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7142    |    7145    |   22485 K  |   22478 K  |
|       from large pool |     919    |     920    |   10761 K  |   10760 K  |
|       from small pool |    6223    |    6226    |   11723 K  |   11717 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7142    |    7145    |   22485 K  |   22478 K  |
|       from large pool |     919    |     920    |   10761 K  |   10760 K  |
|       from small pool |    6223    |    6226    |   11723 K  |   11717 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     838    |     839    |    5270    |    4432    |
|       from large pool |     625    |     625    |    2886    |    2261    |
|       from small pool |     213    |     214    |    2384    |    2171    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     517    |     518    |   12658 K  |   12658 K  |
|       from large pool |     138    |     138    |    7033 K  |    7032 K  |
|       from small pool |     379    |     380    |    5625 K  |    5625 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:09:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:09:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:09:22]    INFO >> epoch 007:   1445 / 1539 loss=3.662, wps=4100.6, ups=5.73, wpb=716, bsz=716, num_updates=10650, lr=0.000295, gnorm=5.785, clip=0, train_wall=8, gb_free=68.1, wall=1862 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:09:30]    INFO >> epoch 007:   1495 / 1539 loss=3.601, wps=4837.2, ups=6.78, wpb=713.5, bsz=713.5, num_updates=10700, lr=0.000295, gnorm=5.736, clip=0, train_wall=7, gb_free=70.4, wall=1869 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:09:37]    INFO >> epoch 007 | loss 3.63 | wps 4236.9 | ups 5.95 | wpb 712.7 | bsz 712.7 | num_updates 10744 | lr 0.000295 | gnorm 5.464 | clip 0.2 | train_wall 226 | gb_free 70.2 | wall 1876 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:09:37] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:09:52]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.809 | wps 10346.7 | wpb 5412.5 | bsz 5412.5 | num_updates 10744 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:09:53]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:09:53]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 7 @ 10744 updates, score 3.809) (writing took 0.013661 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:09:53] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:09:54]    INFO >> epoch 008:      6 / 1539 loss=3.511, wps=1592.2, ups=2.18, wpb=730.3, bsz=730.3, num_updates=10750, lr=0.000262, gnorm=4.619, clip=0, train_wall=8, gb_free=68.7, wall=1892 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:10:02]    INFO >> epoch 008:     56 / 1539 loss=3.462, wps=4646.6, ups=6.32, wpb=735.8, bsz=735.8, num_updates=10800, lr=0.000262, gnorm=5.271, clip=0, train_wall=7, gb_free=73.9, wall=1900 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:10:10]    INFO >> epoch 008:    106 / 1539 loss=3.562, wps=4605.4, ups=6.01, wpb=766.8, bsz=766.8, num_updates=10850, lr=0.000262, gnorm=5.841, clip=2, train_wall=8, gb_free=73.2, wall=1908 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:10:18]    INFO >> epoch 008:    156 / 1539 loss=3.692, wps=4263.8, ups=6.54, wpb=652.3, bsz=652.3, num_updates=10900, lr=0.000262, gnorm=4.39, clip=0, train_wall=7, gb_free=75.6, wall=1916 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:10:27]    INFO >> epoch 008:    206 / 1539 loss=3.615, wps=5067.8, ups=6.07, wpb=834.7, bsz=834.7, num_updates=10950, lr=0.000262, gnorm=5.049, clip=0, train_wall=8, gb_free=73.5, wall=1924 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:10:35]    INFO >> epoch 008:    256 / 1539 loss=3.551, wps=4723.2, ups=6.61, wpb=714.1, bsz=714.1, num_updates=11000, lr=0.000262, gnorm=5.845, clip=0, train_wall=7, gb_free=71.5, wall=1932 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:10:43]    INFO >> epoch 008:    306 / 1539 loss=3.79, wps=4481.4, ups=6.22, wpb=720.8, bsz=720.8, num_updates=11050, lr=0.000262, gnorm=5.839, clip=0, train_wall=8, gb_free=71.8, wall=1940 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:10:51]    INFO >> epoch 008:    356 / 1539 loss=3.615, wps=4001.8, ups=6.01, wpb=665.6, bsz=665.6, num_updates=11100, lr=0.000262, gnorm=5.369, clip=0, train_wall=8, gb_free=74.7, wall=1948 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:11:00]    INFO >> epoch 008:    406 / 1539 loss=3.697, wps=4585.5, ups=6.49, wpb=707, bsz=707, num_updates=11150, lr=0.000262, gnorm=5.306, clip=0, train_wall=7, gb_free=72.1, wall=1956 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:11:08]    INFO >> epoch 008:    456 / 1539 loss=3.679, wps=5369.8, ups=6.13, wpb=875.6, bsz=875.6, num_updates=11200, lr=0.000262, gnorm=5.713, clip=0, train_wall=8, gb_free=72.6, wall=1964 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:11:16]    INFO >> epoch 008:    506 / 1539 loss=3.592, wps=4794.9, ups=6.4, wpb=749.6, bsz=749.6, num_updates=11250, lr=0.000262, gnorm=6.37, clip=0, train_wall=7, gb_free=74.3, wall=1972 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:11:24]    INFO >> epoch 008:    556 / 1539 loss=3.663, wps=4205.4, ups=6.46, wpb=651.2, bsz=651.2, num_updates=11300, lr=0.000262, gnorm=4.901, clip=0, train_wall=7, gb_free=71.8, wall=1979 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:11:33]    INFO >> epoch 008:    606 / 1539 loss=3.743, wps=4327.6, ups=6.34, wpb=682.8, bsz=682.8, num_updates=11350, lr=0.000262, gnorm=5.191, clip=0, train_wall=7, gb_free=75.2, wall=1987 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:11:41]    INFO >> epoch 008:    656 / 1539 loss=3.68, wps=4364, ups=6.86, wpb=636.2, bsz=636.2, num_updates=11400, lr=0.000262, gnorm=4.692, clip=0, train_wall=7, gb_free=72.5, wall=1995 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:11:48]    INFO >> epoch 008:    706 / 1539 loss=3.656, wps=4584.3, ups=6.63, wpb=691.9, bsz=691.9, num_updates=11450, lr=0.000262, gnorm=5.253, clip=0, train_wall=7, gb_free=70, wall=2002 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:11:56]    INFO >> epoch 008:    756 / 1539 loss=3.588, wps=4629, ups=6.37, wpb=727, bsz=727, num_updates=11500, lr=0.000262, gnorm=5.354, clip=0, train_wall=7, gb_free=70.9, wall=2010 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:12:05]    INFO >> epoch 008:    806 / 1539 loss=3.519, wps=4476.8, ups=6.45, wpb=694.2, bsz=694.2, num_updates=11550, lr=0.000262, gnorm=5.808, clip=0, train_wall=7, gb_free=68.9, wall=2018 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:12:12]    INFO >> epoch 008:    856 / 1539 loss=3.708, wps=4093, ups=6.87, wpb=595.7, bsz=595.7, num_updates=11600, lr=0.000262, gnorm=4.608, clip=0, train_wall=7, gb_free=71.5, wall=2025 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:12:21]    INFO >> epoch 008:    906 / 1539 loss=3.313, wps=4953.7, ups=6.09, wpb=813.3, bsz=813.3, num_updates=11650, lr=0.000262, gnorm=5.475, clip=0, train_wall=8, gb_free=74, wall=2033 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:12:28]    INFO >> epoch 008:    956 / 1539 loss=3.669, wps=4551.5, ups=6.75, wpb=674.1, bsz=674.1, num_updates=11700, lr=0.000262, gnorm=5.18, clip=0, train_wall=7, gb_free=72.9, wall=2041 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:12:35] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 2.12 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:12:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:12:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:12:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 59        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78273 MiB |  78333 MiB | 377371 GiB | 377294 GiB |
|       from large pool |  78192 MiB |  78252 MiB | 375231 GiB | 375154 GiB |
|       from small pool |     80 MiB |     82 MiB |   2139 GiB |   2139 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78273 MiB |  78333 MiB | 377371 GiB | 377294 GiB |
|       from large pool |  78192 MiB |  78252 MiB | 375231 GiB | 375154 GiB |
|       from small pool |     80 MiB |     82 MiB |   2139 GiB |   2139 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78222 MiB |  78281 MiB | 376569 GiB | 376492 GiB |
|       from large pool |  78141 MiB |  78201 MiB | 374432 GiB | 374356 GiB |
|       from small pool |     80 MiB |     81 MiB |   2136 GiB |   2136 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB |    830 GiB | 770022 MiB |
|       from large pool |  80418 MiB |  80418 MiB |    825 GiB | 765058 MiB |
|       from small pool |     84 MiB |    246 MiB |      4 GiB |   4964 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2168 MiB |   7147 MiB | 358201 GiB | 358199 GiB |
|       from large pool |   2165 MiB |   7138 MiB | 355780 GiB | 355778 GiB |
|       from small pool |      3 MiB |     29 MiB |   2420 GiB |   2420 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1796    |    1799    |   24927 K  |   24926 K  |
|       from large pool |     455    |     456    |   11890 K  |   11890 K  |
|       from small pool |    1341    |    1344    |   13037 K  |   13035 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1796    |    1799    |   24927 K  |   24926 K  |
|       from large pool |     455    |     456    |   11890 K  |   11890 K  |
|       from small pool |    1341    |    1344    |   13037 K  |   13035 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     242    |     559    |    5862    |    5620    |
|       from large pool |     200    |     436    |    3338    |    3138    |
|       from small pool |      42    |     123    |    2524    |    2482    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     161    |     163    |   14093 K  |   14093 K  |
|       from large pool |     116    |     116    |    7817 K  |    7816 K  |
|       from small pool |      45    |      54    |    6276 K  |    6276 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:12:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:12:35] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:12:38]    INFO >> epoch 008:   1007 / 1539 loss=3.645, wps=3558.5, ups=5.9, wpb=602.8, bsz=602.8, num_updates=11750, lr=0.000262, gnorm=4.354, clip=0, train_wall=7, gb_free=70.6, wall=2049 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:12:49]    INFO >> epoch 008:   1057 / 1539 loss=3.511, wps=3838.3, ups=4.63, wpb=828.4, bsz=828.4, num_updates=11800, lr=0.000262, gnorm=5.529, clip=2, train_wall=10, gb_free=71.7, wall=2060 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:12:56]    INFO >> epoch 008:   1107 / 1539 loss=3.61, wps=4466.5, ups=6.76, wpb=661.2, bsz=661.2, num_updates=11850, lr=0.000262, gnorm=5.045, clip=0, train_wall=7, gb_free=72.8, wall=2067 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:13:03]    INFO >> epoch 008:   1157 / 1539 loss=3.814, wps=3810.6, ups=6.84, wpb=557.5, bsz=557.5, num_updates=11900, lr=0.000262, gnorm=4.489, clip=0, train_wall=7, gb_free=73.2, wall=2075 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:13:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 493.25 MiB is free. Including non-PyTorch memory, this process has 78.63 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:13:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 61        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76956 MiB |  77478 MiB | 382821 GiB | 382746 GiB |
|       from large pool |  76943 MiB |  77465 MiB | 380654 GiB | 380579 GiB |
|       from small pool |     12 MiB |     19 MiB |   2167 GiB |   2167 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76956 MiB |  77478 MiB | 382821 GiB | 382746 GiB |
|       from large pool |  76943 MiB |  77465 MiB | 380654 GiB | 380579 GiB |
|       from small pool |     12 MiB |     19 MiB |   2167 GiB |   2167 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 382008 GiB | 381933 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 379844 GiB | 379769 GiB |
|       from small pool |     12 MiB |     19 MiB |   2164 GiB |   2164 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80012 MiB |  80054 MiB |    903 GiB |    825 GiB |
|       from large pool |  79988 MiB |  79988 MiB |    898 GiB |    820 GiB |
|       from small pool |     24 MiB |     66 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3055 MiB |   8643 MiB | 364118 GiB | 364115 GiB |
|       from large pool |   3044 MiB |   8631 MiB | 361666 GiB | 361663 GiB |
|       from small pool |     11 MiB |     19 MiB |   2452 GiB |   2452 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   25272 K  |   25272 K  |
|       from large pool |     315    |     322    |   12070 K  |   12070 K  |
|       from small pool |     291    |     348    |   13202 K  |   13201 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   25272 K  |   25272 K  |
|       from large pool |     315    |     322    |   12070 K  |   12070 K  |
|       from small pool |     291    |     348    |   13202 K  |   13201 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     112    |    5924    |    5833    |
|       from large pool |      79    |      79    |    3381    |    3302    |
|       from small pool |      12    |      33    |    2543    |    2531    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      98    |      98    |   14279 K  |   14279 K  |
|       from large pool |      69    |      69    |    7931 K  |    7930 K  |
|       from small pool |      29    |      50    |    6348 K  |    6348 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:13:13]    INFO >> epoch 008:   1208 / 1539 loss=3.631, wps=4079, ups=5.71, wpb=715, bsz=715, num_updates=11950, lr=0.000262, gnorm=4.814, clip=0, train_wall=8, gb_free=76, wall=2083 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:13:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.61 GiB is allocated by PyTorch, and 1012.79 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:13:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 62        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79415 MiB |  79475 MiB | 385104 GiB | 385026 GiB |
|       from large pool |  79016 MiB |  79076 MiB | 382922 GiB | 382845 GiB |
|       from small pool |    398 MiB |    399 MiB |   2181 GiB |   2181 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79415 MiB |  79475 MiB | 385104 GiB | 385026 GiB |
|       from large pool |  79016 MiB |  79076 MiB | 382922 GiB | 382845 GiB |
|       from small pool |    398 MiB |    399 MiB |   2181 GiB |   2181 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79398 MiB |  79458 MiB | 384287 GiB | 384209 GiB |
|       from large pool |  79002 MiB |  79061 MiB | 382108 GiB | 382031 GiB |
|       from small pool |    396 MiB |    397 MiB |   2178 GiB |   2178 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80488 MiB |    903 GiB |    825 GiB |
|       from large pool |  80048 MiB |  80048 MiB |    898 GiB |    820 GiB |
|       from small pool |    440 MiB |    440 MiB |      5 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1012 MiB |   5703 MiB | 366750 GiB | 366749 GiB |
|       from large pool |    971 MiB |   5696 MiB | 364281 GiB | 364280 GiB |
|       from small pool |     41 MiB |     42 MiB |   2468 GiB |   2468 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7351    |    7354    |   25429 K  |   25421 K  |
|       from large pool |     938    |     939    |   12139 K  |   12138 K  |
|       from small pool |    6413    |    6416    |   13290 K  |   13283 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7351    |    7354    |   25429 K  |   25421 K  |
|       from large pool |     938    |     939    |   12139 K  |   12138 K  |
|       from small pool |    6413    |    6416    |   13290 K  |   13283 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     300    |     300    |    6133    |    5833    |
|       from large pool |      80    |      80    |    3382    |    3302    |
|       from small pool |     220    |     220    |    2751    |    2531    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     468    |     468    |   14366 K  |   14366 K  |
|       from large pool |      76    |      76    |    7973 K  |    7973 K  |
|       from small pool |     392    |     392    |    6393 K  |    6392 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:13:22]    INFO >> epoch 008:   1259 / 1539 loss=3.572, wps=4780.5, ups=5.62, wpb=850.7, bsz=850.7, num_updates=12000, lr=0.000262, gnorm=6.316, clip=2, train_wall=8, gb_free=73.2, wall=2092 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:13:30]    INFO >> epoch 008:   1309 / 1539 loss=3.655, wps=4415.1, ups=6.27, wpb=704.3, bsz=704.3, num_updates=12050, lr=0.000262, gnorm=5.446, clip=0, train_wall=7, gb_free=72.7, wall=2100 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:13:38]    INFO >> epoch 008:   1359 / 1539 loss=3.657, wps=4327.8, ups=6.51, wpb=664.7, bsz=664.7, num_updates=12100, lr=0.000262, gnorm=4.526, clip=0, train_wall=7, gb_free=69.7, wall=2108 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:13:40] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 803.25 MiB is free. Including non-PyTorch memory, this process has 78.33 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 2.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:13:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 33           |        cudaMalloc retries: 64        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76325 MiB |  78600 MiB | 388762 GiB | 388687 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 386562 GiB | 386487 GiB |
|       from small pool |     18 MiB |     19 MiB |   2200 GiB |   2200 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76325 MiB |  78600 MiB | 388762 GiB | 388687 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 386562 GiB | 386487 GiB |
|       from small pool |     18 MiB |     19 MiB |   2200 GiB |   2200 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 387938 GiB | 387863 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 385741 GiB | 385666 GiB |
|       from small pool |     18 MiB |     19 MiB |   2197 GiB |   2197 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79702 MiB |  80428 MiB |    974 GiB |    897 GiB |
|       from large pool |  79678 MiB |  79988 MiB |    969 GiB |    891 GiB |
|       from small pool |     24 MiB |    440 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3376 MiB |   4508 MiB | 370853 GiB | 370849 GiB |
|       from large pool |   3371 MiB |   4502 MiB | 368363 GiB | 368359 GiB |
|       from small pool |      5 MiB |     27 MiB |   2490 GiB |   2490 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     637    |     643    |   25655 K  |   25655 K  |
|       from large pool |     340    |     346    |   12256 K  |   12256 K  |
|       from small pool |     297    |     348    |   13399 K  |   13398 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     637    |     643    |   25655 K  |   25655 K  |
|       from large pool |     340    |     346    |   12256 K  |   12256 K  |
|       from small pool |     297    |     348    |   13399 K  |   13398 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     108    |     299    |    6192    |    6084    |
|       from large pool |      96    |      96    |    3441    |    3345    |
|       from small pool |      12    |     220    |    2751    |    2739    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     116    |     116    |   14490 K  |   14489 K  |
|       from large pool |      91    |      91    |    8047 K  |    8047 K  |
|       from small pool |      25    |      54    |    6442 K  |    6442 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:13:40] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:13:49]    INFO >> epoch 008:   1410 / 1539 loss=3.643, wps=4028.7, ups=4.89, wpb=824.4, bsz=824.4, num_updates=12150, lr=0.000262, gnorm=5.385, clip=0, train_wall=8, gb_free=73, wall=2118 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:13:58]    INFO >> epoch 008:   1460 / 1539 loss=3.668, wps=4139, ups=5.79, wpb=715.2, bsz=715.2, num_updates=12200, lr=0.000262, gnorm=5.135, clip=0, train_wall=8, gb_free=75, wall=2127 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:14:06]    INFO >> epoch 008:   1510 / 1539 loss=3.586, wps=4349, ups=6.53, wpb=666.4, bsz=666.4, num_updates=12250, lr=0.000262, gnorm=5.497, clip=0, train_wall=7, gb_free=67.3, wall=2134 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:14:10]    INFO >> epoch 008 | loss 3.62 | wps 4160.6 | ups 5.84 | wpb 712.7 | bsz 712.7 | num_updates 12279 | lr 0.000262 | gnorm 5.261 | clip 0.2 | train_wall 229 | gb_free 74.8 | wall 2139 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:14:10] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:14:26]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.819 | wps 10712.4 | wpb 5412.5 | bsz 5412.5 | num_updates 12279 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:14:26]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:14:26]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 8 @ 12279 updates, score 3.819) (writing took 0.013759 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:14:26] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:14:29]    INFO >> epoch 009:     21 / 1539 loss=3.624, wps=1514.3, ups=2.26, wpb=668.9, bsz=668.9, num_updates=12300, lr=0.000227, gnorm=5.159, clip=0, train_wall=7, gb_free=73.1, wall=2157 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:14:37]    INFO >> epoch 009:     71 / 1539 loss=3.671, wps=4209.4, ups=6.34, wpb=663.6, bsz=663.6, num_updates=12350, lr=0.000227, gnorm=4.562, clip=0, train_wall=7, gb_free=73.5, wall=2164 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:14:45]    INFO >> epoch 009:    121 / 1539 loss=3.533, wps=4405.1, ups=6.4, wpb=688.4, bsz=688.4, num_updates=12400, lr=0.000227, gnorm=5.582, clip=0, train_wall=7, gb_free=73.7, wall=2172 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:14:55]    INFO >> epoch 009:    171 / 1539 loss=3.44, wps=5054.2, ups=6, wpb=842.4, bsz=842.4, num_updates=12450, lr=0.000227, gnorm=5.481, clip=0, train_wall=8, gb_free=71.8, wall=2181 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:15:02]    INFO >> epoch 009:    221 / 1539 loss=3.64, wps=4604.1, ups=6.66, wpb=691.5, bsz=691.5, num_updates=12500, lr=0.000227, gnorm=5.024, clip=0, train_wall=7, gb_free=71.5, wall=2188 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:15:09]    INFO >> epoch 009:    271 / 1539 loss=3.566, wps=4828.2, ups=6.86, wpb=703.6, bsz=703.6, num_updates=12550, lr=0.000227, gnorm=5.857, clip=0, train_wall=7, gb_free=70, wall=2195 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:15:17]    INFO >> epoch 009:    321 / 1539 loss=3.65, wps=4847.2, ups=6.18, wpb=784.5, bsz=784.5, num_updates=12600, lr=0.000227, gnorm=4.829, clip=0, train_wall=8, gb_free=66.3, wall=2203 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:15:28]    INFO >> epoch 009:    371 / 1539 loss=3.538, wps=3996.3, ups=5.7, wpb=701, bsz=701, num_updates=12650, lr=0.000227, gnorm=5.128, clip=0, train_wall=8, gb_free=65.8, wall=2212 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:15:28] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 165.25 MiB is free. Including non-PyTorch memory, this process has 78.96 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:15:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:15:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:15:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 34           |        cudaMalloc retries: 66        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75055 MiB |  75998 MiB | 409586 GiB | 409512 GiB |
|       from large pool |  75042 MiB |  75985 MiB | 407263 GiB | 407189 GiB |
|       from small pool |     12 MiB |     16 MiB |   2323 GiB |   2323 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75055 MiB |  75998 MiB | 409586 GiB | 409512 GiB |
|       from large pool |  75042 MiB |  75985 MiB | 407263 GiB | 407189 GiB |
|       from small pool |     12 MiB |     16 MiB |   2323 GiB |   2323 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 408722 GiB | 408649 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 406402 GiB | 406329 GiB |
|       from small pool |     12 MiB |     16 MiB |   2319 GiB |   2319 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80340 MiB |  80340 MiB |    979 GiB |    901 GiB |
|       from large pool |  80316 MiB |  80316 MiB |    974 GiB |    895 GiB |
|       from small pool |     24 MiB |    124 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5284 MiB |   9096 MiB | 391527 GiB | 391522 GiB |
|       from large pool |   5273 MiB |   9085 MiB | 388901 GiB | 388896 GiB |
|       from small pool |     11 MiB |     21 MiB |   2625 GiB |   2625 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   26995 K  |   26995 K  |
|       from large pool |     314    |     322    |   12839 K  |   12839 K  |
|       from small pool |     291    |     341    |   14155 K  |   14155 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   26995 K  |   26995 K  |
|       from large pool |     314    |     322    |   12839 K  |   12839 K  |
|       from small pool |     291    |     341    |   14155 K  |   14155 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     103    |     158    |    6249    |    6146    |
|       from large pool |      91    |      96    |    3448    |    3357    |
|       from small pool |      12    |      62    |    2801    |    2789    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     100    |     100    |   15254 K  |   15254 K  |
|       from large pool |      73    |      73    |    8420 K  |    8420 K  |
|       from small pool |      27    |      48    |    6833 K  |    6833 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:15:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:15:28] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:15:36]    INFO >> epoch 009:    422 / 1539 loss=3.638, wps=4214.5, ups=5.88, wpb=716.4, bsz=716.4, num_updates=12700, lr=0.000227, gnorm=5.003, clip=0, train_wall=7, gb_free=73.1, wall=2221 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:15:44]    INFO >> epoch 009:    472 / 1539 loss=3.518, wps=4632.2, ups=6.7, wpb=691.3, bsz=691.3, num_updates=12750, lr=0.000227, gnorm=5.038, clip=2, train_wall=7, gb_free=74, wall=2228 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:15:51]    INFO >> epoch 009:    522 / 1539 loss=3.602, wps=4948.4, ups=6.5, wpb=760.8, bsz=760.8, num_updates=12800, lr=0.000227, gnorm=4.56, clip=0, train_wall=7, gb_free=72.7, wall=2236 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:16:01]    INFO >> epoch 009:    572 / 1539 loss=3.617, wps=4582.2, ups=6.1, wpb=750.9, bsz=750.9, num_updates=12850, lr=0.000227, gnorm=4.966, clip=0, train_wall=8, gb_free=74.2, wall=2244 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:16:02] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 35.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.30 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:16:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:16:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:16:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 35           |        cudaMalloc retries: 67        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79090 MiB |  79150 MiB | 415679 GiB | 415602 GiB |
|       from large pool |  79001 MiB |  79061 MiB | 413324 GiB | 413247 GiB |
|       from small pool |     89 MiB |     90 MiB |   2355 GiB |   2354 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79090 MiB |  79150 MiB | 415679 GiB | 415602 GiB |
|       from large pool |  79001 MiB |  79061 MiB | 413324 GiB | 413247 GiB |
|       from small pool |     89 MiB |     90 MiB |   2355 GiB |   2354 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79063 MiB |  79123 MiB | 414803 GiB | 414726 GiB |
|       from large pool |  78974 MiB |  79034 MiB | 412452 GiB | 412374 GiB |
|       from small pool |     89 MiB |     90 MiB |   2351 GiB |   2351 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80470 MiB |  80470 MiB |    979 GiB |    901 GiB |
|       from large pool |  80376 MiB |  80376 MiB |    974 GiB |    895 GiB |
|       from small pool |     94 MiB |     94 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1319 MiB |   7421 MiB | 398239 GiB | 398238 GiB |
|       from large pool |   1314 MiB |   7410 MiB | 395577 GiB | 395576 GiB |
|       from small pool |      4 MiB |     25 MiB |   2662 GiB |   2662 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1950    |    1953    |   27385 K  |   27383 K  |
|       from large pool |     469    |     470    |   13039 K  |   13038 K  |
|       from small pool |    1481    |    1484    |   14346 K  |   14345 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1950    |    1953    |   27385 K  |   27383 K  |
|       from large pool |     469    |     470    |   13039 K  |   13038 K  |
|       from small pool |    1481    |    1484    |   14346 K  |   14345 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     139    |     139    |    6285    |    6146    |
|       from large pool |      92    |      92    |    3449    |    3357    |
|       from small pool |      47    |      47    |    2836    |    2789    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     128    |     128    |   15464 K  |   15464 K  |
|       from large pool |      76    |      78    |    8546 K  |    8546 K  |
|       from small pool |      52    |      61    |    6918 K  |    6918 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:16:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:16:02] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:16:09]    INFO >> epoch 009:    623 / 1539 loss=3.571, wps=4148.7, ups=5.85, wpb=709.4, bsz=709.4, num_updates=12900, lr=0.000227, gnorm=5.266, clip=0, train_wall=7, gb_free=65.9, wall=2253 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:16:17]    INFO >> epoch 009:    673 / 1539 loss=3.621, wps=4335.9, ups=6.58, wpb=658.7, bsz=658.7, num_updates=12950, lr=0.000227, gnorm=4.747, clip=0, train_wall=7, gb_free=71.5, wall=2260 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:16:24]    INFO >> epoch 009:    723 / 1539 loss=3.596, wps=4533.1, ups=6.75, wpb=671.1, bsz=671.1, num_updates=13000, lr=0.000227, gnorm=5.315, clip=0, train_wall=7, gb_free=71.2, wall=2268 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:16:34]    INFO >> epoch 009:    773 / 1539 loss=3.525, wps=4325.6, ups=6.23, wpb=693.9, bsz=693.9, num_updates=13050, lr=0.000227, gnorm=5.69, clip=0, train_wall=8, gb_free=57.9, wall=2276 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:16:42] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 165.25 MiB is free. Including non-PyTorch memory, this process has 78.96 GiB memory in use. Of the allocated memory 77.59 GiB is allocated by PyTorch, and 888.96 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:16:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:16:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:16:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 36           |        cudaMalloc retries: 68        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79053 MiB |  79451 MiB | 422841 GiB | 422764 GiB |
|       from large pool |  79034 MiB |  79432 MiB | 420450 GiB | 420372 GiB |
|       from small pool |     18 MiB |     19 MiB |   2391 GiB |   2391 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79053 MiB |  79451 MiB | 422841 GiB | 422764 GiB |
|       from large pool |  79034 MiB |  79432 MiB | 420450 GiB | 420372 GiB |
|       from small pool |     18 MiB |     19 MiB |   2391 GiB |   2391 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79031 MiB |  79429 MiB | 421950 GiB | 421873 GiB |
|       from large pool |  79012 MiB |  79410 MiB | 419562 GiB | 419485 GiB |
|       from small pool |     18 MiB |     19 MiB |   2388 GiB |   2388 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80340 MiB |  80410 MiB |    979 GiB |    901 GiB |
|       from large pool |  80316 MiB |  80316 MiB |    974 GiB |    895 GiB |
|       from small pool |     24 MiB |     94 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1286 MiB |   5521 MiB | 406124 GiB | 406123 GiB |
|       from large pool |   1281 MiB |   5515 MiB | 403419 GiB | 403418 GiB |
|       from small pool |      5 MiB |     21 MiB |   2704 GiB |   2704 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     647    |   27845 K  |   27844 K  |
|       from large pool |     341    |     347    |   13278 K  |   13278 K  |
|       from small pool |     300    |     348    |   14567 K  |   14566 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     647    |   27845 K  |   27844 K  |
|       from large pool |     341    |     347    |   13278 K  |   13278 K  |
|       from small pool |     300    |     348    |   14567 K  |   14566 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     103    |     138    |    6285    |    6182    |
|       from large pool |      91    |      91    |    3449    |    3358    |
|       from small pool |      12    |      47    |    2836    |    2824    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      99    |     102    |   15713 K  |   15713 K  |
|       from large pool |      71    |      74    |    8697 K  |    8697 K  |
|       from small pool |      28    |      51    |    7015 K  |    7015 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:16:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:16:42] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:16:42]    INFO >> epoch 009:    824 / 1539 loss=3.645, wps=4266.4, ups=5.93, wpb=719.6, bsz=719.6, num_updates=13100, lr=0.000227, gnorm=4.828, clip=0, train_wall=7, gb_free=70.5, wall=2284 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:16:49]    INFO >> epoch 009:    874 / 1539 loss=3.617, wps=4492.7, ups=6.97, wpb=644.4, bsz=644.4, num_updates=13150, lr=0.000227, gnorm=4.344, clip=0, train_wall=7, gb_free=74.8, wall=2291 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:16:57]    INFO >> epoch 009:    924 / 1539 loss=3.655, wps=4206.6, ups=6.8, wpb=618.7, bsz=618.7, num_updates=13200, lr=0.000227, gnorm=5.098, clip=0, train_wall=7, gb_free=73.5, wall=2299 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:17:08]    INFO >> epoch 009:    974 / 1539 loss=3.531, wps=4465.4, ups=5.19, wpb=859.7, bsz=859.7, num_updates=13250, lr=0.000227, gnorm=5.618, clip=0, train_wall=9, gb_free=70.8, wall=2308 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:17:16]    INFO >> epoch 009:   1024 / 1539 loss=3.639, wps=4500, ups=6.26, wpb=718.9, bsz=718.9, num_updates=13300, lr=0.000227, gnorm=4.574, clip=0, train_wall=7, gb_free=69.9, wall=2316 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:17:24]    INFO >> epoch 009:   1074 / 1539 loss=3.461, wps=4580.2, ups=6.31, wpb=725.8, bsz=725.8, num_updates=13350, lr=0.000227, gnorm=5.539, clip=0, train_wall=7, gb_free=65.3, wall=2324 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:17:32]    INFO >> epoch 009:   1124 / 1539 loss=3.586, wps=4686.6, ups=6.16, wpb=760.3, bsz=760.3, num_updates=13400, lr=0.000227, gnorm=5.214, clip=0, train_wall=8, gb_free=73.5, wall=2332 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:17:41]    INFO >> epoch 009:   1174 / 1539 loss=3.637, wps=4453.7, ups=6.65, wpb=669.6, bsz=669.6, num_updates=13450, lr=0.000227, gnorm=4.988, clip=0, train_wall=7, gb_free=72.7, wall=2340 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:17:49]    INFO >> epoch 009:   1224 / 1539 loss=3.326, wps=4999.6, ups=5.8, wpb=862, bsz=862, num_updates=13500, lr=0.000227, gnorm=5.557, clip=0, train_wall=8, gb_free=69, wall=2348 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:17:57]    INFO >> epoch 009:   1274 / 1539 loss=3.605, wps=4903.9, ups=6.32, wpb=776.3, bsz=776.3, num_updates=13550, lr=0.000227, gnorm=4.848, clip=0, train_wall=7, gb_free=72.7, wall=2356 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:18:05]    INFO >> epoch 009:   1324 / 1539 loss=3.609, wps=4291.2, ups=6.02, wpb=712.6, bsz=712.6, num_updates=13600, lr=0.000227, gnorm=5.717, clip=0, train_wall=8, gb_free=75.1, wall=2365 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:18:15]    INFO >> epoch 009:   1374 / 1539 loss=3.289, wps=4598.6, ups=6.13, wpb=749.6, bsz=749.6, num_updates=13650, lr=0.000227, gnorm=4.498, clip=0, train_wall=8, gb_free=69.9, wall=2373 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:18:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 47.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.36 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:18:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:18:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:18:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 37           |        cudaMalloc retries: 70        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78127 MiB |  78187 MiB | 440587 GiB | 440511 GiB |
|       from large pool |  77744 MiB |  77804 MiB | 438093 GiB | 438017 GiB |
|       from small pool |    383 MiB |    384 MiB |   2494 GiB |   2493 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78127 MiB |  78187 MiB | 440587 GiB | 440511 GiB |
|       from large pool |  77744 MiB |  77804 MiB | 438093 GiB | 438017 GiB |
|       from small pool |    383 MiB |    384 MiB |   2494 GiB |   2493 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77897 MiB |  77956 MiB | 439658 GiB | 439582 GiB |
|       from large pool |  77516 MiB |  77575 MiB | 437167 GiB | 437092 GiB |
|       from small pool |    381 MiB |    382 MiB |   2490 GiB |   2490 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80458 MiB |  80460 MiB |   1006 GiB |    928 GiB |
|       from large pool |  80036 MiB |  80036 MiB |   1000 GiB |    922 GiB |
|       from small pool |    422 MiB |    424 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2270 MiB |   7290 MiB | 423445 GiB | 423443 GiB |
|       from large pool |   2231 MiB |   7283 MiB | 420622 GiB | 420620 GiB |
|       from small pool |     38 MiB |     40 MiB |   2822 GiB |   2822 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7076    |    7079    |   29063 K  |   29056 K  |
|       from large pool |     913    |     914    |   13873 K  |   13872 K  |
|       from small pool |    6163    |    6166    |   15190 K  |   15184 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7076    |    7079    |   29063 K  |   29056 K  |
|       from large pool |     913    |     914    |   13873 K  |   13872 K  |
|       from small pool |    6163    |    6166    |   15190 K  |   15184 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     742    |     743    |    6935    |    6193    |
|       from large pool |     531    |     531    |    3899    |    3368    |
|       from small pool |     211    |     212    |    3036    |    2825    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     505    |     505    |   16399 K  |   16398 K  |
|       from large pool |     129    |     129    |    9086 K  |    9086 K  |
|       from small pool |     376    |     376    |    7312 K  |    7312 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:18:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:18:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:18:23]    INFO >> epoch 009:   1425 / 1539 loss=3.483, wps=4151.2, ups=6.13, wpb=676.7, bsz=676.7, num_updates=13700, lr=0.000227, gnorm=5.631, clip=0, train_wall=7, gb_free=72, wall=2381 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:18:31]    INFO >> epoch 009:   1475 / 1539 loss=3.691, wps=4126.8, ups=6.51, wpb=634.1, bsz=634.1, num_updates=13750, lr=0.000227, gnorm=4.489, clip=0, train_wall=7, gb_free=72.9, wall=2389 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:18:39]    INFO >> epoch 009:   1525 / 1539 loss=3.612, wps=3984.8, ups=6.39, wpb=623.3, bsz=623.3, num_updates=13800, lr=0.000227, gnorm=5.114, clip=0, train_wall=7, gb_free=67.9, wall=2396 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:18:41]    INFO >> epoch 009 | loss 3.57 | wps 4211.9 | ups 5.91 | wpb 712.7 | bsz 712.7 | num_updates 13814 | lr 0.000227 | gnorm 5.103 | clip 0.1 | train_wall 228 | gb_free 74.2 | wall 2399 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:18:41] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:18:56]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.776 | wps 10815.4 | wpb 5412.5 | bsz 5412.5 | num_updates 13814 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:18:57]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:18:57]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 9 @ 13814 updates, score 3.776) (writing took 0.015754 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:18:57] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:19:04]    INFO >> epoch 010:     36 / 1539 loss=3.637, wps=1472.9, ups=2.1, wpb=701.4, bsz=701.4, num_updates=13850, lr=0.000193, gnorm=4.727, clip=0, train_wall=9, gb_free=72.8, wall=2420 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:19:11]    INFO >> epoch 010:     86 / 1539 loss=3.512, wps=4578.6, ups=6.39, wpb=716.9, bsz=716.9, num_updates=13900, lr=0.000193, gnorm=6.195, clip=0, train_wall=7, gb_free=69.6, wall=2428 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:19:20]    INFO >> epoch 010:    136 / 1539 loss=3.547, wps=4116.1, ups=6.57, wpb=626.5, bsz=626.5, num_updates=13950, lr=0.000193, gnorm=4.642, clip=0, train_wall=7, gb_free=74.1, wall=2436 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:19:29]    INFO >> epoch 010:    186 / 1539 loss=3.516, wps=4140.3, ups=5.56, wpb=744.2, bsz=744.2, num_updates=14000, lr=0.000193, gnorm=5.624, clip=2, train_wall=8, gb_free=74.2, wall=2445 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:19:38]    INFO >> epoch 010:    236 / 1539 loss=3.324, wps=4954.5, ups=5.74, wpb=863.9, bsz=863.9, num_updates=14050, lr=0.000193, gnorm=5.489, clip=0, train_wall=8, gb_free=74.4, wall=2453 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:19:46]    INFO >> epoch 010:    286 / 1539 loss=3.578, wps=4256.9, ups=6.3, wpb=676, bsz=676, num_updates=14100, lr=0.000193, gnorm=5.373, clip=0, train_wall=7, gb_free=73.9, wall=2461 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:19:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 901.25 MiB is free. Including non-PyTorch memory, this process has 78.24 GiB memory in use. Of the allocated memory 74.10 GiB is allocated by PyTorch, and 3.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:19:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:19:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:19:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 38           |        cudaMalloc retries: 72        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72064 MiB |  75874 MiB | 458286 GiB | 458216 GiB |
|       from large pool |  72046 MiB |  75856 MiB | 455686 GiB | 455616 GiB |
|       from small pool |     17 MiB |     18 MiB |   2600 GiB |   2600 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72064 MiB |  75874 MiB | 458286 GiB | 458216 GiB |
|       from large pool |  72046 MiB |  75856 MiB | 455686 GiB | 455616 GiB |
|       from small pool |     17 MiB |     18 MiB |   2600 GiB |   2600 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  75852 MiB | 457323 GiB | 457252 GiB |
|       from large pool |  72027 MiB |  75835 MiB | 454726 GiB | 454656 GiB |
|       from small pool |     17 MiB |     18 MiB |   2596 GiB |   2596 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79604 MiB |  79804 MiB |   1030 GiB |    952 GiB |
|       from large pool |  79578 MiB |  79578 MiB |   1023 GiB |    946 GiB |
|       from small pool |     26 MiB |    226 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4811 MiB |   6891 MiB | 438610 GiB | 438606 GiB |
|       from large pool |   4803 MiB |   6882 MiB | 435671 GiB | 435666 GiB |
|       from small pool |      8 MiB |     23 MiB |   2939 GiB |   2939 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     615    |   30202 K  |   30201 K  |
|       from large pool |     308    |     317    |   14352 K  |   14352 K  |
|       from small pool |     298    |     342    |   15849 K  |   15849 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     615    |   30202 K  |   30201 K  |
|       from large pool |     308    |     317    |   14352 K  |   14352 K  |
|       from small pool |     298    |     342    |   15849 K  |   15849 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     151    |     251    |    7044    |    6893    |
|       from large pool |     138    |     138    |    3908    |    3770    |
|       from small pool |      13    |     113    |    3136    |    3123    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     141    |     143    |   17068 K  |   17068 K  |
|       from large pool |     114    |     116    |    9404 K  |    9404 K  |
|       from small pool |      27    |      47    |    7664 K  |    7664 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:19:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:19:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:19:56]    INFO >> epoch 010:    337 / 1539 loss=3.543, wps=4270.5, ups=6.07, wpb=703.9, bsz=703.9, num_updates=14150, lr=0.000193, gnorm=5.638, clip=0, train_wall=7, gb_free=67.2, wall=2470 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:20:04]    INFO >> epoch 010:    387 / 1539 loss=3.685, wps=4435.6, ups=6.13, wpb=723.5, bsz=723.5, num_updates=14200, lr=0.000193, gnorm=4.676, clip=0, train_wall=8, gb_free=73.4, wall=2478 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:20:12]    INFO >> epoch 010:    437 / 1539 loss=3.598, wps=4024.2, ups=6.37, wpb=631.4, bsz=631.4, num_updates=14250, lr=0.000193, gnorm=4.072, clip=0, train_wall=7, gb_free=72.6, wall=2486 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:20:20]    INFO >> epoch 010:    487 / 1539 loss=3.601, wps=4036.3, ups=6.04, wpb=668.5, bsz=668.5, num_updates=14300, lr=0.000193, gnorm=4.362, clip=0, train_wall=8, gb_free=72.5, wall=2494 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:20:29]    INFO >> epoch 010:    537 / 1539 loss=3.386, wps=4412.1, ups=6.83, wpb=645.7, bsz=645.7, num_updates=14350, lr=0.000193, gnorm=5.241, clip=2, train_wall=7, gb_free=75, wall=2501 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:20:37]    INFO >> epoch 010:    587 / 1539 loss=3.625, wps=4077.9, ups=6.13, wpb=665.2, bsz=665.2, num_updates=14400, lr=0.000193, gnorm=4.775, clip=0, train_wall=8, gb_free=71.4, wall=2509 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:20:45]    INFO >> epoch 010:    637 / 1539 loss=3.541, wps=4648.2, ups=6.46, wpb=719.7, bsz=719.7, num_updates=14450, lr=0.000193, gnorm=5.632, clip=0, train_wall=7, gb_free=69.1, wall=2517 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:20:52]    INFO >> epoch 010:    687 / 1539 loss=3.555, wps=4741, ups=6.53, wpb=726.6, bsz=726.6, num_updates=14500, lr=0.000193, gnorm=5.206, clip=0, train_wall=7, gb_free=73.3, wall=2525 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:20:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 53.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 76.23 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:20:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:20:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:20:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 39           |        cudaMalloc retries: 73        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78000 MiB |  78060 MiB | 469354 GiB | 469278 GiB |
|       from large pool |  77616 MiB |  77676 MiB | 466693 GiB | 466617 GiB |
|       from small pool |    383 MiB |    385 MiB |   2661 GiB |   2660 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78000 MiB |  78060 MiB | 469354 GiB | 469278 GiB |
|       from large pool |  77616 MiB |  77676 MiB | 466693 GiB | 466617 GiB |
|       from small pool |    383 MiB |    385 MiB |   2661 GiB |   2660 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77957 MiB |  78016 MiB | 468366 GiB | 468290 GiB |
|       from large pool |  77575 MiB |  77635 MiB | 465709 GiB | 465633 GiB |
|       from small pool |    381 MiB |    383 MiB |   2657 GiB |   2656 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80452 MiB |  80454 MiB |   1033 GiB |    955 GiB |
|       from large pool |  80030 MiB |  80030 MiB |   1027 GiB |    948 GiB |
|       from small pool |    422 MiB |    424 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2391 MiB |   6658 MiB | 449489 GiB | 449487 GiB |
|       from large pool |   2353 MiB |   6651 MiB | 446479 GiB | 446477 GiB |
|       from small pool |     38 MiB |     40 MiB |   3009 GiB |   3009 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7087    |    7090    |   30947 K  |   30940 K  |
|       from large pool |     914    |     915    |   14731 K  |   14730 K  |
|       from small pool |    6173    |    6176    |   16216 K  |   16209 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7087    |    7090    |   30947 K  |   30940 K  |
|       from large pool |     914    |     915    |   14731 K  |   14730 K  |
|       from small pool |    6173    |    6176    |   16216 K  |   16209 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     401    |     402    |    7296    |    6895    |
|       from large pool |     190    |     190    |    3961    |    3771    |
|       from small pool |     211    |     212    |    3335    |    3124    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     503    |     506    |   17482 K  |   17482 K  |
|       from large pool |     127    |     127    |    9653 K  |    9653 K  |
|       from small pool |     376    |     379    |    7829 K  |    7828 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:20:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:20:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:21:02]    INFO >> epoch 010:    738 / 1539 loss=3.678, wps=3896.4, ups=6.06, wpb=642.7, bsz=642.7, num_updates=14550, lr=0.000193, gnorm=4.891, clip=0, train_wall=7, gb_free=68.7, wall=2533 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:21:10]    INFO >> epoch 010:    788 / 1539 loss=3.571, wps=4377.6, ups=6.05, wpb=723.3, bsz=723.3, num_updates=14600, lr=0.000193, gnorm=4.57, clip=0, train_wall=8, gb_free=67.8, wall=2541 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:21:18]    INFO >> epoch 010:    838 / 1539 loss=3.562, wps=4205.5, ups=6.24, wpb=673.9, bsz=673.9, num_updates=14650, lr=0.000193, gnorm=4.959, clip=0, train_wall=7, gb_free=66.8, wall=2549 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:21:27]    INFO >> epoch 010:    888 / 1539 loss=3.315, wps=4832.5, ups=5.69, wpb=848.7, bsz=848.7, num_updates=14700, lr=0.000193, gnorm=5.234, clip=0, train_wall=8, gb_free=76.2, wall=2558 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:21:36]    INFO >> epoch 010:    938 / 1539 loss=3.617, wps=4541.1, ups=6.36, wpb=713.5, bsz=713.5, num_updates=14750, lr=0.000193, gnorm=4.962, clip=0, train_wall=7, gb_free=74.7, wall=2566 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:21:44]    INFO >> epoch 010:    988 / 1539 loss=3.692, wps=4564.5, ups=6.61, wpb=690.7, bsz=690.7, num_updates=14800, lr=0.000193, gnorm=4.979, clip=0, train_wall=7, gb_free=74.4, wall=2573 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:21:51]    INFO >> epoch 010:   1038 / 1539 loss=3.627, wps=4370.1, ups=6.51, wpb=671.7, bsz=671.7, num_updates=14850, lr=0.000193, gnorm=4.669, clip=0, train_wall=7, gb_free=70, wall=2581 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:21:59]    INFO >> epoch 010:   1088 / 1539 loss=3.602, wps=4971.4, ups=6.18, wpb=803.8, bsz=803.8, num_updates=14900, lr=0.000193, gnorm=5.276, clip=0, train_wall=8, gb_free=72.8, wall=2589 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:22:09]    INFO >> epoch 010:   1138 / 1539 loss=3.595, wps=3931.3, ups=6.42, wpb=612.7, bsz=612.7, num_updates=14950, lr=0.000193, gnorm=4.44, clip=0, train_wall=7, gb_free=71.8, wall=2597 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:22:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 29.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.30 GiB is allocated by PyTorch, and 2.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:22:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:22:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:22:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 40           |        cudaMalloc retries: 75        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78067 MiB |  78127 MiB | 483183 GiB | 483107 GiB |
|       from large pool |  77988 MiB |  78048 MiB | 480446 GiB | 480370 GiB |
|       from small pool |     79 MiB |     80 MiB |   2736 GiB |   2736 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78067 MiB |  78127 MiB | 483183 GiB | 483107 GiB |
|       from large pool |  77988 MiB |  78048 MiB | 480446 GiB | 480370 GiB |
|       from small pool |     79 MiB |     80 MiB |   2736 GiB |   2736 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78041 MiB |  78101 MiB | 482164 GiB | 482088 GiB |
|       from large pool |  77963 MiB |  78022 MiB | 479431 GiB | 479355 GiB |
|       from small pool |     78 MiB |     79 MiB |   2732 GiB |   2732 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80476 MiB |  80478 MiB |   1034 GiB |    955 GiB |
|       from large pool |  80394 MiB |  80394 MiB |   1028 GiB |    949 GiB |
|       from small pool |     82 MiB |    422 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2348 MiB |  10074 MiB | 462318 GiB | 462316 GiB |
|       from large pool |   2345 MiB |  10062 MiB | 459222 GiB | 459219 GiB |
|       from small pool |      2 MiB |     23 MiB |   3096 GiB |   3096 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1763    |    1766    |   31868 K  |   31866 K  |
|       from large pool |     452    |     453    |   15195 K  |   15195 K  |
|       from small pool |    1311    |    1314    |   16672 K  |   16671 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1763    |    1766    |   31868 K  |   31866 K  |
|       from large pool |     452    |     453    |   15195 K  |   15195 K  |
|       from small pool |    1311    |    1314    |   16672 K  |   16671 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     223    |     400    |    7326    |    7103    |
|       from large pool |     182    |     189    |    3963    |    3781    |
|       from small pool |      41    |     211    |    3363    |    3322    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     191    |     192    |   18002 K  |   18002 K  |
|       from large pool |     147    |     152    |    9963 K  |    9963 K  |
|       from small pool |      44    |      52    |    8038 K  |    8038 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:22:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:22:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:22:18]    INFO >> epoch 010:   1189 / 1539 loss=3.587, wps=4289.2, ups=5.44, wpb=787.9, bsz=787.9, num_updates=15000, lr=0.000193, gnorm=4.819, clip=0, train_wall=8, gb_free=70.1, wall=2606 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:22:26]    INFO >> epoch 010:   1239 / 1539 loss=3.537, wps=4715.7, ups=6, wpb=786.3, bsz=786.3, num_updates=15050, lr=0.000193, gnorm=5.278, clip=0, train_wall=8, gb_free=73.2, wall=2615 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:22:35]    INFO >> epoch 010:   1289 / 1539 loss=3.558, wps=4468.8, ups=5.83, wpb=766.5, bsz=766.5, num_updates=15100, lr=0.000193, gnorm=4.715, clip=0, train_wall=8, gb_free=72.7, wall=2623 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:22:44]    INFO >> epoch 010:   1339 / 1539 loss=3.607, wps=4317.2, ups=6.39, wpb=675.5, bsz=675.5, num_updates=15150, lr=0.000193, gnorm=5.208, clip=0, train_wall=7, gb_free=74.4, wall=2631 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:22:52]    INFO >> epoch 010:   1389 / 1539 loss=3.423, wps=4723.6, ups=6.33, wpb=746.1, bsz=746.1, num_updates=15200, lr=0.000193, gnorm=5.568, clip=0, train_wall=7, gb_free=73.6, wall=2639 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:22:59]    INFO >> epoch 010:   1439 / 1539 loss=3.54, wps=4370.6, ups=6.43, wpb=679.6, bsz=679.6, num_updates=15250, lr=0.000193, gnorm=5.304, clip=0, train_wall=7, gb_free=70.1, wall=2647 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:23:05] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 911.25 MiB is free. Including non-PyTorch memory, this process has 78.23 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 3.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:23:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:23:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:23:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 41           |        cudaMalloc retries: 78        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75058 MiB |  76000 MiB | 491923 GiB | 491849 GiB |
|       from large pool |  75045 MiB |  75988 MiB | 489139 GiB | 489065 GiB |
|       from small pool |     12 MiB |     24 MiB |   2784 GiB |   2783 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75058 MiB |  76000 MiB | 491923 GiB | 491849 GiB |
|       from large pool |  75045 MiB |  75988 MiB | 489139 GiB | 489065 GiB |
|       from small pool |     12 MiB |     24 MiB |   2784 GiB |   2783 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 490883 GiB | 490810 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 488103 GiB | 488030 GiB |
|       from small pool |     12 MiB |     24 MiB |   2780 GiB |   2780 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79594 MiB |  79594 MiB |   1070 GiB |    993 GiB |
|       from large pool |  79568 MiB |  79568 MiB |   1064 GiB |    986 GiB |
|       from small pool |     26 MiB |    210 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4535 MiB |   9492 MiB | 470276 GiB | 470272 GiB |
|       from large pool |   4522 MiB |   9478 MiB | 467125 GiB | 467120 GiB |
|       from small pool |     13 MiB |     31 MiB |   3151 GiB |   3151 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   32444 K  |   32443 K  |
|       from large pool |     314    |     322    |   15487 K  |   15486 K  |
|       from small pool |     291    |     356    |   16957 K  |   16957 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   32444 K  |   32443 K  |
|       from large pool |     314    |     322    |   15487 K  |   15486 K  |
|       from small pool |     291    |     356    |   16957 K  |   16957 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     164    |     281    |    7455    |    7291    |
|       from large pool |     151    |     176    |    4028    |    3877    |
|       from small pool |      13    |     105    |    3427    |    3414    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     149    |     151    |   18327 K  |   18327 K  |
|       from large pool |     123    |     125    |   10158 K  |   10158 K  |
|       from small pool |      26    |      62    |    8168 K  |    8168 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:23:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:23:05] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:23:08]    INFO >> epoch 010:   1490 / 1539 loss=3.616, wps=4055.5, ups=5.84, wpb=694.7, bsz=694.7, num_updates=15300, lr=0.000193, gnorm=4.95, clip=0, train_wall=7, gb_free=64.7, wall=2655 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:23:16]    INFO >> epoch 010 | loss 3.556 | wps 4137.1 | ups 5.81 | wpb 712.7 | bsz 712.7 | num_updates 15349 | lr 0.000193 | gnorm 5.032 | clip 0.1 | train_wall 232 | gb_free 70.8 | wall 2663 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:23:16] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:23:30]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.761 | wps 10720.2 | wpb 5412.5 | bsz 5412.5 | num_updates 15349 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:23:31]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:23:31]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 10 @ 15349 updates, score 3.761) (writing took 0.019015 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 16:23:31] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:23:31]    INFO >> epoch 011:      1 / 1539 loss=3.616, wps=1606.9, ups=2.21, wpb=728.6, bsz=728.6, num_updates=15350, lr=0.000161, gnorm=4.475, clip=0, train_wall=8, gb_free=68.7, wall=2678 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:23:38]    INFO >> epoch 011:     51 / 1539 loss=3.646, wps=4197.4, ups=6.48, wpb=647.3, bsz=647.3, num_updates=15400, lr=0.000161, gnorm=4.653, clip=0, train_wall=7, gb_free=75, wall=2686 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:23:49]    INFO >> epoch 011:    101 / 1539 loss=3.556, wps=4546.8, ups=6.23, wpb=729.7, bsz=729.7, num_updates=15450, lr=0.000161, gnorm=5.43, clip=0, train_wall=8, gb_free=74.4, wall=2694 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:23:57]    INFO >> epoch 011:    151 / 1539 loss=3.669, wps=4444.8, ups=6.67, wpb=666, bsz=666, num_updates=15500, lr=0.000161, gnorm=4.871, clip=0, train_wall=7, gb_free=71.4, wall=2701 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:24:05]    INFO >> epoch 011:    201 / 1539 loss=3.522, wps=4752.2, ups=6.18, wpb=769.4, bsz=769.4, num_updates=15550, lr=0.000161, gnorm=4.692, clip=0, train_wall=8, gb_free=71.9, wall=2709 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:24:12]    INFO >> epoch 011:    251 / 1539 loss=3.525, wps=4840.7, ups=6.64, wpb=729.4, bsz=729.4, num_updates=15600, lr=0.000161, gnorm=5.427, clip=0, train_wall=7, gb_free=73.3, wall=2717 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:24:21]    INFO >> epoch 011:    301 / 1539 loss=3.531, wps=3949.2, ups=6.34, wpb=623.3, bsz=623.3, num_updates=15650, lr=0.000161, gnorm=4.844, clip=0, train_wall=7, gb_free=74.6, wall=2725 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:24:30]    INFO >> epoch 011:    351 / 1539 loss=3.529, wps=4562.1, ups=6.07, wpb=751.3, bsz=751.3, num_updates=15700, lr=0.000161, gnorm=5.757, clip=2, train_wall=8, gb_free=72, wall=2733 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:24:37]    INFO >> epoch 011:    401 / 1539 loss=3.573, wps=4536, ups=6.7, wpb=677.5, bsz=677.5, num_updates=15750, lr=0.000161, gnorm=5.03, clip=0, train_wall=7, gb_free=72.3, wall=2740 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:24:45]    INFO >> epoch 011:    451 / 1539 loss=3.606, wps=4177.2, ups=6.69, wpb=624.1, bsz=624.1, num_updates=15800, lr=0.000161, gnorm=5.342, clip=0, train_wall=7, gb_free=72.8, wall=2748 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:24:55]    INFO >> epoch 011:    501 / 1539 loss=3.71, wps=3524.9, ups=5.81, wpb=606.7, bsz=606.7, num_updates=15850, lr=0.000161, gnorm=4.549, clip=0, train_wall=8, gb_free=75, wall=2756 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:25:02]    INFO >> epoch 011:    551 / 1539 loss=3.633, wps=4118.2, ups=6.38, wpb=645.8, bsz=645.8, num_updates=15900, lr=0.000161, gnorm=4.746, clip=2, train_wall=7, gb_free=73.9, wall=2764 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:25:03] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 33.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.32 GiB is allocated by PyTorch, and 2.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:25:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:25:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:25:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 42           |        cudaMalloc retries: 79        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78095 MiB |  78155 MiB | 514020 GiB | 513944 GiB |
|       from large pool |  77710 MiB |  77770 MiB | 511104 GiB | 511028 GiB |
|       from small pool |    385 MiB |    386 MiB |   2916 GiB |   2916 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78095 MiB |  78155 MiB | 514020 GiB | 513944 GiB |
|       from large pool |  77710 MiB |  77770 MiB | 511104 GiB | 511028 GiB |
|       from small pool |    385 MiB |    386 MiB |   2916 GiB |   2916 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78077 MiB |  78137 MiB | 512935 GiB | 512859 GiB |
|       from large pool |  77694 MiB |  77753 MiB | 510023 GiB | 509947 GiB |
|       from small pool |    383 MiB |    384 MiB |   2912 GiB |   2912 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80472 MiB |  80474 MiB |   1071 GiB |    993 GiB |
|       from large pool |  80048 MiB |  80048 MiB |   1064 GiB |    986 GiB |
|       from small pool |    424 MiB |    426 MiB |      7 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2316 MiB |   7005 MiB | 489661 GiB | 489659 GiB |
|       from large pool |   2277 MiB |   7000 MiB | 486363 GiB | 486361 GiB |
|       from small pool |     38 MiB |     40 MiB |   3298 GiB |   3298 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7109    |    7112    |   33914 K  |   33907 K  |
|       from large pool |     916    |     917    |   16134 K  |   16134 K  |
|       from small pool |    6193    |    6196    |   17779 K  |   17773 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7109    |    7112    |   33914 K  |   33907 K  |
|       from large pool |     916    |     917    |   16134 K  |   16134 K  |
|       from small pool |    6193    |    6196    |   17779 K  |   17773 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     371    |     372    |    7663    |    7292    |
|       from large pool |     159    |     159    |    4036    |    3877    |
|       from small pool |     212    |     213    |    3627    |    3415    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     522    |     524    |   19167 K  |   19167 K  |
|       from large pool |     145    |     145    |   10591 K  |   10591 K  |
|       from small pool |     377    |     379    |    8575 K  |    8575 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:25:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:25:03] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:25:11]    INFO >> epoch 011:    602 / 1539 loss=3.558, wps=3966.9, ups=5.72, wpb=694.1, bsz=694.1, num_updates=15950, lr=0.000161, gnorm=4.617, clip=0, train_wall=8, gb_free=71.6, wall=2773 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:25:20]    INFO >> epoch 011:    652 / 1539 loss=3.534, wps=4822.7, ups=5.92, wpb=815.3, bsz=815.3, num_updates=16000, lr=0.000161, gnorm=4.541, clip=0, train_wall=8, gb_free=71.5, wall=2781 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:25:29]    INFO >> epoch 011:    702 / 1539 loss=3.563, wps=4247.3, ups=6.42, wpb=661.1, bsz=661.1, num_updates=16050, lr=0.000161, gnorm=4.656, clip=0, train_wall=7, gb_free=71, wall=2789 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:25:38]    INFO >> epoch 011:    752 / 1539 loss=3.447, wps=4118.6, ups=5.63, wpb=731.8, bsz=731.8, num_updates=16100, lr=0.000161, gnorm=5.354, clip=0, train_wall=8, gb_free=72.6, wall=2798 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:25:43] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 78.03 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 3.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:25:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:25:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:25:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 43           |        cudaMalloc retries: 80        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75056 MiB |  75999 MiB | 521029 GiB | 520955 GiB |
|       from large pool |  75044 MiB |  75986 MiB | 518075 GiB | 518001 GiB |
|       from small pool |     12 MiB |     21 MiB |   2953 GiB |   2953 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75056 MiB |  75999 MiB | 521029 GiB | 520955 GiB |
|       from large pool |  75044 MiB |  75986 MiB | 518075 GiB | 518001 GiB |
|       from small pool |     12 MiB |     21 MiB |   2953 GiB |   2953 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 519927 GiB | 519854 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 516977 GiB | 516904 GiB |
|       from small pool |     12 MiB |     21 MiB |   2949 GiB |   2949 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79396 MiB |  80412 MiB |   1071 GiB |    994 GiB |
|       from large pool |  79372 MiB |  79988 MiB |   1064 GiB |    987 GiB |
|       from small pool |     24 MiB |    424 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4279 MiB |   8182 MiB | 496383 GiB | 496378 GiB |
|       from large pool |   4267 MiB |   8171 MiB | 493041 GiB | 493037 GiB |
|       from small pool |     11 MiB |     27 MiB |   3341 GiB |   3341 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   34373 K  |   34372 K  |
|       from large pool |     314    |     322    |   16366 K  |   16366 K  |
|       from small pool |     291    |     356    |   18006 K  |   18006 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   34373 K  |   34372 K  |
|       from large pool |     314    |     322    |   16366 K  |   16366 K  |
|       from small pool |     291    |     356    |   18006 K  |   18006 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     165    |     370    |    7663    |    7498    |
|       from large pool |     153    |     158    |    4036    |    3883    |
|       from small pool |      12    |     212    |    3627    |    3615    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     154    |     157    |   19428 K  |   19427 K  |
|       from large pool |     127    |     130    |   10745 K  |   10745 K  |
|       from small pool |      27    |      60    |    8682 K  |    8682 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:25:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:25:43] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:25:46]    INFO >> epoch 011:    803 / 1539 loss=3.626, wps=3683.6, ups=6.28, wpb=586.3, bsz=586.3, num_updates=16150, lr=0.000161, gnorm=4.684, clip=0, train_wall=7, gb_free=62.8, wall=2806 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:25:53]    INFO >> epoch 011:    853 / 1539 loss=3.49, wps=4268.9, ups=6.5, wpb=657, bsz=657, num_updates=16200, lr=0.000161, gnorm=5.373, clip=0, train_wall=7, gb_free=74.8, wall=2814 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:26:02]    INFO >> epoch 011:    903 / 1539 loss=3.623, wps=4991.6, ups=6.4, wpb=779.8, bsz=779.8, num_updates=16250, lr=0.000161, gnorm=4.782, clip=0, train_wall=7, gb_free=72.7, wall=2822 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:26:10]    INFO >> epoch 011:    953 / 1539 loss=3.553, wps=4693.2, ups=6.29, wpb=746, bsz=746, num_updates=16300, lr=0.000161, gnorm=4.827, clip=0, train_wall=7, gb_free=72, wall=2830 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:26:19]    INFO >> epoch 011:   1003 / 1539 loss=3.487, wps=4554.9, ups=6.17, wpb=737.8, bsz=737.8, num_updates=16350, lr=0.000161, gnorm=5.584, clip=2, train_wall=8, gb_free=71.3, wall=2838 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:26:26]    INFO >> epoch 011:   1053 / 1539 loss=3.499, wps=4351.8, ups=6.28, wpb=692.9, bsz=692.9, num_updates=16400, lr=0.000161, gnorm=4.162, clip=0, train_wall=7, gb_free=73.7, wall=2846 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:26:32] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 19.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.94 GiB is allocated by PyTorch, and 1.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:26:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:26:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:26:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 44           |        cudaMalloc retries: 82        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78725 MiB |  78785 MiB | 529610 GiB | 529533 GiB |
|       from large pool |  78639 MiB |  78699 MiB | 526609 GiB | 526533 GiB |
|       from small pool |     85 MiB |     86 MiB |   3000 GiB |   3000 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78725 MiB |  78785 MiB | 529610 GiB | 529533 GiB |
|       from large pool |  78639 MiB |  78699 MiB | 526609 GiB | 526533 GiB |
|       from small pool |     85 MiB |     86 MiB |   3000 GiB |   3000 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78703 MiB |  78762 MiB | 528490 GiB | 528413 GiB |
|       from large pool |  78617 MiB |  78677 MiB | 525494 GiB | 525417 GiB |
|       from small pool |     85 MiB |     86 MiB |   2996 GiB |   2996 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80486 MiB |  80486 MiB |   1072 GiB |    994 GiB |
|       from large pool |  80396 MiB |  80396 MiB |   1065 GiB |    987 GiB |
|       from small pool |     90 MiB |    124 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1700 MiB |   9160 MiB | 504805 GiB | 504803 GiB |
|       from large pool |   1696 MiB |   9152 MiB | 501410 GiB | 501408 GiB |
|       from small pool |      4 MiB |     23 MiB |   3395 GiB |   3395 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1884    |    1887    |   34936 K  |   34934 K  |
|       from large pool |     463    |     464    |   16651 K  |   16651 K  |
|       from small pool |    1421    |    1424    |   18284 K  |   18282 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1884    |    1887    |   34936 K  |   34934 K  |
|       from large pool |     463    |     464    |   16651 K  |   16651 K  |
|       from small pool |    1421    |    1424    |   18284 K  |   18282 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     201    |     217    |    7718    |    7517    |
|       from large pool |     156    |     156    |    4040    |    3884    |
|       from small pool |      45    |      62    |    3678    |    3633    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     170    |     170    |   19742 K  |   19741 K  |
|       from large pool |     123    |     125    |   10933 K  |   10933 K  |
|       from small pool |      47    |      52    |    8808 K  |    8808 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:26:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:26:32] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:26:37]    INFO >> epoch 011:   1104 / 1539 loss=3.534, wps=4267.7, ups=5.59, wpb=764, bsz=764, num_updates=16450, lr=0.000161, gnorm=5.364, clip=0, train_wall=8, gb_free=75, wall=2855 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:26:46]    INFO >> epoch 011:   1154 / 1539 loss=3.534, wps=5137.3, ups=5.65, wpb=909, bsz=909, num_updates=16500, lr=0.000161, gnorm=5.478, clip=0, train_wall=8, gb_free=69.3, wall=2863 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:26:54]    INFO >> epoch 011:   1204 / 1539 loss=3.597, wps=3780.8, ups=6.22, wpb=608.2, bsz=608.2, num_updates=16550, lr=0.000161, gnorm=4.344, clip=0, train_wall=8, gb_free=68.8, wall=2871 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:27:02]    INFO >> epoch 011:   1254 / 1539 loss=3.516, wps=4713.8, ups=5.69, wpb=828.9, bsz=828.9, num_updates=16600, lr=0.000161, gnorm=5.084, clip=0, train_wall=8, gb_free=73.1, wall=2880 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:27:12]    INFO >> epoch 011:   1304 / 1539 loss=3.535, wps=4557.6, ups=6.17, wpb=739.1, bsz=739.1, num_updates=16650, lr=0.000161, gnorm=4.959, clip=0, train_wall=8, gb_free=71.9, wall=2888 (progress_bar.py:258, log())[0m
[33m[2025-11-21 16:27:15] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process has 78.05 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 16:27:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:27:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:27:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 45           |        cudaMalloc retries: 86        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76321 MiB |  78595 MiB | 536742 GiB | 536667 GiB |
|       from large pool |  76302 MiB |  78578 MiB | 533702 GiB | 533628 GiB |
|       from small pool |     18 MiB |     23 MiB |   3039 GiB |   3039 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76321 MiB |  78595 MiB | 536742 GiB | 536667 GiB |
|       from large pool |  76302 MiB |  78578 MiB | 533702 GiB | 533628 GiB |
|       from small pool |     18 MiB |     23 MiB |   3039 GiB |   3039 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 535606 GiB | 535532 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 532571 GiB | 532497 GiB |
|       from small pool |     18 MiB |     23 MiB |   3035 GiB |   3035 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79418 MiB |  79536 MiB |   1178 GiB |   1101 GiB |
|       from large pool |  79392 MiB |  79460 MiB |   1171 GiB |   1094 GiB |
|       from small pool |     26 MiB |     76 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3096 MiB |   4417 MiB | 511218 GiB | 511215 GiB |
|       from large pool |   3089 MiB |   4409 MiB | 507778 GiB | 507775 GiB |
|       from small pool |      7 MiB |     31 MiB |   3439 GiB |   3439 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   35400 K  |   35399 K  |
|       from large pool |     340    |     346    |   16881 K  |   16881 K  |
|       from small pool |     300    |     356    |   18518 K  |   18518 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   35400 K  |   35399 K  |
|       from large pool |     340    |     346    |   16881 K  |   16881 K  |
|       from small pool |     300    |     356    |   18518 K  |   18518 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     109    |     212    |    7929    |    7820    |
|       from large pool |      96    |     174    |    4167    |    4071    |
|       from small pool |      13    |      38    |    3762    |    3749    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     128    |     130    |   20008 K  |   20008 K  |
|       from large pool |     101    |     103    |   11090 K  |   11089 K  |
|       from small pool |      27    |      55    |    8918 K  |    8918 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:27:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 16:27:15] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 16:27:22]    INFO >> epoch 011:   1355 / 1539 loss=3.53, wps=3346.4, ups=4.86, wpb=689.1, bsz=689.1, num_updates=16700, lr=0.000161, gnorm=5.149, clip=0, train_wall=8, gb_free=56.8, wall=2899 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:27:31]    INFO >> epoch 011:   1405 / 1539 loss=3.658, wps=4722.3, ups=5.97, wpb=791.3, bsz=791.3, num_updates=16750, lr=0.000161, gnorm=5.055, clip=0, train_wall=8, gb_free=73.5, wall=2907 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:27:39]    INFO >> epoch 011:   1455 / 1539 loss=3.624, wps=4136.9, ups=6.22, wpb=665.3, bsz=665.3, num_updates=16800, lr=0.000161, gnorm=4.771, clip=0, train_wall=8, gb_free=65.3, wall=2915 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:27:49]    INFO >> epoch 011:   1505 / 1539 loss=3.189, wps=4728.4, ups=5.78, wpb=818.6, bsz=818.6, num_updates=16850, lr=0.000161, gnorm=5.457, clip=0, train_wall=8, gb_free=72.5, wall=2924 (progress_bar.py:258, log())[0m
[32m[2025-11-21 16:27:54]    INFO >> epoch 011 | loss 3.549 | wps 4110.7 | ups 5.77 | wpb 712.7 | bsz 712.7 | num_updates 16884 | lr 0.000161 | gnorm 4.994 | clip 0.2 | train_wall 232 | gb_free 73.9 | wall 2929 (progress_bar.py:267, print())[0m
[33m[2025-11-21 16:27:54] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 16:28:08]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.766 | wps 10621.6 | wpb 5412.5 | bsz 5412.5 | num_updates 16884 | best_loss 4.689 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:28:09]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:28:09]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_last.pt (epoch 11 @ 16884 updates, score 3.766) (writing took 0.013845 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-21 16:28:09]    INFO >> æ—©åœ: éªŒè¯æ€§èƒ½å·²10è½®æœªæå‡ (train_enhanced.py:616, single_main())[0m
[32m[2025-11-21 16:28:09]    INFO >> è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ 2876.5 ç§’ (train_enhanced.py:626, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 16:28:09]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 16:28:09]    INFO >> æ‰€æœ‰æ—¥å¿—å·²ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs (train_enhanced.py:631, single_main())[0m
[32m[2025-11-21 16:28:09]    INFO >> 
================================================================================ (train_enhanced.py:634, single_main())[0m
[32m[2025-11-21 16:28:09]    INFO >> å¼€å§‹æµ‹è¯•... (train_enhanced.py:635, single_main())[0m
[32m[2025-11-21 16:28:09]    INFO >> ================================================================================ (train_enhanced.py:636, single_main())[0m
[32m[2025-11-21 16:28:09]    INFO >> åŠ è½½æœ€ä½³checkpoint: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/checkpoint_best.pt (train_enhanced.py:50, run_test_after_training())[0m
[32m[2025-11-21 16:28:09]    INFO >> æµ‹è¯•é›†: test (train_enhanced.py:51, run_test_after_training())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/checkpoint_utils.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(
[32m[2025-11-21 16:29:02]    INFO >> 
================================================================================ (train_enhanced.py:168, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> æµ‹è¯•ç»“æžœ: (train_enhanced.py:169, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> -------------------------------------------------------------------------------- (train_enhanced.py:170, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> å¹³å‡Loss:      3.8948 (train_enhanced.py:171, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> Acc@1:         22.18% (train_enhanced.py:172, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> Acc@5:         60.94% (train_enhanced.py:173, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> Acc@1 (å«any): 22.18% (train_enhanced.py:174, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> Acc@5 (å«any): 60.94% (train_enhanced.py:175, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> ================================================================================ (train_enhanced.py:176, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> æµ‹è¯•ç»“æžœå·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/checkpoints/res.txt (train_enhanced.py:187, run_test_after_training())[0m
[32m[2025-11-21 16:29:02]    INFO >> è®­ç»ƒæ—¥å¿—å·²æ›´æ–°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs (train_enhanced.py:222, run_test_after_training())[0m
[TrainingLogger] æ—¥å¿—ç›®å½•: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs
[TrainingLogger] åŽŸå§‹è¾“å‡ºå°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/training_output.log
[TrainingLogger] Epoch 1 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 2 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 3 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 4 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 5 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 6 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 7 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 8 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 9 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 10 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
[TrainingLogger] Epoch 11 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_015/logs/metrics.json
