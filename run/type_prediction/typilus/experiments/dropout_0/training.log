[32m[2025-11-21 14:14:01]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/config.yml (train_enhanced.py:666, cli_main())[0m
[32m[2025-11-21 14:14:01]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:694, cli_main())[0m
[32m[2025-11-21 14:14:01]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs (train_enhanced.py:561, single_main())[0m
[32m[2025-11-21 14:14:01]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 14:14:01]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 14:14:01]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-21 14:14:10]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 64, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.0, inplace=False)
      (1): Linear(in_features=64, out_features=64, bias=False)
      (2): Dropout(p=0.0, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(64, 64)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(128, 64)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=False)
      (1): Dropout(p=0.0, inplace=False)
      (2): Linear(in_features=64, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:568, single_main())[0m
[32m[2025-11-21 14:14:10]    INFO >> æ¨¡åž‹: typilus, æŸå¤±å‡½æ•°: TypilusCriterion (train_enhanced.py:569, single_main())[0m
[32m[2025-11-21 14:14:10]    INFO >> æ¨¡åž‹å‚æ•°: 847843 (å¯è®­ç»ƒ: 847843) (train_enhanced.py:570, single_main())[0m
[32m[2025-11-21 14:14:10]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 14:14:10]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80579 MB ; used memory = 1340 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 14:14:10]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 14:14:10]    INFO >> ä½¿ç”¨ 1 ä¸ªGPUè®­ç»ƒ (train_enhanced.py:576, single_main())[0m
[32m[2025-11-21 14:14:10]    INFO >> no existing checkpoint found /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-21 14:14:10]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-21 14:15:18]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-21 14:15:18] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 14:15:26]    INFO >> epoch 001:     50 / 1539 loss=5.627, wps=5212.9, ups=7.21, wpb=720, bsz=720, num_updates=50, lr=0.0004, gnorm=1.175, clip=0, train_wall=7, gb_free=74.4, wall=72 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:15:28] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 15.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 73.64 GiB is allocated by PyTorch, and 4.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:15:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:15:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:15:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75351 MiB |  75411 MiB |   1663 GiB |   1589 GiB |
|       from large pool |  74995 MiB |  75055 MiB |   1652 GiB |   1578 GiB |
|       from small pool |    356 MiB |    357 MiB |     11 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75351 MiB |  75411 MiB |   1663 GiB |   1589 GiB |
|       from large pool |  74995 MiB |  75055 MiB |   1652 GiB |   1578 GiB |
|       from small pool |    356 MiB |    357 MiB |     11 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75205 MiB |  75264 MiB |   1658 GiB |   1584 GiB |
|       from large pool |  74850 MiB |  74910 MiB |   1647 GiB |   1573 GiB |
|       from small pool |    354 MiB |    355 MiB |     11 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80492 MiB |  80492 MiB |  91734 MiB |  11242 MiB |
|       from large pool |  80098 MiB |  80120 MiB |  91336 MiB |  11238 MiB |
|       from small pool |    394 MiB |    394 MiB |    398 MiB |      4 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5080 MiB |   5613 MiB |    793 GiB |    788 GiB |
|       from large pool |   5042 MiB |   5588 MiB |    780 GiB |    775 GiB |
|       from small pool |     37 MiB |     38 MiB |     13 GiB |     13 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6682    |    6685    |  125203    |  118521    |
|       from large pool |     828    |     829    |   49276    |   48448    |
|       from small pool |    5854    |    5857    |   75927    |   70073    |
|---------------------------------------------------------------------------|
| Active allocs         |    6682    |    6685    |  125203    |  118521    |
|       from large pool |     828    |     829    |   49276    |   48448    |
|       from small pool |    5854    |    5857    |   75927    |   70073    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     760    |     789    |     990    |     230    |
|       from large pool |     563    |     607    |     791    |     228    |
|       from small pool |     197    |     197    |     199    |       2    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     676    |     676    |   76683    |   76007    |
|       from large pool |     322    |     322    |   37136    |   36814    |
|       from small pool |     354    |     354    |   39547    |   39193    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:15:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:15:28] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:15:33]    INFO >> epoch 001:    101 / 1539 loss=4.072, wps=4430.9, ups=7.26, wpb=610.7, bsz=610.7, num_updates=100, lr=0.0004, gnorm=1.535, clip=0, train_wall=6, gb_free=75.7, wall=79 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:15:41]    INFO >> epoch 001:    151 / 1539 loss=3.445, wps=5573.4, ups=6.82, wpb=816.7, bsz=816.7, num_updates=150, lr=0.0004, gnorm=1.517, clip=0, train_wall=7, gb_free=74.3, wall=86 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:15:47]    INFO >> epoch 001:    201 / 1539 loss=3.506, wps=4967.4, ups=7.74, wpb=641.7, bsz=641.7, num_updates=200, lr=0.0004, gnorm=1.52, clip=0, train_wall=6, gb_free=74.9, wall=93 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:15:54]    INFO >> epoch 001:    251 / 1539 loss=3.436, wps=4788.9, ups=7.51, wpb=637.9, bsz=637.9, num_updates=250, lr=0.0004, gnorm=1.675, clip=0, train_wall=6, gb_free=71.7, wall=99 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:16:03]    INFO >> epoch 001:    301 / 1539 loss=3.446, wps=5293.7, ups=6.73, wpb=786.5, bsz=786.5, num_updates=300, lr=0.0004, gnorm=1.617, clip=0, train_wall=7, gb_free=73.9, wall=107 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:16:09]    INFO >> epoch 001:    351 / 1539 loss=3.272, wps=5087.8, ups=7.58, wpb=671.6, bsz=671.6, num_updates=350, lr=0.0004, gnorm=1.667, clip=0, train_wall=6, gb_free=72.6, wall=114 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:16:16]    INFO >> epoch 001:    401 / 1539 loss=3.261, wps=5867.2, ups=6.89, wpb=851.7, bsz=851.7, num_updates=400, lr=0.0004, gnorm=1.966, clip=0, train_wall=7, gb_free=73.4, wall=121 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:16:26] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.91 GiB is allocated by PyTorch, and 711.06 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:16:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:16:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:16:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79716 MiB |  79776 MiB |  11725 GiB |  11647 GiB |
|       from large pool |  79575 MiB |  79635 MiB |  11661 GiB |  11583 GiB |
|       from small pool |    141 MiB |    142 MiB |     64 GiB |     63 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79716 MiB |  79776 MiB |  11725 GiB |  11647 GiB |
|       from large pool |  79575 MiB |  79635 MiB |  11661 GiB |  11583 GiB |
|       from small pool |    141 MiB |    142 MiB |     64 GiB |     63 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79601 MiB |  79660 MiB |  11695 GiB |  11617 GiB |
|       from large pool |  79459 MiB |  79519 MiB |  11631 GiB |  11553 GiB |
|       from small pool |    141 MiB |    142 MiB |     64 GiB |     63 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 166570 MiB |  86082 MiB |
|       from large pool |  80342 MiB |  80342 MiB | 166046 MiB |  85704 MiB |
|       from small pool |    146 MiB |    394 MiB |    524 MiB |    378 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 728130 KiB |   2495 MiB |   5498 GiB |   5498 GiB |
|       from large pool | 723709 KiB |   2491 MiB |   5424 GiB |   5423 GiB |
|       from small pool |   4421 KiB |     17 MiB |     74 GiB |     74 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2876    |    2879    |     781 K  |     778 K  |
|       from large pool |     496    |     497    |     357 K  |     357 K  |
|       from small pool |    2380    |    2383    |     423 K  |     421 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2876    |    2879    |     781 K  |     778 K  |
|       from large pool |     496    |     497    |     357 K  |     357 K  |
|       from small pool |    2380    |    2383    |     423 K  |     421 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     373    |     759    |    1313    |     940    |
|       from large pool |     300    |     562    |    1051    |     751    |
|       from small pool |      73    |     197    |     262    |     189    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     142    |     144    |  485024    |  484882    |
|       from large pool |      74    |      75    |  277574    |  277500    |
|       from small pool |      68    |      70    |  207450    |  207382    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:16:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:16:26] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:16:30]    INFO >> epoch 001:    452 / 1539 loss=3.263, wps=2371.9, ups=3.76, wpb=631.6, bsz=631.6, num_updates=450, lr=0.0004, gnorm=1.689, clip=0, train_wall=6, gb_free=72, wall=134 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:16:41]    INFO >> epoch 001:    502 / 1539 loss=3.49, wps=3940.4, ups=5.3, wpb=743.1, bsz=743.1, num_updates=500, lr=0.0004, gnorm=1.686, clip=0, train_wall=9, gb_free=72.7, wall=144 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:16:47]    INFO >> epoch 001:    552 / 1539 loss=3.211, wps=4780.7, ups=7.28, wpb=657, bsz=657, num_updates=550, lr=0.0004, gnorm=1.876, clip=0, train_wall=6, gb_free=65.9, wall=150 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:16:54]    INFO >> epoch 001:    602 / 1539 loss=3.306, wps=4840.3, ups=7.24, wpb=668.6, bsz=668.6, num_updates=600, lr=0.0004, gnorm=1.98, clip=0, train_wall=6, gb_free=73.4, wall=157 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:17:01]    INFO >> epoch 001:    652 / 1539 loss=3.335, wps=4979.4, ups=6.99, wpb=712.2, bsz=712.2, num_updates=650, lr=0.0004, gnorm=1.717, clip=0, train_wall=6, gb_free=73.7, wall=164 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:17:11]    INFO >> epoch 001:    702 / 1539 loss=3.21, wps=4376.2, ups=6.51, wpb=672.5, bsz=672.5, num_updates=700, lr=0.0004, gnorm=1.756, clip=0, train_wall=7, gb_free=74.2, wall=172 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:17:18]    INFO >> epoch 001:    752 / 1539 loss=3.083, wps=5299.6, ups=7, wpb=757.4, bsz=757.4, num_updates=750, lr=0.0004, gnorm=1.677, clip=0, train_wall=6, gb_free=73.8, wall=179 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:17:24]    INFO >> epoch 001:    802 / 1539 loss=3.411, wps=5413.2, ups=7.46, wpb=725.2, bsz=725.2, num_updates=800, lr=0.0004, gnorm=1.95, clip=0, train_wall=6, gb_free=73.5, wall=186 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:17:32]    INFO >> epoch 001:    852 / 1539 loss=3.369, wps=4419, ups=6.89, wpb=641.1, bsz=641.1, num_updates=850, lr=0.0004, gnorm=1.838, clip=0, train_wall=7, gb_free=71.9, wall=193 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:17:40]    INFO >> epoch 001:    902 / 1539 loss=3.265, wps=4907.3, ups=7.43, wpb=660.1, bsz=660.1, num_updates=900, lr=0.0004, gnorm=1.747, clip=0, train_wall=6, gb_free=72.3, wall=200 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:17:47]    INFO >> epoch 001:    952 / 1539 loss=3.246, wps=5118.8, ups=7.18, wpb=713.2, bsz=713.2, num_updates=950, lr=0.0004, gnorm=1.753, clip=0, train_wall=6, gb_free=72, wall=207 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:17:49] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 376.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 237.25 MiB is free. Including non-PyTorch memory, this process has 78.88 GiB memory in use. Of the allocated memory 77.19 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:17:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:17:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:17:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  66231 MiB |  79042 MiB |  26729 GiB |  26664 GiB |
|       from large pool |  66222 MiB |  79033 MiB |  26597 GiB |  26532 GiB |
|       from small pool |      8 MiB |     18 MiB |    131 GiB |    131 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  66231 MiB |  79042 MiB |  26729 GiB |  26664 GiB |
|       from large pool |  66222 MiB |  79033 MiB |  26597 GiB |  26532 GiB |
|       from small pool |      8 MiB |     18 MiB |    131 GiB |    131 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  66225 MiB |  79032 MiB |  26678 GiB |  26613 GiB |
|       from large pool |  66216 MiB |  79023 MiB |  26546 GiB |  26482 GiB |
|       from small pool |      8 MiB |     18 MiB |    131 GiB |    131 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80268 MiB |  80268 MiB | 285424 MiB | 205156 MiB |
|       from large pool |  80246 MiB |  80246 MiB | 284826 MiB | 204580 MiB |
|       from small pool |     22 MiB |     96 MiB |    598 MiB |    576 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2472 MiB |   5499 MiB |  19910 GiB |  19907 GiB |
|       from large pool |   2459 MiB |   5485 MiB |  19758 GiB |  19755 GiB |
|       from small pool |     13 MiB |     17 MiB |    151 GiB |    151 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     543    |     585    |    1675 K  |    1674 K  |
|       from large pool |     260    |     302    |     802 K  |     802 K  |
|       from small pool |     283    |     354    |     872 K  |     872 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     543    |     585    |    1675 K  |    1674 K  |
|       from large pool |     260    |     302    |     802 K  |     802 K  |
|       from small pool |     283    |     354    |     872 K  |     872 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     115    |     169    |    1487    |    1372    |
|       from large pool |     104    |     121    |    1188    |    1084    |
|       from small pool |      11    |      48    |     299    |     288    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     115    |     120    |     969 K  |     969 K  |
|       from large pool |      93    |      98    |     561 K  |     561 K  |
|       from small pool |      22    |      43    |     407 K  |     407 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:17:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:17:49] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:17:55]    INFO >> epoch 001:   1003 / 1539 loss=3.272, wps=3872.8, ups=5.96, wpb=649.4, bsz=649.4, num_updates=1000, lr=0.0004, gnorm=1.879, clip=0, train_wall=6, gb_free=72.3, wall=215 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:18:02]    INFO >> epoch 001:   1053 / 1539 loss=3.154, wps=5622.6, ups=6.84, wpb=822.6, bsz=822.6, num_updates=1050, lr=0.0004, gnorm=1.94, clip=0, train_wall=7, gb_free=68.3, wall=223 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:18:10] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 519.25 MiB is free. Including non-PyTorch memory, this process has 78.61 GiB memory in use. Of the allocated memory 72.51 GiB is allocated by PyTorch, and 5.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:18:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:18:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:18:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 9         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63155 MiB |  75126 MiB |  30822 GiB |  30760 GiB |
|       from large pool |  63144 MiB |  75115 MiB |  30668 GiB |  30607 GiB |
|       from small pool |     11 MiB |     15 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63155 MiB |  75126 MiB |  30822 GiB |  30760 GiB |
|       from large pool |  63144 MiB |  75115 MiB |  30668 GiB |  30607 GiB |
|       from small pool |     11 MiB |     15 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63147 MiB |  75114 MiB |  30766 GiB |  30704 GiB |
|       from large pool |  63136 MiB |  75103 MiB |  30612 GiB |  30551 GiB |
|       from small pool |     11 MiB |     15 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79986 MiB |  80256 MiB | 305464 MiB | 225478 MiB |
|       from large pool |  79964 MiB |  80234 MiB | 304684 MiB | 224720 MiB |
|       from small pool |     22 MiB |    204 MiB |    780 MiB |    758 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8174 MiB |  10475 MiB |  24343 GiB |  24335 GiB |
|       from large pool |   8163 MiB |  10464 MiB |  24166 GiB |  24158 GiB |
|       from small pool |     10 MiB |     17 MiB |    177 GiB |    177 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     513    |     555    |    1930 K  |    1930 K  |
|       from large pool |     230    |     272    |     918 K  |     918 K  |
|       from small pool |     283    |     354    |    1012 K  |    1011 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     513    |     555    |    1930 K  |    1930 K  |
|       from large pool |     230    |     272    |     918 K  |     918 K  |
|       from small pool |     283    |     354    |    1012 K  |    1011 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      98    |     204    |    1597    |    1499    |
|       from large pool |      87    |     102    |    1207    |    1120    |
|       from small pool |      11    |     102    |     390    |     379    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     105    |     108    |    1110 K  |    1110 K  |
|       from large pool |      83    |      86    |     633 K  |     633 K  |
|       from small pool |      22    |      42    |     477 K  |     477 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:18:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:18:10] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:18:11]    INFO >> epoch 001:   1104 / 1539 loss=3.099, wps=5478.3, ups=5.89, wpb=929.6, bsz=929.6, num_updates=1100, lr=0.0004, gnorm=2, clip=0, train_wall=7, gb_free=73, wall=231 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:18:19]    INFO >> epoch 001:   1154 / 1539 loss=3.466, wps=5088.7, ups=7.35, wpb=692.6, bsz=692.6, num_updates=1150, lr=0.0004, gnorm=1.917, clip=0, train_wall=6, gb_free=73.3, wall=238 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:18:26]    INFO >> epoch 001:   1204 / 1539 loss=3.14, wps=5060, ups=7.46, wpb=678.4, bsz=678.4, num_updates=1200, lr=0.0004, gnorm=1.843, clip=0, train_wall=6, gb_free=71.4, wall=245 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:18:33]    INFO >> epoch 001:   1254 / 1539 loss=3.356, wps=5280.2, ups=7.22, wpb=730.9, bsz=730.9, num_updates=1250, lr=0.0004, gnorm=1.915, clip=0, train_wall=6, gb_free=71.5, wall=252 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:18:40]    INFO >> epoch 001:   1304 / 1539 loss=3.375, wps=5186.5, ups=7.05, wpb=735.4, bsz=735.4, num_updates=1300, lr=0.0004, gnorm=1.744, clip=0, train_wall=6, gb_free=74.4, wall=259 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:18:48]    INFO >> epoch 001:   1354 / 1539 loss=3.274, wps=4723.4, ups=7.19, wpb=657.2, bsz=657.2, num_updates=1350, lr=0.0004, gnorm=1.933, clip=0, train_wall=6, gb_free=73.5, wall=266 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:18:55]    INFO >> epoch 001:   1404 / 1539 loss=3.293, wps=5009.9, ups=7.03, wpb=712.5, bsz=712.5, num_updates=1400, lr=0.0004, gnorm=2.03, clip=0, train_wall=6, gb_free=73.4, wall=273 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:19:02]    INFO >> epoch 001:   1454 / 1539 loss=3.243, wps=5160.3, ups=7.35, wpb=702.2, bsz=702.2, num_updates=1450, lr=0.0004, gnorm=1.947, clip=0, train_wall=6, gb_free=71.8, wall=280 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:19:09]    INFO >> epoch 001:   1504 / 1539 loss=3.285, wps=4936.8, ups=7.09, wpb=695.9, bsz=695.9, num_updates=1500, lr=0.0004, gnorm=2.194, clip=0, train_wall=6, gb_free=71, wall=287 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:19:14]    INFO >> epoch 001 | loss 3.398 | wps 4836 | ups 6.78 | wpb 712.7 | bsz 712.7 | num_updates 1535 | lr 0.0004 | gnorm 1.798 | clip 0 | train_wall 196 | gb_free 76.7 | wall 291 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:19:14] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:19:27]    INFO >> epoch 001 | valid on 'valid' subset | loss 3.481 | wps 11507.2 | wpb 5412.5 | bsz 5412.5 | num_updates 1535 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-21 14:19:28]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:19:28]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_best.pt (epoch 1 @ 1535 updates, score 3.481) (writing took 0.024328 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:19:28] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 14:19:30]    INFO >> epoch 002:     15 / 1539 loss=3.248, wps=1774.4, ups=2.43, wpb=731.3, bsz=731.3, num_updates=1550, lr=0.0004, gnorm=1.987, clip=0, train_wall=6, gb_free=74.5, wall=307 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:19:36]    INFO >> epoch 002:     65 / 1539 loss=3.309, wps=4951.3, ups=7.52, wpb=658.8, bsz=658.8, num_updates=1600, lr=0.0004, gnorm=1.779, clip=0, train_wall=6, gb_free=73.5, wall=314 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:19:43]    INFO >> epoch 002:    115 / 1539 loss=3.27, wps=5015.6, ups=7.02, wpb=714.6, bsz=714.6, num_updates=1650, lr=0.0004, gnorm=1.91, clip=0, train_wall=7, gb_free=66, wall=321 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:19:51]    INFO >> epoch 002:    165 / 1539 loss=3.119, wps=5113.7, ups=6.92, wpb=738.8, bsz=738.8, num_updates=1700, lr=0.0004, gnorm=1.655, clip=0, train_wall=7, gb_free=73.8, wall=328 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:20:00]    INFO >> epoch 002:    215 / 1539 loss=3.377, wps=4966.8, ups=7.05, wpb=704.2, bsz=704.2, num_updates=1750, lr=0.0004, gnorm=2.095, clip=0, train_wall=7, gb_free=71.4, wall=335 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:20:08]    INFO >> epoch 002:    265 / 1539 loss=2.884, wps=5454.5, ups=6.24, wpb=873.5, bsz=873.5, num_updates=1800, lr=0.0004, gnorm=1.743, clip=0, train_wall=8, gb_free=74.8, wall=343 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:20:15]    INFO >> epoch 002:    315 / 1539 loss=3.288, wps=4672.2, ups=7.24, wpb=645.1, bsz=645.1, num_updates=1850, lr=0.0004, gnorm=1.863, clip=0, train_wall=7, gb_free=72.1, wall=350 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:20:22]    INFO >> epoch 002:    365 / 1539 loss=3.341, wps=4741.7, ups=7.23, wpb=655.8, bsz=655.8, num_updates=1900, lr=0.0004, gnorm=2.025, clip=0, train_wall=7, gb_free=74.1, wall=357 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:20:31]    INFO >> epoch 002:    415 / 1539 loss=3.297, wps=4705.8, ups=6.6, wpb=713.2, bsz=713.2, num_updates=1950, lr=0.0004, gnorm=1.666, clip=0, train_wall=7, gb_free=76.3, wall=365 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:20:32] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB. GPU 2 has a total capacity of 79.14 GiB of which 991.25 MiB is free. Including non-PyTorch memory, this process has 78.15 GiB memory in use. Of the allocated memory 75.54 GiB is allocated by PyTorch, and 2.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:20:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:20:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:20:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  66230 MiB |  77540 MiB |  58600 GiB |  58535 GiB |
|       from large pool |  66221 MiB |  77531 MiB |  58297 GiB |  58232 GiB |
|       from small pool |      8 MiB |     21 MiB |    303 GiB |    303 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  66230 MiB |  77540 MiB |  58600 GiB |  58535 GiB |
|       from large pool |  66221 MiB |  77531 MiB |  58297 GiB |  58232 GiB |
|       from small pool |      8 MiB |     21 MiB |    303 GiB |    303 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  66225 MiB |  77534 MiB |  58506 GiB |  58441 GiB |
|       from large pool |  66216 MiB |  77525 MiB |  58203 GiB |  58139 GiB |
|       from small pool |      8 MiB |     21 MiB |    302 GiB |    302 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79514 MiB |  79696 MiB | 313830 MiB | 234316 MiB |
|       from large pool |  79492 MiB |  79492 MiB | 312868 MiB | 233376 MiB |
|       from small pool |     22 MiB |    204 MiB |    962 MiB |    940 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5099 MiB |   6843 MiB |  53186 GiB |  53181 GiB |
|       from large pool |   5086 MiB |   6830 MiB |  52843 GiB |  52838 GiB |
|       from small pool |     13 MiB |     17 MiB |    343 GiB |    343 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     545    |     586    |    3667 K  |    3666 K  |
|       from large pool |     260    |     301    |    1671 K  |    1671 K  |
|       from small pool |     285    |     355    |    1996 K  |    1995 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     545    |     586    |    3667 K  |    3666 K  |
|       from large pool |     260    |     301    |    1671 K  |    1671 K  |
|       from small pool |     285    |     355    |    1996 K  |    1995 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     182    |    1691    |    1600    |
|       from large pool |      80    |      80    |    1210    |    1130    |
|       from small pool |      11    |     102    |     481    |     470    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      94    |      94    |    2069 K  |    2069 K  |
|       from large pool |      73    |      73    |    1102 K  |    1102 K  |
|       from small pool |      21    |      44    |     966 K  |     966 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:20:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:20:32] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:20:39]    INFO >> epoch 002:    466 / 1539 loss=3.206, wps=4598.7, ups=6.3, wpb=729.4, bsz=729.4, num_updates=2000, lr=0.0004, gnorm=1.974, clip=0, train_wall=7, gb_free=71.8, wall=373 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:20:46]    INFO >> epoch 002:    516 / 1539 loss=3.434, wps=4874.5, ups=6.88, wpb=708.4, bsz=708.4, num_updates=2050, lr=0.0004, gnorm=2.013, clip=0, train_wall=7, gb_free=71.4, wall=380 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:20:54]    INFO >> epoch 002:    566 / 1539 loss=3.321, wps=4333.2, ups=6.96, wpb=622.3, bsz=622.3, num_updates=2100, lr=0.0004, gnorm=1.854, clip=0, train_wall=7, gb_free=74.1, wall=387 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:21:02]    INFO >> epoch 002:    616 / 1539 loss=3.32, wps=5738.6, ups=6.59, wpb=871.1, bsz=871.1, num_updates=2150, lr=0.0004, gnorm=2.003, clip=0, train_wall=7, gb_free=68.9, wall=395 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:21:10]    INFO >> epoch 002:    666 / 1539 loss=3.173, wps=5054.8, ups=6.54, wpb=772.9, bsz=772.9, num_updates=2200, lr=0.0004, gnorm=1.873, clip=0, train_wall=7, gb_free=68.7, wall=402 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:21:17]    INFO >> epoch 002:    716 / 1539 loss=3.165, wps=4961.1, ups=7, wpb=708.6, bsz=708.6, num_updates=2250, lr=0.0004, gnorm=1.659, clip=0, train_wall=7, gb_free=73.2, wall=409 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:21:24]    INFO >> epoch 002:    766 / 1539 loss=3.175, wps=4577.6, ups=6.88, wpb=665.2, bsz=665.2, num_updates=2300, lr=0.0004, gnorm=1.742, clip=0, train_wall=7, gb_free=75.8, wall=417 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:21:32]    INFO >> epoch 002:    816 / 1539 loss=3.397, wps=4564.4, ups=6.96, wpb=656, bsz=656, num_updates=2350, lr=0.0004, gnorm=1.847, clip=0, train_wall=7, gb_free=72.3, wall=424 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:21:41]    INFO >> epoch 002:    866 / 1539 loss=3.431, wps=4674.7, ups=6.49, wpb=719.8, bsz=719.8, num_updates=2400, lr=0.0004, gnorm=2.08, clip=0, train_wall=7, gb_free=75.8, wall=432 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:21:48]    INFO >> epoch 002:    916 / 1539 loss=3.231, wps=4550.3, ups=6.78, wpb=671.5, bsz=671.5, num_updates=2450, lr=0.0004, gnorm=1.864, clip=0, train_wall=7, gb_free=73.2, wall=439 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:21:55]    INFO >> epoch 002:    966 / 1539 loss=3.198, wps=4408, ups=6.91, wpb=637.9, bsz=637.9, num_updates=2500, lr=0.0004, gnorm=1.746, clip=0, train_wall=7, gb_free=69.4, wall=446 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:21:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 5.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.05 GiB is allocated by PyTorch, and 1.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:21:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:21:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:21:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 11        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78842 MiB |  78902 MiB |  73881 GiB |  73804 GiB |
|       from large pool |  78450 MiB |  78510 MiB |  73502 GiB |  73425 GiB |
|       from small pool |    392 MiB |    393 MiB |    379 GiB |    379 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78842 MiB |  78902 MiB |  73881 GiB |  73804 GiB |
|       from large pool |  78450 MiB |  78510 MiB |  73502 GiB |  73425 GiB |
|       from small pool |    392 MiB |    393 MiB |    379 GiB |    379 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78748 MiB |  78807 MiB |  73766 GiB |  73689 GiB |
|       from large pool |  78358 MiB |  78417 MiB |  73387 GiB |  73310 GiB |
|       from small pool |    390 MiB |    391 MiB |    379 GiB |    378 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80500 MiB |  80502 MiB | 323002 MiB | 242502 MiB |
|       from large pool |  80068 MiB |  80068 MiB | 321628 MiB | 241560 MiB |
|       from small pool |    432 MiB |    434 MiB |   1374 MiB |    942 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1597 MiB |   5160 MiB |  70444 GiB |  70442 GiB |
|       from large pool |   1557 MiB |   5155 MiB |  70012 GiB |  70011 GiB |
|       from small pool |     39 MiB |     41 MiB |    431 GiB |    431 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7333    |    7336    |    4635 K  |    4628 K  |
|       from large pool |     887    |     888    |    2133 K  |    2132 K  |
|       from small pool |    6446    |    6449    |    2501 K  |    2495 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7333    |    7336    |    4635 K  |    4628 K  |
|       from large pool |     887    |     888    |    2133 K  |    2132 K  |
|       from small pool |    6446    |    6449    |    2501 K  |    2495 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     439    |     440    |    2043    |    1604    |
|       from large pool |     223    |     223    |    1356    |    1133    |
|       from small pool |     216    |     217    |     687    |     471    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     465    |     466    |    2590 K  |    2589 K  |
|       from large pool |      75    |      75    |    1385 K  |    1385 K  |
|       from small pool |     390    |     391    |    1204 K  |    1203 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:21:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:21:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 14:22:01] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 49.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:22:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:22:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:22:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 13        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78272 MiB |  78332 MiB |  74786 GiB |  74710 GiB |
|       from large pool |  78144 MiB |  78204 MiB |  74401 GiB |  74325 GiB |
|       from small pool |    127 MiB |    128 MiB |    384 GiB |    384 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78272 MiB |  78332 MiB |  74786 GiB |  74710 GiB |
|       from large pool |  78144 MiB |  78204 MiB |  74401 GiB |  74325 GiB |
|       from small pool |    127 MiB |    128 MiB |    384 GiB |    384 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78218 MiB |  78278 MiB |  74669 GiB |  74593 GiB |
|       from large pool |  78091 MiB |  78150 MiB |  74285 GiB |  74209 GiB |
|       from small pool |    127 MiB |    128 MiB |    384 GiB |    384 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80456 MiB |  80458 MiB | 329240 MiB | 248784 MiB |
|       from large pool |  80324 MiB |  80324 MiB | 327754 MiB | 247430 MiB |
|       from small pool |    132 MiB |    432 MiB |   1486 MiB |   1354 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2123 MiB |   9172 MiB |  71231 GiB |  71229 GiB |
|       from large pool |   2119 MiB |   9165 MiB |  70793 GiB |  70791 GiB |
|       from small pool |      4 MiB |     19 MiB |    438 GiB |    438 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2625    |    2628    |    4695 K  |    4692 K  |
|       from large pool |     473    |     474    |    2159 K  |    2158 K  |
|       from small pool |    2152    |    2155    |    2536 K  |    2534 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2625    |    2628    |    4695 K  |    4692 K  |
|       from large pool |     473    |     474    |    2159 K  |    2158 K  |
|       from small pool |    2152    |    2155    |    2536 K  |    2534 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     289    |     438    |    2178    |    1889    |
|       from large pool |     223    |     223    |    1435    |    1212    |
|       from small pool |      66    |     216    |     743    |     677    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     193    |     194    |    2627 K  |    2627 K  |
|       from large pool |     130    |     134    |    1403 K  |    1403 K  |
|       from small pool |      63    |      64    |    1223 K  |    1223 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:22:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:22:01] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:22:04]    INFO >> epoch 002:   1018 / 1539 loss=3.245, wps=4027.2, ups=5.85, wpb=687.9, bsz=687.9, num_updates=2550, lr=0.0004, gnorm=1.984, clip=0, train_wall=7, gb_free=73.1, wall=455 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:22:13]    INFO >> epoch 002:   1068 / 1539 loss=3.086, wps=4995.5, ups=6.54, wpb=763.7, bsz=763.7, num_updates=2600, lr=0.0004, gnorm=1.661, clip=0, train_wall=7, gb_free=73.7, wall=462 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:22:21]    INFO >> epoch 002:   1118 / 1539 loss=3.077, wps=5125.8, ups=6.47, wpb=791.9, bsz=791.9, num_updates=2650, lr=0.0004, gnorm=1.811, clip=0, train_wall=7, gb_free=69.4, wall=470 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:22:28]    INFO >> epoch 002:   1168 / 1539 loss=3.408, wps=5113.2, ups=6.64, wpb=770.1, bsz=770.1, num_updates=2700, lr=0.0004, gnorm=2.11, clip=0, train_wall=7, gb_free=72.5, wall=478 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:22:36]    INFO >> epoch 002:   1218 / 1539 loss=3.354, wps=4760.2, ups=6.7, wpb=710, bsz=710, num_updates=2750, lr=0.0004, gnorm=1.965, clip=0, train_wall=7, gb_free=71.1, wall=485 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:22:39] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 121.25 MiB is free. Including non-PyTorch memory, this process has 79.00 GiB memory in use. Of the allocated memory 76.03 GiB is allocated by PyTorch, and 2.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:22:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:22:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:22:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 15        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77368 MiB |  77855 MiB |  81624 GiB |  81548 GiB |
|       from large pool |  77357 MiB |  77844 MiB |  81204 GiB |  81129 GiB |
|       from small pool |     11 MiB |     14 MiB |    419 GiB |    419 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77368 MiB |  77855 MiB |  81624 GiB |  81548 GiB |
|       from large pool |  77357 MiB |  77844 MiB |  81204 GiB |  81129 GiB |
|       from small pool |     11 MiB |     14 MiB |    419 GiB |    419 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77354 MiB |  77841 MiB |  81497 GiB |  81422 GiB |
|       from large pool |  77342 MiB |  77830 MiB |  81078 GiB |  81003 GiB |
|       from small pool |     11 MiB |     14 MiB |    418 GiB |    418 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80384 MiB |  80482 MiB | 338664 MiB | 258280 MiB |
|       from large pool |  80362 MiB |  80362 MiB | 337092 MiB | 256730 MiB |
|       from small pool |     22 MiB |    218 MiB |   1572 MiB |   1550 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2745 MiB |   6422 MiB |  78054 GiB |  78052 GiB |
|       from large pool |   2734 MiB |   6411 MiB |  77576 GiB |  77573 GiB |
|       from small pool |     10 MiB |     17 MiB |    478 GiB |    478 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     564    |     568    |    5121 K  |    5120 K  |
|       from large pool |     279    |     283    |    2358 K  |    2358 K  |
|       from small pool |     285    |     356    |    2762 K  |    2762 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     564    |     568    |    5121 K  |    5120 K  |
|       from large pool |     279    |     283    |    2358 K  |    2358 K  |
|       from small pool |     285    |     356    |    2762 K  |    2762 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     331    |    2230    |    2142    |
|       from large pool |      77    |     222    |    1444    |    1367    |
|       from small pool |      11    |     109    |     786    |     775    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      98    |      99    |    2867 K  |    2867 K  |
|       from large pool |      75    |      76    |    1533 K  |    1533 K  |
|       from small pool |      23    |      49    |    1334 K  |    1334 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:22:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:22:39] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:22:45]    INFO >> epoch 002:   1269 / 1539 loss=3.188, wps=4624.8, ups=6.04, wpb=765.3, bsz=765.3, num_updates=2800, lr=0.0004, gnorm=1.733, clip=0, train_wall=7, gb_free=70.7, wall=493 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:22:52]    INFO >> epoch 002:   1319 / 1539 loss=3.163, wps=4735.1, ups=7.16, wpb=661.5, bsz=661.5, num_updates=2850, lr=0.0004, gnorm=1.675, clip=0, train_wall=7, gb_free=75.1, wall=500 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:22:59]    INFO >> epoch 002:   1369 / 1539 loss=3.271, wps=4994, ups=6.86, wpb=728.5, bsz=728.5, num_updates=2900, lr=0.0004, gnorm=1.842, clip=0, train_wall=7, gb_free=70.7, wall=508 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:23:07]    INFO >> epoch 002:   1419 / 1539 loss=3.207, wps=4570, ups=6.99, wpb=653.6, bsz=653.6, num_updates=2950, lr=0.0004, gnorm=1.682, clip=0, train_wall=7, gb_free=65.1, wall=515 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:23:16]    INFO >> epoch 002:   1469 / 1539 loss=3.294, wps=4365.3, ups=6.21, wpb=702.5, bsz=702.5, num_updates=3000, lr=0.0004, gnorm=1.686, clip=0, train_wall=7, gb_free=70.5, wall=523 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:23:24]    INFO >> epoch 002:   1519 / 1539 loss=3.269, wps=4452.7, ups=6.61, wpb=673.3, bsz=673.3, num_updates=3050, lr=0.0004, gnorm=1.835, clip=0, train_wall=7, gb_free=74.3, wall=530 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:23:27]    INFO >> epoch 002 | loss 3.248 | wps 4518.4 | ups 6.34 | wpb 712.7 | bsz 712.7 | num_updates 3070 | lr 0.0004 | gnorm 1.847 | clip 0 | train_wall 213 | gb_free 72.6 | wall 534 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:23:27] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:23:40]    INFO >> epoch 002 | valid on 'valid' subset | loss 3.466 | wps 11475.4 | wpb 5412.5 | bsz 5412.5 | num_updates 3070 | best_loss 3.481 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:23:40]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:23:40]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 2 @ 3070 updates, score 3.466) (writing took 0.017383 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:23:40] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:23:45]    INFO >> epoch 003:     30 / 1539 loss=3.286, wps=1622.1, ups=2.38, wpb=681.1, bsz=681.1, num_updates=3100, lr=0.000392, gnorm=1.839, clip=0, train_wall=7, gb_free=70.9, wall=551 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:23:53]    INFO >> epoch 003:     80 / 1539 loss=3.371, wps=5162.8, ups=6.68, wpb=772.8, bsz=772.8, num_updates=3150, lr=0.000392, gnorm=1.733, clip=0, train_wall=7, gb_free=73.6, wall=559 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:23:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 938.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 41.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.37 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:23:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:23:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:23:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 17        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73478 MiB |  79415 MiB |  96974 GiB |  96902 GiB |
|       from large pool |  73469 MiB |  79406 MiB |  96467 GiB |  96395 GiB |
|       from small pool |      8 MiB |     19 MiB |    506 GiB |    506 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73478 MiB |  79415 MiB |  96974 GiB |  96902 GiB |
|       from large pool |  73469 MiB |  79406 MiB |  96467 GiB |  96395 GiB |
|       from small pool |      8 MiB |     19 MiB |    506 GiB |    506 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73471 MiB |  79407 MiB |  96828 GiB |  96756 GiB |
|       from large pool |  73462 MiB |  79398 MiB |  96322 GiB |  96250 GiB |
|       from small pool |      8 MiB |     19 MiB |    506 GiB |    506 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80464 MiB |  80464 MiB | 339136 MiB | 258672 MiB |
|       from large pool |  80440 MiB |  80440 MiB | 337468 MiB | 257028 MiB |
|       from small pool |     24 MiB |    118 MiB |   1668 MiB |   1644 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4257 MiB |   5673 MiB |  93086 GiB |  93082 GiB |
|       from large pool |   4242 MiB |   5658 MiB |  92513 GiB |  92509 GiB |
|       from small pool |     15 MiB |     21 MiB |    572 GiB |    572 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     589    |    6061 K  |    6060 K  |
|       from large pool |     286    |     304    |    2725 K  |    2725 K  |
|       from small pool |     285    |     356    |    3335 K  |    3335 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     589    |    6061 K  |    6060 K  |
|       from large pool |     286    |     304    |    2725 K  |    2725 K  |
|       from small pool |     285    |     356    |    3335 K  |    3335 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     135    |    2279    |    2191    |
|       from large pool |      76    |      76    |    1445    |    1369    |
|       from small pool |      12    |      59    |     834    |     822    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      99    |      99    |    3402 K  |    3402 K  |
|       from large pool |      78    |      78    |    1755 K  |    1755 K  |
|       from small pool |      21    |      44    |    1646 K  |    1646 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:23:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:23:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:24:01]    INFO >> epoch 003:    131 / 1539 loss=3.282, wps=5229.4, ups=6.26, wpb=835, bsz=835, num_updates=3200, lr=0.000392, gnorm=1.931, clip=0, train_wall=7, gb_free=72, wall=567 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:24:09]    INFO >> epoch 003:    181 / 1539 loss=3.386, wps=4599.7, ups=7, wpb=656.8, bsz=656.8, num_updates=3250, lr=0.000392, gnorm=1.819, clip=0, train_wall=7, gb_free=73.2, wall=574 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:24:13] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 929.25 MiB is free. Including non-PyTorch memory, this process has 78.21 GiB memory in use. Of the allocated memory 72.51 GiB is allocated by PyTorch, and 5.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:24:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:24:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:24:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63159 MiB |  75129 MiB | 100307 GiB | 100246 GiB |
|       from large pool |  63148 MiB |  75118 MiB |  99782 GiB |  99721 GiB |
|       from small pool |     11 MiB |     21 MiB |    525 GiB |    525 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63159 MiB |  75129 MiB | 100307 GiB | 100246 GiB |
|       from large pool |  63148 MiB |  75118 MiB |  99782 GiB |  99721 GiB |
|       from small pool |     11 MiB |     21 MiB |    525 GiB |    525 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63147 MiB |  75114 MiB | 100157 GiB | 100095 GiB |
|       from large pool |  63136 MiB |  75103 MiB |  99632 GiB |  99571 GiB |
|       from small pool |     11 MiB |     21 MiB |    524 GiB |    524 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79576 MiB |  79818 MiB | 341218 MiB | 261642 MiB |
|       from large pool |  79554 MiB |  79614 MiB | 339370 MiB | 259816 MiB |
|       from small pool |     22 MiB |    204 MiB |   1848 MiB |   1826 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8788 MiB |  10848 MiB |  97046 GiB |  97037 GiB |
|       from large pool |   8777 MiB |  10837 MiB |  96452 GiB |  96443 GiB |
|       from small pool |     10 MiB |     21 MiB |    594 GiB |    594 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     515    |     557    |    6276 K  |    6275 K  |
|       from large pool |     230    |     272    |    2822 K  |    2822 K  |
|       from small pool |     285    |     356    |    3453 K  |    3453 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     515    |     557    |    6276 K  |    6275 K  |
|       from large pool |     230    |     272    |    2822 K  |    2822 K  |
|       from small pool |     285    |     356    |    3453 K  |    3453 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      86    |     178    |    2370    |    2284    |
|       from large pool |      75    |      76    |    1446    |    1371    |
|       from small pool |      11    |     102    |     924    |     913    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      97    |     100    |    3519 K  |    3519 K  |
|       from large pool |      76    |      79    |    1813 K  |    1812 K  |
|       from small pool |      21    |      47    |    1706 K  |    1706 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:24:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:24:13] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:24:17]    INFO >> epoch 003:    232 / 1539 loss=2.879, wps=4630.8, ups=6.04, wpb=767.2, bsz=767.2, num_updates=3300, lr=0.000392, gnorm=1.638, clip=0, train_wall=7, gb_free=74.1, wall=582 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:24:25]    INFO >> epoch 003:    282 / 1539 loss=3.195, wps=5151.2, ups=6.85, wpb=751.9, bsz=751.9, num_updates=3350, lr=0.000392, gnorm=1.763, clip=0, train_wall=7, gb_free=70.8, wall=590 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:24:33]    INFO >> epoch 003:    332 / 1539 loss=3.453, wps=4932.6, ups=6.24, wpb=790.4, bsz=790.4, num_updates=3400, lr=0.000392, gnorm=1.912, clip=0, train_wall=8, gb_free=73.9, wall=598 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:24:41]    INFO >> epoch 003:    382 / 1539 loss=3.248, wps=4421.9, ups=6.42, wpb=688.4, bsz=688.4, num_updates=3450, lr=0.000392, gnorm=1.807, clip=0, train_wall=7, gb_free=72.7, wall=605 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:24:49]    INFO >> epoch 003:    432 / 1539 loss=3.251, wps=4443.2, ups=6.75, wpb=657.8, bsz=657.8, num_updates=3500, lr=0.000392, gnorm=1.693, clip=0, train_wall=7, gb_free=66.5, wall=613 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:24:58]    INFO >> epoch 003:    482 / 1539 loss=3.095, wps=4916.8, ups=6.52, wpb=754, bsz=754, num_updates=3550, lr=0.000392, gnorm=1.833, clip=0, train_wall=7, gb_free=73.2, wall=620 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:25:05]    INFO >> epoch 003:    532 / 1539 loss=3.326, wps=5166.8, ups=6.71, wpb=770.5, bsz=770.5, num_updates=3600, lr=0.000392, gnorm=2.21, clip=0, train_wall=7, gb_free=73.9, wall=628 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:25:13]    INFO >> epoch 003:    582 / 1539 loss=3.182, wps=4742.2, ups=6.59, wpb=720.1, bsz=720.1, num_updates=3650, lr=0.000392, gnorm=1.6, clip=0, train_wall=7, gb_free=71.7, wall=636 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:25:20]    INFO >> epoch 003:    632 / 1539 loss=3.315, wps=4839.2, ups=7.17, wpb=674.5, bsz=674.5, num_updates=3700, lr=0.000392, gnorm=1.805, clip=0, train_wall=7, gb_free=66.7, wall=642 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:25:27]    INFO >> epoch 003:    682 / 1539 loss=3.297, wps=5017, ups=6.63, wpb=756.5, bsz=756.5, num_updates=3750, lr=0.000392, gnorm=1.706, clip=0, train_wall=7, gb_free=75.2, wall=650 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:25:37]    INFO >> epoch 003:    732 / 1539 loss=3.02, wps=4911.7, ups=6.06, wpb=810.3, bsz=810.3, num_updates=3800, lr=0.000392, gnorm=1.776, clip=0, train_wall=8, gb_free=74, wall=658 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:25:44]    INFO >> epoch 003:    782 / 1539 loss=2.907, wps=5233.6, ups=6.58, wpb=795, bsz=795, num_updates=3850, lr=0.000392, gnorm=1.572, clip=0, train_wall=7, gb_free=73.8, wall=666 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:25:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 35.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.28 GiB is allocated by PyTorch, and 1.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:25:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:25:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:25:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 20        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79072 MiB |  79132 MiB | 116837 GiB | 116759 GiB |
|       from large pool |  78937 MiB |  78997 MiB | 116228 GiB | 116151 GiB |
|       from small pool |    135 MiB |    136 MiB |    608 GiB |    608 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79072 MiB |  79132 MiB | 116837 GiB | 116759 GiB |
|       from large pool |  78937 MiB |  78997 MiB | 116228 GiB | 116151 GiB |
|       from small pool |    135 MiB |    136 MiB |    608 GiB |    608 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78999 MiB |  79059 MiB | 116664 GiB | 116587 GiB |
|       from large pool |  78864 MiB |  78924 MiB | 116056 GiB | 115979 GiB |
|       from small pool |    135 MiB |    136 MiB |    607 GiB |    607 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80470 MiB |  80472 MiB | 349842 MiB | 269372 MiB |
|       from large pool |  80330 MiB |  80330 MiB | 347774 MiB | 267444 MiB |
|       from small pool |    140 MiB |    238 MiB |   2068 MiB |   1928 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1337 MiB |   5589 MiB | 116735 GiB | 116734 GiB |
|       from large pool |   1332 MiB |   5580 MiB | 116044 GiB | 116043 GiB |
|       from small pool |      4 MiB |     19 MiB |    690 GiB |    690 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2768    |    2771    |    7313 K  |    7310 K  |
|       from large pool |     486    |     487    |    3311 K  |    3310 K  |
|       from small pool |    2282    |    2285    |    4001 K  |    3999 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2768    |    2771    |    7313 K  |    7310 K  |
|       from large pool |     486    |     487    |    3311 K  |    3310 K  |
|       from small pool |    2282    |    2285    |    4001 K  |    3999 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     267    |     314    |    2606    |    2339    |
|       from large pool |     197    |     197    |    1572    |    1375    |
|       from small pool |      70    |     119    |    1034    |     964    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     145    |     146    |    4075 K  |    4075 K  |
|       from large pool |      77    |      83    |    2103 K  |    2103 K  |
|       from small pool |      68    |      69    |    1972 K  |    1972 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:25:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:25:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:25:52]    INFO >> epoch 003:    833 / 1539 loss=3.417, wps=4663.7, ups=6.34, wpb=736, bsz=736, num_updates=3900, lr=0.000392, gnorm=1.868, clip=0, train_wall=7, gb_free=73.2, wall=674 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:26:00]    INFO >> epoch 003:    883 / 1539 loss=3.337, wps=4810.5, ups=6.71, wpb=717, bsz=717, num_updates=3950, lr=0.000392, gnorm=1.833, clip=0, train_wall=7, gb_free=72.7, wall=681 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:26:07]    INFO >> epoch 003:    933 / 1539 loss=3.133, wps=4765.7, ups=6.53, wpb=729.5, bsz=729.5, num_updates=4000, lr=0.000392, gnorm=1.706, clip=0, train_wall=7, gb_free=67.6, wall=689 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:26:16]    INFO >> epoch 003:    983 / 1539 loss=3.313, wps=3987.9, ups=5.83, wpb=684.3, bsz=684.3, num_updates=4050, lr=0.000392, gnorm=1.768, clip=0, train_wall=8, gb_free=71.7, wall=697 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:26:23]    INFO >> epoch 003:   1033 / 1539 loss=3.368, wps=4457.3, ups=7, wpb=636.3, bsz=636.3, num_updates=4100, lr=0.000392, gnorm=1.711, clip=0, train_wall=7, gb_free=68.5, wall=705 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:26:30]    INFO >> epoch 003:   1083 / 1539 loss=3.235, wps=4780.7, ups=7.09, wpb=673.9, bsz=673.9, num_updates=4150, lr=0.000392, gnorm=2.017, clip=0, train_wall=7, gb_free=73.1, wall=712 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:26:40] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 37.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.52 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:26:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:26:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:26:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 22        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78293 MiB |  78353 MiB | 126431 GiB | 126355 GiB |
|       from large pool |  77907 MiB |  77967 MiB | 125775 GiB | 125699 GiB |
|       from small pool |    385 MiB |    386 MiB |    656 GiB |    655 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78293 MiB |  78353 MiB | 126431 GiB | 126355 GiB |
|       from large pool |  77907 MiB |  77967 MiB | 125775 GiB | 125699 GiB |
|       from small pool |    385 MiB |    386 MiB |    656 GiB |    655 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78087 MiB |  78147 MiB | 126244 GiB | 126168 GiB |
|       from large pool |  77704 MiB |  77763 MiB | 125589 GiB | 125513 GiB |
|       from small pool |    383 MiB |    384 MiB |    655 GiB |    654 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80468 MiB |  80504 MiB | 366268 MiB | 285800 MiB |
|       from large pool |  80042 MiB |  80270 MiB | 363914 MiB | 283872 MiB |
|       from small pool |    426 MiB |    426 MiB |   2354 MiB |   1928 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2114 MiB |   5523 MiB | 125970 GiB | 125968 GiB |
|       from large pool |   2074 MiB |   5518 MiB | 125225 GiB | 125223 GiB |
|       from small pool |     40 MiB |     41 MiB |    745 GiB |    745 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7212    |    7215    |    7911 K  |    7904 K  |
|       from large pool |     876    |     877    |    3597 K  |    3596 K  |
|       from small pool |    6336    |    6339    |    4314 K  |    4307 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7212    |    7215    |    7911 K  |    7904 K  |
|       from large pool |     876    |     877    |    3597 K  |    3596 K  |
|       from small pool |    6336    |    6339    |    4314 K  |    4307 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     672    |     672    |    3018    |    2346    |
|       from large pool |     459    |     459    |    1841    |    1382    |
|       from small pool |     213    |     213    |    1177    |     964    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     485    |     485    |    4407 K  |    4406 K  |
|       from large pool |     101    |     101    |    2289 K  |    2289 K  |
|       from small pool |     384    |     384    |    2117 K  |    2117 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:26:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:26:40] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:26:41]    INFO >> epoch 003:   1134 / 1539 loss=3.187, wps=4366, ups=6.38, wpb=684.8, bsz=684.8, num_updates=4200, lr=0.000392, gnorm=1.807, clip=0, train_wall=7, gb_free=73.2, wall=719 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:26:48]    INFO >> epoch 003:   1184 / 1539 loss=3.212, wps=4585.9, ups=6.81, wpb=673.6, bsz=673.6, num_updates=4250, lr=0.000392, gnorm=1.707, clip=0, train_wall=7, gb_free=73.6, wall=727 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:26:56]    INFO >> epoch 003:   1234 / 1539 loss=3.356, wps=4749.6, ups=6.61, wpb=718.1, bsz=718.1, num_updates=4300, lr=0.000392, gnorm=2.167, clip=0, train_wall=7, gb_free=70.7, wall=734 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:27:03]    INFO >> epoch 003:   1284 / 1539 loss=3.075, wps=4421.2, ups=7.13, wpb=619.7, bsz=619.7, num_updates=4350, lr=0.000392, gnorm=1.682, clip=0, train_wall=7, gb_free=73.7, wall=741 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:27:11]    INFO >> epoch 003:   1334 / 1539 loss=3.202, wps=4986.8, ups=7.04, wpb=708, bsz=708, num_updates=4400, lr=0.000392, gnorm=2.092, clip=0, train_wall=7, gb_free=72.3, wall=748 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:27:18]    INFO >> epoch 003:   1384 / 1539 loss=3.219, wps=4985.9, ups=7.2, wpb=692.7, bsz=692.7, num_updates=4450, lr=0.000392, gnorm=1.674, clip=0, train_wall=7, gb_free=74.4, wall=755 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:27:25]    INFO >> epoch 003:   1434 / 1539 loss=3.298, wps=4702.2, ups=7.22, wpb=651.7, bsz=651.7, num_updates=4500, lr=0.000392, gnorm=1.803, clip=0, train_wall=7, gb_free=72.3, wall=762 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:27:32]    INFO >> epoch 003:   1484 / 1539 loss=3.172, wps=4382.1, ups=6.65, wpb=658.9, bsz=658.9, num_updates=4550, lr=0.000392, gnorm=1.783, clip=0, train_wall=7, gb_free=72.6, wall=770 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:27:40]    INFO >> epoch 003:   1534 / 1539 loss=3.407, wps=4675.8, ups=7.05, wpb=662.9, bsz=662.9, num_updates=4600, lr=0.000392, gnorm=1.927, clip=0, train_wall=7, gb_free=72.2, wall=777 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:27:40]    INFO >> epoch 003 | loss 3.235 | wps 4482.5 | ups 6.29 | wpb 712.7 | bsz 712.7 | num_updates 4605 | lr 0.000392 | gnorm 1.807 | clip 0 | train_wall 214 | gb_free 74.5 | wall 778 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:27:40] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:27:55]    INFO >> epoch 003 | valid on 'valid' subset | loss 3.479 | wps 11260.7 | wpb 5412.5 | bsz 5412.5 | num_updates 4605 | best_loss 3.481 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:27:55]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:27:55]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 3 @ 4605 updates, score 3.479) (writing took 0.013638 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:27:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:28:02]    INFO >> epoch 004:     45 / 1539 loss=3.161, wps=1763.5, ups=2.35, wpb=751.7, bsz=751.7, num_updates=4650, lr=0.000376, gnorm=1.937, clip=0, train_wall=7, gb_free=72.9, wall=798 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:28:10]    INFO >> epoch 004:     95 / 1539 loss=3.206, wps=4974.4, ups=6.58, wpb=756, bsz=756, num_updates=4700, lr=0.000376, gnorm=1.753, clip=0, train_wall=7, gb_free=68.1, wall=806 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:28:21]    INFO >> epoch 004:    145 / 1539 loss=2.93, wps=5310.1, ups=5.06, wpb=1048.6, bsz=1048.6, num_updates=4750, lr=0.000376, gnorm=1.962, clip=0, train_wall=9, gb_free=75.2, wall=816 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:28:28]    INFO >> epoch 004:    195 / 1539 loss=3.074, wps=5001.7, ups=7.13, wpb=701.3, bsz=701.3, num_updates=4800, lr=0.000376, gnorm=1.723, clip=0, train_wall=7, gb_free=72, wall=823 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:28:36]    INFO >> epoch 004:    245 / 1539 loss=3.353, wps=4946.7, ups=6.65, wpb=743.6, bsz=743.6, num_updates=4850, lr=0.000376, gnorm=1.996, clip=0, train_wall=7, gb_free=73.7, wall=830 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:28:43]    INFO >> epoch 004:    295 / 1539 loss=3.24, wps=4644.4, ups=7.17, wpb=647.4, bsz=647.4, num_updates=4900, lr=0.000376, gnorm=1.761, clip=0, train_wall=7, gb_free=74.8, wall=837 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:28:50]    INFO >> epoch 004:    345 / 1539 loss=3.12, wps=4788.4, ups=6.97, wpb=687.4, bsz=687.4, num_updates=4950, lr=0.000376, gnorm=1.669, clip=0, train_wall=7, gb_free=70.1, wall=844 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:28:57]    INFO >> epoch 004:    395 / 1539 loss=3.122, wps=5141.1, ups=6.75, wpb=761.1, bsz=761.1, num_updates=5000, lr=0.000376, gnorm=1.728, clip=0, train_wall=7, gb_free=68.1, wall=852 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:29:05]    INFO >> epoch 004:    445 / 1539 loss=3.428, wps=3952.6, ups=6.27, wpb=630.5, bsz=630.5, num_updates=5050, lr=0.000376, gnorm=1.83, clip=0, train_wall=7, gb_free=71.6, wall=860 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:29:12]    INFO >> epoch 004:    495 / 1539 loss=3.13, wps=4923.1, ups=6.84, wpb=720.1, bsz=720.1, num_updates=5100, lr=0.000376, gnorm=1.638, clip=0, train_wall=7, gb_free=74.1, wall=867 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:29:20]    INFO >> epoch 004:    545 / 1539 loss=3.281, wps=4674.9, ups=6.66, wpb=701.6, bsz=701.6, num_updates=5150, lr=0.000376, gnorm=1.892, clip=0, train_wall=7, gb_free=73.6, wall=875 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:29:27]    INFO >> epoch 004:    595 / 1539 loss=3.194, wps=4168.3, ups=6.62, wpb=629.8, bsz=629.8, num_updates=5200, lr=0.000376, gnorm=1.815, clip=0, train_wall=7, gb_free=71.2, wall=882 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:29:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 25.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.75 GiB is allocated by PyTorch, and 1.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:29:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:29:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:29:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 24        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78527 MiB |  78587 MiB | 158536 GiB | 158459 GiB |
|       from large pool |  78397 MiB |  78457 MiB | 157709 GiB | 157632 GiB |
|       from small pool |    130 MiB |    131 MiB |    827 GiB |    827 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78527 MiB |  78587 MiB | 158536 GiB | 158459 GiB |
|       from large pool |  78397 MiB |  78457 MiB | 157709 GiB | 157632 GiB |
|       from small pool |    130 MiB |    131 MiB |    827 GiB |    827 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78458 MiB |  78518 MiB | 158303 GiB | 158227 GiB |
|       from large pool |  78329 MiB |  78388 MiB | 157477 GiB | 157401 GiB |
|       from small pool |    129 MiB |    130 MiB |    826 GiB |    826 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80480 MiB |  80482 MiB | 386756 MiB | 306276 MiB |
|       from large pool |  80346 MiB |  80346 MiB | 384288 MiB | 303942 MiB |
|       from small pool |    134 MiB |    426 MiB |   2468 MiB |   2334 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1892 MiB |   9014 MiB | 155328 GiB | 155326 GiB |
|       from large pool |   1888 MiB |   9007 MiB | 154391 GiB | 154389 GiB |
|       from small pool |      3 MiB |     21 MiB |    937 GiB |    937 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2669    |    2672    |    9920 K  |    9917 K  |
|       from large pool |     477    |     478    |    4476 K  |    4476 K  |
|       from small pool |    2192    |    2195    |    5443 K  |    5441 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2669    |    2672    |    9920 K  |    9917 K  |
|       from large pool |     477    |     478    |    4476 K  |    4476 K  |
|       from small pool |    2192    |    2195    |    5443 K  |    5441 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     316    |     671    |    3196    |    2880    |
|       from large pool |     249    |     458    |    1962    |    1713    |
|       from small pool |      67    |     213    |    1234    |    1167    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     191    |     192    |    5544 K  |    5544 K  |
|       from large pool |     125    |     125    |    2861 K  |    2861 K  |
|       from small pool |      66    |      67    |    2682 K  |    2682 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:29:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:29:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:29:36]    INFO >> epoch 004:    646 / 1539 loss=3.25, wps=4037.4, ups=6.02, wpb=671.1, bsz=671.1, num_updates=5250, lr=0.000376, gnorm=1.703, clip=0, train_wall=7, gb_free=69, wall=890 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:29:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 938.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 41.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.37 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:29:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:29:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:29:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73478 MiB |  79417 MiB | 160997 GiB | 160925 GiB |
|       from large pool |  73469 MiB |  79408 MiB | 160159 GiB | 160087 GiB |
|       from small pool |      8 MiB |     24 MiB |    838 GiB |    838 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73478 MiB |  79417 MiB | 160997 GiB | 160925 GiB |
|       from large pool |  73469 MiB |  79408 MiB | 160159 GiB | 160087 GiB |
|       from small pool |      8 MiB |     24 MiB |    838 GiB |    838 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73471 MiB |  79407 MiB | 160761 GiB | 160689 GiB |
|       from large pool |  73462 MiB |  79398 MiB | 159924 GiB | 159852 GiB |
|       from small pool |      8 MiB |     24 MiB |    837 GiB |    837 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80464 MiB |  80464 MiB | 442508 MiB | 362044 MiB |
|       from large pool |  80440 MiB |  80440 MiB | 440040 MiB | 359600 MiB |
|       from small pool |     24 MiB |    134 MiB |   2468 MiB |   2444 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3505 MiB |   5366 MiB | 157616 GiB | 157613 GiB |
|       from large pool |   3490 MiB |   5351 MiB | 156666 GiB | 156663 GiB |
|       from small pool |     15 MiB |     23 MiB |    949 GiB |    949 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     589    |   10069 K  |   10068 K  |
|       from large pool |     286    |     304    |    4552 K  |    4551 K  |
|       from small pool |     285    |     356    |    5517 K  |    5516 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     589    |   10069 K  |   10068 K  |
|       from large pool |     286    |     304    |    4552 K  |    4551 K  |
|       from small pool |     285    |     356    |    5517 K  |    5516 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     131    |     315    |    3254    |    3123    |
|       from large pool |     119    |     248    |    2020    |    1901    |
|       from small pool |      12    |      67    |    1234    |    1222    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     139    |     139    |    5625 K  |    5625 K  |
|       from large pool |     115    |     115    |    2910 K  |    2910 K  |
|       from small pool |      24    |      48    |    2714 K  |    2714 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:29:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:29:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:29:45]    INFO >> epoch 004:    697 / 1539 loss=3.29, wps=3481.9, ups=5.47, wpb=636.6, bsz=636.6, num_updates=5300, lr=0.000376, gnorm=1.785, clip=0, train_wall=7, gb_free=68, wall=900 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:29:53]    INFO >> epoch 004:    747 / 1539 loss=3.111, wps=4005.9, ups=5.95, wpb=673.1, bsz=673.1, num_updates=5350, lr=0.000376, gnorm=1.529, clip=0, train_wall=8, gb_free=73.1, wall=908 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:30:01]    INFO >> epoch 004:    797 / 1539 loss=3.186, wps=4788.3, ups=6.74, wpb=710.1, bsz=710.1, num_updates=5400, lr=0.000376, gnorm=1.577, clip=0, train_wall=7, gb_free=72.4, wall=915 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:30:08]    INFO >> epoch 004:    847 / 1539 loss=3.256, wps=5061.2, ups=6.77, wpb=747.5, bsz=747.5, num_updates=5450, lr=0.000376, gnorm=1.644, clip=0, train_wall=7, gb_free=63.9, wall=923 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:30:16]    INFO >> epoch 004:    897 / 1539 loss=3.229, wps=4956, ups=6.37, wpb=778.3, bsz=778.3, num_updates=5500, lr=0.000376, gnorm=1.904, clip=0, train_wall=7, gb_free=70.4, wall=931 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:30:23]    INFO >> epoch 004:    947 / 1539 loss=3.117, wps=4587.6, ups=7.09, wpb=646.7, bsz=646.7, num_updates=5550, lr=0.000376, gnorm=1.636, clip=0, train_wall=7, gb_free=67.8, wall=938 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:30:36]    INFO >> epoch 004:    997 / 1539 loss=3.494, wps=4805.6, ups=6.28, wpb=764.9, bsz=764.9, num_updates=5600, lr=0.000376, gnorm=1.803, clip=0, train_wall=8, gb_free=75.6, wall=946 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:30:44]    INFO >> epoch 004:   1047 / 1539 loss=3.374, wps=4361.9, ups=6.94, wpb=628.6, bsz=628.6, num_updates=5650, lr=0.000376, gnorm=1.863, clip=0, train_wall=7, gb_free=72.6, wall=953 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:30:51]    INFO >> epoch 004:   1097 / 1539 loss=3.115, wps=4948.4, ups=6.78, wpb=729.6, bsz=729.6, num_updates=5700, lr=0.000376, gnorm=1.789, clip=0, train_wall=7, gb_free=71.3, wall=960 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:30:58]    INFO >> epoch 004:   1147 / 1539 loss=3.307, wps=4638.4, ups=6.86, wpb=676.4, bsz=676.4, num_updates=5750, lr=0.000376, gnorm=1.675, clip=0, train_wall=7, gb_free=57, wall=968 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:31:07]    INFO >> epoch 004:   1197 / 1539 loss=3.323, wps=4938.6, ups=6.54, wpb=754.7, bsz=754.7, num_updates=5800, lr=0.000376, gnorm=1.719, clip=0, train_wall=7, gb_free=75.7, wall=975 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:31:14]    INFO >> epoch 004:   1247 / 1539 loss=3.163, wps=4501.7, ups=7.17, wpb=627.8, bsz=627.8, num_updates=5850, lr=0.000376, gnorm=1.532, clip=0, train_wall=7, gb_free=75.7, wall=982 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:31:22]    INFO >> epoch 004:   1297 / 1539 loss=3.244, wps=4174.1, ups=6.48, wpb=644.6, bsz=644.6, num_updates=5900, lr=0.000376, gnorm=1.777, clip=0, train_wall=7, gb_free=74.7, wall=990 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:31:30]    INFO >> epoch 004:   1347 / 1539 loss=3.18, wps=5164, ups=6.45, wpb=801.1, bsz=801.1, num_updates=5950, lr=0.000376, gnorm=1.908, clip=0, train_wall=7, gb_free=67.1, wall=998 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:31:37] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 711.25 MiB is free. Including non-PyTorch memory, this process has 78.42 GiB memory in use. Of the allocated memory 72.51 GiB is allocated by PyTorch, and 5.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:31:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:31:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:31:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63159 MiB |  75129 MiB | 180473 GiB | 180411 GiB |
|       from large pool |  63148 MiB |  75118 MiB | 179542 GiB | 179480 GiB |
|       from small pool |     11 MiB |     13 MiB |    931 GiB |    931 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63159 MiB |  75129 MiB | 180473 GiB | 180411 GiB |
|       from large pool |  63148 MiB |  75118 MiB | 179542 GiB | 179480 GiB |
|       from small pool |     11 MiB |     13 MiB |    931 GiB |    931 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63147 MiB |  75114 MiB | 180209 GiB | 180147 GiB |
|       from large pool |  63136 MiB |  75103 MiB | 179279 GiB | 179217 GiB |
|       from small pool |     11 MiB |     13 MiB |    930 GiB |    930 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79794 MiB |  79794 MiB | 456346 MiB | 376552 MiB |
|       from large pool |  79772 MiB |  79772 MiB | 453664 MiB | 373892 MiB |
|       from small pool |     22 MiB |    238 MiB |   2682 MiB |   2660 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8518 MiB |  11894 MiB | 178616 GiB | 178607 GiB |
|       from large pool |   8507 MiB |  11883 MiB | 177559 GiB | 177551 GiB |
|       from small pool |     10 MiB |     17 MiB |   1056 GiB |   1056 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     515    |     557    |   11266 K  |   11265 K  |
|       from large pool |     230    |     272    |    5136 K  |    5136 K  |
|       from small pool |     285    |     336    |    6130 K  |    6129 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     515    |     557    |   11266 K  |   11265 K  |
|       from large pool |     230    |     272    |    5136 K  |    5136 K  |
|       from small pool |     285    |     336    |    6130 K  |    6129 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      99    |     235    |    3373    |    3274    |
|       from large pool |      88    |     116    |    2032    |    1944    |
|       from small pool |      11    |     119    |    1341    |    1330    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     106    |     107    |    6265 K  |    6265 K  |
|       from large pool |      81    |      82    |    3270 K  |    3270 K  |
|       from small pool |      25    |      38    |    2995 K  |    2995 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:31:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:31:37] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:31:40]    INFO >> epoch 004:   1398 / 1539 loss=3.219, wps=4583.6, ups=5.79, wpb=791.5, bsz=791.5, num_updates=6000, lr=0.000376, gnorm=1.912, clip=0, train_wall=7, gb_free=71.8, wall=1006 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:31:47]    INFO >> epoch 004:   1448 / 1539 loss=3.351, wps=4999.8, ups=7.02, wpb=711.7, bsz=711.7, num_updates=6050, lr=0.000376, gnorm=1.803, clip=0, train_wall=7, gb_free=73.7, wall=1013 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:31:54]    INFO >> epoch 004:   1498 / 1539 loss=3.386, wps=4667.3, ups=6.84, wpb=682, bsz=682, num_updates=6100, lr=0.000376, gnorm=1.961, clip=0, train_wall=7, gb_free=74.3, wall=1021 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:31:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.99 GiB is allocated by PyTorch, and 1.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:31:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:31:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:31:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 29        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78779 MiB |  78839 MiB | 183746 GiB | 183669 GiB |
|       from large pool |  78387 MiB |  78447 MiB | 182796 GiB | 182719 GiB |
|       from small pool |    391 MiB |    392 MiB |    950 GiB |    950 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78779 MiB |  78839 MiB | 183746 GiB | 183669 GiB |
|       from large pool |  78387 MiB |  78447 MiB | 182796 GiB | 182719 GiB |
|       from small pool |    391 MiB |    392 MiB |    950 GiB |    950 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78688 MiB |  78747 MiB | 183478 GiB | 183401 GiB |
|       from large pool |  78298 MiB |  78358 MiB | 182529 GiB | 182452 GiB |
|       from small pool |    389 MiB |    390 MiB |    949 GiB |    948 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 465158 MiB | 384670 MiB |
|       from large pool |  80056 MiB |  80056 MiB | 462064 MiB | 382008 MiB |
|       from small pool |    432 MiB |    434 MiB |   3094 MiB |   2662 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1648 MiB |   5018 MiB | 182378 GiB | 182376 GiB |
|       from large pool |   1608 MiB |   5012 MiB | 181299 GiB | 181298 GiB |
|       from small pool |     40 MiB |     41 MiB |   1078 GiB |   1078 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7322    |    7325    |   11491 K  |   11483 K  |
|       from large pool |     886    |     887    |    5235 K  |    5234 K  |
|       from small pool |    6436    |    6439    |    6255 K  |    6249 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7322    |    7325    |   11491 K  |   11483 K  |
|       from large pool |     886    |     887    |    5235 K  |    5234 K  |
|       from small pool |    6436    |    6439    |    6255 K  |    6249 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     436    |     437    |    3719    |    3283    |
|       from large pool |     220    |     220    |    2172    |    1952    |
|       from small pool |     216    |     217    |    1547    |    1331    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     466    |     467    |    6386 K  |    6385 K  |
|       from large pool |      76    |      76    |    3329 K  |    3329 K  |
|       from small pool |     390    |     391    |    3056 K  |    3056 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:31:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:31:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:32:00]    INFO >> epoch 004 | loss 3.227 | wps 4390.1 | ups 6.16 | wpb 712.7 | bsz 712.7 | num_updates 6140 | lr 0.000376 | gnorm 1.77 | clip 0 | train_wall 217 | gb_free 70.3 | wall 1027 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:32:00] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:32:15]    INFO >> epoch 004 | valid on 'valid' subset | loss 3.486 | wps 10873.6 | wpb 5412.5 | bsz 5412.5 | num_updates 6140 | best_loss 3.486 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:32:16]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:32:16]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_best.pt (epoch 4 @ 6140 updates, score 3.486) (writing took 0.018010 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:32:16] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:32:17]    INFO >> epoch 005:     10 / 1539 loss=3.447, wps=1474.5, ups=2.29, wpb=643.2, bsz=643.2, num_updates=6150, lr=0.000354, gnorm=1.573, clip=0, train_wall=7, gb_free=72.3, wall=1043 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:32:25]    INFO >> epoch 005:     60 / 1539 loss=3.174, wps=4465, ups=6.34, wpb=704.5, bsz=704.5, num_updates=6200, lr=0.000354, gnorm=1.6, clip=0, train_wall=7, gb_free=73.5, wall=1050 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:32:33]    INFO >> epoch 005:    110 / 1539 loss=3.241, wps=4716.4, ups=6.69, wpb=705.4, bsz=705.4, num_updates=6250, lr=0.000354, gnorm=1.723, clip=0, train_wall=7, gb_free=71.7, wall=1058 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:32:40]    INFO >> epoch 005:    160 / 1539 loss=3.215, wps=4478.2, ups=7, wpb=639.6, bsz=639.6, num_updates=6300, lr=0.000354, gnorm=1.674, clip=0, train_wall=7, gb_free=74.1, wall=1065 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:32:48]    INFO >> epoch 005:    210 / 1539 loss=3.124, wps=4561.2, ups=6.79, wpb=671.6, bsz=671.6, num_updates=6350, lr=0.000354, gnorm=1.6, clip=0, train_wall=7, gb_free=71.2, wall=1072 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:32:57]    INFO >> epoch 005:    260 / 1539 loss=3.358, wps=5419.6, ups=6.05, wpb=895.2, bsz=895.2, num_updates=6400, lr=0.000354, gnorm=1.79, clip=0, train_wall=8, gb_free=68.1, wall=1081 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:33:04]    INFO >> epoch 005:    310 / 1539 loss=3.149, wps=4808.5, ups=6.82, wpb=704.8, bsz=704.8, num_updates=6450, lr=0.000354, gnorm=1.685, clip=0, train_wall=7, gb_free=72, wall=1088 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:33:12]    INFO >> epoch 005:    360 / 1539 loss=2.798, wps=5052.1, ups=6.54, wpb=772.8, bsz=772.8, num_updates=6500, lr=0.000354, gnorm=1.684, clip=0, train_wall=7, gb_free=69.7, wall=1096 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:33:18] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 27.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.15 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:33:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:33:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:33:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77914 MiB |  77974 MiB | 199835 GiB | 199759 GiB |
|       from large pool |  77531 MiB |  77591 MiB | 198785 GiB | 198709 GiB |
|       from small pool |    383 MiB |    384 MiB |   1050 GiB |   1050 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77914 MiB |  77974 MiB | 199835 GiB | 199759 GiB |
|       from large pool |  77531 MiB |  77591 MiB | 198785 GiB | 198709 GiB |
|       from small pool |    383 MiB |    384 MiB |   1050 GiB |   1050 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77847 MiB |  77907 MiB | 199543 GiB | 199467 GiB |
|       from large pool |  77466 MiB |  77526 MiB | 198494 GiB | 198418 GiB |
|       from small pool |    380 MiB |    382 MiB |   1048 GiB |   1048 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80478 MiB |  80488 MiB | 465218 MiB | 384740 MiB |
|       from large pool |  80056 MiB |  80056 MiB | 462124 MiB | 382068 MiB |
|       from small pool |    422 MiB |    432 MiB |   3094 MiB |   2672 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2503 MiB |   6459 MiB | 195877 GiB | 195875 GiB |
|       from large pool |   2464 MiB |   6453 MiB | 194688 GiB | 194686 GiB |
|       from small pool |     38 MiB |     40 MiB |   1189 GiB |   1189 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7168    |    7171    |   12551 K  |   12543 K  |
|       from large pool |     872    |     873    |    5636 K  |    5636 K  |
|       from small pool |    6296    |    6299    |    6914 K  |    6907 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7168    |    7171    |   12551 K  |   12543 K  |
|       from large pool |     872    |     873    |    5636 K  |    5636 K  |
|       from small pool |    6296    |    6299    |    6914 K  |    6907 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     431    |     436    |    3720    |    3289    |
|       from large pool |     220    |     220    |    2173    |    1953    |
|       from small pool |     211    |     216    |    1547    |    1336    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     493    |     494    |    7015 K  |    7015 K  |
|       from large pool |     111    |     111    |    3594 K  |    3594 K  |
|       from small pool |     382    |     383    |    3421 K  |    3421 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:33:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:33:18] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:33:19]    INFO >> epoch 005:    411 / 1539 loss=3.297, wps=4546.5, ups=6.67, wpb=681.6, bsz=681.6, num_updates=6550, lr=0.000354, gnorm=1.739, clip=0, train_wall=7, gb_free=74, wall=1103 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:33:28]    INFO >> epoch 005:    461 / 1539 loss=3.283, wps=4206.3, ups=6.92, wpb=608, bsz=608, num_updates=6600, lr=0.000354, gnorm=1.7, clip=0, train_wall=7, gb_free=74.8, wall=1110 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:33:40]    INFO >> epoch 005:    511 / 1539 loss=3.1, wps=3054.2, ups=4.18, wpb=730.5, bsz=730.5, num_updates=6650, lr=0.000354, gnorm=1.599, clip=0, train_wall=12, gb_free=70.1, wall=1122 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:33:47]    INFO >> epoch 005:    561 / 1539 loss=3.375, wps=4156, ups=6.7, wpb=620.7, bsz=620.7, num_updates=6700, lr=0.000354, gnorm=1.673, clip=0, train_wall=7, gb_free=72.7, wall=1130 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:33:52] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.55 GiB is allocated by PyTorch, and 2.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:33:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:33:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:33:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78329 MiB |  78389 MiB | 204932 GiB | 204855 GiB |
|       from large pool |  78200 MiB |  78260 MiB | 203857 GiB | 203781 GiB |
|       from small pool |    128 MiB |    129 MiB |   1074 GiB |   1074 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78329 MiB |  78389 MiB | 204932 GiB | 204855 GiB |
|       from large pool |  78200 MiB |  78260 MiB | 203857 GiB | 203781 GiB |
|       from small pool |    128 MiB |    129 MiB |   1074 GiB |   1074 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78278 MiB |  78338 MiB | 204632 GiB | 204556 GiB |
|       from large pool |  78150 MiB |  78210 MiB | 203559 GiB | 203483 GiB |
|       from small pool |    127 MiB |    129 MiB |   1072 GiB |   1072 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 465582 MiB | 385094 MiB |
|       from large pool |  80356 MiB |  80356 MiB | 462484 MiB | 382128 MiB |
|       from small pool |    132 MiB |    422 MiB |   3098 MiB |   2966 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2098 MiB |   8462 MiB | 200686 GiB | 200684 GiB |
|       from large pool |   2095 MiB |   8453 MiB | 199469 GiB | 199467 GiB |
|       from small pool |      3 MiB |     19 MiB |   1216 GiB |   1216 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2636    |    2639    |   12862 K  |   12859 K  |
|       from large pool |     474    |     475    |    5790 K  |    5790 K  |
|       from small pool |    2162    |    2165    |    7071 K  |    7069 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2636    |    2639    |   12862 K  |   12859 K  |
|       from large pool |     474    |     475    |    5790 K  |    5790 K  |
|       from small pool |    2162    |    2165    |    7071 K  |    7069 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     291    |     431    |    3728    |    3437    |
|       from large pool |     225    |     225    |    2179    |    1954    |
|       from small pool |      66    |     211    |    1549    |    1483    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     204    |     205    |    7188 K  |    7188 K  |
|       from large pool |     140    |     141    |    3694 K  |    3694 K  |
|       from small pool |      64    |      65    |    3493 K  |    3493 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:33:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:33:52] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:33:57]    INFO >> epoch 005:    612 / 1539 loss=3.321, wps=4505.6, ups=5.96, wpb=756, bsz=756, num_updates=6750, lr=0.000354, gnorm=1.707, clip=0, train_wall=7, gb_free=70.3, wall=1138 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:34:04]    INFO >> epoch 005:    662 / 1539 loss=3.341, wps=4941.8, ups=6.71, wpb=736.8, bsz=736.8, num_updates=6800, lr=0.000354, gnorm=1.6, clip=0, train_wall=7, gb_free=74.3, wall=1146 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:34:12]    INFO >> epoch 005:    712 / 1539 loss=3.36, wps=4667.9, ups=6.4, wpb=729, bsz=729, num_updates=6850, lr=0.000354, gnorm=2.002, clip=0, train_wall=7, gb_free=71.6, wall=1153 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:34:20]    INFO >> epoch 005:    762 / 1539 loss=3.25, wps=4888.6, ups=6.82, wpb=717.1, bsz=717.1, num_updates=6900, lr=0.000354, gnorm=1.747, clip=0, train_wall=7, gb_free=73.1, wall=1161 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:34:29]    INFO >> epoch 005:    812 / 1539 loss=3.294, wps=4158, ups=6.17, wpb=673.7, bsz=673.7, num_updates=6950, lr=0.000354, gnorm=1.945, clip=0, train_wall=8, gb_free=69.9, wall=1169 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:34:36]    INFO >> epoch 005:    862 / 1539 loss=3.024, wps=4574.7, ups=6.82, wpb=670.9, bsz=670.9, num_updates=7000, lr=0.000354, gnorm=1.588, clip=0, train_wall=7, gb_free=66.7, wall=1176 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:34:44]    INFO >> epoch 005:    912 / 1539 loss=3.222, wps=4405.4, ups=6.6, wpb=667, bsz=667, num_updates=7050, lr=0.000354, gnorm=1.77, clip=0, train_wall=7, gb_free=71.9, wall=1184 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:34:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.79 GiB is free. Including non-PyTorch memory, this process has 77.33 GiB memory in use. Of the allocated memory 71.77 GiB is allocated by PyTorch, and 5.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:34:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:34:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:34:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 34        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73095 MiB |  73493 MiB | 214939 GiB | 214868 GiB |
|       from large pool |  73086 MiB |  73484 MiB | 213819 GiB | 213747 GiB |
|       from small pool |      8 MiB |     24 MiB |   1120 GiB |   1120 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73095 MiB |  73493 MiB | 214939 GiB | 214868 GiB |
|       from large pool |  73086 MiB |  73484 MiB | 213819 GiB | 213747 GiB |
|       from small pool |      8 MiB |     24 MiB |   1120 GiB |   1120 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73086 MiB |  73484 MiB | 214624 GiB | 214553 GiB |
|       from large pool |  73078 MiB |  73475 MiB | 213506 GiB | 213434 GiB |
|       from small pool |      8 MiB |     24 MiB |   1118 GiB |   1118 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78676 MiB |  80428 MiB | 469248 MiB | 390572 MiB |
|       from large pool |  78652 MiB |  80296 MiB | 466150 MiB | 387498 MiB |
|       from small pool |     24 MiB |    132 MiB |   3098 MiB |   3074 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5580 MiB |   7715 MiB | 210005 GiB | 210000 GiB |
|       from large pool |   5565 MiB |   7700 MiB | 208736 GiB | 208730 GiB |
|       from small pool |     15 MiB |     21 MiB |   1269 GiB |   1269 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     577    |   13466 K  |   13465 K  |
|       from large pool |     286    |     292    |    6090 K  |    6090 K  |
|       from small pool |     285    |     356    |    7375 K  |    7375 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     577    |   13466 K  |   13465 K  |
|       from large pool |     286    |     292    |    6090 K  |    6090 K  |
|       from small pool |     285    |     356    |    7375 K  |    7375 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     153    |     290    |    3730    |    3577    |
|       from large pool |     141    |     224    |    2181    |    2040    |
|       from small pool |      12    |      66    |    1549    |    1537    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     158    |     158    |    7520 K  |    7520 K  |
|       from large pool |     136    |     136    |    3889 K  |    3889 K  |
|       from small pool |      22    |      46    |    3630 K  |    3630 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:34:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:34:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:34:52]    INFO >> epoch 005:    963 / 1539 loss=3.182, wps=4170, ups=6.16, wpb=676.6, bsz=676.6, num_updates=7100, lr=0.000354, gnorm=1.806, clip=0, train_wall=7, gb_free=70.6, wall=1192 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:35:02]    INFO >> epoch 005:   1013 / 1539 loss=2.993, wps=5459.5, ups=5.72, wpb=954.2, bsz=954.2, num_updates=7150, lr=0.000354, gnorm=1.782, clip=0, train_wall=8, gb_free=70.5, wall=1201 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:35:10]    INFO >> epoch 005:   1063 / 1539 loss=3.007, wps=4942, ups=6.25, wpb=791.1, bsz=791.1, num_updates=7200, lr=0.000354, gnorm=1.713, clip=0, train_wall=8, gb_free=65, wall=1209 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:35:17]    INFO >> epoch 005:   1113 / 1539 loss=3.317, wps=4353.2, ups=6.95, wpb=626.1, bsz=626.1, num_updates=7250, lr=0.000354, gnorm=1.677, clip=0, train_wall=7, gb_free=74.3, wall=1216 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:35:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 78.03 GiB memory in use. Of the allocated memory 71.51 GiB is allocated by PyTorch, and 6.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:35:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:35:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:35:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 36        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72703 MiB |  73225 MiB | 220144 GiB | 220073 GiB |
|       from large pool |  72692 MiB |  73214 MiB | 218996 GiB | 218925 GiB |
|       from small pool |     11 MiB |     21 MiB |   1147 GiB |   1147 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72703 MiB |  73225 MiB | 220144 GiB | 220073 GiB |
|       from large pool |  72692 MiB |  73214 MiB | 218996 GiB | 218925 GiB |
|       from small pool |     11 MiB |     21 MiB |   1147 GiB |   1147 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72691 MiB |  73213 MiB | 219821 GiB | 219750 GiB |
|       from large pool |  72680 MiB |  73202 MiB | 218675 GiB | 218604 GiB |
|       from small pool |     11 MiB |     21 MiB |   1145 GiB |   1145 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79388 MiB |  80316 MiB | 473754 MiB | 394366 MiB |
|       from large pool |  79364 MiB |  80098 MiB | 470462 MiB | 391098 MiB |
|       from small pool |     24 MiB |    218 MiB |   3292 MiB |   3268 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6684 MiB |  10454 MiB | 215209 GiB | 215202 GiB |
|       from large pool |   6671 MiB |  10441 MiB | 213908 GiB | 213901 GiB |
|       from small pool |     12 MiB |     21 MiB |   1300 GiB |   1300 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     544    |     551    |   13790 K  |   13789 K  |
|       from large pool |     259    |     266    |    6239 K  |    6239 K  |
|       from small pool |     285    |     356    |    7551 K  |    7550 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     544    |     551    |   13790 K  |   13789 K  |
|       from large pool |     259    |     266    |    6239 K  |    6239 K  |
|       from small pool |     285    |     356    |    7551 K  |    7550 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      99    |     253    |    3833    |    3734    |
|       from large pool |      87    |     144    |    2187    |    2100    |
|       from small pool |      12    |     109    |    1646    |    1634    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     103    |     103    |    7700 K  |    7700 K  |
|       from large pool |      79    |      79    |    3983 K  |    3983 K  |
|       from small pool |      24    |      46    |    3716 K  |    3716 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:35:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:35:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:35:26]    INFO >> epoch 005:   1164 / 1539 loss=3.313, wps=4581.8, ups=5.87, wpb=780.4, bsz=780.4, num_updates=7300, lr=0.000354, gnorm=1.917, clip=0, train_wall=7, gb_free=69.8, wall=1224 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:35:33]    INFO >> epoch 005:   1214 / 1539 loss=3.223, wps=4351.3, ups=6.62, wpb=657.7, bsz=657.7, num_updates=7350, lr=0.000354, gnorm=1.907, clip=0, train_wall=7, gb_free=68.9, wall=1232 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:35:41]    INFO >> epoch 005:   1264 / 1539 loss=3.22, wps=4681.6, ups=7.37, wpb=635.1, bsz=635.1, num_updates=7400, lr=0.000354, gnorm=1.626, clip=0, train_wall=6, gb_free=73.6, wall=1239 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:35:49]    INFO >> epoch 005:   1314 / 1539 loss=3.248, wps=4353.6, ups=6.68, wpb=651.5, bsz=651.5, num_updates=7450, lr=0.000354, gnorm=1.746, clip=0, train_wall=7, gb_free=70.2, wall=1246 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:35:56]    INFO >> epoch 005:   1364 / 1539 loss=3.208, wps=4895.5, ups=6.75, wpb=725.6, bsz=725.6, num_updates=7500, lr=0.000354, gnorm=1.548, clip=0, train_wall=7, gb_free=74.8, wall=1254 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:36:04]    INFO >> epoch 005:   1414 / 1539 loss=3.354, wps=4311.9, ups=6.84, wpb=630.1, bsz=630.1, num_updates=7550, lr=0.000354, gnorm=1.889, clip=0, train_wall=7, gb_free=74.4, wall=1261 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:36:11]    INFO >> epoch 005:   1464 / 1539 loss=3.267, wps=4887.7, ups=6.4, wpb=763.7, bsz=763.7, num_updates=7600, lr=0.000354, gnorm=1.887, clip=0, train_wall=7, gb_free=49.4, wall=1269 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:36:19]    INFO >> epoch 005:   1514 / 1539 loss=3.193, wps=5044.2, ups=6.28, wpb=802.7, bsz=802.7, num_updates=7650, lr=0.000354, gnorm=2.017, clip=0, train_wall=8, gb_free=55.2, wall=1277 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:36:23]    INFO >> epoch 005 | loss 3.217 | wps 4309 | ups 6.05 | wpb 712.7 | bsz 712.7 | num_updates 7675 | lr 0.000354 | gnorm 1.743 | clip 0 | train_wall 223 | gb_free 63.9 | wall 1281 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:36:23] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:36:37]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.489 | wps 11214.5 | wpb 5412.5 | bsz 5412.5 | num_updates 7675 | best_loss 3.489 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:36:37]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:36:37]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_best.pt (epoch 5 @ 7675 updates, score 3.489) (writing took 0.023158 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:36:37] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:36:44]    INFO >> epoch 006:     25 / 1539 loss=3.286, wps=1656.3, ups=2.34, wpb=708.8, bsz=708.8, num_updates=7700, lr=0.000327, gnorm=1.69, clip=0, train_wall=7, gb_free=75.1, wall=1298 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:36:52]    INFO >> epoch 006:     75 / 1539 loss=3.335, wps=5371.4, ups=6.22, wpb=863.7, bsz=863.7, num_updates=7750, lr=0.000327, gnorm=2.011, clip=0, train_wall=8, gb_free=75.3, wall=1306 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:36:59]    INFO >> epoch 006:    125 / 1539 loss=3.316, wps=4673.3, ups=6.92, wpb=675.3, bsz=675.3, num_updates=7800, lr=0.000327, gnorm=1.737, clip=0, train_wall=7, gb_free=70.5, wall=1313 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:37:06]    INFO >> epoch 006:    175 / 1539 loss=3.149, wps=4647.2, ups=6.75, wpb=689, bsz=689, num_updates=7850, lr=0.000327, gnorm=1.581, clip=0, train_wall=7, gb_free=74.4, wall=1321 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:37:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 78.02 GiB memory in use. Of the allocated memory 71.51 GiB is allocated by PyTorch, and 6.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:37:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 37        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72702 MiB |  73224 MiB | 241121 GiB | 241050 GiB |
|       from large pool |  72691 MiB |  73213 MiB | 239858 GiB | 239787 GiB |
|       from small pool |     11 MiB |     15 MiB |   1263 GiB |   1263 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72702 MiB |  73224 MiB | 241121 GiB | 241050 GiB |
|       from large pool |  72691 MiB |  73213 MiB | 239858 GiB | 239787 GiB |
|       from small pool |     11 MiB |     15 MiB |   1263 GiB |   1263 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72691 MiB |  73213 MiB | 240771 GiB | 240700 GiB |
|       from large pool |  72680 MiB |  73202 MiB | 239510 GiB | 239439 GiB |
|       from small pool |     11 MiB |     15 MiB |   1261 GiB |   1261 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79386 MiB |  79460 MiB | 473826 MiB | 394440 MiB |
|       from large pool |  79364 MiB |  79364 MiB | 470462 MiB | 391098 MiB |
|       from small pool |     22 MiB |     96 MiB |   3364 MiB |   3342 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6683 MiB |  10904 MiB | 236510 GiB | 236503 GiB |
|       from large pool |   6672 MiB |  10893 MiB | 235081 GiB | 235075 GiB |
|       from small pool |     10 MiB |     17 MiB |   1428 GiB |   1428 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     544    |     551    |   15087 K  |   15087 K  |
|       from large pool |     259    |     266    |    6778 K  |    6777 K  |
|       from small pool |     285    |     356    |    8309 K  |    8309 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     544    |     551    |   15087 K  |   15087 K  |
|       from large pool |     259    |     266    |    6778 K  |    6777 K  |
|       from small pool |     285    |     356    |    8309 K  |    8309 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      98    |     135    |    3869    |    3771    |
|       from large pool |      87    |      87    |    2187    |    2100    |
|       from small pool |      11    |      48    |    1682    |    1671    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     106    |     106    |    8424 K  |    8424 K  |
|       from large pool |      82    |      82    |    4310 K  |    4310 K  |
|       from small pool |      24    |      43    |    4114 K  |    4114 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 14:37:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 45.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.88 GiB is allocated by PyTorch, and 1.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:37:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 38        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78661 MiB |  78721 MiB | 242225 GiB | 242149 GiB |
|       from large pool |  78529 MiB |  78589 MiB | 240957 GiB | 240880 GiB |
|       from small pool |    132 MiB |    133 MiB |   1268 GiB |   1268 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78661 MiB |  78721 MiB | 242225 GiB | 242149 GiB |
|       from large pool |  78529 MiB |  78589 MiB | 240957 GiB | 240880 GiB |
|       from small pool |    132 MiB |    133 MiB |   1268 GiB |   1268 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78639 MiB |  78698 MiB | 241874 GiB | 241797 GiB |
|       from large pool |  78507 MiB |  78567 MiB | 240607 GiB | 240531 GiB |
|       from small pool |    131 MiB |    132 MiB |   1266 GiB |   1266 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80460 MiB |  80462 MiB | 474902 MiB | 394442 MiB |
|       from large pool |  80324 MiB |  80324 MiB | 471422 MiB | 391098 MiB |
|       from small pool |    136 MiB |    138 MiB |   3480 MiB |   3344 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1738 MiB |   7080 MiB | 237731 GiB | 237730 GiB |
|       from large pool |   1734 MiB |   7073 MiB | 236297 GiB | 236295 GiB |
|       from small pool |      3 MiB |     17 MiB |   1434 GiB |   1434 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2702    |    2705    |   15153 K  |   15150 K  |
|       from large pool |     480    |     481    |    6806 K  |    6806 K  |
|       from small pool |    2222    |    2225    |    8346 K  |    8343 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2702    |    2705    |   15153 K  |   15150 K  |
|       from large pool |     480    |     481    |    6806 K  |    6806 K  |
|       from small pool |    2222    |    2225    |    8346 K  |    8343 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     171    |     172    |    3943    |    3772    |
|       from large pool |     103    |     103    |    2203    |    2100    |
|       from small pool |      68    |      69    |    1740    |    1672    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     147    |     149    |    8460 K  |    8460 K  |
|       from large pool |      84    |      85    |    4327 K  |    4327 K  |
|       from small pool |      63    |      65    |    4132 K  |    4132 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:37:17]    INFO >> epoch 006:    227 / 1539 loss=3.237, wps=3966.4, ups=5.51, wpb=719.2, bsz=719.2, num_updates=7900, lr=0.000327, gnorm=1.644, clip=0, train_wall=7, gb_free=71.6, wall=1330 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:37:24]    INFO >> epoch 006:    277 / 1539 loss=3.167, wps=4520.9, ups=7.23, wpb=625.7, bsz=625.7, num_updates=7950, lr=0.000327, gnorm=1.679, clip=0, train_wall=7, gb_free=74.7, wall=1337 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:37:31]    INFO >> epoch 006:    327 / 1539 loss=2.869, wps=5368, ups=6.46, wpb=830.5, bsz=830.5, num_updates=8000, lr=0.000327, gnorm=1.771, clip=0, train_wall=7, gb_free=70.5, wall=1344 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:37:39]    INFO >> epoch 006:    377 / 1539 loss=3.219, wps=4391.8, ups=6.68, wpb=657.6, bsz=657.6, num_updates=8050, lr=0.000327, gnorm=1.735, clip=0, train_wall=7, gb_free=71.3, wall=1352 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:37:46]    INFO >> epoch 006:    427 / 1539 loss=3.204, wps=5212, ups=6.75, wpb=772.1, bsz=772.1, num_updates=8100, lr=0.000327, gnorm=2.025, clip=0, train_wall=7, gb_free=75.1, wall=1359 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:37:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB. GPU 2 has a total capacity of 79.14 GiB of which 219.25 MiB is free. Including non-PyTorch memory, this process has 78.90 GiB memory in use. Of the allocated memory 75.54 GiB is allocated by PyTorch, and 2.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:37:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 39        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  66232 MiB |  77542 MiB | 248293 GiB | 248228 GiB |
|       from large pool |  66223 MiB |  77534 MiB | 246994 GiB | 246929 GiB |
|       from small pool |      8 MiB |     21 MiB |   1298 GiB |   1298 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  66232 MiB |  77542 MiB | 248293 GiB | 248228 GiB |
|       from large pool |  66223 MiB |  77534 MiB | 246994 GiB | 246929 GiB |
|       from small pool |      8 MiB |     21 MiB |   1298 GiB |   1298 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  66225 MiB |  77534 MiB | 247933 GiB | 247868 GiB |
|       from large pool |  66216 MiB |  77525 MiB | 246636 GiB | 246572 GiB |
|       from small pool |      8 MiB |     21 MiB |   1296 GiB |   1296 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80286 MiB |  80468 MiB | 474970 MiB | 394684 MiB |
|       from large pool |  80264 MiB |  80264 MiB | 471422 MiB | 391158 MiB |
|       from small pool |     22 MiB |    204 MiB |   3548 MiB |   3526 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5869 MiB |   7612 MiB | 244383 GiB | 244378 GiB |
|       from large pool |   5856 MiB |   7599 MiB | 242914 GiB | 242908 GiB |
|       from small pool |     13 MiB |     23 MiB |   1469 GiB |   1469 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     545    |     586    |   15537 K  |   15537 K  |
|       from large pool |     260    |     301    |    6990 K  |    6990 K  |
|       from small pool |     285    |     356    |    8547 K  |    8546 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     545    |     586    |   15537 K  |   15537 K  |
|       from large pool |     260    |     301    |    6990 K  |    6990 K  |
|       from small pool |     285    |     356    |    8547 K  |    8546 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     113    |     204    |    3977    |    3864    |
|       from large pool |     102    |     102    |    2203    |    2101    |
|       from small pool |      11    |     102    |    1774    |    1763    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     117    |     118    |    8669 K  |    8669 K  |
|       from large pool |      96    |      97    |    4441 K  |    4441 K  |
|       from small pool |      21    |      48    |    4228 K  |    4228 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:37:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:37:55]    INFO >> epoch 006:    478 / 1539 loss=3.233, wps=4449.8, ups=6.3, wpb=705.8, bsz=705.8, num_updates=8150, lr=0.000327, gnorm=1.765, clip=0, train_wall=7, gb_free=65.3, wall=1367 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:38:03]    INFO >> epoch 006:    528 / 1539 loss=3.104, wps=4413.2, ups=6.24, wpb=707.6, bsz=707.6, num_updates=8200, lr=0.000327, gnorm=1.711, clip=0, train_wall=8, gb_free=72.1, wall=1375 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:38:11]    INFO >> epoch 006:    578 / 1539 loss=3.368, wps=5178.9, ups=6.49, wpb=797.6, bsz=797.6, num_updates=8250, lr=0.000327, gnorm=1.687, clip=0, train_wall=7, gb_free=74.2, wall=1383 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:38:18]    INFO >> epoch 006:    628 / 1539 loss=3.234, wps=4671, ups=7.24, wpb=644.9, bsz=644.9, num_updates=8300, lr=0.000327, gnorm=1.667, clip=0, train_wall=6, gb_free=70.7, wall=1390 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:38:26]    INFO >> epoch 006:    678 / 1539 loss=3.327, wps=4825.8, ups=6.61, wpb=730.2, bsz=730.2, num_updates=8350, lr=0.000327, gnorm=1.614, clip=0, train_wall=7, gb_free=73.7, wall=1397 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:38:33]    INFO >> epoch 006:    728 / 1539 loss=3.147, wps=4321.4, ups=7.01, wpb=616.1, bsz=616.1, num_updates=8400, lr=0.000327, gnorm=1.676, clip=0, train_wall=7, gb_free=62.4, wall=1405 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:38:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.69 GiB is allocated by PyTorch, and 1.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:38:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:38:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:38:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 40        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78474 MiB |  78534 MiB | 256294 GiB | 256218 GiB |
|       from large pool |  78085 MiB |  78145 MiB | 254952 GiB | 254876 GiB |
|       from small pool |    388 MiB |    389 MiB |   1342 GiB |   1341 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78474 MiB |  78534 MiB | 256294 GiB | 256218 GiB |
|       from large pool |  78085 MiB |  78145 MiB | 254952 GiB | 254876 GiB |
|       from small pool |    388 MiB |    389 MiB |   1342 GiB |   1341 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78388 MiB |  78447 MiB | 255923 GiB | 255846 GiB |
|       from large pool |  78001 MiB |  78061 MiB | 254582 GiB | 254506 GiB |
|       from small pool |    386 MiB |    387 MiB |   1340 GiB |   1339 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 483358 MiB | 402870 MiB |
|       from large pool |  80060 MiB |  80060 MiB | 479402 MiB | 399342 MiB |
|       from small pool |    428 MiB |    430 MiB |   3956 MiB |   3528 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1953 MiB |   5223 MiB | 253170 GiB | 253168 GiB |
|       from large pool |   1914 MiB |   5219 MiB | 251650 GiB | 251648 GiB |
|       from small pool |     39 MiB |     41 MiB |   1519 GiB |   1519 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7267    |    7270    |   16070 K  |   16063 K  |
|       from large pool |     881    |     882    |    7236 K  |    7235 K  |
|       from small pool |    6386    |    6389    |    8834 K  |    8827 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7267    |    7270    |   16070 K  |   16063 K  |
|       from large pool |     881    |     882    |    7236 K  |    7235 K  |
|       from small pool |    6386    |    6389    |    8834 K  |    8827 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     446    |     447    |    4314    |    3868    |
|       from large pool |     232    |     232    |    2336    |    2104    |
|       from small pool |     214    |     215    |    1978    |    1764    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     481    |     482    |    8958 K  |    8957 K  |
|       from large pool |      94    |      94    |    4592 K  |    4592 K  |
|       from small pool |     387    |     388    |    4365 K  |    4365 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:38:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:38:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:38:40]    INFO >> epoch 006:    779 / 1539 loss=3.162, wps=4252.8, ups=6.62, wpb=642.5, bsz=642.5, num_updates=8450, lr=0.000327, gnorm=1.521, clip=0, train_wall=7, gb_free=74.2, wall=1412 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:38:48]    INFO >> epoch 006:    829 / 1539 loss=3.259, wps=4438.6, ups=6.76, wpb=656.8, bsz=656.8, num_updates=8500, lr=0.000327, gnorm=1.754, clip=0, train_wall=7, gb_free=66.9, wall=1420 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:38:58]    INFO >> epoch 006:    879 / 1539 loss=3.272, wps=4884.4, ups=6.7, wpb=729.6, bsz=729.6, num_updates=8550, lr=0.000327, gnorm=1.569, clip=0, train_wall=7, gb_free=70.4, wall=1427 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:39:06]    INFO >> epoch 006:    929 / 1539 loss=3.339, wps=5005.5, ups=6.06, wpb=826.1, bsz=826.1, num_updates=8600, lr=0.000327, gnorm=1.909, clip=0, train_wall=8, gb_free=73.2, wall=1435 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:39:14]    INFO >> epoch 006:    979 / 1539 loss=3.209, wps=4645.8, ups=6.63, wpb=701.1, bsz=701.1, num_updates=8650, lr=0.000327, gnorm=1.801, clip=0, train_wall=7, gb_free=75.2, wall=1443 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:39:22]    INFO >> epoch 006:   1029 / 1539 loss=3.133, wps=4402, ups=5.78, wpb=761.1, bsz=761.1, num_updates=8700, lr=0.000327, gnorm=1.575, clip=0, train_wall=8, gb_free=17.4, wall=1451 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:39:30]    INFO >> epoch 006:   1079 / 1539 loss=3.36, wps=4855.8, ups=6.58, wpb=737.8, bsz=737.8, num_updates=8750, lr=0.000327, gnorm=2.054, clip=0, train_wall=7, gb_free=76.9, wall=1459 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:39:38]    INFO >> epoch 006:   1129 / 1539 loss=3.084, wps=4605.9, ups=6.09, wpb=755.9, bsz=755.9, num_updates=8800, lr=0.000327, gnorm=1.78, clip=0, train_wall=8, gb_free=73.1, wall=1467 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:39:46]    INFO >> epoch 006:   1179 / 1539 loss=3.376, wps=4889.2, ups=6.7, wpb=729.8, bsz=729.8, num_updates=8850, lr=0.000327, gnorm=1.93, clip=0, train_wall=7, gb_free=75.9, wall=1475 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:39:53]    INFO >> epoch 006:   1229 / 1539 loss=3.31, wps=4306.4, ups=6.35, wpb=678, bsz=678, num_updates=8900, lr=0.000327, gnorm=1.717, clip=0, train_wall=7, gb_free=73.1, wall=1483 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:40:01]    INFO >> epoch 006:   1279 / 1539 loss=3.339, wps=4411.7, ups=6.74, wpb=654.3, bsz=654.3, num_updates=8950, lr=0.000327, gnorm=1.749, clip=0, train_wall=7, gb_free=63.2, wall=1490 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:40:11]    INFO >> epoch 006:   1329 / 1539 loss=3.225, wps=4826.6, ups=6.6, wpb=731.2, bsz=731.2, num_updates=9000, lr=0.000327, gnorm=1.839, clip=0, train_wall=7, gb_free=73.2, wall=1498 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:40:18]    INFO >> epoch 006:   1379 / 1539 loss=3.104, wps=4312.3, ups=7.2, wpb=598.8, bsz=598.8, num_updates=9050, lr=0.000327, gnorm=1.684, clip=0, train_wall=7, gb_free=68, wall=1505 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:40:26]    INFO >> epoch 006:   1429 / 1539 loss=3.21, wps=4991.3, ups=6.5, wpb=767.4, bsz=767.4, num_updates=9100, lr=0.000327, gnorm=1.938, clip=0, train_wall=7, gb_free=70, wall=1512 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:40:34]    INFO >> epoch 006:   1479 / 1539 loss=3.015, wps=4616.3, ups=6.41, wpb=719.9, bsz=719.9, num_updates=9150, lr=0.000327, gnorm=1.675, clip=0, train_wall=7, gb_free=52, wall=1520 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:40:42]    INFO >> epoch 006:   1529 / 1539 loss=3.049, wps=4791.5, ups=7.12, wpb=673, bsz=673, num_updates=9200, lr=0.000327, gnorm=1.612, clip=0, train_wall=7, gb_free=71.4, wall=1527 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:40:43]    INFO >> epoch 006 | loss 3.212 | wps 4413.6 | ups 6.19 | wpb 712.7 | bsz 712.7 | num_updates 9210 | lr 0.000327 | gnorm 1.745 | clip 0 | train_wall 218 | gb_free 72.4 | wall 1529 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:40:43] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:40:57]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.497 | wps 11058.7 | wpb 5412.5 | bsz 5412.5 | num_updates 9210 | best_loss 3.497 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:40:57]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:40:57]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_best.pt (epoch 6 @ 9210 updates, score 3.497) (writing took 0.018779 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:40:57] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:41:04]    INFO >> epoch 007:     40 / 1539 loss=3.141, wps=1584.4, ups=2.25, wpb=703.3, bsz=703.3, num_updates=9250, lr=0.000295, gnorm=1.909, clip=0, train_wall=7, gb_free=73.7, wall=1549 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:41:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.05 GiB is free. Including non-PyTorch memory, this process has 77.07 GiB memory in use. Of the allocated memory 71.77 GiB is allocated by PyTorch, and 4.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:41:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:41:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:41:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 42        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73095 MiB |  73493 MiB | 284602 GiB | 284530 GiB |
|       from large pool |  73087 MiB |  73484 MiB | 283109 GiB | 283038 GiB |
|       from small pool |      8 MiB |     25 MiB |   1492 GiB |   1492 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73095 MiB |  73493 MiB | 284602 GiB | 284530 GiB |
|       from large pool |  73087 MiB |  73484 MiB | 283109 GiB | 283038 GiB |
|       from small pool |      8 MiB |     25 MiB |   1492 GiB |   1492 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73086 MiB |  73484 MiB | 284189 GiB | 284118 GiB |
|       from large pool |  73078 MiB |  73475 MiB | 282699 GiB | 282628 GiB |
|       from small pool |      8 MiB |     25 MiB |   1489 GiB |   1489 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78408 MiB |  80428 MiB | 487024 MiB | 408616 MiB |
|       from large pool |  78386 MiB |  80000 MiB | 483068 MiB | 404682 MiB |
|       from small pool |     22 MiB |    428 MiB |   3956 MiB |   3934 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5312 MiB |   7818 MiB | 277623 GiB | 277618 GiB |
|       from large pool |   5298 MiB |   7805 MiB | 275936 GiB | 275931 GiB |
|       from small pool |     13 MiB |     27 MiB |   1686 GiB |   1686 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     577    |   17806 K  |   17805 K  |
|       from large pool |     286    |     292    |    7987 K  |    7987 K  |
|       from small pool |     285    |     356    |    9818 K  |    9818 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     577    |   17806 K  |   17805 K  |
|       from large pool |     286    |     292    |    7987 K  |    7987 K  |
|       from small pool |     285    |     356    |    9818 K  |    9818 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     156    |     445    |    4316    |    4160    |
|       from large pool |     145    |     231    |    2338    |    2193    |
|       from small pool |      11    |     214    |    1978    |    1967    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     158    |     159    |    9949 K  |    9949 K  |
|       from large pool |     137    |     138    |    5085 K  |    5085 K  |
|       from small pool |      21    |      53    |    4863 K  |    4863 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:41:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:41:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:41:13]    INFO >> epoch 007:     91 / 1539 loss=3.198, wps=4367.6, ups=6.5, wpb=672, bsz=672, num_updates=9300, lr=0.000295, gnorm=1.593, clip=0, train_wall=7, gb_free=72.4, wall=1557 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:41:21]    INFO >> epoch 007:    141 / 1539 loss=3.264, wps=4451.8, ups=6.69, wpb=665.8, bsz=665.8, num_updates=9350, lr=0.000295, gnorm=1.709, clip=0, train_wall=7, gb_free=69.5, wall=1564 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:41:28]    INFO >> epoch 007:    191 / 1539 loss=3.138, wps=5600.1, ups=6.4, wpb=874.9, bsz=874.9, num_updates=9400, lr=0.000295, gnorm=1.789, clip=0, train_wall=7, gb_free=71.6, wall=1572 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:41:36]    INFO >> epoch 007:    241 / 1539 loss=3.324, wps=5003.8, ups=6.26, wpb=798.9, bsz=798.9, num_updates=9450, lr=0.000295, gnorm=1.695, clip=0, train_wall=8, gb_free=74.5, wall=1580 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:41:45]    INFO >> epoch 007:    291 / 1539 loss=3.206, wps=4732.4, ups=6.7, wpb=706.1, bsz=706.1, num_updates=9500, lr=0.000295, gnorm=1.651, clip=0, train_wall=7, gb_free=69.1, wall=1588 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:41:51] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 76.77 GiB is allocated by PyTorch, and 1.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:41:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:41:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:41:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 44        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78552 MiB |  78612 MiB | 292352 GiB | 292275 GiB |
|       from large pool |  78421 MiB |  78481 MiB | 290820 GiB | 290744 GiB |
|       from small pool |    130 MiB |    132 MiB |   1531 GiB |   1531 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78552 MiB |  78612 MiB | 292352 GiB | 292275 GiB |
|       from large pool |  78421 MiB |  78481 MiB | 290820 GiB | 290744 GiB |
|       from small pool |    130 MiB |    132 MiB |   1531 GiB |   1531 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78519 MiB |  78578 MiB | 291927 GiB | 291850 GiB |
|       from large pool |  78388 MiB |  78448 MiB | 290398 GiB | 290321 GiB |
|       from small pool |    130 MiB |    131 MiB |   1529 GiB |   1528 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80502 MiB | 489202 MiB | 408700 MiB |
|       from large pool |  80366 MiB |  80366 MiB | 485048 MiB | 404682 MiB |
|       from small pool |    136 MiB |    218 MiB |   4154 MiB |   4018 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1889 MiB |   8627 MiB | 285302 GiB | 285300 GiB |
|       from large pool |   1884 MiB |   8616 MiB | 283570 GiB | 283568 GiB |
|       from small pool |      5 MiB |     23 MiB |   1731 GiB |   1731 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2680    |    2683    |   18295 K  |   18292 K  |
|       from large pool |     478    |     479    |    8218 K  |    8218 K  |
|       from small pool |    2202    |    2205    |   10076 K  |   10074 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2680    |    2683    |   18295 K  |   18292 K  |
|       from large pool |     478    |     479    |    8218 K  |    8218 K  |
|       from small pool |    2202    |    2205    |   10076 K  |   10074 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     246    |     285    |    4448    |    4202    |
|       from large pool |     178    |     178    |    2371    |    2193    |
|       from small pool |      68    |     109    |    2077    |    2009    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     194    |     194    |   10220 K  |   10220 K  |
|       from large pool |     130    |     134    |    5234 K  |    5234 K  |
|       from small pool |      64    |      64    |    4985 K  |    4985 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:41:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:41:51] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:41:54]    INFO >> epoch 007:    342 / 1539 loss=3.248, wps=3999.7, ups=5.82, wpb=686.9, bsz=686.9, num_updates=9550, lr=0.000295, gnorm=1.791, clip=0, train_wall=7, gb_free=70.9, wall=1596 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:42:01]    INFO >> epoch 007:    392 / 1539 loss=3.273, wps=4149.6, ups=6.96, wpb=596.2, bsz=596.2, num_updates=9600, lr=0.000295, gnorm=1.66, clip=0, train_wall=7, gb_free=75.1, wall=1603 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:42:09]    INFO >> epoch 007:    442 / 1539 loss=3.063, wps=4365.6, ups=6.39, wpb=683.5, bsz=683.5, num_updates=9650, lr=0.000295, gnorm=1.754, clip=0, train_wall=7, gb_free=76.5, wall=1611 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:42:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.29 GiB is free. Including non-PyTorch memory, this process has 77.82 GiB memory in use. Of the allocated memory 71.51 GiB is allocated by PyTorch, and 5.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:42:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:42:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:42:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 46        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72704 MiB |  73227 MiB | 296484 GiB | 296413 GiB |
|       from large pool |  72693 MiB |  73216 MiB | 294935 GiB | 294864 GiB |
|       from small pool |     11 MiB |     18 MiB |   1549 GiB |   1549 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72704 MiB |  73227 MiB | 296484 GiB | 296413 GiB |
|       from large pool |  72693 MiB |  73216 MiB | 294935 GiB | 294864 GiB |
|       from small pool |     11 MiB |     18 MiB |   1549 GiB |   1549 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72691 MiB |  73213 MiB | 296053 GiB | 295982 GiB |
|       from large pool |  72680 MiB |  73202 MiB | 294507 GiB | 294436 GiB |
|       from small pool |     11 MiB |     18 MiB |   1546 GiB |   1546 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79180 MiB |  80442 MiB | 493514 MiB | 414334 MiB |
|       from large pool |  79158 MiB |  80306 MiB | 489360 MiB | 410202 MiB |
|       from small pool |     22 MiB |    136 MiB |   4154 MiB |   4132 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6475 MiB |  10245 MiB | 289217 GiB | 289211 GiB |
|       from large pool |   6464 MiB |  10234 MiB | 287465 GiB | 287459 GiB |
|       from small pool |     10 MiB |     19 MiB |   1751 GiB |   1751 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     544    |     551    |   18533 K  |   18532 K  |
|       from large pool |     259    |     266    |    8338 K  |    8338 K  |
|       from small pool |     285    |     356    |   10194 K  |   10193 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     544    |     551    |   18533 K  |   18532 K  |
|       from large pool |     259    |     266    |    8338 K  |    8338 K  |
|       from small pool |     285    |     356    |   10194 K  |   10193 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     103    |     245    |    4454    |    4351    |
|       from large pool |      92    |     177    |    2377    |    2285    |
|       from small pool |      11    |      68    |    2077    |    2066    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     102    |     103    |   10351 K  |   10351 K  |
|       from large pool |      81    |      82    |    5313 K  |    5313 K  |
|       from small pool |      21    |      43    |    5037 K  |    5037 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:42:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:42:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:42:18]    INFO >> epoch 007:    493 / 1539 loss=3.331, wps=4414.5, ups=6.01, wpb=734.8, bsz=734.8, num_updates=9700, lr=0.000295, gnorm=1.772, clip=0, train_wall=7, gb_free=66.5, wall=1620 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:42:26]    INFO >> epoch 007:    543 / 1539 loss=3.265, wps=4455.9, ups=7.01, wpb=635.3, bsz=635.3, num_updates=9750, lr=0.000295, gnorm=1.72, clip=0, train_wall=7, gb_free=73, wall=1627 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:42:33]    INFO >> epoch 007:    593 / 1539 loss=3.319, wps=4660.8, ups=6.86, wpb=679.2, bsz=679.2, num_updates=9800, lr=0.000295, gnorm=1.879, clip=0, train_wall=7, gb_free=72.7, wall=1634 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:42:41]    INFO >> epoch 007:    643 / 1539 loss=3.16, wps=4521, ups=6.53, wpb=692.4, bsz=692.4, num_updates=9850, lr=0.000295, gnorm=1.925, clip=0, train_wall=7, gb_free=74.7, wall=1642 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:42:48]    INFO >> epoch 007:    693 / 1539 loss=3.147, wps=5454.1, ups=6.45, wpb=845.2, bsz=845.2, num_updates=9900, lr=0.000295, gnorm=1.749, clip=0, train_wall=7, gb_free=71.4, wall=1649 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:42:57]    INFO >> epoch 007:    743 / 1539 loss=3.226, wps=4812.4, ups=6.72, wpb=715.8, bsz=715.8, num_updates=9950, lr=0.000295, gnorm=1.81, clip=0, train_wall=7, gb_free=74.2, wall=1657 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:43:04]    INFO >> epoch 007:    793 / 1539 loss=3.28, wps=4494.5, ups=6.88, wpb=653.1, bsz=653.1, num_updates=10000, lr=0.000295, gnorm=1.685, clip=0, train_wall=7, gb_free=75, wall=1664 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:43:12]    INFO >> epoch 007:    843 / 1539 loss=3.121, wps=4741.9, ups=6.7, wpb=707.7, bsz=707.7, num_updates=10050, lr=0.000295, gnorm=1.745, clip=0, train_wall=7, gb_free=71.9, wall=1672 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:43:19]    INFO >> epoch 007:    893 / 1539 loss=3.206, wps=4579.8, ups=6.55, wpb=699.7, bsz=699.7, num_updates=10100, lr=0.000295, gnorm=1.732, clip=0, train_wall=7, gb_free=72.4, wall=1679 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:43:28]    INFO >> epoch 007:    943 / 1539 loss=3.288, wps=4703.2, ups=6.73, wpb=699.1, bsz=699.1, num_updates=10150, lr=0.000295, gnorm=1.781, clip=0, train_wall=7, gb_free=50.1, wall=1687 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:43:36]    INFO >> epoch 007:    993 / 1539 loss=3.262, wps=4701.7, ups=6.46, wpb=727.9, bsz=727.9, num_updates=10200, lr=0.000295, gnorm=1.762, clip=0, train_wall=7, gb_free=68.8, wall=1694 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:43:43]    INFO >> epoch 007:   1043 / 1539 loss=3.312, wps=4816.2, ups=6.72, wpb=716.6, bsz=716.6, num_updates=10250, lr=0.000295, gnorm=1.898, clip=0, train_wall=7, gb_free=67.5, wall=1702 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:43:51]    INFO >> epoch 007:   1093 / 1539 loss=2.987, wps=4408.3, ups=6.5, wpb=677.8, bsz=677.8, num_updates=10300, lr=0.000295, gnorm=1.741, clip=0, train_wall=7, gb_free=69.1, wall=1710 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:43:59]    INFO >> epoch 007:   1143 / 1539 loss=3.241, wps=4931.2, ups=6.17, wpb=799.6, bsz=799.6, num_updates=10350, lr=0.000295, gnorm=1.673, clip=0, train_wall=8, gb_free=73.4, wall=1718 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:44:07]    INFO >> epoch 007:   1193 / 1539 loss=3.235, wps=5129.6, ups=6.33, wpb=809.8, bsz=809.8, num_updates=10400, lr=0.000295, gnorm=1.858, clip=0, train_wall=7, gb_free=72, wall=1726 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:44:14]    INFO >> epoch 007:   1243 / 1539 loss=3.224, wps=4776.4, ups=6.9, wpb=691.8, bsz=691.8, num_updates=10450, lr=0.000295, gnorm=1.623, clip=0, train_wall=7, gb_free=74.5, wall=1733 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:44:22]    INFO >> epoch 007:   1293 / 1539 loss=3.225, wps=4329, ups=6.82, wpb=634.9, bsz=634.9, num_updates=10500, lr=0.000295, gnorm=1.696, clip=0, train_wall=7, gb_free=73.8, wall=1740 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:44:29]    INFO >> epoch 007:   1343 / 1539 loss=3.239, wps=4859.4, ups=6.94, wpb=700.4, bsz=700.4, num_updates=10550, lr=0.000295, gnorm=1.797, clip=0, train_wall=7, gb_free=73.3, wall=1747 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:44:39]    INFO >> epoch 007:   1393 / 1539 loss=3.087, wps=4570.8, ups=6.58, wpb=694.2, bsz=694.2, num_updates=10600, lr=0.000295, gnorm=1.594, clip=0, train_wall=7, gb_free=72.3, wall=1755 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:44:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.86 GiB is allocated by PyTorch, and 1.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:44:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:44:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:44:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 47        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78648 MiB |  78708 MiB | 322775 GiB | 322698 GiB |
|       from large pool |  78258 MiB |  78318 MiB | 321095 GiB | 321019 GiB |
|       from small pool |    390 MiB |    392 MiB |   1679 GiB |   1679 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78648 MiB |  78708 MiB | 322775 GiB | 322698 GiB |
|       from large pool |  78258 MiB |  78318 MiB | 321095 GiB | 321019 GiB |
|       from small pool |    390 MiB |    392 MiB |   1679 GiB |   1679 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78628 MiB |  78687 MiB | 322309 GiB | 322232 GiB |
|       from large pool |  78239 MiB |  78298 MiB | 320631 GiB | 320555 GiB |
|       from small pool |    388 MiB |    390 MiB |   1677 GiB |   1676 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 494824 MiB | 414336 MiB |
|       from large pool |  80058 MiB |  80058 MiB | 490260 MiB | 410202 MiB |
|       from small pool |    430 MiB |    432 MiB |   4564 MiB |   4134 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1779 MiB |   4616 MiB | 319343 GiB | 319341 GiB |
|       from large pool |   1739 MiB |   4612 MiB | 317440 GiB | 317438 GiB |
|       from small pool |     39 MiB |     41 MiB |   1902 GiB |   1902 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7311    |    7314    |   20195 K  |   20188 K  |
|       from large pool |     885    |     886    |    9137 K  |    9136 K  |
|       from small pool |    6426    |    6429    |   11057 K  |   11051 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7311    |    7314    |   20195 K  |   20188 K  |
|       from large pool |     885    |     886    |    9137 K  |    9136 K  |
|       from small pool |    6426    |    6429    |   11057 K  |   11051 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     322    |     323    |    4674    |    4352    |
|       from large pool |     107    |     107    |    2392    |    2285    |
|       from small pool |     215    |     216    |    2282    |    2067    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     476    |     478    |   11234 K  |   11233 K  |
|       from large pool |      88    |      88    |    5794 K  |    5794 K  |
|       from small pool |     388    |     390    |    5439 K  |    5439 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:44:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:44:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:44:47]    INFO >> epoch 007:   1444 / 1539 loss=3.265, wps=4411.3, ups=6.15, wpb=716.9, bsz=716.9, num_updates=10650, lr=0.000295, gnorm=1.729, clip=0, train_wall=7, gb_free=68.6, wall=1763 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:44:55]    INFO >> epoch 007:   1494 / 1539 loss=3.314, wps=4968, ups=6.98, wpb=711.5, bsz=711.5, num_updates=10700, lr=0.000295, gnorm=1.61, clip=0, train_wall=7, gb_free=67.1, wall=1770 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:45:02]    INFO >> epoch 007 | loss 3.207 | wps 4395.5 | ups 6.17 | wpb 712.7 | bsz 712.7 | num_updates 10745 | lr 0.000295 | gnorm 1.736 | clip 0 | train_wall 218 | gb_free 70.4 | wall 1777 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:45:02] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:45:16]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.489 | wps 11465.4 | wpb 5412.5 | bsz 5412.5 | num_updates 10745 | best_loss 3.497 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:45:17]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:45:17]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 7 @ 10745 updates, score 3.489) (writing took 0.014930 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:45:17] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:45:17]    INFO >> epoch 008:      5 / 1539 loss=2.872, wps=1691.2, ups=2.33, wpb=726.4, bsz=726.4, num_updates=10750, lr=0.000262, gnorm=1.471, clip=0, train_wall=7, gb_free=71.6, wall=1792 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:45:25]    INFO >> epoch 008:     55 / 1539 loss=2.916, wps=4823.7, ups=6.46, wpb=746.9, bsz=746.9, num_updates=10800, lr=0.000262, gnorm=1.524, clip=0, train_wall=7, gb_free=74.3, wall=1799 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:45:33]    INFO >> epoch 008:    105 / 1539 loss=3.216, wps=4853.1, ups=6.35, wpb=764.8, bsz=764.8, num_updates=10850, lr=0.000262, gnorm=1.741, clip=0, train_wall=7, gb_free=72.1, wall=1807 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:45:40]    INFO >> epoch 008:    155 / 1539 loss=3.172, wps=4390.9, ups=6.72, wpb=653.2, bsz=653.2, num_updates=10900, lr=0.000262, gnorm=1.465, clip=0, train_wall=7, gb_free=66.3, wall=1815 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:45:50]    INFO >> epoch 008:    205 / 1539 loss=3.016, wps=5123.4, ups=6.14, wpb=834, bsz=834, num_updates=10950, lr=0.000262, gnorm=1.759, clip=0, train_wall=8, gb_free=74.6, wall=1823 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:45:57]    INFO >> epoch 008:    255 / 1539 loss=3.171, wps=4746.8, ups=6.71, wpb=707.5, bsz=707.5, num_updates=11000, lr=0.000262, gnorm=1.899, clip=0, train_wall=7, gb_free=76.3, wall=1830 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:46:05]    INFO >> epoch 008:    305 / 1539 loss=3.394, wps=4695.2, ups=6.5, wpb=722.6, bsz=722.6, num_updates=11050, lr=0.000262, gnorm=1.785, clip=0, train_wall=7, gb_free=72.4, wall=1838 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:46:13]    INFO >> epoch 008:    355 / 1539 loss=3.227, wps=4177.1, ups=6.21, wpb=672.6, bsz=672.6, num_updates=11100, lr=0.000262, gnorm=1.803, clip=0, train_wall=8, gb_free=74.1, wall=1846 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:46:22]    INFO >> epoch 008:    405 / 1539 loss=3.225, wps=4635.5, ups=6.61, wpb=701, bsz=701, num_updates=11150, lr=0.000262, gnorm=1.633, clip=0, train_wall=7, gb_free=69, wall=1854 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:46:30]    INFO >> epoch 008:    455 / 1539 loss=3.389, wps=5603.6, ups=6.38, wpb=878.7, bsz=878.7, num_updates=11200, lr=0.000262, gnorm=1.874, clip=0, train_wall=7, gb_free=73.4, wall=1862 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:46:37]    INFO >> epoch 008:    505 / 1539 loss=3.239, wps=5083.2, ups=6.76, wpb=751.6, bsz=751.6, num_updates=11250, lr=0.000262, gnorm=1.943, clip=0, train_wall=7, gb_free=69.2, wall=1869 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:46:45]    INFO >> epoch 008:    555 / 1539 loss=3.257, wps=4354, ups=6.75, wpb=645.3, bsz=645.3, num_updates=11300, lr=0.000262, gnorm=1.497, clip=0, train_wall=7, gb_free=72.3, wall=1876 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:46:54]    INFO >> epoch 008:    605 / 1539 loss=3.345, wps=4508.6, ups=6.58, wpb=685.6, bsz=685.6, num_updates=11350, lr=0.000262, gnorm=1.659, clip=0, train_wall=7, gb_free=73.9, wall=1884 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:47:00]    INFO >> epoch 008:    655 / 1539 loss=3.244, wps=4627.6, ups=7.28, wpb=635.9, bsz=635.9, num_updates=11400, lr=0.000262, gnorm=1.74, clip=0, train_wall=6, gb_free=74.2, wall=1891 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:47:08]    INFO >> epoch 008:    705 / 1539 loss=3.211, wps=4664.5, ups=6.79, wpb=686.9, bsz=686.9, num_updates=11450, lr=0.000262, gnorm=1.761, clip=0, train_wall=7, gb_free=71.7, wall=1898 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:47:15]    INFO >> epoch 008:    755 / 1539 loss=3.253, wps=4840.9, ups=6.68, wpb=725.2, bsz=725.2, num_updates=11500, lr=0.000262, gnorm=1.789, clip=0, train_wall=7, gb_free=71.5, wall=1906 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:47:24]    INFO >> epoch 008:    805 / 1539 loss=3.128, wps=4659.8, ups=6.68, wpb=697.6, bsz=697.6, num_updates=11550, lr=0.000262, gnorm=1.817, clip=0, train_wall=7, gb_free=74.7, wall=1913 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:47:31]    INFO >> epoch 008:    855 / 1539 loss=3.157, wps=4335.5, ups=7.23, wpb=599.6, bsz=599.6, num_updates=11600, lr=0.000262, gnorm=1.516, clip=0, train_wall=7, gb_free=72.6, wall=1920 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:47:39]    INFO >> epoch 008:    905 / 1539 loss=3.088, wps=5062.9, ups=6.22, wpb=814.2, bsz=814.2, num_updates=11650, lr=0.000262, gnorm=1.728, clip=0, train_wall=8, gb_free=72, wall=1928 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:47:46]    INFO >> epoch 008:    955 / 1539 loss=3.162, wps=4703, ups=6.98, wpb=674.2, bsz=674.2, num_updates=11700, lr=0.000262, gnorm=1.625, clip=0, train_wall=7, gb_free=70.8, wall=1935 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:47:51] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 11.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 76.93 GiB is allocated by PyTorch, and 1.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:47:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:47:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:47:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 49        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78718 MiB |  78778 MiB | 357267 GiB | 357190 GiB |
|       from large pool |  78585 MiB |  78645 MiB | 355398 GiB | 355322 GiB |
|       from small pool |    132 MiB |    133 MiB |   1868 GiB |   1868 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78718 MiB |  78778 MiB | 357267 GiB | 357190 GiB |
|       from large pool |  78585 MiB |  78645 MiB | 355398 GiB | 355322 GiB |
|       from small pool |    132 MiB |    133 MiB |   1868 GiB |   1868 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78699 MiB |  78758 MiB | 356751 GiB | 356674 GiB |
|       from large pool |  78567 MiB |  78626 MiB | 354885 GiB | 354808 GiB |
|       from small pool |    132 MiB |    133 MiB |   1865 GiB |   1865 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80494 MiB |  80496 MiB | 495188 MiB | 414694 MiB |
|       from large pool |  80358 MiB |  80358 MiB | 490620 MiB | 410262 MiB |
|       from small pool |    136 MiB |    430 MiB |   4568 MiB |   4432 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1715 MiB |   7712 MiB | 354758 GiB | 354756 GiB |
|       from large pool |   1712 MiB |   7705 MiB | 352642 GiB | 352641 GiB |
|       from small pool |      3 MiB |     19 MiB |   2115 GiB |   2115 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2713    |    2716    |   22401 K  |   22398 K  |
|       from large pool |     481    |     482    |   10099 K  |   10098 K  |
|       from small pool |    2232    |    2235    |   12302 K  |   12300 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2713    |    2716    |   22401 K  |   22398 K  |
|       from large pool |     481    |     482    |   10099 K  |   10098 K  |
|       from small pool |    2232    |    2235    |   12302 K  |   12300 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     180    |     322    |    4682    |    4502    |
|       from large pool |     112    |     112    |    2398    |    2286    |
|       from small pool |      68    |     215    |    2284    |    2216    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     159    |     161    |   12459 K  |   12459 K  |
|       from large pool |      94    |      95    |    6388 K  |    6388 K  |
|       from small pool |      65    |      67    |    6071 K  |    6071 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:47:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:47:51] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:47:54]    INFO >> epoch 008:   1006 / 1539 loss=3.189, wps=3814.4, ups=6.47, wpb=589.5, bsz=589.5, num_updates=11750, lr=0.000262, gnorm=1.562, clip=0, train_wall=7, gb_free=71.6, wall=1943 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:48:04]    INFO >> epoch 008:   1056 / 1539 loss=3.067, wps=4831.1, ups=5.76, wpb=839.3, bsz=839.3, num_updates=11800, lr=0.000262, gnorm=1.512, clip=0, train_wall=8, gb_free=72.6, wall=1952 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:48:11]    INFO >> epoch 008:   1106 / 1539 loss=3.163, wps=4598.2, ups=6.9, wpb=666.3, bsz=666.3, num_updates=11850, lr=0.000262, gnorm=1.779, clip=0, train_wall=7, gb_free=70.5, wall=1959 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:48:18]    INFO >> epoch 008:   1156 / 1539 loss=3.355, wps=3900.8, ups=7.04, wpb=553.7, bsz=553.7, num_updates=11900, lr=0.000262, gnorm=1.727, clip=0, train_wall=7, gb_free=72.8, wall=1966 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:48:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.23 GiB is free. Including non-PyTorch memory, this process has 77.88 GiB memory in use. Of the allocated memory 71.51 GiB is allocated by PyTorch, and 5.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:48:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 50        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72704 MiB |  73226 MiB | 362358 GiB | 362287 GiB |
|       from large pool |  72693 MiB |  73215 MiB | 360466 GiB | 360395 GiB |
|       from small pool |     11 MiB |     19 MiB |   1891 GiB |   1891 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72704 MiB |  73226 MiB | 362358 GiB | 362287 GiB |
|       from large pool |  72693 MiB |  73215 MiB | 360466 GiB | 360395 GiB |
|       from small pool |     11 MiB |     19 MiB |   1891 GiB |   1891 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72691 MiB |  73213 MiB | 361835 GiB | 361764 GiB |
|       from large pool |  72680 MiB |  73202 MiB | 359946 GiB | 359875 GiB |
|       from small pool |     11 MiB |     19 MiB |   1888 GiB |   1888 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79242 MiB |  80434 MiB | 495188 MiB | 415946 MiB |
|       from large pool |  79218 MiB |  80298 MiB | 490620 MiB | 411402 MiB |
|       from small pool |     24 MiB |    136 MiB |   4568 MiB |   4544 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6537 MiB |   9102 MiB | 360117 GiB | 360111 GiB |
|       from large pool |   6524 MiB |   9089 MiB | 357975 GiB | 357969 GiB |
|       from small pool |     12 MiB |     21 MiB |   2142 GiB |   2142 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     544    |     551    |   22706 K  |   22705 K  |
|       from large pool |     259    |     266    |   10250 K  |   10250 K  |
|       from small pool |     285    |     348    |   12455 K  |   12454 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     544    |     551    |   22706 K  |   22705 K  |
|       from large pool |     259    |     266    |   10250 K  |   10250 K  |
|       from small pool |     285    |     348    |   12455 K  |   12454 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     105    |     179    |    4682    |    4577    |
|       from large pool |      93    |     111    |    2398    |    2305    |
|       from small pool |      12    |      68    |    2284    |    2272    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     112    |     112    |   12621 K  |   12621 K  |
|       from large pool |      84    |      84    |    6482 K  |    6482 K  |
|       from small pool |      28    |      43    |    6139 K  |    6139 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:48:27]    INFO >> epoch 008:   1207 / 1539 loss=3.194, wps=4274.8, ups=5.97, wpb=716.6, bsz=716.6, num_updates=11950, lr=0.000262, gnorm=1.685, clip=0, train_wall=7, gb_free=70.3, wall=1974 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:48:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.75 GiB is allocated by PyTorch, and 1.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:48:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 51        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78528 MiB |  78588 MiB | 364513 GiB | 364437 GiB |
|       from large pool |  78138 MiB |  78198 MiB | 362609 GiB | 362532 GiB |
|       from small pool |    389 MiB |    390 MiB |   1904 GiB |   1904 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78528 MiB |  78588 MiB | 364513 GiB | 364437 GiB |
|       from large pool |  78138 MiB |  78198 MiB | 362609 GiB | 362532 GiB |
|       from small pool |    389 MiB |    390 MiB |   1904 GiB |   1904 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78508 MiB |  78567 MiB | 363988 GiB | 363911 GiB |
|       from large pool |  78120 MiB |  78179 MiB | 362086 GiB | 362010 GiB |
|       from small pool |    387 MiB |    388 MiB |   1901 GiB |   1901 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 496436 MiB | 415948 MiB |
|       from large pool |  80058 MiB |  80058 MiB | 491460 MiB | 411402 MiB |
|       from small pool |    430 MiB |    432 MiB |   4976 MiB |   4546 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1899 MiB |   4678 MiB | 362552 GiB | 362550 GiB |
|       from large pool |   1859 MiB |   4672 MiB | 360395 GiB | 360393 GiB |
|       from small pool |     40 MiB |     41 MiB |   2156 GiB |   2156 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7289    |    7292    |   22848 K  |   22840 K  |
|       from large pool |     883    |     884    |   10309 K  |   10308 K  |
|       from small pool |    6406    |    6409    |   12538 K  |   12532 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7289    |    7292    |   22848 K  |   22840 K  |
|       from large pool |     883    |     884    |   10309 K  |   10308 K  |
|       from small pool |    6406    |    6409    |   12538 K  |   12532 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     322    |     323    |    4900    |    4578    |
|       from large pool |     107    |     107    |    2412    |    2305    |
|       from small pool |     215    |     216    |    2488    |    2273    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     480    |     480    |   12699 K  |   12699 K  |
|       from large pool |      91    |      91    |    6517 K  |    6517 K  |
|       from small pool |     389    |     389    |    6182 K  |    6181 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:48:37]    INFO >> epoch 008:   1258 / 1539 loss=3.382, wps=5020.3, ups=5.92, wpb=848.4, bsz=848.4, num_updates=12000, lr=0.000262, gnorm=2.071, clip=0, train_wall=7, gb_free=70.3, wall=1983 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:48:44]    INFO >> epoch 008:   1308 / 1539 loss=3.163, wps=4773.1, ups=6.83, wpb=698.5, bsz=698.5, num_updates=12050, lr=0.000262, gnorm=1.727, clip=0, train_wall=7, gb_free=71.5, wall=1990 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:48:51]    INFO >> epoch 008:   1358 / 1539 loss=3.065, wps=4432.9, ups=6.59, wpb=672.3, bsz=672.3, num_updates=12100, lr=0.000262, gnorm=1.723, clip=0, train_wall=7, gb_free=67.6, wall=1998 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:48:52] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB. GPU 2 has a total capacity of 79.14 GiB of which 485.25 MiB is free. Including non-PyTorch memory, this process has 78.64 GiB memory in use. Of the allocated memory 75.54 GiB is allocated by PyTorch, and 2.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:48:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 52        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  66232 MiB |  77542 MiB | 367951 GiB | 367886 GiB |
|       from large pool |  66223 MiB |  77533 MiB | 366031 GiB | 365966 GiB |
|       from small pool |      8 MiB |     18 MiB |   1920 GiB |   1920 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  66232 MiB |  77542 MiB | 367951 GiB | 367886 GiB |
|       from large pool |  66223 MiB |  77533 MiB | 366031 GiB | 365966 GiB |
|       from small pool |      8 MiB |     18 MiB |   1920 GiB |   1920 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  66225 MiB |  77534 MiB | 367421 GiB | 367356 GiB |
|       from large pool |  66216 MiB |  77525 MiB | 365503 GiB | 365439 GiB |
|       from small pool |      8 MiB |     18 MiB |   1917 GiB |   1917 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80020 MiB |  80428 MiB | 496436 MiB | 416416 MiB |
|       from large pool |  79998 MiB |  79998 MiB | 491460 MiB | 411462 MiB |
|       from small pool |     22 MiB |    430 MiB |   4976 MiB |   4954 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5603 MiB |   7347 MiB | 366252 GiB | 366247 GiB |
|       from large pool |   5590 MiB |   7333 MiB | 364077 GiB | 364072 GiB |
|       from small pool |     13 MiB |     21 MiB |   2175 GiB |   2175 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     545    |     586    |   23048 K  |   23047 K  |
|       from large pool |     260    |     301    |   10408 K  |   10407 K  |
|       from small pool |     285    |     347    |   12640 K  |   12639 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     545    |     586    |   23048 K  |   23047 K  |
|       from large pool |     260    |     301    |   10408 K  |   10407 K  |
|       from small pool |     285    |     347    |   12640 K  |   12639 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     117    |     321    |    4900    |    4783    |
|       from large pool |     106    |     106    |    2412    |    2306    |
|       from small pool |      11    |     215    |    2488    |    2477    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     123    |     128    |   12807 K  |   12807 K  |
|       from large pool |     100    |     105    |    6578 K  |    6578 K  |
|       from small pool |      23    |      45    |    6229 K  |    6229 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:48:52] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:49:00]    INFO >> epoch 008:   1409 / 1539 loss=3.136, wps=4889.9, ups=5.92, wpb=825.5, bsz=825.5, num_updates=12150, lr=0.000262, gnorm=1.77, clip=0, train_wall=7, gb_free=69.5, wall=2006 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:49:10]    INFO >> epoch 008:   1459 / 1539 loss=3.337, wps=4330.7, ups=6.04, wpb=716.4, bsz=716.4, num_updates=12200, lr=0.000262, gnorm=1.822, clip=0, train_wall=8, gb_free=66.7, wall=2014 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:49:17]    INFO >> epoch 008:   1509 / 1539 loss=3.217, wps=4472.4, ups=6.77, wpb=660.9, bsz=660.9, num_updates=12250, lr=0.000262, gnorm=1.798, clip=0, train_wall=7, gb_free=73.3, wall=2022 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:49:22]    INFO >> epoch 008 | loss 3.198 | wps 4393 | ups 6.16 | wpb 712.7 | bsz 712.7 | num_updates 12280 | lr 0.000262 | gnorm 1.724 | clip 0 | train_wall 219 | gb_free 74.9 | wall 2026 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:49:22] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:49:35]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.524 | wps 11223.6 | wpb 5412.5 | bsz 5412.5 | num_updates 12280 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:49:35]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:49:35]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_best.pt (epoch 8 @ 12280 updates, score 3.524) (writing took 0.019832 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:49:35] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:49:39]    INFO >> epoch 009:     20 / 1539 loss=3.179, wps=1580.2, ups=2.36, wpb=669.4, bsz=669.4, num_updates=12300, lr=0.000227, gnorm=1.807, clip=0, train_wall=7, gb_free=73.2, wall=2043 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:49:47]    INFO >> epoch 009:     70 / 1539 loss=3.232, wps=4470, ups=6.72, wpb=664.8, bsz=664.8, num_updates=12350, lr=0.000227, gnorm=1.74, clip=0, train_wall=7, gb_free=73.2, wall=2050 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:49:54]    INFO >> epoch 009:    120 / 1539 loss=3.18, wps=4781.7, ups=6.94, wpb=688.8, bsz=688.8, num_updates=12400, lr=0.000227, gnorm=1.636, clip=0, train_wall=7, gb_free=75.3, wall=2058 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:50:02]    INFO >> epoch 009:    170 / 1539 loss=3.201, wps=5366, ups=6.44, wpb=833.1, bsz=833.1, num_updates=12450, lr=0.000227, gnorm=1.961, clip=0, train_wall=7, gb_free=71.9, wall=2065 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:50:09]    INFO >> epoch 009:    220 / 1539 loss=3.252, wps=4886.6, ups=7.01, wpb=696.6, bsz=696.6, num_updates=12500, lr=0.000227, gnorm=1.77, clip=0, train_wall=7, gb_free=70.4, wall=2073 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:50:17]    INFO >> epoch 009:    270 / 1539 loss=3.19, wps=4973, ups=7, wpb=710.9, bsz=710.9, num_updates=12550, lr=0.000227, gnorm=1.671, clip=0, train_wall=7, gb_free=75, wall=2080 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:50:25]    INFO >> epoch 009:    320 / 1539 loss=3.254, wps=4937.6, ups=6.41, wpb=770.7, bsz=770.7, num_updates=12600, lr=0.000227, gnorm=1.856, clip=0, train_wall=7, gb_free=69.4, wall=2088 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:50:34]    INFO >> epoch 009:    370 / 1539 loss=3.306, wps=4313.1, ups=6.04, wpb=714.7, bsz=714.7, num_updates=12650, lr=0.000227, gnorm=1.693, clip=0, train_wall=8, gb_free=71.8, wall=2096 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:50:35] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.34 GiB is free. Including non-PyTorch memory, this process has 77.78 GiB memory in use. Of the allocated memory 72.51 GiB is allocated by PyTorch, and 4.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:50:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:50:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:50:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 33           |        cudaMalloc retries: 53        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63159 MiB |  75129 MiB | 387779 GiB | 387717 GiB |
|       from large pool |  63148 MiB |  75118 MiB | 385748 GiB | 385687 GiB |
|       from small pool |     11 MiB |     16 MiB |   2030 GiB |   2030 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63159 MiB |  75129 MiB | 387779 GiB | 387717 GiB |
|       from large pool |  63148 MiB |  75118 MiB | 385748 GiB | 385687 GiB |
|       from small pool |     11 MiB |     16 MiB |   2030 GiB |   2030 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63147 MiB |  75114 MiB | 387221 GiB | 387160 GiB |
|       from large pool |  63136 MiB |  75103 MiB | 385194 GiB | 385132 GiB |
|       from small pool |     11 MiB |     16 MiB |   2027 GiB |   2027 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79132 MiB |  80068 MiB | 504668 MiB | 425536 MiB |
|       from large pool |  79110 MiB |  79950 MiB | 499596 MiB | 420486 MiB |
|       from small pool |     22 MiB |    118 MiB |   5072 MiB |   5050 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9758 MiB |   9758 MiB | 385321 GiB | 385311 GiB |
|       from large pool |   9747 MiB |   9747 MiB | 383025 GiB | 383016 GiB |
|       from small pool |     10 MiB |     19 MiB |   2295 GiB |   2295 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     515    |     557    |   24267 K  |   24266 K  |
|       from large pool |     230    |     272    |   10906 K  |   10906 K  |
|       from small pool |     285    |     342    |   13360 K  |   13360 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     515    |     557    |   24267 K  |   24266 K  |
|       from large pool |     230    |     272    |   10906 K  |   10906 K  |
|       from small pool |     285    |     342    |   13360 K  |   13360 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     108    |     170    |    4956    |    4848    |
|       from large pool |      97    |     111    |    2420    |    2323    |
|       from small pool |      11    |      59    |    2536    |    2525    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     109    |     109    |   13490 K  |   13490 K  |
|       from large pool |      87    |      87    |    6887 K  |    6887 K  |
|       from small pool |      22    |      38    |    6602 K  |    6602 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:50:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:50:35] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:50:42]    INFO >> epoch 009:    421 / 1539 loss=3.239, wps=4319.7, ups=6.05, wpb=714.3, bsz=714.3, num_updates=12700, lr=0.000227, gnorm=1.803, clip=0, train_wall=7, gb_free=73.9, wall=2104 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:50:51]    INFO >> epoch 009:    471 / 1539 loss=3.043, wps=4651.4, ups=6.72, wpb=691.9, bsz=691.9, num_updates=12750, lr=0.000227, gnorm=1.544, clip=0, train_wall=7, gb_free=74.7, wall=2112 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:50:58]    INFO >> epoch 009:    521 / 1539 loss=2.936, wps=4901.9, ups=6.46, wpb=759.2, bsz=759.2, num_updates=12800, lr=0.000227, gnorm=1.591, clip=0, train_wall=7, gb_free=73.7, wall=2119 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:51:06]    INFO >> epoch 009:    571 / 1539 loss=3.332, wps=4765.6, ups=6.33, wpb=752.6, bsz=752.6, num_updates=12850, lr=0.000227, gnorm=1.946, clip=0, train_wall=7, gb_free=72.3, wall=2127 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:51:07] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 31.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.05 GiB is allocated by PyTorch, and 1.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:51:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:51:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:51:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 34           |        cudaMalloc retries: 54        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78834 MiB |  78894 MiB | 393520 GiB | 393443 GiB |
|       from large pool |  78700 MiB |  78760 MiB | 391462 GiB | 391386 GiB |
|       from small pool |    133 MiB |    134 MiB |   2057 GiB |   2057 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78834 MiB |  78894 MiB | 393520 GiB | 393443 GiB |
|       from large pool |  78700 MiB |  78760 MiB | 391462 GiB | 391386 GiB |
|       from small pool |    133 MiB |    134 MiB |   2057 GiB |   2057 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78759 MiB |  78819 MiB | 392955 GiB | 392878 GiB |
|       from large pool |  78626 MiB |  78686 MiB | 390901 GiB | 390824 GiB |
|       from small pool |    132 MiB |    133 MiB |   2054 GiB |   2054 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80474 MiB |  80474 MiB | 512224 MiB | 431750 MiB |
|       from large pool |  80336 MiB |  80336 MiB | 507036 MiB | 426700 MiB |
|       from small pool |    138 MiB |    138 MiB |   5188 MiB |   5050 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1579 MiB |   6069 MiB | 391807 GiB | 391806 GiB |
|       from large pool |   1575 MiB |   6062 MiB | 389480 GiB | 389478 GiB |
|       from small pool |      4 MiB |     17 MiB |   2327 GiB |   2327 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2724    |    2727    |   24616 K  |   24613 K  |
|       from large pool |     482    |     483    |   11075 K  |   11075 K  |
|       from small pool |    2242    |    2245    |   13541 K  |   13538 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2724    |    2727    |   24616 K  |   24613 K  |
|       from large pool |     482    |     483    |   11075 K  |   11075 K  |
|       from small pool |    2242    |    2245    |   13541 K  |   13538 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     283    |     283    |    5138    |    4855    |
|       from large pool |     214    |     214    |    2544    |    2330    |
|       from small pool |      69    |      69    |    2594    |    2525    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     150    |     152    |   13675 K  |   13675 K  |
|       from large pool |      86    |      89    |    6989 K  |    6989 K  |
|       from small pool |      64    |      66    |    6686 K  |    6685 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:51:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:51:07] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:51:14]    INFO >> epoch 009:    622 / 1539 loss=3.247, wps=4242, ups=6.06, wpb=699.8, bsz=699.8, num_updates=12900, lr=0.000227, gnorm=1.758, clip=0, train_wall=7, gb_free=71.1, wall=2135 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:51:22]    INFO >> epoch 009:    672 / 1539 loss=3.27, wps=4422.3, ups=6.88, wpb=643.2, bsz=643.2, num_updates=12950, lr=0.000227, gnorm=1.668, clip=0, train_wall=7, gb_free=64.2, wall=2143 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:51:29]    INFO >> epoch 009:    722 / 1539 loss=3.135, wps=4744.9, ups=6.82, wpb=695.7, bsz=695.7, num_updates=13000, lr=0.000227, gnorm=1.749, clip=0, train_wall=7, gb_free=73.6, wall=2150 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:51:37]    INFO >> epoch 009:    772 / 1539 loss=3.345, wps=4435.8, ups=6.55, wpb=676.8, bsz=676.8, num_updates=13050, lr=0.000227, gnorm=1.822, clip=0, train_wall=7, gb_free=72.8, wall=2158 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:51:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 78.03 GiB memory in use. Of the allocated memory 71.77 GiB is allocated by PyTorch, and 5.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:51:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:51:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:51:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 35           |        cudaMalloc retries: 56        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73095 MiB |  73493 MiB | 400254 GiB | 400182 GiB |
|       from large pool |  73086 MiB |  73484 MiB | 398165 GiB | 398093 GiB |
|       from small pool |      8 MiB |     18 MiB |   2088 GiB |   2088 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73095 MiB |  73493 MiB | 400254 GiB | 400182 GiB |
|       from large pool |  73086 MiB |  73484 MiB | 398165 GiB | 398093 GiB |
|       from small pool |      8 MiB |     18 MiB |   2088 GiB |   2088 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73086 MiB |  73484 MiB | 399678 GiB | 399607 GiB |
|       from large pool |  73078 MiB |  73475 MiB | 397593 GiB | 397521 GiB |
|       from small pool |      8 MiB |     18 MiB |   2085 GiB |   2085 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79396 MiB |  80414 MiB | 514952 MiB | 435556 MiB |
|       from large pool |  79374 MiB |  80276 MiB | 509764 MiB | 430390 MiB |
|       from small pool |     22 MiB |    138 MiB |   5188 MiB |   5166 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6180 MiB |   8177 MiB | 398079 GiB | 398073 GiB |
|       from large pool |   6167 MiB |   8163 MiB | 395716 GiB | 395710 GiB |
|       from small pool |     13 MiB |     17 MiB |   2363 GiB |   2363 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     577    |   25023 K  |   25022 K  |
|       from large pool |     286    |     292    |   11277 K  |   11277 K  |
|       from small pool |     285    |     348    |   13745 K  |   13745 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     577    |   25023 K  |   25022 K  |
|       from large pool |     286    |     292    |   11277 K  |   11277 K  |
|       from small pool |     285    |     348    |   13745 K  |   13745 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     168    |     282    |    5139    |    4971    |
|       from large pool |     157    |     213    |    2545    |    2388    |
|       from small pool |      11    |      69    |    2594    |    2583    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     169    |     171    |   13899 K  |   13898 K  |
|       from large pool |     148    |     150    |    7121 K  |    7121 K  |
|       from small pool |      21    |      40    |    6777 K  |    6777 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:51:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:51:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:51:45]    INFO >> epoch 009:    823 / 1539 loss=3.195, wps=4477.7, ups=6.11, wpb=732.7, bsz=732.7, num_updates=13100, lr=0.000227, gnorm=1.846, clip=0, train_wall=7, gb_free=74, wall=2166 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:51:55]    INFO >> epoch 009:    873 / 1539 loss=3.235, wps=4606.4, ups=7.14, wpb=644.7, bsz=644.7, num_updates=13150, lr=0.000227, gnorm=1.578, clip=0, train_wall=7, gb_free=69.6, wall=2173 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:52:02]    INFO >> epoch 009:    923 / 1539 loss=3.336, wps=4363.5, ups=7.05, wpb=618.7, bsz=618.7, num_updates=13200, lr=0.000227, gnorm=1.785, clip=0, train_wall=7, gb_free=67.4, wall=2180 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:52:11]    INFO >> epoch 009:    973 / 1539 loss=3.099, wps=4801.8, ups=5.56, wpb=863.2, bsz=863.2, num_updates=13250, lr=0.000227, gnorm=1.68, clip=0, train_wall=8, gb_free=70.1, wall=2189 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:52:19]    INFO >> epoch 009:   1023 / 1539 loss=3.122, wps=4567.7, ups=6.37, wpb=717.2, bsz=717.2, num_updates=13300, lr=0.000227, gnorm=1.731, clip=0, train_wall=7, gb_free=70.1, wall=2197 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:52:27]    INFO >> epoch 009:   1073 / 1539 loss=3.237, wps=4893.1, ups=6.7, wpb=730.3, bsz=730.3, num_updates=13350, lr=0.000227, gnorm=1.655, clip=0, train_wall=7, gb_free=69.1, wall=2204 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:52:35]    INFO >> epoch 009:   1123 / 1539 loss=3.261, wps=4762.9, ups=6.3, wpb=756.5, bsz=756.5, num_updates=13400, lr=0.000227, gnorm=1.78, clip=0, train_wall=7, gb_free=71.7, wall=2212 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:52:43]    INFO >> epoch 009:   1173 / 1539 loss=3.201, wps=4509.7, ups=6.74, wpb=669.3, bsz=669.3, num_updates=13450, lr=0.000227, gnorm=1.678, clip=0, train_wall=7, gb_free=73.6, wall=2220 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:52:51]    INFO >> epoch 009:   1223 / 1539 loss=3.01, wps=5113.4, ups=6.05, wpb=844.6, bsz=844.6, num_updates=13500, lr=0.000227, gnorm=1.679, clip=0, train_wall=8, gb_free=74.2, wall=2228 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:53:00]    INFO >> epoch 009:   1273 / 1539 loss=3.155, wps=5122.9, ups=6.49, wpb=789.1, bsz=789.1, num_updates=13550, lr=0.000227, gnorm=1.749, clip=0, train_wall=7, gb_free=72.1, wall=2236 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:53:08]    INFO >> epoch 009:   1323 / 1539 loss=3.326, wps=4505.2, ups=6.29, wpb=716.7, bsz=716.7, num_updates=13600, lr=0.000227, gnorm=1.925, clip=0, train_wall=7, gb_free=74.7, wall=2243 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:53:16]    INFO >> epoch 009:   1373 / 1539 loss=2.831, wps=4777.5, ups=6.4, wpb=746, bsz=746, num_updates=13650, lr=0.000227, gnorm=1.504, clip=0, train_wall=7, gb_free=72.4, wall=2251 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:53:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 47.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.30 GiB is allocated by PyTorch, and 2.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:53:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:53:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:53:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 36           |        cudaMalloc retries: 57        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78068 MiB |  78128 MiB | 416972 GiB | 416895 GiB |
|       from large pool |  77683 MiB |  77743 MiB | 414794 GiB | 414718 GiB |
|       from small pool |    384 MiB |    386 MiB |   2177 GiB |   2177 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78068 MiB |  78128 MiB | 416972 GiB | 416895 GiB |
|       from large pool |  77683 MiB |  77743 MiB | 414794 GiB | 414718 GiB |
|       from small pool |    384 MiB |    386 MiB |   2177 GiB |   2177 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78027 MiB |  78087 MiB | 416371 GiB | 416295 GiB |
|       from large pool |  77644 MiB |  77704 MiB | 414197 GiB | 414121 GiB |
|       from small pool |    382 MiB |    383 MiB |   2174 GiB |   2173 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80458 MiB |  80460 MiB | 516136 MiB | 435678 MiB |
|       from large pool |  80034 MiB |  80034 MiB | 510544 MiB | 430510 MiB |
|       from small pool |    424 MiB |    426 MiB |   5592 MiB |   5168 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2329 MiB |   5159 MiB | 414342 GiB | 414340 GiB |
|       from large pool |   2290 MiB |   5153 MiB | 411876 GiB | 411873 GiB |
|       from small pool |     39 MiB |     41 MiB |   2466 GiB |   2466 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7201    |    7204    |   26111 K  |   26104 K  |
|       from large pool |     875    |     876    |   11781 K  |   11780 K  |
|       from small pool |    6326    |    6329    |   14330 K  |   14324 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7201    |    7204    |   26111 K  |   26104 K  |
|       from large pool |     875    |     876    |   11781 K  |   11780 K  |
|       from small pool |    6326    |    6329    |   14330 K  |   14324 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     380    |     381    |    5354    |    4974    |
|       from large pool |     168    |     168    |    2558    |    2390    |
|       from small pool |     212    |     213    |    2796    |    2584    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     494    |     495    |   14505 K  |   14505 K  |
|       from large pool |     109    |     109    |    7442 K  |    7441 K  |
|       from small pool |     385    |     386    |    7063 K  |    7063 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:53:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:53:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:53:24]    INFO >> epoch 009:   1424 / 1539 loss=3.178, wps=4354.1, ups=6.42, wpb=678.2, bsz=678.2, num_updates=13700, lr=0.000227, gnorm=1.644, clip=0, train_wall=7, gb_free=70.4, wall=2259 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:53:31]    INFO >> epoch 009:   1474 / 1539 loss=3.238, wps=4307, ups=6.8, wpb=633.1, bsz=633.1, num_updates=13750, lr=0.000227, gnorm=1.787, clip=0, train_wall=7, gb_free=70.7, wall=2266 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:53:40]    INFO >> epoch 009:   1524 / 1539 loss=3.211, wps=4289, ups=6.88, wpb=623.5, bsz=623.5, num_updates=13800, lr=0.000227, gnorm=1.829, clip=0, train_wall=7, gb_free=62.4, wall=2274 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:53:42]    INFO >> epoch 009 | loss 3.193 | wps 4382.6 | ups 6.15 | wpb 712.7 | bsz 712.7 | num_updates 13815 | lr 0.000227 | gnorm 1.74 | clip 0 | train_wall 219 | gb_free 74.3 | wall 2276 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:53:42] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:53:55]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.499 | wps 11151.6 | wpb 5412.5 | bsz 5412.5 | num_updates 13815 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:53:56]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:53:56]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 9 @ 13815 updates, score 3.499) (writing took 0.014414 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:53:56] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:54:02]    INFO >> epoch 010:     35 / 1539 loss=3.047, wps=1559.8, ups=2.22, wpb=703.3, bsz=703.3, num_updates=13850, lr=0.000193, gnorm=1.805, clip=0, train_wall=8, gb_free=74.2, wall=2296 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:54:11]    INFO >> epoch 010:     85 / 1539 loss=3.192, wps=4770.8, ups=6.7, wpb=711.9, bsz=711.9, num_updates=13900, lr=0.000193, gnorm=1.978, clip=0, train_wall=7, gb_free=69.9, wall=2304 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:54:18]    INFO >> epoch 010:    135 / 1539 loss=3.167, wps=4366.7, ups=6.9, wpb=632.6, bsz=632.6, num_updates=13950, lr=0.000193, gnorm=1.686, clip=0, train_wall=7, gb_free=72.3, wall=2311 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:54:26]    INFO >> epoch 010:    185 / 1539 loss=3.255, wps=4553.5, ups=6.08, wpb=748.6, bsz=748.6, num_updates=14000, lr=0.000193, gnorm=1.9, clip=0, train_wall=8, gb_free=70, wall=2319 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:54:35]    INFO >> epoch 010:    235 / 1539 loss=3.015, wps=5249.6, ups=6.08, wpb=862.7, bsz=862.7, num_updates=14050, lr=0.000193, gnorm=1.753, clip=0, train_wall=8, gb_free=32.2, wall=2327 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:54:43]    INFO >> epoch 010:    285 / 1539 loss=3.229, wps=4452.1, ups=6.57, wpb=677.9, bsz=677.9, num_updates=14100, lr=0.000193, gnorm=1.696, clip=0, train_wall=7, gb_free=73.6, wall=2335 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:54:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.29 GiB is free. Including non-PyTorch memory, this process has 77.83 GiB memory in use. Of the allocated memory 71.77 GiB is allocated by PyTorch, and 5.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:54:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:54:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:54:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 37           |        cudaMalloc retries: 58        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73095 MiB |  73493 MiB | 433846 GiB | 433774 GiB |
|       from large pool |  73086 MiB |  73484 MiB | 431573 GiB | 431501 GiB |
|       from small pool |      8 MiB |     13 MiB |   2273 GiB |   2273 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73095 MiB |  73493 MiB | 433846 GiB | 433774 GiB |
|       from large pool |  73086 MiB |  73484 MiB | 431573 GiB | 431501 GiB |
|       from small pool |      8 MiB |     13 MiB |   2273 GiB |   2273 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73086 MiB |  73484 MiB | 433221 GiB | 433150 GiB |
|       from large pool |  73078 MiB |  73475 MiB | 430951 GiB | 430880 GiB |
|       from small pool |      8 MiB |     13 MiB |   2269 GiB |   2269 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79186 MiB |  80398 MiB | 516136 MiB | 436950 MiB |
|       from large pool |  79164 MiB |  79974 MiB | 510544 MiB | 431380 MiB |
|       from small pool |     22 MiB |    424 MiB |   5592 MiB |   5570 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5970 MiB |   7721 MiB | 428325 GiB | 428319 GiB |
|       from large pool |   5957 MiB |   7708 MiB | 425753 GiB | 425747 GiB |
|       from small pool |     13 MiB |     17 MiB |   2571 GiB |   2571 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     577    |   27152 K  |   27152 K  |
|       from large pool |     286    |     292    |   12192 K  |   12192 K  |
|       from small pool |     285    |     342    |   14959 K  |   14959 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     577    |   27152 K  |   27152 K  |
|       from large pool |     286    |     292    |   12192 K  |   12192 K  |
|       from small pool |     285    |     342    |   14959 K  |   14959 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     168    |     379    |    5354    |    5186    |
|       from large pool |     157    |     167    |    2558    |    2401    |
|       from small pool |      11    |     212    |    2796    |    2785    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     168    |     170    |   15107 K  |   15107 K  |
|       from large pool |     145    |     147    |    7710 K  |    7710 K  |
|       from small pool |      23    |      42    |    7397 K  |    7397 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:54:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:54:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:54:51]    INFO >> epoch 010:    336 / 1539 loss=3.125, wps=4519.7, ups=6.44, wpb=702.1, bsz=702.1, num_updates=14150, lr=0.000193, gnorm=1.791, clip=0, train_wall=7, gb_free=71.5, wall=2343 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:54:59]    INFO >> epoch 010:    386 / 1539 loss=3.261, wps=4598.9, ups=6.39, wpb=719.9, bsz=719.9, num_updates=14200, lr=0.000193, gnorm=1.748, clip=0, train_wall=7, gb_free=71.5, wall=2351 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:55:07]    INFO >> epoch 010:    436 / 1539 loss=3.127, wps=4247.9, ups=6.7, wpb=634.1, bsz=634.1, num_updates=14250, lr=0.000193, gnorm=1.539, clip=0, train_wall=7, gb_free=73.6, wall=2358 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:55:16]    INFO >> epoch 010:    486 / 1539 loss=3.178, wps=4244.1, ups=6.33, wpb=670.3, bsz=670.3, num_updates=14300, lr=0.000193, gnorm=1.725, clip=0, train_wall=7, gb_free=62.2, wall=2366 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:55:23]    INFO >> epoch 010:    536 / 1539 loss=3.093, wps=4600.3, ups=7.14, wpb=644.7, bsz=644.7, num_updates=14350, lr=0.000193, gnorm=1.62, clip=0, train_wall=7, gb_free=74.1, wall=2373 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:55:30]    INFO >> epoch 010:    586 / 1539 loss=3.308, wps=4287.5, ups=6.52, wpb=657.2, bsz=657.2, num_updates=14400, lr=0.000193, gnorm=1.743, clip=0, train_wall=7, gb_free=72.5, wall=2381 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:55:38]    INFO >> epoch 010:    636 / 1539 loss=3.23, wps=4926.7, ups=6.81, wpb=724, bsz=724, num_updates=14450, lr=0.000193, gnorm=1.696, clip=0, train_wall=7, gb_free=70.3, wall=2388 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:55:45]    INFO >> epoch 010:    686 / 1539 loss=3.217, wps=4858.4, ups=6.64, wpb=731.6, bsz=731.6, num_updates=14500, lr=0.000193, gnorm=1.67, clip=0, train_wall=7, gb_free=73.7, wall=2396 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:55:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 15.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.36 GiB is allocated by PyTorch, and 2.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:55:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:55:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:55:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 38           |        cudaMalloc retries: 59        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78131 MiB |  78191 MiB | 444284 GiB | 444208 GiB |
|       from large pool |  77746 MiB |  77806 MiB | 441959 GiB | 441883 GiB |
|       from small pool |    385 MiB |    386 MiB |   2324 GiB |   2324 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78131 MiB |  78191 MiB | 444284 GiB | 444208 GiB |
|       from large pool |  77746 MiB |  77806 MiB | 441959 GiB | 441883 GiB |
|       from small pool |    385 MiB |    386 MiB |   2324 GiB |   2324 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78087 MiB |  78147 MiB | 443645 GiB | 443569 GiB |
|       from large pool |  77704 MiB |  77763 MiB | 441323 GiB | 441248 GiB |
|       from small pool |    383 MiB |    384 MiB |   2321 GiB |   2320 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80490 MiB |  80490 MiB | 517560 MiB | 437070 MiB |
|       from large pool |  80064 MiB |  80064 MiB | 511564 MiB | 431500 MiB |
|       from small pool |    426 MiB |    426 MiB |   5996 MiB |   5570 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2298 MiB |   5190 MiB | 438571 GiB | 438569 GiB |
|       from large pool |   2257 MiB |   5185 MiB | 435940 GiB | 435938 GiB |
|       from small pool |     40 MiB |     41 MiB |   2631 GiB |   2631 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7212    |    7215    |   27814 K  |   27807 K  |
|       from large pool |     876    |     877    |   12512 K  |   12511 K  |
|       from small pool |    6336    |    6339    |   15301 K  |   15295 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7212    |    7215    |   27814 K  |   27807 K  |
|       from large pool |     876    |     877    |   12512 K  |   12511 K  |
|       from small pool |    6336    |    6339    |   15301 K  |   15295 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     385    |     385    |    5573    |    5188    |
|       from large pool |     172    |     172    |    2575    |    2403    |
|       from small pool |     213    |     213    |    2998    |    2785    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     493    |     494    |   15471 K  |   15471 K  |
|       from large pool |     108    |     108    |    7916 K  |    7916 K  |
|       from small pool |     385    |     386    |    7555 K  |    7554 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:55:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:55:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:55:54]    INFO >> epoch 010:    737 / 1539 loss=3.248, wps=4121.9, ups=6.55, wpb=629.4, bsz=629.4, num_updates=14550, lr=0.000193, gnorm=1.703, clip=0, train_wall=7, gb_free=72.2, wall=2403 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:56:02]    INFO >> epoch 010:    787 / 1539 loss=3.114, wps=4626.3, ups=6.33, wpb=730.4, bsz=730.4, num_updates=14600, lr=0.000193, gnorm=1.744, clip=0, train_wall=7, gb_free=74.5, wall=2411 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:56:10]    INFO >> epoch 010:    837 / 1539 loss=3.313, wps=4391.2, ups=6.51, wpb=674.5, bsz=674.5, num_updates=14650, lr=0.000193, gnorm=1.836, clip=0, train_wall=7, gb_free=69.8, wall=2419 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:56:18]    INFO >> epoch 010:    887 / 1539 loss=3.088, wps=5167.8, ups=6.07, wpb=850.8, bsz=850.8, num_updates=14700, lr=0.000193, gnorm=1.881, clip=0, train_wall=8, gb_free=50.1, wall=2427 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:56:27]    INFO >> epoch 010:    937 / 1539 loss=3.146, wps=4838.6, ups=6.75, wpb=716.4, bsz=716.4, num_updates=14750, lr=0.000193, gnorm=1.858, clip=0, train_wall=7, gb_free=70.5, wall=2434 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:56:34]    INFO >> epoch 010:    987 / 1539 loss=3.318, wps=4753.4, ups=6.89, wpb=689.5, bsz=689.5, num_updates=14800, lr=0.000193, gnorm=1.5, clip=0, train_wall=7, gb_free=69, wall=2442 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:56:41]    INFO >> epoch 010:   1037 / 1539 loss=3.299, wps=4607.2, ups=6.84, wpb=673.2, bsz=673.2, num_updates=14850, lr=0.000193, gnorm=1.677, clip=0, train_wall=7, gb_free=70.9, wall=2449 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:56:49]    INFO >> epoch 010:   1087 / 1539 loss=3.135, wps=5082.4, ups=6.36, wpb=798.6, bsz=798.6, num_updates=14900, lr=0.000193, gnorm=1.811, clip=0, train_wall=7, gb_free=74.2, wall=2457 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:56:57]    INFO >> epoch 010:   1137 / 1539 loss=3.137, wps=4104.8, ups=6.72, wpb=611, bsz=611, num_updates=14950, lr=0.000193, gnorm=1.566, clip=0, train_wall=7, gb_free=71.5, wall=2464 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:57:04] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 76.64 GiB is allocated by PyTorch, and 1.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:57:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:57:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:57:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 39           |        cudaMalloc retries: 61        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78422 MiB |  78482 MiB | 457329 GiB | 457252 GiB |
|       from large pool |  78292 MiB |  78352 MiB | 454939 GiB | 454862 GiB |
|       from small pool |    129 MiB |    130 MiB |   2390 GiB |   2389 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78422 MiB |  78482 MiB | 457329 GiB | 457252 GiB |
|       from large pool |  78292 MiB |  78352 MiB | 454939 GiB | 454862 GiB |
|       from small pool |    129 MiB |    130 MiB |   2390 GiB |   2389 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78398 MiB |  78458 MiB | 456669 GiB | 456592 GiB |
|       from large pool |  78269 MiB |  78329 MiB | 454282 GiB | 454206 GiB |
|       from small pool |    129 MiB |    130 MiB |   2386 GiB |   2386 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 517926 MiB | 437428 MiB |
|       from large pool |  80364 MiB |  80364 MiB | 511924 MiB | 431560 MiB |
|       from small pool |    134 MiB |    426 MiB |   6002 MiB |   5868 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2015 MiB |   9719 MiB | 451014 GiB | 451012 GiB |
|       from large pool |   2011 MiB |   9712 MiB | 448308 GiB | 448306 GiB |
|       from small pool |      4 MiB |     17 MiB |   2706 GiB |   2706 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2658    |    2661    |   28636 K  |   28634 K  |
|       from large pool |     476    |     477    |   12905 K  |   12905 K  |
|       from small pool |    2182    |    2185    |   15731 K  |   15729 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2658    |    2661    |   28636 K  |   28634 K  |
|       from large pool |     476    |     477    |   12905 K  |   12905 K  |
|       from small pool |    2182    |    2185    |   15731 K  |   15729 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     244    |     385    |    5582    |    5338    |
|       from large pool |     177    |     177    |    2581    |    2404    |
|       from small pool |      67    |     213    |    3001    |    2934    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     214    |     215    |   15931 K  |   15931 K  |
|       from large pool |     148    |     151    |    8171 K  |    8171 K  |
|       from small pool |      66    |      67    |    7760 K  |    7760 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:57:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:57:04] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:57:06]    INFO >> epoch 010:   1188 / 1539 loss=3.326, wps=4491.4, ups=5.71, wpb=786.7, bsz=786.7, num_updates=15000, lr=0.000193, gnorm=1.754, clip=0, train_wall=8, gb_free=72.7, wall=2473 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:57:13]    INFO >> epoch 010:   1238 / 1539 loss=3.27, wps=4966.3, ups=6.26, wpb=793.2, bsz=793.2, num_updates=15050, lr=0.000193, gnorm=1.828, clip=0, train_wall=8, gb_free=72.7, wall=2481 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:57:22]    INFO >> epoch 010:   1288 / 1539 loss=2.97, wps=4694.4, ups=6.14, wpb=764.7, bsz=764.7, num_updates=15100, lr=0.000193, gnorm=1.714, clip=0, train_wall=8, gb_free=73.6, wall=2489 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:57:32]    INFO >> epoch 010:   1338 / 1539 loss=3.249, wps=4413.2, ups=6.56, wpb=672.9, bsz=672.9, num_updates=15150, lr=0.000193, gnorm=1.839, clip=0, train_wall=7, gb_free=72.5, wall=2497 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:57:40]    INFO >> epoch 010:   1388 / 1539 loss=3.257, wps=4861.4, ups=6.51, wpb=747.1, bsz=747.1, num_updates=15200, lr=0.000193, gnorm=1.902, clip=0, train_wall=7, gb_free=70, wall=2504 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:57:47]    INFO >> epoch 010:   1438 / 1539 loss=3.167, wps=4655.1, ups=6.79, wpb=685.8, bsz=685.8, num_updates=15250, lr=0.000193, gnorm=1.564, clip=0, train_wall=7, gb_free=74.5, wall=2512 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:57:53] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 239.25 MiB is free. Including non-PyTorch memory, this process has 78.88 GiB memory in use. Of the allocated memory 76.03 GiB is allocated by PyTorch, and 2.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:57:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:57:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:57:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 40           |        cudaMalloc retries: 64        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77368 MiB |  77855 MiB | 465578 GiB | 465503 GiB |
|       from large pool |  77357 MiB |  77844 MiB | 463148 GiB | 463073 GiB |
|       from small pool |     11 MiB |     24 MiB |   2430 GiB |   2430 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77368 MiB |  77855 MiB | 465578 GiB | 465503 GiB |
|       from large pool |  77357 MiB |  77844 MiB | 463148 GiB | 463073 GiB |
|       from small pool |     11 MiB |     24 MiB |   2430 GiB |   2430 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77354 MiB |  77841 MiB | 464907 GiB | 464831 GiB |
|       from large pool |  77342 MiB |  77830 MiB | 462480 GiB | 462404 GiB |
|       from small pool |     11 MiB |     24 MiB |   2426 GiB |   2426 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80266 MiB |  80326 MiB | 550316 MiB | 470050 MiB |
|       from large pool |  80244 MiB |  80304 MiB | 544244 MiB | 464000 MiB |
|       from small pool |     22 MiB |    204 MiB |   6072 MiB |   6050 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2897 MiB |   7189 MiB | 459479 GiB | 459476 GiB |
|       from large pool |   2886 MiB |   7178 MiB | 456726 GiB | 456723 GiB |
|       from small pool |     10 MiB |     23 MiB |   2753 GiB |   2753 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     564    |     568    |   29148 K  |   29147 K  |
|       from large pool |     279    |     283    |   13152 K  |   13151 K  |
|       from small pool |     285    |     356    |   15996 K  |   15996 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     564    |     568    |   29148 K  |   29147 K  |
|       from large pool |     279    |     283    |   13152 K  |   13151 K  |
|       from small pool |     285    |     356    |   15996 K  |   15996 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     122    |     237    |    5647    |    5525    |
|       from large pool |     111    |     135    |    2611    |    2500    |
|       from small pool |      11    |     102    |    3036    |    3025    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     122    |     122    |   16212 K  |   16211 K  |
|       from large pool |     100    |     100    |    8326 K  |    8326 K  |
|       from small pool |      22    |      48    |    7885 K  |    7885 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:57:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:57:53] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:57:55]    INFO >> epoch 010:   1489 / 1539 loss=3.465, wps=4180.7, ups=6.27, wpb=667.2, bsz=667.2, num_updates=15300, lr=0.000193, gnorm=1.945, clip=0, train_wall=7, gb_free=68.9, wall=2520 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:58:03]    INFO >> epoch 010:   1539 / 1539 loss=2.99, wps=4760.7, ups=6.42, wpb=741.8, bsz=741.8, num_updates=15350, lr=0.000193, gnorm=1.878, clip=0, train_wall=7, gb_free=71, wall=2528 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:58:03]    INFO >> epoch 010 | loss 3.188 | wps 4349.7 | ups 6.1 | wpb 712.7 | bsz 712.7 | num_updates 15350 | lr 0.000193 | gnorm 1.751 | clip 0 | train_wall 221 | gb_free 71 | wall 2528 (progress_bar.py:267, print())[0m
[33m[2025-11-21 14:58:03] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:58:17]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.489 | wps 10321.5 | wpb 5412.5 | bsz 5412.5 | num_updates 15350 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 14:58:18]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 14:58:18]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 10 @ 15350 updates, score 3.489) (writing took 0.020486 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 14:58:18] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 14:58:25]    INFO >> epoch 011:     50 / 1539 loss=3.248, wps=1459.6, ups=2.22, wpb=658.6, bsz=658.6, num_updates=15400, lr=0.000161, gnorm=1.753, clip=0, train_wall=7, gb_free=72, wall=2550 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:58:36]    INFO >> epoch 011:    100 / 1539 loss=3.278, wps=4658.9, ups=6.39, wpb=728.7, bsz=728.7, num_updates=15450, lr=0.000161, gnorm=1.594, clip=0, train_wall=7, gb_free=72.2, wall=2558 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:58:43]    INFO >> epoch 011:    150 / 1539 loss=3.213, wps=4780.1, ups=7.13, wpb=670.8, bsz=670.8, num_updates=15500, lr=0.000161, gnorm=1.748, clip=0, train_wall=7, gb_free=71.9, wall=2565 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:58:51]    INFO >> epoch 011:    200 / 1539 loss=2.83, wps=4942, ups=6.47, wpb=764, bsz=764, num_updates=15550, lr=0.000161, gnorm=1.7, clip=0, train_wall=7, gb_free=73, wall=2573 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:58:58]    INFO >> epoch 011:    250 / 1539 loss=3.197, wps=5013.6, ups=6.84, wpb=733, bsz=733, num_updates=15600, lr=0.000161, gnorm=1.627, clip=0, train_wall=7, gb_free=70.3, wall=2580 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:59:05]    INFO >> epoch 011:    300 / 1539 loss=3.261, wps=4191.2, ups=6.74, wpb=621.7, bsz=621.7, num_updates=15650, lr=0.000161, gnorm=1.531, clip=0, train_wall=7, gb_free=73.2, wall=2587 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:59:15]    INFO >> epoch 011:    350 / 1539 loss=3.156, wps=4721.9, ups=6.31, wpb=748.4, bsz=748.4, num_updates=15700, lr=0.000161, gnorm=1.942, clip=0, train_wall=7, gb_free=76.1, wall=2595 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:59:22]    INFO >> epoch 011:    400 / 1539 loss=3.285, wps=4634.9, ups=6.83, wpb=678.8, bsz=678.8, num_updates=15750, lr=0.000161, gnorm=1.822, clip=0, train_wall=7, gb_free=72, wall=2603 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:59:29]    INFO >> epoch 011:    450 / 1539 loss=3.362, wps=4321.4, ups=6.96, wpb=620.6, bsz=620.6, num_updates=15800, lr=0.000161, gnorm=1.882, clip=0, train_wall=7, gb_free=72.8, wall=2610 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:59:37]    INFO >> epoch 011:    500 / 1539 loss=3.291, wps=3663.3, ups=6.01, wpb=609.6, bsz=609.6, num_updates=15850, lr=0.000161, gnorm=1.668, clip=0, train_wall=8, gb_free=72.7, wall=2618 (progress_bar.py:258, log())[0m
[32m[2025-11-21 14:59:45]    INFO >> epoch 011:    550 / 1539 loss=3.306, wps=4400.9, ups=6.79, wpb=648.3, bsz=648.3, num_updates=15900, lr=0.000161, gnorm=1.749, clip=0, train_wall=7, gb_free=73.4, wall=2626 (progress_bar.py:258, log())[0m
[33m[2025-11-21 14:59:46] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 19.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.64 GiB is allocated by PyTorch, and 1.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 14:59:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:59:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:59:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 41           |        cudaMalloc retries: 66        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78417 MiB |  78477 MiB | 486641 GiB | 486565 GiB |
|       from large pool |  78030 MiB |  78090 MiB | 484093 GiB | 484016 GiB |
|       from small pool |    387 MiB |    388 MiB |   2548 GiB |   2548 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78417 MiB |  78477 MiB | 486641 GiB | 486565 GiB |
|       from large pool |  78030 MiB |  78090 MiB | 484093 GiB | 484016 GiB |
|       from small pool |    387 MiB |    388 MiB |   2548 GiB |   2548 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78267 MiB |  78327 MiB | 485939 GiB | 485863 GiB |
|       from large pool |  77882 MiB |  77942 MiB | 483395 GiB | 483318 GiB |
|       from small pool |    385 MiB |    386 MiB |   2544 GiB |   2544 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80486 MiB |  80504 MiB | 565602 MiB | 485116 MiB |
|       from large pool |  80058 MiB |  80244 MiB | 559124 MiB | 479066 MiB |
|       from small pool |    428 MiB |    428 MiB |   6478 MiB |   6050 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2008 MiB |   5588 MiB | 480894 GiB | 480892 GiB |
|       from large pool |   1967 MiB |   5584 MiB | 478010 GiB | 478009 GiB |
|       from small pool |     40 MiB |     41 MiB |   2883 GiB |   2883 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7245    |    7248    |   30483 K  |   30476 K  |
|       from large pool |     879    |     880    |   13705 K  |   13704 K  |
|       from small pool |    6366    |    6369    |   16778 K  |   16771 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7245    |    7248    |   30483 K  |   30476 K  |
|       from large pool |     879    |     880    |   13705 K  |   13704 K  |
|       from small pool |    6366    |    6369    |   16778 K  |   16771 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     566    |     566    |    6098    |    5532    |
|       from large pool |     352    |     352    |    2859    |    2507    |
|       from small pool |     214    |     214    |    3239    |    3025    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     489    |     489    |   16949 K  |   16949 K  |
|       from large pool |     104    |     104    |    8661 K  |    8661 K  |
|       from small pool |     385    |     385    |    8288 K  |    8288 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:59:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 14:59:46] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 14:59:53]    INFO >> epoch 011:    601 / 1539 loss=3.216, wps=4084.7, ups=5.87, wpb=696, bsz=696, num_updates=15950, lr=0.000161, gnorm=1.659, clip=0, train_wall=7, gb_free=71.6, wall=2634 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:00:01]    INFO >> epoch 011:    651 / 1539 loss=3.316, wps=5058.6, ups=6.23, wpb=812, bsz=812, num_updates=16000, lr=0.000161, gnorm=1.619, clip=0, train_wall=8, gb_free=69.8, wall=2642 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:00:09]    INFO >> epoch 011:    701 / 1539 loss=3.057, wps=4415.9, ups=6.71, wpb=657.7, bsz=657.7, num_updates=16050, lr=0.000161, gnorm=1.868, clip=0, train_wall=7, gb_free=75.1, wall=2650 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:00:20]    INFO >> epoch 011:    751 / 1539 loss=3.199, wps=4311.2, ups=5.85, wpb=736.4, bsz=736.4, num_updates=16100, lr=0.000161, gnorm=1.644, clip=0, train_wall=8, gb_free=70.8, wall=2658 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:00:26] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 301.25 MiB is free. Including non-PyTorch memory, this process has 78.82 GiB memory in use. Of the allocated memory 76.03 GiB is allocated by PyTorch, and 2.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:00:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:00:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:00:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 42           |        cudaMalloc retries: 68        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77369 MiB |  77856 MiB | 493259 GiB | 493183 GiB |
|       from large pool |  77357 MiB |  77845 MiB | 490678 GiB | 490603 GiB |
|       from small pool |     11 MiB |     21 MiB |   2580 GiB |   2580 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77369 MiB |  77856 MiB | 493259 GiB | 493183 GiB |
|       from large pool |  77357 MiB |  77845 MiB | 490678 GiB | 490603 GiB |
|       from small pool |     11 MiB |     21 MiB |   2580 GiB |   2580 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77354 MiB |  77841 MiB | 492547 GiB | 492471 GiB |
|       from large pool |  77342 MiB |  77830 MiB | 489971 GiB | 489895 GiB |
|       from small pool |     11 MiB |     21 MiB |   2576 GiB |   2576 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80204 MiB |  80426 MiB | 580666 MiB | 500462 MiB |
|       from large pool |  80182 MiB |  80242 MiB | 574188 MiB | 494006 MiB |
|       from small pool |     22 MiB |    428 MiB |   6478 MiB |   6456 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2834 MiB |   4860 MiB | 487011 GiB | 487008 GiB |
|       from large pool |   2824 MiB |   4849 MiB | 484090 GiB | 484088 GiB |
|       from small pool |     10 MiB |     17 MiB |   2920 GiB |   2920 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     564    |     568    |   30891 K  |   30890 K  |
|       from large pool |     279    |     283    |   13901 K  |   13901 K  |
|       from small pool |     285    |     356    |   16989 K  |   16989 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     564    |     568    |   30891 K  |   30890 K  |
|       from large pool |     279    |     283    |   13901 K  |   13901 K  |
|       from small pool |     285    |     356    |   16989 K  |   16989 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     126    |     565    |    6110    |    5984    |
|       from large pool |     115    |     351    |    2871    |    2756    |
|       from small pool |      11    |     214    |    3239    |    3228    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     127    |     127    |   17179 K  |   17179 K  |
|       from large pool |     104    |     104    |    8789 K  |    8789 K  |
|       from small pool |      23    |      49    |    8389 K  |    8389 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:00:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:00:26] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:00:28]    INFO >> epoch 011:    802 / 1539 loss=3.268, wps=3768.2, ups=6.45, wpb=584.6, bsz=584.6, num_updates=16150, lr=0.000161, gnorm=1.721, clip=0, train_wall=6, gb_free=75.4, wall=2666 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:00:35]    INFO >> epoch 011:    852 / 1539 loss=3.155, wps=4382.6, ups=6.71, wpb=653.4, bsz=653.4, num_updates=16200, lr=0.000161, gnorm=1.65, clip=0, train_wall=7, gb_free=75.3, wall=2673 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:00:43]    INFO >> epoch 011:    902 / 1539 loss=3.206, wps=5316, ups=6.75, wpb=787.2, bsz=787.2, num_updates=16250, lr=0.000161, gnorm=1.757, clip=0, train_wall=7, gb_free=73, wall=2681 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:00:50]    INFO >> epoch 011:    952 / 1539 loss=3.18, wps=4727.6, ups=6.43, wpb=735.8, bsz=735.8, num_updates=16300, lr=0.000161, gnorm=1.84, clip=0, train_wall=7, gb_free=72, wall=2688 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:00:58]    INFO >> epoch 011:   1002 / 1539 loss=3.071, wps=4809.9, ups=6.52, wpb=737.4, bsz=737.4, num_updates=16350, lr=0.000161, gnorm=1.806, clip=0, train_wall=7, gb_free=71.3, wall=2696 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:01:06]    INFO >> epoch 011:   1052 / 1539 loss=3.034, wps=4542, ups=6.49, wpb=699.4, bsz=699.4, num_updates=16400, lr=0.000161, gnorm=1.577, clip=0, train_wall=7, gb_free=71.5, wall=2704 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:01:11] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.16 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:01:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:01:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:01:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 43           |        cudaMalloc retries: 69        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78954 MiB |  79014 MiB | 501348 GiB | 501271 GiB |
|       from large pool |  78819 MiB |  78879 MiB | 498727 GiB | 498650 GiB |
|       from small pool |    135 MiB |    136 MiB |   2620 GiB |   2620 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78954 MiB |  79014 MiB | 501348 GiB | 501271 GiB |
|       from large pool |  78819 MiB |  78879 MiB | 498727 GiB | 498650 GiB |
|       from small pool |    135 MiB |    136 MiB |   2620 GiB |   2620 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78939 MiB |  78999 MiB | 500625 GiB | 500548 GiB |
|       from large pool |  78805 MiB |  78864 MiB | 498009 GiB | 497932 GiB |
|       from small pool |    134 MiB |    135 MiB |   2616 GiB |   2616 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80502 MiB | 580964 MiB | 500462 MiB |
|       from large pool |  80362 MiB |  80362 MiB | 574368 MiB | 494006 MiB |
|       from small pool |    140 MiB |    140 MiB |   6596 MiB |   6456 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1487 MiB |   6604 MiB | 496218 GiB | 496217 GiB |
|       from large pool |   1482 MiB |   6595 MiB | 493251 GiB | 493250 GiB |
|       from small pool |      4 MiB |     21 MiB |   2967 GiB |   2967 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2757    |    2760    |   31395 K  |   31392 K  |
|       from large pool |     485    |     486    |   14142 K  |   14142 K  |
|       from small pool |    2272    |    2275    |   17252 K  |   17250 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2757    |    2760    |   31395 K  |   31392 K  |
|       from large pool |     485    |     486    |   14142 K  |   14142 K  |
|       from small pool |    2272    |    2275    |   17252 K  |   17250 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     188    |     188    |    6172    |    5984    |
|       from large pool |     118    |     118    |    2874    |    2756    |
|       from small pool |      70    |      70    |    3298    |    3228    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     172    |     172    |   17446 K  |   17446 K  |
|       from large pool |     103    |     103    |    8934 K  |    8933 K  |
|       from small pool |      69    |      69    |    8512 K  |    8512 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:01:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:01:11] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:01:15]    INFO >> epoch 011:   1103 / 1539 loss=3.157, wps=4367, ups=5.68, wpb=768.3, bsz=768.3, num_updates=16450, lr=0.000161, gnorm=1.795, clip=0, train_wall=8, gb_free=71.6, wall=2713 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:01:26]    INFO >> epoch 011:   1153 / 1539 loss=2.96, wps=5464.4, ups=6.03, wpb=906.2, bsz=906.2, num_updates=16500, lr=0.000161, gnorm=1.86, clip=0, train_wall=8, gb_free=74.5, wall=2721 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:01:33]    INFO >> epoch 011:   1203 / 1539 loss=3.14, wps=3850.1, ups=6.59, wpb=584.3, bsz=584.3, num_updates=16550, lr=0.000161, gnorm=1.759, clip=0, train_wall=7, gb_free=76.3, wall=2729 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:01:42]    INFO >> epoch 011:   1253 / 1539 loss=3.147, wps=4819.1, ups=5.72, wpb=842.2, bsz=842.2, num_updates=16600, lr=0.000161, gnorm=1.759, clip=0, train_wall=8, gb_free=67.1, wall=2737 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:01:50]    INFO >> epoch 011:   1303 / 1539 loss=2.951, wps=4824.8, ups=6.45, wpb=748.1, bsz=748.1, num_updates=16650, lr=0.000161, gnorm=1.772, clip=0, train_wall=7, gb_free=73.8, wall=2745 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:01:53] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 938.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 505.25 MiB is free. Including non-PyTorch memory, this process has 78.62 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 760.17 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:01:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:01:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:01:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 44           |        cudaMalloc retries: 72        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73487 MiB |  79427 MiB | 508033 GiB | 507961 GiB |
|       from large pool |  73479 MiB |  79418 MiB | 505379 GiB | 505307 GiB |
|       from small pool |      8 MiB |     23 MiB |   2654 GiB |   2654 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73487 MiB |  79427 MiB | 508033 GiB | 507961 GiB |
|       from large pool |  73479 MiB |  79418 MiB | 505379 GiB | 505307 GiB |
|       from small pool |      8 MiB |     23 MiB |   2654 GiB |   2654 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73471 MiB |  79407 MiB | 507301 GiB | 507229 GiB |
|       from large pool |  73462 MiB |  79398 MiB | 504651 GiB | 504579 GiB |
|       from small pool |      8 MiB |     23 MiB |   2649 GiB |   2649 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80000 MiB |  80188 MiB | 681082 MiB | 601082 MiB |
|       from large pool |  79978 MiB |  80166 MiB | 674422 MiB | 594444 MiB |
|       from small pool |     22 MiB |    204 MiB |   6660 MiB |   6638 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2844 MiB |   3827 MiB | 503660 GiB | 503658 GiB |
|       from large pool |   2830 MiB |   3814 MiB | 500655 GiB | 500652 GiB |
|       from small pool |     13 MiB |     21 MiB |   3005 GiB |   3005 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     589    |   31808 K  |   31808 K  |
|       from large pool |     286    |     304    |   14337 K  |   14337 K  |
|       from small pool |     285    |     356    |   17471 K  |   17470 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     589    |   31808 K  |   31808 K  |
|       from large pool |     286    |     304    |   14337 K  |   14337 K  |
|       from small pool |     285    |     356    |   17471 K  |   17470 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     144    |     214    |    6312    |    6168    |
|       from large pool |     133    |     134    |    2982    |    2849    |
|       from small pool |      11    |     102    |    3330    |    3319    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     145    |     146    |   17669 K  |   17669 K  |
|       from large pool |     123    |     124    |    9051 K  |    9051 K  |
|       from small pool |      22    |      48    |    8617 K  |    8617 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:01:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:01:53] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:02:01]    INFO >> epoch 011:   1354 / 1539 loss=3.317, wps=3278, ups=5.12, wpb=640.8, bsz=640.8, num_updates=16700, lr=0.000161, gnorm=1.705, clip=0, train_wall=7, gb_free=68.6, wall=2755 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:02:09]    INFO >> epoch 011:   1404 / 1539 loss=3.307, wps=5185.4, ups=6.18, wpb=838.7, bsz=838.7, num_updates=16750, lr=0.000161, gnorm=2.151, clip=0, train_wall=8, gb_free=71.9, wall=2763 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:02:16]    INFO >> epoch 011:   1454 / 1539 loss=3.35, wps=4422.1, ups=6.6, wpb=669.9, bsz=669.9, num_updates=16800, lr=0.000161, gnorm=1.7, clip=0, train_wall=7, gb_free=70.9, wall=2770 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:02:25]    INFO >> epoch 011:   1504 / 1539 loss=3.118, wps=4865.7, ups=5.94, wpb=818.8, bsz=818.8, num_updates=16850, lr=0.000161, gnorm=1.608, clip=0, train_wall=8, gb_free=70.6, wall=2779 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:02:31]    INFO >> epoch 011 | loss 3.182 | wps 4263.6 | ups 5.98 | wpb 712.7 | bsz 712.7 | num_updates 16885 | lr 0.000161 | gnorm 1.742 | clip 0 | train_wall 223 | gb_free 74.1 | wall 2784 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:02:31] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:02:47]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.498 | wps 9988.2 | wpb 5412.5 | bsz 5412.5 | num_updates 16885 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:02:47]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:02:47]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 11 @ 16885 updates, score 3.498) (writing took 0.018486 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:02:47] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:02:49]    INFO >> epoch 012:     15 / 1539 loss=3.253, wps=1403.7, ups=2.18, wpb=644.6, bsz=644.6, num_updates=16900, lr=0.000132, gnorm=1.823, clip=0, train_wall=7, gb_free=66.2, wall=2802 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:02:58]    INFO >> epoch 012:     65 / 1539 loss=3.351, wps=3950.1, ups=5.67, wpb=696.4, bsz=696.4, num_updates=16950, lr=0.000132, gnorm=1.838, clip=0, train_wall=8, gb_free=73.1, wall=2811 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:03:07]    INFO >> epoch 012:    115 / 1539 loss=3.168, wps=4461.5, ups=6.82, wpb=654.1, bsz=654.1, num_updates=17000, lr=0.000132, gnorm=1.718, clip=0, train_wall=7, gb_free=73.4, wall=2818 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:03:15]    INFO >> epoch 012:    165 / 1539 loss=2.945, wps=4566, ups=6.34, wpb=720.2, bsz=720.2, num_updates=17050, lr=0.000132, gnorm=1.704, clip=0, train_wall=7, gb_free=72.6, wall=2826 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:03:18] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.61 GiB is allocated by PyTorch, and 1.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:03:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:03:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:03:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 45           |        cudaMalloc retries: 74        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78388 MiB |  78448 MiB | 524014 GiB | 523938 GiB |
|       from large pool |  78259 MiB |  78319 MiB | 521266 GiB | 521190 GiB |
|       from small pool |    128 MiB |    130 MiB |   2748 GiB |   2747 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78388 MiB |  78448 MiB | 524014 GiB | 523938 GiB |
|       from large pool |  78259 MiB |  78319 MiB | 521266 GiB | 521190 GiB |
|       from small pool |    128 MiB |    130 MiB |   2748 GiB |   2747 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78338 MiB |  78398 MiB | 523260 GiB | 523183 GiB |
|       from large pool |  78210 MiB |  78269 MiB | 520516 GiB | 520440 GiB |
|       from small pool |    128 MiB |    129 MiB |   2743 GiB |   2743 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80488 MiB | 685300 MiB | 604838 MiB |
|       from large pool |  80330 MiB |  80330 MiB | 678442 MiB | 598112 MiB |
|       from small pool |    132 MiB |    218 MiB |   6858 MiB |   6726 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2013 MiB |   6548 MiB | 518619 GiB | 518617 GiB |
|       from large pool |   2010 MiB |   6539 MiB | 515511 GiB | 515509 GiB |
|       from small pool |      3 MiB |     23 MiB |   3108 GiB |   3108 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2647    |    2650    |   32805 K  |   32802 K  |
|       from large pool |     475    |     476    |   14721 K  |   14720 K  |
|       from small pool |    2172    |    2175    |   18084 K  |   18082 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2647    |    2650    |   32805 K  |   32802 K  |
|       from large pool |     475    |     476    |   14721 K  |   14720 K  |
|       from small pool |    2172    |    2175    |   18084 K  |   18082 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     260    |     302    |    6478    |    6218    |
|       from large pool |     194    |     194    |    3049    |    2855    |
|       from small pool |      66    |     109    |    3429    |    3363    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     191    |     194    |   18244 K  |   18244 K  |
|       from large pool |     129    |     132    |    9296 K  |    9296 K  |
|       from small pool |      62    |      65    |    8948 K  |    8948 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:03:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:03:18] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:03:23]    INFO >> epoch 012:    216 / 1539 loss=3.203, wps=4137.2, ups=6.14, wpb=673.7, bsz=673.7, num_updates=17100, lr=0.000132, gnorm=1.804, clip=0, train_wall=7, gb_free=72.6, wall=2834 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:03:30]    INFO >> epoch 012:    266 / 1539 loss=3.293, wps=4404.2, ups=6.43, wpb=684.7, bsz=684.7, num_updates=17150, lr=0.000132, gnorm=1.738, clip=0, train_wall=7, gb_free=73.2, wall=2842 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:03:38]    INFO >> epoch 012:    316 / 1539 loss=3.37, wps=4951, ups=6.84, wpb=724.2, bsz=724.2, num_updates=17200, lr=0.000132, gnorm=1.771, clip=0, train_wall=7, gb_free=72.9, wall=2849 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:03:39] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 393.25 MiB is free. Including non-PyTorch memory, this process has 78.73 GiB memory in use. Of the allocated memory 72.51 GiB is allocated by PyTorch, and 5.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:03:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:03:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:03:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 46           |        cudaMalloc retries: 76        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63157 MiB |  75128 MiB | 527654 GiB | 527592 GiB |
|       from large pool |  63146 MiB |  75117 MiB | 524889 GiB | 524827 GiB |
|       from small pool |     11 MiB |     24 MiB |   2765 GiB |   2765 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63157 MiB |  75128 MiB | 527654 GiB | 527592 GiB |
|       from large pool |  63146 MiB |  75117 MiB | 524889 GiB | 524827 GiB |
|       from small pool |     11 MiB |     24 MiB |   2765 GiB |   2765 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63147 MiB |  75114 MiB | 526893 GiB | 526832 GiB |
|       from large pool |  63136 MiB |  75103 MiB | 524133 GiB | 524071 GiB |
|       from small pool |     11 MiB |     24 MiB |   2760 GiB |   2760 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80112 MiB |  80402 MiB | 707780 MiB | 627668 MiB |
|       from large pool |  80086 MiB |  80270 MiB | 700922 MiB | 620836 MiB |
|       from small pool |     26 MiB |    132 MiB |   6858 MiB |   6832 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8838 MiB |  10224 MiB | 521882 GiB | 521874 GiB |
|       from large pool |   8823 MiB |  10209 MiB | 518755 GiB | 518746 GiB |
|       from small pool |     14 MiB |     23 MiB |   3127 GiB |   3127 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     515    |     557    |   33032 K  |   33032 K  |
|       from large pool |     230    |     272    |   14835 K  |   14834 K  |
|       from small pool |     285    |     356    |   18197 K  |   18197 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     515    |     557    |   33032 K  |   33032 K  |
|       from large pool |     230    |     272    |   14835 K  |   14834 K  |
|       from small pool |     285    |     356    |   18197 K  |   18197 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     259    |    6498    |    6402    |
|       from large pool |      83    |     193    |    3069    |    2986    |
|       from small pool |      13    |      66    |    3429    |    3416    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     101    |     104    |   18371 K  |   18370 K  |
|       from large pool |      78    |      81    |    9372 K  |    9372 K  |
|       from small pool |      23    |      48    |    8998 K  |    8998 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:03:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:03:39] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:03:46]    INFO >> epoch 012:    367 / 1539 loss=3.154, wps=3998.2, ups=5.83, wpb=686.2, bsz=686.2, num_updates=17250, lr=0.000132, gnorm=1.713, clip=0, train_wall=7, gb_free=75.1, wall=2858 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:03:54]    INFO >> epoch 012:    417 / 1539 loss=3.26, wps=4179.8, ups=6.89, wpb=606.7, bsz=606.7, num_updates=17300, lr=0.000132, gnorm=1.798, clip=0, train_wall=7, gb_free=72.9, wall=2865 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:04:02]    INFO >> epoch 012:    467 / 1539 loss=3.311, wps=4513.6, ups=6.14, wpb=735.6, bsz=735.6, num_updates=17350, lr=0.000132, gnorm=1.962, clip=0, train_wall=8, gb_free=74, wall=2873 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:04:12]    INFO >> epoch 012:    517 / 1539 loss=3.2, wps=4824.7, ups=6.45, wpb=747.7, bsz=747.7, num_updates=17400, lr=0.000132, gnorm=1.864, clip=0, train_wall=7, gb_free=73.2, wall=2881 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:04:21]    INFO >> epoch 012:    567 / 1539 loss=3.082, wps=5035.8, ups=5.97, wpb=843.2, bsz=843.2, num_updates=17450, lr=0.000132, gnorm=1.641, clip=0, train_wall=8, gb_free=71.7, wall=2889 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:04:28]    INFO >> epoch 012:    617 / 1539 loss=3.157, wps=4962.7, ups=6.53, wpb=759.8, bsz=759.8, num_updates=17500, lr=0.000132, gnorm=1.869, clip=0, train_wall=7, gb_free=69, wall=2897 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:04:40] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.10 GiB is allocated by PyTorch, and 1.51 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:04:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:04:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:04:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 47           |        cudaMalloc retries: 77        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78889 MiB |  79049 MiB | 537027 GiB | 536950 GiB |
|       from large pool |  78498 MiB |  78657 MiB | 534212 GiB | 534135 GiB |
|       from small pool |    391 MiB |    394 MiB |   2815 GiB |   2814 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78889 MiB |  79049 MiB | 537027 GiB | 536950 GiB |
|       from large pool |  78498 MiB |  78657 MiB | 534212 GiB | 534135 GiB |
|       from small pool |    391 MiB |    394 MiB |   2815 GiB |   2814 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78803 MiB |  78960 MiB | 536254 GiB | 536177 GiB |
|       from large pool |  78414 MiB |  78570 MiB | 533443 GiB | 533366 GiB |
|       from small pool |    389 MiB |    392 MiB |   2810 GiB |   2810 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB | 716288 MiB | 635786 MiB |
|       from large pool |  80070 MiB |  80070 MiB | 709022 MiB | 628952 MiB |
|       from small pool |    432 MiB |    434 MiB |   7266 MiB |   6834 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1552 MiB |   4153 MiB | 532736 GiB | 532735 GiB |
|       from large pool |   1511 MiB |   4123 MiB | 529551 GiB | 529550 GiB |
|       from small pool |     40 MiB |     41 MiB |   3185 GiB |   3185 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7325    |    7358    |   33645 K  |   33638 K  |
|       from large pool |     887    |     890    |   15116 K  |   15115 K  |
|       from small pool |    6438    |    6469    |   18529 K  |   18522 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7325    |    7358    |   33645 K  |   33638 K  |
|       from large pool |     887    |     890    |   15116 K  |   15115 K  |
|       from small pool |    6438    |    6469    |   18529 K  |   18522 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     426    |     427    |    6837    |    6411    |
|       from large pool |     210    |     210    |    3204    |    2994    |
|       from small pool |     216    |     217    |    3633    |    3417    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     472    |     473    |   18702 K  |   18701 K  |
|       from large pool |      78    |      78    |    9542 K  |    9542 K  |
|       from small pool |     394    |     396    |    9159 K  |    9158 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:04:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:04:40] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:04:42]    INFO >> epoch 012:    668 / 1539 loss=2.94, wps=3004.1, ups=3.6, wpb=834.8, bsz=834.8, num_updates=17550, lr=0.000132, gnorm=1.896, clip=0, train_wall=8, gb_free=72.9, wall=2911 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:04:51]    INFO >> epoch 012:    718 / 1539 loss=3.088, wps=4967.4, ups=6.57, wpb=756.5, bsz=756.5, num_updates=17600, lr=0.000132, gnorm=1.68, clip=0, train_wall=7, gb_free=70.5, wall=2918 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:04:59]    INFO >> epoch 012:    768 / 1539 loss=2.886, wps=4993.3, ups=6.09, wpb=819.3, bsz=819.3, num_updates=17650, lr=0.000132, gnorm=1.896, clip=0, train_wall=8, gb_free=77, wall=2927 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:05:07]    INFO >> epoch 012:    818 / 1539 loss=3.284, wps=4436.6, ups=6.7, wpb=662.3, bsz=662.3, num_updates=17700, lr=0.000132, gnorm=1.768, clip=0, train_wall=7, gb_free=71.1, wall=2934 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:05:15]    INFO >> epoch 012:    868 / 1539 loss=3.272, wps=4807.4, ups=6.26, wpb=767.6, bsz=767.6, num_updates=17750, lr=0.000132, gnorm=1.799, clip=0, train_wall=7, gb_free=64.5, wall=2942 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:05:24]    INFO >> epoch 012:    918 / 1539 loss=3.251, wps=4866.1, ups=6, wpb=811.1, bsz=811.1, num_updates=17800, lr=0.000132, gnorm=1.729, clip=0, train_wall=8, gb_free=68.5, wall=2950 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:05:32]    INFO >> epoch 012:    968 / 1539 loss=3.249, wps=4594.9, ups=6.61, wpb=694.9, bsz=694.9, num_updates=17850, lr=0.000132, gnorm=1.714, clip=0, train_wall=7, gb_free=73.8, wall=2958 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:05:40]    INFO >> epoch 012:   1018 / 1539 loss=3.257, wps=4112.8, ups=6.21, wpb=662.6, bsz=662.6, num_updates=17900, lr=0.000132, gnorm=1.778, clip=0, train_wall=8, gb_free=67, wall=2966 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:05:48]    INFO >> epoch 012:   1068 / 1539 loss=3.215, wps=4297.8, ups=6.38, wpb=673.9, bsz=673.9, num_updates=17950, lr=0.000132, gnorm=1.714, clip=0, train_wall=7, gb_free=72, wall=2974 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:05:57]    INFO >> epoch 012:   1118 / 1539 loss=3.211, wps=3908.2, ups=6.56, wpb=595.8, bsz=595.8, num_updates=18000, lr=0.000132, gnorm=1.54, clip=0, train_wall=7, gb_free=76.2, wall=2981 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:05:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.78 GiB is free. Including non-PyTorch memory, this process has 77.34 GiB memory in use. Of the allocated memory 71.77 GiB is allocated by PyTorch, and 5.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:05:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:05:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:05:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 48           |        cudaMalloc retries: 79        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73097 MiB |  73495 MiB | 550158 GiB | 550087 GiB |
|       from large pool |  73088 MiB |  73486 MiB | 547277 GiB | 547206 GiB |
|       from small pool |      8 MiB |     13 MiB |   2880 GiB |   2880 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73097 MiB |  73495 MiB | 550158 GiB | 550087 GiB |
|       from large pool |  73088 MiB |  73486 MiB | 547277 GiB | 547206 GiB |
|       from small pool |      8 MiB |     13 MiB |   2880 GiB |   2880 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73086 MiB |  73484 MiB | 549365 GiB | 549294 GiB |
|       from large pool |  73078 MiB |  73475 MiB | 546489 GiB | 546417 GiB |
|       from small pool |      8 MiB |     13 MiB |   2876 GiB |   2876 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78682 MiB |  80442 MiB | 719016 MiB | 640334 MiB |
|       from large pool |  78658 MiB |  80010 MiB | 711750 MiB | 633092 MiB |
|       from small pool |     24 MiB |    432 MiB |   7266 MiB |   7242 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5464 MiB |   6691 MiB | 545298 GiB | 545292 GiB |
|       from large pool |   5449 MiB |   6676 MiB | 542037 GiB | 542032 GiB |
|       from small pool |     15 MiB |     19 MiB |   3260 GiB |   3260 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     568    |     574    |   34463 K  |   34463 K  |
|       from large pool |     286    |     292    |   15506 K  |   15506 K  |
|       from small pool |     282    |     356    |   18956 K  |   18956 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     568    |     574    |   34463 K  |   34463 K  |
|       from large pool |     286    |     292    |   15506 K  |   15506 K  |
|       from small pool |     282    |     356    |   18956 K  |   18956 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     154    |     425    |    6838    |    6684    |
|       from large pool |     142    |     209    |    3205    |    3063    |
|       from small pool |      12    |     216    |    3633    |    3621    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     155    |     157    |   19161 K  |   19161 K  |
|       from large pool |     131    |     133    |    9797 K  |    9797 K  |
|       from small pool |      24    |      52    |    9363 K  |    9363 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:05:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:05:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:06:05]    INFO >> epoch 012:   1169 / 1539 loss=3.415, wps=3858.3, ups=5.81, wpb=664.4, bsz=664.4, num_updates=18050, lr=0.000132, gnorm=2.004, clip=0, train_wall=7, gb_free=74.2, wall=2990 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:06:14]    INFO >> epoch 012:   1219 / 1539 loss=3.21, wps=4936.6, ups=6.1, wpb=809.2, bsz=809.2, num_updates=18100, lr=0.000132, gnorm=1.748, clip=0, train_wall=8, gb_free=68.6, wall=2998 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:06:23]    INFO >> epoch 012:   1269 / 1539 loss=3.14, wps=4522.4, ups=5.62, wpb=804.1, bsz=804.1, num_updates=18150, lr=0.000132, gnorm=1.711, clip=0, train_wall=8, gb_free=63.9, wall=3007 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:06:31]    INFO >> epoch 012:   1319 / 1539 loss=3.246, wps=4799.1, ups=6.9, wpb=695.5, bsz=695.5, num_updates=18200, lr=0.000132, gnorm=1.729, clip=0, train_wall=7, gb_free=72.5, wall=3014 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:06:39]    INFO >> epoch 012:   1369 / 1539 loss=3.184, wps=4075.5, ups=6.26, wpb=651.4, bsz=651.4, num_updates=18250, lr=0.000132, gnorm=1.619, clip=0, train_wall=8, gb_free=75, wall=3022 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:06:47]    INFO >> epoch 012:   1419 / 1539 loss=3.053, wps=4161.7, ups=6.59, wpb=631.7, bsz=631.7, num_updates=18300, lr=0.000132, gnorm=1.576, clip=0, train_wall=7, gb_free=61.2, wall=3030 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:06:54]    INFO >> epoch 012:   1469 / 1539 loss=2.917, wps=4619.8, ups=6.58, wpb=701.7, bsz=701.7, num_updates=18350, lr=0.000132, gnorm=1.572, clip=0, train_wall=7, gb_free=69.3, wall=3038 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:07:03]    INFO >> epoch 012:   1519 / 1539 loss=3.219, wps=4593.6, ups=6.43, wpb=714.6, bsz=714.6, num_updates=18400, lr=0.000132, gnorm=1.663, clip=0, train_wall=7, gb_free=62.3, wall=3045 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:07:06]    INFO >> epoch 012 | loss 3.177 | wps 4144.6 | ups 5.82 | wpb 712.7 | bsz 712.7 | num_updates 18420 | lr 0.000132 | gnorm 1.75 | clip 0 | train_wall 226 | gb_free 72.7 | wall 3048 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:07:06] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:07:22]    INFO >> epoch 012 | valid on 'valid' subset | loss 3.507 | wps 9785.7 | wpb 5412.5 | bsz 5412.5 | num_updates 18420 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:07:22]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:07:22]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 12 @ 18420 updates, score 3.507) (writing took 0.013904 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:07:22] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:07:27]    INFO >> epoch 013:     30 / 1539 loss=3.082, wps=1527.9, ups=2.08, wpb=734.4, bsz=734.4, num_updates=18450, lr=0.000105, gnorm=1.693, clip=0, train_wall=8, gb_free=72.1, wall=3069 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:07:36]    INFO >> epoch 013:     80 / 1539 loss=3.083, wps=4957.7, ups=6.54, wpb=757.6, bsz=757.6, num_updates=18500, lr=0.000105, gnorm=1.654, clip=0, train_wall=7, gb_free=75.7, wall=3077 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:07:44]    INFO >> epoch 013:    130 / 1539 loss=3.17, wps=4199.7, ups=6.23, wpb=674.4, bsz=674.4, num_updates=18550, lr=0.000105, gnorm=1.64, clip=0, train_wall=8, gb_free=71, wall=3085 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:07:52]    INFO >> epoch 013:    180 / 1539 loss=2.884, wps=4708.9, ups=6.26, wpb=751.9, bsz=751.9, num_updates=18600, lr=0.000105, gnorm=1.628, clip=0, train_wall=7, gb_free=71.2, wall=3093 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:08:01]    INFO >> epoch 013:    230 / 1539 loss=3.213, wps=4295.8, ups=6.14, wpb=699.6, bsz=699.6, num_updates=18650, lr=0.000105, gnorm=1.771, clip=0, train_wall=8, gb_free=70.4, wall=3101 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:08:08]    INFO >> epoch 013:    280 / 1539 loss=3.12, wps=4340.3, ups=6.7, wpb=647.6, bsz=647.6, num_updates=18700, lr=0.000105, gnorm=1.608, clip=0, train_wall=7, gb_free=74.1, wall=3109 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:08:16]    INFO >> epoch 013:    330 / 1539 loss=3.215, wps=4406.5, ups=6.14, wpb=717.6, bsz=717.6, num_updates=18750, lr=0.000105, gnorm=1.6, clip=0, train_wall=8, gb_free=71.1, wall=3117 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:08:24]    INFO >> epoch 013:    380 / 1539 loss=3.339, wps=4136.1, ups=6.4, wpb=646.4, bsz=646.4, num_updates=18800, lr=0.000105, gnorm=1.86, clip=0, train_wall=7, gb_free=71.7, wall=3125 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:08:32]    INFO >> epoch 013:    430 / 1539 loss=3.245, wps=4404, ups=6.12, wpb=719.3, bsz=719.3, num_updates=18850, lr=0.000105, gnorm=1.669, clip=0, train_wall=8, gb_free=70.7, wall=3133 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:08:43]    INFO >> epoch 013:    480 / 1539 loss=3.243, wps=4626.3, ups=6.44, wpb=718.7, bsz=718.7, num_updates=18900, lr=0.000105, gnorm=1.66, clip=0, train_wall=7, gb_free=72.7, wall=3141 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:08:49] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.90 GiB is free. Including non-PyTorch memory, this process has 77.22 GiB memory in use. Of the allocated memory 71.77 GiB is allocated by PyTorch, and 4.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:08:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:08:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:08:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 49           |        cudaMalloc retries: 80        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73097 MiB |  73495 MiB | 580485 GiB | 580414 GiB |
|       from large pool |  73088 MiB |  73486 MiB | 577447 GiB | 577375 GiB |
|       from small pool |      8 MiB |     18 MiB |   3038 GiB |   3038 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73097 MiB |  73495 MiB | 580485 GiB | 580414 GiB |
|       from large pool |  73088 MiB |  73486 MiB | 577447 GiB | 577375 GiB |
|       from small pool |      8 MiB |     18 MiB |   3038 GiB |   3038 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73086 MiB |  73484 MiB | 579647 GiB | 579575 GiB |
|       from large pool |  73078 MiB |  73475 MiB | 576613 GiB | 576542 GiB |
|       from small pool |      8 MiB |     18 MiB |   3033 GiB |   3033 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78560 MiB |  78656 MiB | 719110 MiB | 640550 MiB |
|       from large pool |  78538 MiB |  78538 MiB | 711750 MiB | 633212 MiB |
|       from small pool |     22 MiB |    118 MiB |   7360 MiB |   7338 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5342 MiB |   6689 MiB | 573422 GiB | 573416 GiB |
|       from large pool |   5329 MiB |   6676 MiB | 569986 GiB | 569981 GiB |
|       from small pool |     13 MiB |     17 MiB |   3435 GiB |   3435 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     577    |   36318 K  |   36318 K  |
|       from large pool |     286    |     292    |   16323 K  |   16322 K  |
|       from small pool |     285    |     348    |   19995 K  |   19995 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     577    |   36318 K  |   36318 K  |
|       from large pool |     286    |     292    |   16323 K  |   16322 K  |
|       from small pool |     285    |     348    |   19995 K  |   19995 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     151    |     199    |    6885    |    6734    |
|       from large pool |     140    |     140    |    3205    |    3065    |
|       from small pool |      11    |      59    |    3680    |    3669    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     153    |     155    |   20215 K  |   20215 K  |
|       from large pool |     130    |     132    |   10323 K  |   10323 K  |
|       from small pool |      23    |      43    |    9892 K  |    9892 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:08:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:08:49] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:08:52]    INFO >> epoch 013:    531 / 1539 loss=3.357, wps=4219.7, ups=5.47, wpb=771.3, bsz=771.3, num_updates=18950, lr=0.000105, gnorm=1.858, clip=0, train_wall=8, gb_free=71.5, wall=3150 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:09:00]    INFO >> epoch 013:    581 / 1539 loss=3.212, wps=3732.3, ups=6.15, wpb=606.6, bsz=606.6, num_updates=19000, lr=0.000105, gnorm=1.779, clip=0, train_wall=8, gb_free=72.9, wall=3158 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:09:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.24 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:09:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:09:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:09:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 50           |        cudaMalloc retries: 81        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78006 MiB |  78066 MiB | 583418 GiB | 583342 GiB |
|       from large pool |  77622 MiB |  77682 MiB | 580363 GiB | 580287 GiB |
|       from small pool |    384 MiB |    385 MiB |   3054 GiB |   3054 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78006 MiB |  78066 MiB | 583418 GiB | 583342 GiB |
|       from large pool |  77622 MiB |  77682 MiB | 580363 GiB | 580287 GiB |
|       from small pool |    384 MiB |    385 MiB |   3054 GiB |   3054 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77967 MiB |  78027 MiB | 582576 GiB | 582499 GiB |
|       from large pool |  77585 MiB |  77644 MiB | 579525 GiB | 579450 GiB |
|       from small pool |    382 MiB |    383 MiB |   3050 GiB |   3049 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80462 MiB | 721132 MiB | 640670 MiB |
|       from large pool |  80038 MiB |  80038 MiB | 713370 MiB | 633332 MiB |
|       from small pool |    424 MiB |    424 MiB |   7762 MiB |   7338 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2395 MiB |   5326 MiB | 576331 GiB | 576329 GiB |
|       from large pool |   2355 MiB |   5322 MiB | 572877 GiB | 572874 GiB |
|       from small pool |     39 MiB |     40 MiB |   3454 GiB |   3454 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7190    |    7193    |   36513 K  |   36506 K  |
|       from large pool |     874    |     875    |   16409 K  |   16408 K  |
|       from small pool |    6316    |    6319    |   20104 K  |   20097 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7190    |    7193    |   36513 K  |   36506 K  |
|       from large pool |     874    |     875    |   16409 K  |   16408 K  |
|       from small pool |    6316    |    6319    |   20104 K  |   20097 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     377    |     377    |    7113    |    6736    |
|       from large pool |     165    |     165    |    3232    |    3067    |
|       from small pool |     212    |     212    |    3881    |    3669    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     494    |     495    |   20325 K  |   20325 K  |
|       from large pool |     112    |     112    |   10379 K  |   10379 K  |
|       from small pool |     382    |     383    |    9946 K  |    9945 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:09:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:09:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:09:08]    INFO >> epoch 013:    632 / 1539 loss=3.003, wps=4212.3, ups=5.99, wpb=703.6, bsz=703.6, num_updates=19050, lr=0.000105, gnorm=1.759, clip=0, train_wall=7, gb_free=72.3, wall=3166 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:09:19]    INFO >> epoch 013:    682 / 1539 loss=3.106, wps=3859, ups=5.56, wpb=694.4, bsz=694.4, num_updates=19100, lr=0.000105, gnorm=1.871, clip=0, train_wall=8, gb_free=66.3, wall=3175 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:09:26]    INFO >> epoch 013:    732 / 1539 loss=3.359, wps=4242.8, ups=6.4, wpb=662.6, bsz=662.6, num_updates=19150, lr=0.000105, gnorm=1.812, clip=0, train_wall=7, gb_free=73.3, wall=3183 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:09:34]    INFO >> epoch 013:    782 / 1539 loss=3.189, wps=4292.2, ups=6.91, wpb=621.6, bsz=621.6, num_updates=19200, lr=0.000105, gnorm=1.624, clip=0, train_wall=7, gb_free=72.7, wall=3190 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:09:41]    INFO >> epoch 013:    832 / 1539 loss=3.29, wps=4417.9, ups=6.53, wpb=676.3, bsz=676.3, num_updates=19250, lr=0.000105, gnorm=1.711, clip=0, train_wall=7, gb_free=71.2, wall=3198 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:09:51]    INFO >> epoch 013:    882 / 1539 loss=3.084, wps=4556.1, ups=5.86, wpb=777.1, bsz=777.1, num_updates=19300, lr=0.000105, gnorm=1.719, clip=0, train_wall=8, gb_free=75.5, wall=3206 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:09:59]    INFO >> epoch 013:    932 / 1539 loss=3.174, wps=4372.2, ups=6.37, wpb=686.3, bsz=686.3, num_updates=19350, lr=0.000105, gnorm=1.699, clip=0, train_wall=7, gb_free=74.1, wall=3214 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:10:07]    INFO >> epoch 013:    982 / 1539 loss=3.056, wps=4492.3, ups=6.61, wpb=679.4, bsz=679.4, num_updates=19400, lr=0.000105, gnorm=1.821, clip=0, train_wall=7, gb_free=73.5, wall=3222 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:10:14]    INFO >> epoch 013:   1032 / 1539 loss=3.242, wps=4943.6, ups=6.66, wpb=742.2, bsz=742.2, num_updates=19450, lr=0.000105, gnorm=1.953, clip=0, train_wall=7, gb_free=72.8, wall=3229 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:10:24]    INFO >> epoch 013:   1082 / 1539 loss=3.109, wps=4158.4, ups=5.93, wpb=700.9, bsz=700.9, num_updates=19500, lr=0.000105, gnorm=1.833, clip=0, train_wall=8, gb_free=72.1, wall=3238 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:10:32]    INFO >> epoch 013:   1132 / 1539 loss=3.461, wps=4681.7, ups=5.85, wpb=799.8, bsz=799.8, num_updates=19550, lr=0.000105, gnorm=1.855, clip=0, train_wall=8, gb_free=75.5, wall=3246 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:10:41]    INFO >> epoch 013:   1182 / 1539 loss=3.052, wps=4642.9, ups=5.97, wpb=777.1, bsz=777.1, num_updates=19600, lr=0.000105, gnorm=1.613, clip=0, train_wall=8, gb_free=72.1, wall=3255 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:10:48]    INFO >> epoch 013:   1232 / 1539 loss=3.183, wps=4610.7, ups=6.48, wpb=711.5, bsz=711.5, num_updates=19650, lr=0.000105, gnorm=1.638, clip=0, train_wall=7, gb_free=67.1, wall=3262 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:10:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.17 GiB. GPU 2 has a total capacity of 79.14 GiB of which 57.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 74.62 GiB is allocated by PyTorch, and 3.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:10:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:10:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:10:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 51           |        cudaMalloc retries: 83        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  70623 MiB |  76889 MiB | 600528 GiB | 600459 GiB |
|       from large pool |  70612 MiB |  76878 MiB | 597389 GiB | 597320 GiB |
|       from small pool |     11 MiB |     20 MiB |   3138 GiB |   3138 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  70623 MiB |  76889 MiB | 600528 GiB | 600459 GiB |
|       from large pool |  70612 MiB |  76878 MiB | 597389 GiB | 597320 GiB |
|       from small pool |     11 MiB |     20 MiB |   3138 GiB |   3138 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  70612 MiB |  76876 MiB | 599659 GiB | 599590 GiB |
|       from large pool |  70600 MiB |  76865 MiB | 596525 GiB | 596456 GiB |
|       from small pool |     11 MiB |     20 MiB |   3133 GiB |   3133 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80448 MiB |  80448 MiB | 726858 MiB | 646410 MiB |
|       from large pool |  80424 MiB |  80424 MiB | 719096 MiB | 638672 MiB |
|       from small pool |     24 MiB |    424 MiB |   7762 MiB |   7738 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6000 MiB |   9535 MiB | 592400 GiB | 592394 GiB |
|       from large pool |   5987 MiB |   9522 MiB | 588848 GiB | 588842 GiB |
|       from small pool |     12 MiB |     21 MiB |   3551 GiB |   3551 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     541    |     559    |   37581 K  |   37580 K  |
|       from large pool |     256    |     274    |   16925 K  |   16925 K  |
|       from small pool |     285    |     356    |   20655 K  |   20655 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     541    |     559    |   37581 K  |   37580 K  |
|       from large pool |     256    |     274    |   16925 K  |   16925 K  |
|       from small pool |     285    |     356    |   20655 K  |   20655 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     376    |    7116    |    7025    |
|       from large pool |      79    |     164    |    3235    |    3156    |
|       from small pool |      12    |     212    |    3881    |    3869    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     105    |     105    |   20919 K  |   20919 K  |
|       from large pool |      79    |      79    |   10716 K  |   10716 K  |
|       from small pool |      26    |      41    |   10203 K  |   10203 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:10:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:10:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:10:58]    INFO >> epoch 013:   1283 / 1539 loss=3.315, wps=3878.1, ups=6.02, wpb=644.6, bsz=644.6, num_updates=19700, lr=0.000105, gnorm=1.97, clip=0, train_wall=7, gb_free=75.8, wall=3271 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:11:06]    INFO >> epoch 013:   1333 / 1539 loss=3.244, wps=4410.6, ups=6.42, wpb=687, bsz=687, num_updates=19750, lr=0.000105, gnorm=1.742, clip=0, train_wall=7, gb_free=73.8, wall=3278 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:11:14]    INFO >> epoch 013:   1383 / 1539 loss=3.172, wps=5046, ups=6.16, wpb=819.3, bsz=819.3, num_updates=19800, lr=0.000105, gnorm=2.025, clip=0, train_wall=8, gb_free=72.6, wall=3287 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:11:22]    INFO >> epoch 013:   1433 / 1539 loss=3.3, wps=4477.6, ups=6.56, wpb=682.8, bsz=682.8, num_updates=19850, lr=0.000105, gnorm=1.694, clip=0, train_wall=7, gb_free=73.2, wall=3294 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:11:31]    INFO >> epoch 013:   1483 / 1539 loss=3.018, wps=4247.1, ups=6.24, wpb=681, bsz=681, num_updates=19900, lr=0.000105, gnorm=1.754, clip=0, train_wall=8, gb_free=72.7, wall=3302 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:11:32] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 45.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.31 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:11:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:11:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:11:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 52           |        cudaMalloc retries: 84        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79103 MiB |  79163 MiB | 607258 GiB | 607180 GiB |
|       from large pool |  78966 MiB |  79026 MiB | 604085 GiB | 604008 GiB |
|       from small pool |    136 MiB |    137 MiB |   3172 GiB |   3172 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79103 MiB |  79163 MiB | 607258 GiB | 607180 GiB |
|       from large pool |  78966 MiB |  79026 MiB | 604085 GiB | 604008 GiB |
|       from small pool |    136 MiB |    137 MiB |   3172 GiB |   3172 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79060 MiB |  79119 MiB | 606379 GiB | 606302 GiB |
|       from large pool |  78924 MiB |  78983 MiB | 603211 GiB | 603134 GiB |
|       from small pool |    135 MiB |    136 MiB |   3167 GiB |   3167 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80460 MiB |  80462 MiB | 730696 MiB | 650236 MiB |
|       from large pool |  80320 MiB |  80320 MiB | 722816 MiB | 642496 MiB |
|       from small pool |    140 MiB |    142 MiB |   7880 MiB |   7740 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1296 MiB |   7046 MiB | 600203 GiB | 600202 GiB |
|       from large pool |   1293 MiB |   7039 MiB | 596612 GiB | 596611 GiB |
|       from small pool |      3 MiB |     17 MiB |   3590 GiB |   3590 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2779    |    2782    |   38014 K  |   38011 K  |
|       from large pool |     487    |     488    |   17132 K  |   17132 K  |
|       from small pool |    2292    |    2295    |   20881 K  |   20879 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2779    |    2782    |   38014 K  |   38011 K  |
|       from large pool |     487    |     488    |   17132 K  |   17132 K  |
|       from small pool |    2292    |    2295    |   20881 K  |   20879 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     209    |     210    |    7237    |    7028    |
|       from large pool |     139    |     139    |    3297    |    3158    |
|       from small pool |      70    |      71    |    3940    |    3870    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     153    |     154    |   21151 K  |   21151 K  |
|       from large pool |      88    |      92    |   10841 K  |   10841 K  |
|       from small pool |      65    |      66    |   10310 K  |   10310 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:11:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:11:32] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:11:40]    INFO >> epoch 013:   1534 / 1539 loss=2.875, wps=4553, ups=5.34, wpb=852.5, bsz=852.5, num_updates=19950, lr=0.000105, gnorm=1.806, clip=0, train_wall=8, gb_free=69.5, wall=3312 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:11:41]    INFO >> epoch 013 | loss 3.175 | wps 4137 | ups 5.8 | wpb 712.7 | bsz 712.7 | num_updates 19955 | lr 0.000105 | gnorm 1.757 | clip 0 | train_wall 231 | gb_free 71.6 | wall 3313 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:11:41] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:11:57]    INFO >> epoch 013 | valid on 'valid' subset | loss 3.501 | wps 9747.5 | wpb 5412.5 | bsz 5412.5 | num_updates 19955 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:11:57]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:11:57]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 13 @ 19955 updates, score 3.501) (writing took 0.014492 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:11:57] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:12:06]    INFO >> epoch 014:     45 / 1539 loss=3.215, wps=1521.6, ups=2.07, wpb=736.4, bsz=736.4, num_updates=20000, lr=8.3e-05, gnorm=1.719, clip=0, train_wall=8, gb_free=67.2, wall=3336 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:12:14]    INFO >> epoch 014:     95 / 1539 loss=3.358, wps=4304.9, ups=6.33, wpb=679.6, bsz=679.6, num_updates=20050, lr=8.3e-05, gnorm=1.752, clip=0, train_wall=7, gb_free=70.9, wall=3344 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:12:22]    INFO >> epoch 014:    145 / 1539 loss=3.04, wps=4508.6, ups=6.14, wpb=734.8, bsz=734.8, num_updates=20100, lr=8.3e-05, gnorm=1.794, clip=0, train_wall=8, gb_free=68.5, wall=3352 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:12:29]    INFO >> epoch 014:    195 / 1539 loss=3.331, wps=4521.7, ups=6.88, wpb=657.2, bsz=657.2, num_updates=20150, lr=8.3e-05, gnorm=1.62, clip=0, train_wall=7, gb_free=70.9, wall=3359 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:12:40]    INFO >> epoch 014:    245 / 1539 loss=2.808, wps=5075.9, ups=5.61, wpb=904.5, bsz=904.5, num_updates=20200, lr=8.3e-05, gnorm=1.637, clip=0, train_wall=8, gb_free=73, wall=3368 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:12:47]    INFO >> epoch 014:    295 / 1539 loss=3.109, wps=4120, ups=6.29, wpb=654.7, bsz=654.7, num_updates=20250, lr=8.3e-05, gnorm=1.475, clip=0, train_wall=7, gb_free=74.1, wall=3376 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:12:49] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 47.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.54 GiB is allocated by PyTorch, and 2.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:12:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:12:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:12:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 53           |        cudaMalloc retries: 86        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78320 MiB |  78380 MiB | 621751 GiB | 621674 GiB |
|       from large pool |  77934 MiB |  77994 MiB | 618486 GiB | 618409 GiB |
|       from small pool |    386 MiB |    387 MiB |   3265 GiB |   3264 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78320 MiB |  78380 MiB | 621751 GiB | 621674 GiB |
|       from large pool |  77934 MiB |  77994 MiB | 618486 GiB | 618409 GiB |
|       from small pool |    386 MiB |    387 MiB |   3265 GiB |   3264 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78147 MiB |  78207 MiB | 620851 GiB | 620775 GiB |
|       from large pool |  77763 MiB |  77823 MiB | 617591 GiB | 617515 GiB |
|       from small pool |    384 MiB |    385 MiB |   3260 GiB |   3259 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80458 MiB |  80504 MiB | 747124 MiB | 666666 MiB |
|       from large pool |  80032 MiB |  80260 MiB | 738956 MiB | 658924 MiB |
|       from small pool |    426 MiB |    428 MiB |   8168 MiB |   7742 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2077 MiB |   6219 MiB | 612937 GiB | 612935 GiB |
|       from large pool |   2037 MiB |   6216 MiB | 609245 GiB | 609243 GiB |
|       from small pool |     39 MiB |     41 MiB |   3692 GiB |   3692 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7223    |    7226    |   38963 K  |   38956 K  |
|       from large pool |     877    |     878    |   17475 K  |   17474 K  |
|       from small pool |    6346    |    6349    |   21487 K  |   21481 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7223    |    7226    |   38963 K  |   38956 K  |
|       from large pool |     877    |     878    |   17475 K  |   17474 K  |
|       from small pool |    6346    |    6349    |   21487 K  |   21481 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     614    |     615    |    7650    |    7036    |
|       from large pool |     401    |     401    |    3566    |    3165    |
|       from small pool |     213    |     214    |    4084    |    3871    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     491    |     491    |   21710 K  |   21709 K  |
|       from large pool |     106    |     106    |   11061 K  |   11061 K  |
|       from small pool |     385    |     385    |   10648 K  |   10647 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:12:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:12:49] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:12:56]    INFO >> epoch 014:    346 / 1539 loss=3.15, wps=3710.5, ups=5.57, wpb=666.2, bsz=666.2, num_updates=20300, lr=8.3e-05, gnorm=1.55, clip=0, train_wall=8, gb_free=74.4, wall=3385 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:13:06]    INFO >> epoch 014:    396 / 1539 loss=3.119, wps=4004.7, ups=5.43, wpb=736.9, bsz=736.9, num_updates=20350, lr=8.3e-05, gnorm=1.742, clip=0, train_wall=9, gb_free=17.4, wall=3394 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:13:15]    INFO >> epoch 014:    446 / 1539 loss=3.305, wps=4212.1, ups=6.52, wpb=646, bsz=646, num_updates=20400, lr=8.3e-05, gnorm=1.982, clip=0, train_wall=7, gb_free=74.1, wall=3402 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:13:23]    INFO >> epoch 014:    496 / 1539 loss=3.092, wps=4190.5, ups=6.33, wpb=661.7, bsz=661.7, num_updates=20450, lr=8.3e-05, gnorm=1.75, clip=0, train_wall=7, gb_free=73.1, wall=3410 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:13:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 27.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.93 GiB is allocated by PyTorch, and 1.66 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:13:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:13:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:13:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 54           |        cudaMalloc retries: 88        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78717 MiB |  78777 MiB | 628367 GiB | 628290 GiB |
|       from large pool |  78585 MiB |  78645 MiB | 625071 GiB | 624994 GiB |
|       from small pool |    132 MiB |    133 MiB |   3295 GiB |   3295 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78717 MiB |  78777 MiB | 628367 GiB | 628290 GiB |
|       from large pool |  78585 MiB |  78645 MiB | 625071 GiB | 624994 GiB |
|       from small pool |    132 MiB |    133 MiB |   3295 GiB |   3295 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78639 MiB |  78698 MiB | 627457 GiB | 627380 GiB |
|       from large pool |  78507 MiB |  78567 MiB | 624167 GiB | 624090 GiB |
|       from small pool |    131 MiB |    132 MiB |   3290 GiB |   3290 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80478 MiB |  80480 MiB | 764348 MiB | 683870 MiB |
|       from large pool |  80342 MiB |  80342 MiB | 756066 MiB | 675724 MiB |
|       from small pool |    136 MiB |    138 MiB |   8282 MiB |   8146 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1700 MiB |   8075 MiB | 619242 GiB | 619241 GiB |
|       from large pool |   1696 MiB |   8066 MiB | 615516 GiB | 615514 GiB |
|       from small pool |      3 MiB |     19 MiB |   3726 GiB |   3726 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2702    |    2705    |   39360 K  |   39358 K  |
|       from large pool |     480    |     481    |   17672 K  |   17671 K  |
|       from small pool |    2222    |    2225    |   21688 K  |   21686 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2702    |    2705    |   39360 K  |   39358 K  |
|       from large pool |     480    |     481    |   17672 K  |   17671 K  |
|       from small pool |    2222    |    2225    |   21688 K  |   21686 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     325    |     326    |    7843    |    7518    |
|       from large pool |     257    |     257    |    3702    |    3445    |
|       from small pool |      68    |      69    |    4141    |    4073    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     181    |     182    |   21927 K  |   21927 K  |
|       from large pool |     117    |     120    |   11188 K  |   11188 K  |
|       from small pool |      64    |      65    |   10739 K  |   10739 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:13:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:13:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:13:32]    INFO >> epoch 014:    547 / 1539 loss=3.404, wps=4388.6, ups=5.17, wpb=848.8, bsz=848.8, num_updates=20500, lr=8.3e-05, gnorm=1.9, clip=0, train_wall=9, gb_free=50.1, wall=3419 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:13:40]    INFO >> epoch 014:    597 / 1539 loss=3.194, wps=4765.2, ups=6.19, wpb=770.4, bsz=770.4, num_updates=20550, lr=8.3e-05, gnorm=1.625, clip=0, train_wall=8, gb_free=73.2, wall=3427 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:13:50]    INFO >> epoch 014:    647 / 1539 loss=3.241, wps=4344.1, ups=6.25, wpb=695.2, bsz=695.2, num_updates=20600, lr=8.3e-05, gnorm=1.775, clip=0, train_wall=8, gb_free=73.4, wall=3435 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:13:58]    INFO >> epoch 014:    697 / 1539 loss=3.073, wps=4348.3, ups=6.01, wpb=723.9, bsz=723.9, num_updates=20650, lr=8.3e-05, gnorm=1.59, clip=0, train_wall=8, gb_free=74, wall=3444 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:14:06]    INFO >> epoch 014:    747 / 1539 loss=3.078, wps=4273.9, ups=6.31, wpb=677.7, bsz=677.7, num_updates=20700, lr=8.3e-05, gnorm=1.625, clip=0, train_wall=7, gb_free=71.6, wall=3452 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:14:14]    INFO >> epoch 014:    797 / 1539 loss=3.227, wps=4331.6, ups=6.33, wpb=684.6, bsz=684.6, num_updates=20750, lr=8.3e-05, gnorm=1.906, clip=0, train_wall=7, gb_free=74.7, wall=3460 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:14:24]    INFO >> epoch 014:    847 / 1539 loss=3.259, wps=4062.9, ups=5.81, wpb=698.9, bsz=698.9, num_updates=20800, lr=8.3e-05, gnorm=1.814, clip=0, train_wall=8, gb_free=70.9, wall=3468 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:14:32]    INFO >> epoch 014:    897 / 1539 loss=3.133, wps=4595.7, ups=6.26, wpb=733.7, bsz=733.7, num_updates=20850, lr=8.3e-05, gnorm=1.96, clip=0, train_wall=7, gb_free=58.1, wall=3476 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:14:40]    INFO >> epoch 014:    947 / 1539 loss=3.131, wps=4463, ups=6.25, wpb=713.7, bsz=713.7, num_updates=20900, lr=8.3e-05, gnorm=1.631, clip=0, train_wall=7, gb_free=71.3, wall=3484 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:14:47]    INFO >> epoch 014:    997 / 1539 loss=3.1, wps=3971.1, ups=6.55, wpb=606.5, bsz=606.5, num_updates=20950, lr=8.3e-05, gnorm=1.645, clip=0, train_wall=7, gb_free=70.6, wall=3492 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:14:55] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB. GPU 2 has a total capacity of 79.14 GiB of which 137.25 MiB is free. Including non-PyTorch memory, this process has 78.98 GiB memory in use. Of the allocated memory 75.55 GiB is allocated by PyTorch, and 2.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:14:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:14:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:14:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 55           |        cudaMalloc retries: 91        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  66234 MiB |  77546 MiB | 642296 GiB | 642231 GiB |
|       from large pool |  66225 MiB |  77537 MiB | 638934 GiB | 638869 GiB |
|       from small pool |      8 MiB |     16 MiB |   3362 GiB |   3362 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  66234 MiB |  77546 MiB | 642296 GiB | 642231 GiB |
|       from large pool |  66225 MiB |  77537 MiB | 638934 GiB | 638869 GiB |
|       from small pool |      8 MiB |     16 MiB |   3362 GiB |   3362 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  66225 MiB |  77534 MiB | 641365 GiB | 641301 GiB |
|       from large pool |  66216 MiB |  77525 MiB | 638008 GiB | 637944 GiB |
|       from small pool |      8 MiB |     16 MiB |   3356 GiB |   3356 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80368 MiB |  80368 MiB |    775 GiB | 713678 MiB |
|       from large pool |  80344 MiB |  80344 MiB |    767 GiB | 705318 MiB |
|       from small pool |     24 MiB |    238 MiB |      8 GiB |   8360 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5641 MiB |   7443 MiB | 632330 GiB | 632325 GiB |
|       from large pool |   5626 MiB |   7428 MiB | 628528 GiB | 628522 GiB |
|       from small pool |     15 MiB |     21 MiB |   3802 GiB |   3802 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     545    |     586    |   40218 K  |   40218 K  |
|       from large pool |     260    |     301    |   18088 K  |   18088 K  |
|       from small pool |     285    |     342    |   22129 K  |   22129 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     545    |     586    |   40218 K  |   40218 K  |
|       from large pool |     260    |     301    |   18088 K  |   18088 K  |
|       from small pool |     285    |     342    |   22129 K  |   22129 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     147    |     376    |    7906    |    7759    |
|       from large pool |     135    |     257    |    3714    |    3579    |
|       from small pool |      12    |     119    |    4192    |    4180    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     151    |     153    |   22403 K  |   22403 K  |
|       from large pool |     127    |     129    |   11460 K  |   11460 K  |
|       from small pool |      24    |      43    |   10943 K  |   10942 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:14:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:14:55] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:14:56]    INFO >> epoch 014:   1048 / 1539 loss=3.261, wps=3642.4, ups=5.48, wpb=664.4, bsz=664.4, num_updates=21000, lr=8.3e-05, gnorm=1.728, clip=0, train_wall=8, gb_free=69.9, wall=3501 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:15:05]    INFO >> epoch 014:   1098 / 1539 loss=3.105, wps=4916.9, ups=6.01, wpb=818.7, bsz=818.7, num_updates=21050, lr=8.3e-05, gnorm=1.818, clip=0, train_wall=8, gb_free=73.5, wall=3509 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:15:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.17 GiB. GPU 2 has a total capacity of 79.14 GiB of which 801.25 MiB is free. Including non-PyTorch memory, this process has 78.33 GiB memory in use. Of the allocated memory 74.62 GiB is allocated by PyTorch, and 3.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:15:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:15:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:15:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 56           |        cudaMalloc retries: 93        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  70626 MiB |  76892 MiB | 644235 GiB | 644166 GiB |
|       from large pool |  70615 MiB |  76881 MiB | 640864 GiB | 640795 GiB |
|       from small pool |     11 MiB |     23 MiB |   3371 GiB |   3371 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  70626 MiB |  76892 MiB | 644235 GiB | 644166 GiB |
|       from large pool |  70615 MiB |  76881 MiB | 640864 GiB | 640795 GiB |
|       from small pool |     11 MiB |     23 MiB |   3371 GiB |   3371 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  70612 MiB |  76876 MiB | 643302 GiB | 643233 GiB |
|       from large pool |  70600 MiB |  76865 MiB | 639935 GiB | 639866 GiB |
|       from small pool |     11 MiB |     23 MiB |   3366 GiB |   3366 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79704 MiB |  79704 MiB |    786 GiB | 725432 MiB |
|       from large pool |  79680 MiB |  79680 MiB |    777 GiB | 716978 MiB |
|       from small pool |     24 MiB |    118 MiB |      8 GiB |   8454 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5253 MiB |   9024 MiB | 634272 GiB | 634267 GiB |
|       from large pool |   5240 MiB |   9011 MiB | 630459 GiB | 630454 GiB |
|       from small pool |     12 MiB |     21 MiB |   3812 GiB |   3812 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     541    |     559    |   40332 K  |   40331 K  |
|       from large pool |     256    |     274    |   18141 K  |   18141 K  |
|       from small pool |     285    |     356    |   22190 K  |   22190 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     541    |     559    |   40332 K  |   40331 K  |
|       from large pool |     256    |     274    |   18141 K  |   18141 K  |
|       from small pool |     285    |     356    |   22190 K  |   22190 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     194    |    7961    |    7865    |
|       from large pool |      84    |     135    |    3722    |    3638    |
|       from small pool |      12    |      59    |    4239    |    4227    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     100    |     100    |   22465 K  |   22465 K  |
|       from large pool |      75    |      75    |   11493 K  |   11493 K  |
|       from small pool |      25    |      47    |   10972 K  |   10972 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:15:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:15:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:15:13]    INFO >> epoch 014:   1149 / 1539 loss=3.081, wps=3920.5, ups=5.76, wpb=680.4, bsz=680.4, num_updates=21100, lr=8.3e-05, gnorm=1.662, clip=0, train_wall=7, gb_free=74.1, wall=3518 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:15:22]    INFO >> epoch 014:   1199 / 1539 loss=3.295, wps=4130.6, ups=6.24, wpb=662.1, bsz=662.1, num_updates=21150, lr=8.3e-05, gnorm=1.655, clip=0, train_wall=8, gb_free=74.7, wall=3526 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:15:32]    INFO >> epoch 014:   1249 / 1539 loss=3.18, wps=3855.5, ups=6.6, wpb=584.3, bsz=584.3, num_updates=21200, lr=8.3e-05, gnorm=1.537, clip=0, train_wall=7, gb_free=73.9, wall=3534 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:15:39]    INFO >> epoch 014:   1299 / 1539 loss=3.266, wps=4088.6, ups=6.65, wpb=614.8, bsz=614.8, num_updates=21250, lr=8.3e-05, gnorm=1.799, clip=0, train_wall=7, gb_free=73.9, wall=3541 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:15:47]    INFO >> epoch 014:   1349 / 1539 loss=3.201, wps=4409.1, ups=6.23, wpb=707.7, bsz=707.7, num_updates=21300, lr=8.3e-05, gnorm=1.908, clip=0, train_wall=8, gb_free=72.3, wall=3549 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:15:55]    INFO >> epoch 014:   1399 / 1539 loss=3.04, wps=4491.5, ups=6.15, wpb=729.9, bsz=729.9, num_updates=21350, lr=8.3e-05, gnorm=1.787, clip=0, train_wall=8, gb_free=72.5, wall=3557 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:16:06]    INFO >> epoch 014:   1449 / 1539 loss=3.081, wps=5067.2, ups=5.63, wpb=899.8, bsz=899.8, num_updates=21400, lr=8.3e-05, gnorm=2.111, clip=0, train_wall=8, gb_free=72.2, wall=3566 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:16:14]    INFO >> epoch 014:   1499 / 1539 loss=3.29, wps=4300.7, ups=6.31, wpb=681.8, bsz=681.8, num_updates=21450, lr=8.3e-05, gnorm=1.842, clip=0, train_wall=7, gb_free=71.2, wall=3574 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:16:20]    INFO >> epoch 014 | loss 3.171 | wps 4079.2 | ups 5.72 | wpb 712.7 | bsz 712.7 | num_updates 21490 | lr 8.3e-05 | gnorm 1.754 | clip 0 | train_wall 234 | gb_free 71.7 | wall 3581 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:16:20] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:16:39]    INFO >> epoch 014 | valid on 'valid' subset | loss 3.498 | wps 8901.9 | wpb 5412.5 | bsz 5412.5 | num_updates 21490 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:16:39]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:16:39]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 14 @ 21490 updates, score 3.498) (writing took 0.014620 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:16:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:16:40]    INFO >> epoch 015:     10 / 1539 loss=3.287, wps=1522.8, ups=1.96, wpb=777.5, bsz=777.5, num_updates=21500, lr=6.4e-05, gnorm=2.092, clip=0, train_wall=8, gb_free=72.5, wall=3600 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:16:48]    INFO >> epoch 015:     60 / 1539 loss=3.18, wps=4093.9, ups=6.49, wpb=631.1, bsz=631.1, num_updates=21550, lr=6.4e-05, gnorm=1.726, clip=0, train_wall=7, gb_free=74.3, wall=3607 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:16:56]    INFO >> epoch 015:    110 / 1539 loss=3.102, wps=4440.2, ups=6.18, wpb=718.1, bsz=718.1, num_updates=21600, lr=6.4e-05, gnorm=1.626, clip=0, train_wall=8, gb_free=73.6, wall=3615 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:17:06]    INFO >> epoch 015:    160 / 1539 loss=3.241, wps=4149.2, ups=6.26, wpb=663.1, bsz=663.1, num_updates=21650, lr=6.4e-05, gnorm=1.669, clip=0, train_wall=8, gb_free=71.2, wall=3623 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:17:13]    INFO >> epoch 015:    210 / 1539 loss=3.111, wps=4092.1, ups=6.34, wpb=645.9, bsz=645.9, num_updates=21700, lr=6.4e-05, gnorm=1.59, clip=0, train_wall=7, gb_free=73.9, wall=3631 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:17:23]    INFO >> epoch 015:    260 / 1539 loss=3.174, wps=5593.1, ups=5.37, wpb=1041.2, bsz=1041.2, num_updates=21750, lr=6.4e-05, gnorm=2.024, clip=0, train_wall=9, gb_free=67.7, wall=3641 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:17:31]    INFO >> epoch 015:    310 / 1539 loss=3.195, wps=4531.2, ups=6.44, wpb=703.4, bsz=703.4, num_updates=21800, lr=6.4e-05, gnorm=1.67, clip=0, train_wall=7, gb_free=71.5, wall=3648 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:17:33] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 1.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:17:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:17:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:17:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 57           |        cudaMalloc retries: 95        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79172 MiB |  79232 MiB | 669457 GiB | 669380 GiB |
|       from large pool |  79035 MiB |  79095 MiB | 665943 GiB | 665866 GiB |
|       from small pool |    136 MiB |    138 MiB |   3514 GiB |   3514 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79172 MiB |  79232 MiB | 669457 GiB | 669380 GiB |
|       from large pool |  79035 MiB |  79095 MiB | 665943 GiB | 665866 GiB |
|       from small pool |    136 MiB |    138 MiB |   3514 GiB |   3514 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79120 MiB |  79179 MiB | 668490 GiB | 668413 GiB |
|       from large pool |  78983 MiB |  79043 MiB | 664982 GiB | 664905 GiB |
|       from small pool |    136 MiB |    137 MiB |   3508 GiB |   3508 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80498 MiB |    790 GiB | 729354 MiB |
|       from large pool |  80356 MiB |  80356 MiB |    782 GiB | 720802 MiB |
|       from small pool |    142 MiB |    238 MiB |      8 GiB |   8552 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1265 MiB |   6300 MiB | 660601 GiB | 660600 GiB |
|       from large pool |   1260 MiB |   6293 MiB | 656631 GiB | 656629 GiB |
|       from small pool |      5 MiB |     19 MiB |   3970 GiB |   3970 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2790    |    2793    |   41940 K  |   41938 K  |
|       from large pool |     488    |     489    |   18815 K  |   18814 K  |
|       from small pool |    2302    |    2305    |   23125 K  |   23123 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2790    |    2793    |   41940 K  |   41938 K  |
|       from large pool |     488    |     489    |   18815 K  |   18814 K  |
|       from small pool |    2302    |    2305    |   23125 K  |   23123 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     228    |     274    |    8144    |    7916    |
|       from large pool |     157    |     157    |    3797    |    3640    |
|       from small pool |      71    |     119    |    4347    |    4276    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     150    |     150    |   23357 K  |   23357 K  |
|       from large pool |      83    |      86    |   11901 K  |   11901 K  |
|       from small pool |      67    |      67    |   11455 K  |   11455 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:17:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:17:33] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:17:41]    INFO >> epoch 015:    361 / 1539 loss=3.156, wps=4067.4, ups=5.49, wpb=740.3, bsz=740.3, num_updates=21850, lr=6.4e-05, gnorm=1.717, clip=0, train_wall=8, gb_free=69.3, wall=3657 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:17:49]    INFO >> epoch 015:    411 / 1539 loss=3.292, wps=4285, ups=6.53, wpb=656, bsz=656, num_updates=21900, lr=6.4e-05, gnorm=1.83, clip=0, train_wall=7, gb_free=72.7, wall=3665 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:17:56]    INFO >> epoch 015:    461 / 1539 loss=3.227, wps=4270.6, ups=6.58, wpb=648.7, bsz=648.7, num_updates=21950, lr=6.4e-05, gnorm=1.697, clip=0, train_wall=7, gb_free=73.4, wall=3673 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:18:05]    INFO >> epoch 015:    511 / 1539 loss=2.869, wps=4962.7, ups=5.6, wpb=885.8, bsz=885.8, num_updates=22000, lr=6.4e-05, gnorm=1.866, clip=0, train_wall=8, gb_free=69.8, wall=3682 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:18:15]    INFO >> epoch 015:    561 / 1539 loss=3.125, wps=4594.8, ups=6.05, wpb=759.1, bsz=759.1, num_updates=22050, lr=6.4e-05, gnorm=1.66, clip=0, train_wall=8, gb_free=75.1, wall=3690 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:18:17] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 823.25 MiB is free. Including non-PyTorch memory, this process has 78.31 GiB memory in use. Of the allocated memory 72.51 GiB is allocated by PyTorch, and 5.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:18:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:18:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:18:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 58           |        cudaMalloc retries: 97        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63158 MiB |  75128 MiB | 676614 GiB | 676552 GiB |
|       from large pool |  63147 MiB |  75117 MiB | 673064 GiB | 673002 GiB |
|       from small pool |     11 MiB |     13 MiB |   3549 GiB |   3549 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63158 MiB |  75128 MiB | 676614 GiB | 676552 GiB |
|       from large pool |  63147 MiB |  75117 MiB | 673064 GiB | 673002 GiB |
|       from small pool |     11 MiB |     13 MiB |   3549 GiB |   3549 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63147 MiB |  75114 MiB | 675636 GiB | 675575 GiB |
|       from large pool |  63136 MiB |  75103 MiB | 672092 GiB | 672031 GiB |
|       from small pool |     11 MiB |     13 MiB |   3544 GiB |   3544 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79682 MiB |  80500 MiB |    794 GiB | 734036 MiB |
|       from large pool |  79660 MiB |  80296 MiB |    786 GiB | 725302 MiB |
|       from small pool |     22 MiB |    204 MiB |      8 GiB |   8734 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9991 MiB |   9991 MiB | 667497 GiB | 667487 GiB |
|       from large pool |   9980 MiB |   9980 MiB | 663485 GiB | 663475 GiB |
|       from small pool |     10 MiB |     17 MiB |   4011 GiB |   4011 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     515    |     557    |   42383 K  |   42383 K  |
|       from large pool |     230    |     272    |   19026 K  |   19025 K  |
|       from small pool |     285    |     342    |   23357 K  |   23357 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     515    |     557    |   42383 K  |   42383 K  |
|       from large pool |     230    |     272    |   19026 K  |   19025 K  |
|       from small pool |     285    |     342    |   23357 K  |   23357 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      95    |     258    |    8177    |    8082    |
|       from large pool |      84    |     156    |    3799    |    3715    |
|       from small pool |      11    |     102    |    4378    |    4367    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     101    |     102    |   23608 K  |   23608 K  |
|       from large pool |      78    |      79    |   12039 K  |   12039 K  |
|       from small pool |      23    |      42    |   11568 K  |   11568 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:18:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:18:17] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:18:23]    INFO >> epoch 015:    612 / 1539 loss=3.193, wps=4196.3, ups=6.07, wpb=691.8, bsz=691.8, num_updates=22100, lr=6.4e-05, gnorm=1.775, clip=0, train_wall=7, gb_free=72.8, wall=3698 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:18:32]    INFO >> epoch 015:    662 / 1539 loss=3.143, wps=4230.6, ups=5.77, wpb=733.6, bsz=733.6, num_updates=22150, lr=6.4e-05, gnorm=1.933, clip=0, train_wall=8, gb_free=73.4, wall=3707 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:18:40]    INFO >> epoch 015:    712 / 1539 loss=3.274, wps=4567.4, ups=6.17, wpb=740.6, bsz=740.6, num_updates=22200, lr=6.4e-05, gnorm=1.98, clip=0, train_wall=8, gb_free=74.4, wall=3715 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:18:50]    INFO >> epoch 015:    762 / 1539 loss=2.944, wps=3964.2, ups=5.68, wpb=698.4, bsz=698.4, num_updates=22250, lr=6.4e-05, gnorm=1.585, clip=0, train_wall=8, gb_free=72.2, wall=3724 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:18:58]    INFO >> epoch 015:    812 / 1539 loss=3.213, wps=4313.6, ups=6.25, wpb=690.6, bsz=690.6, num_updates=22300, lr=6.4e-05, gnorm=1.77, clip=0, train_wall=8, gb_free=72.8, wall=3732 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:19:03] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 938.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 23.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.37 GiB is allocated by PyTorch, and 1.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:19:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:19:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:19:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 59           |        cudaMalloc retries: 99        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73481 MiB |  79417 MiB | 684297 GiB | 684225 GiB |
|       from large pool |  73472 MiB |  79408 MiB | 680712 GiB | 680641 GiB |
|       from small pool |      8 MiB |     12 MiB |   3584 GiB |   3584 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73481 MiB |  79417 MiB | 684297 GiB | 684225 GiB |
|       from large pool |  73472 MiB |  79408 MiB | 680712 GiB | 680641 GiB |
|       from small pool |      8 MiB |     12 MiB |   3584 GiB |   3584 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73471 MiB |  79407 MiB | 683309 GiB | 683238 GiB |
|       from large pool |  73462 MiB |  79398 MiB | 679731 GiB | 679659 GiB |
|       from small pool |      8 MiB |     12 MiB |   3578 GiB |   3578 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80482 MiB |  80482 MiB |    801 GiB | 740628 MiB |
|       from large pool |  80460 MiB |  80460 MiB |    793 GiB | 731834 MiB |
|       from small pool |     22 MiB |     82 MiB |      8 GiB |   8794 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4272 MiB |   4272 MiB | 676190 GiB | 676185 GiB |
|       from large pool |   4259 MiB |   4259 MiB | 672138 GiB | 672134 GiB |
|       from small pool |     13 MiB |     17 MiB |   4051 GiB |   4051 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     589    |   42836 K  |   42836 K  |
|       from large pool |     286    |     304    |   19250 K  |   19250 K  |
|       from small pool |     285    |     342    |   23586 K  |   23585 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     589    |   42836 K  |   42836 K  |
|       from large pool |     286    |     304    |   19250 K  |   19250 K  |
|       from small pool |     285    |     342    |   23586 K  |   23585 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     125    |    8211    |    8115    |
|       from large pool |      85    |      85    |    3803    |    3718    |
|       from small pool |      11    |      41    |    4408    |    4397    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     110    |     110    |   23846 K  |   23846 K  |
|       from large pool |      89    |      89    |   12174 K  |   12174 K  |
|       from small pool |      21    |      39    |   11672 K  |   11672 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:19:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:19:03] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:19:06]    INFO >> epoch 015:    863 / 1539 loss=3.252, wps=3930, ups=5.9, wpb=666.5, bsz=666.5, num_updates=22350, lr=6.4e-05, gnorm=1.666, clip=0, train_wall=7, gb_free=68.9, wall=3740 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:19:15]    INFO >> epoch 015:    913 / 1539 loss=3.185, wps=4556.9, ups=6.03, wpb=755.6, bsz=755.6, num_updates=22400, lr=6.4e-05, gnorm=1.901, clip=0, train_wall=8, gb_free=63.2, wall=3748 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:19:24]    INFO >> epoch 015:    963 / 1539 loss=3.219, wps=4589.2, ups=6.12, wpb=750.1, bsz=750.1, num_updates=22450, lr=6.4e-05, gnorm=1.669, clip=0, train_wall=8, gb_free=72.4, wall=3757 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:19:32]    INFO >> epoch 015:   1013 / 1539 loss=3.122, wps=4374.7, ups=6.69, wpb=653.6, bsz=653.6, num_updates=22500, lr=6.4e-05, gnorm=1.712, clip=0, train_wall=7, gb_free=73.1, wall=3764 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:19:40]    INFO >> epoch 015:   1063 / 1539 loss=3.12, wps=3745.5, ups=6.32, wpb=593, bsz=593, num_updates=22550, lr=6.4e-05, gnorm=1.637, clip=0, train_wall=7, gb_free=73, wall=3772 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:19:48]    INFO >> epoch 015:   1113 / 1539 loss=3.189, wps=4231.1, ups=6.14, wpb=689.6, bsz=689.6, num_updates=22600, lr=6.4e-05, gnorm=1.728, clip=0, train_wall=8, gb_free=69.7, wall=3780 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:19:57]    INFO >> epoch 015:   1163 / 1539 loss=3.224, wps=3799.6, ups=6.33, wpb=600.6, bsz=600.6, num_updates=22650, lr=6.4e-05, gnorm=1.746, clip=0, train_wall=7, gb_free=68.7, wall=3788 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:20:05]    INFO >> epoch 015:   1213 / 1539 loss=3.274, wps=4351.1, ups=6.16, wpb=706.3, bsz=706.3, num_updates=22700, lr=6.4e-05, gnorm=1.855, clip=0, train_wall=8, gb_free=63.2, wall=3796 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:20:13]    INFO >> epoch 015:   1263 / 1539 loss=3.211, wps=5052.4, ups=6.4, wpb=789.9, bsz=789.9, num_updates=22750, lr=6.4e-05, gnorm=1.811, clip=0, train_wall=7, gb_free=72.3, wall=3804 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:20:21]    INFO >> epoch 015:   1313 / 1539 loss=3.122, wps=4939.5, ups=5.98, wpb=826.5, bsz=826.5, num_updates=22800, lr=6.4e-05, gnorm=1.972, clip=0, train_wall=8, gb_free=74.7, wall=3812 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:20:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.99 GiB is allocated by PyTorch, and 1.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:20:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:20:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:20:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 60           |        cudaMalloc retries: 103       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78782 MiB |  78842 MiB | 697328 GiB | 697251 GiB |
|       from large pool |  78390 MiB |  78450 MiB | 693679 GiB | 693602 GiB |
|       from small pool |    392 MiB |    393 MiB |   3649 GiB |   3648 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78782 MiB |  78842 MiB | 697328 GiB | 697251 GiB |
|       from large pool |  78390 MiB |  78450 MiB | 693679 GiB | 693602 GiB |
|       from small pool |    392 MiB |    393 MiB |   3649 GiB |   3648 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78748 MiB |  78807 MiB | 696323 GiB | 696246 GiB |
|       from large pool |  78358 MiB |  78417 MiB | 692679 GiB | 692603 GiB |
|       from small pool |    390 MiB |    391 MiB |   3643 GiB |   3643 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB |    804 GiB | 743420 MiB |
|       from large pool |  80072 MiB |  80072 MiB |    795 GiB | 734622 MiB |
|       from small pool |    432 MiB |    434 MiB |      9 GiB |   8798 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1661 MiB |   4274 MiB | 691152 GiB | 691150 GiB |
|       from large pool |   1621 MiB |   4237 MiB | 687025 GiB | 687024 GiB |
|       from small pool |     39 MiB |     41 MiB |   4126 GiB |   4126 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7333    |    7336    |   43662 K  |   43655 K  |
|       from large pool |     887    |     888    |   19646 K  |   19645 K  |
|       from small pool |    6446    |    6449    |   24016 K  |   24009 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7333    |    7336    |   43662 K  |   43655 K  |
|       from large pool |     887    |     888    |   19646 K  |   19645 K  |
|       from small pool |    6446    |    6449    |   24016 K  |   24009 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     339    |     339    |    8458    |    8119    |
|       from large pool |     123    |     123    |    3843    |    3720    |
|       from small pool |     216    |     217    |    4615    |    4399    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     471    |     472    |   24283 K  |   24283 K  |
|       from large pool |      80    |      80    |   12413 K  |   12413 K  |
|       from small pool |     391    |     392    |   11870 K  |   11870 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:20:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:20:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:20:32]    INFO >> epoch 015:   1364 / 1539 loss=3.189, wps=3706.6, ups=5.45, wpb=679.8, bsz=679.8, num_updates=22850, lr=6.4e-05, gnorm=1.707, clip=0, train_wall=8, gb_free=72.1, wall=3822 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:20:40]    INFO >> epoch 015:   1414 / 1539 loss=3.098, wps=4671.4, ups=6, wpb=778.1, bsz=778.1, num_updates=22900, lr=6.4e-05, gnorm=1.667, clip=0, train_wall=8, gb_free=74.1, wall=3830 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:20:48]    INFO >> epoch 015:   1464 / 1539 loss=3.106, wps=4441.3, ups=6.66, wpb=666.5, bsz=666.5, num_updates=22950, lr=6.4e-05, gnorm=1.719, clip=0, train_wall=7, gb_free=75.7, wall=3837 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:20:55]    INFO >> epoch 015:   1514 / 1539 loss=3.393, wps=4192.2, ups=6.41, wpb=653.7, bsz=653.7, num_updates=23000, lr=6.4e-05, gnorm=1.778, clip=0, train_wall=7, gb_free=71, wall=3845 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:21:01]    INFO >> epoch 015 | loss 3.168 | wps 4077.4 | ups 5.72 | wpb 712.7 | bsz 712.7 | num_updates 23025 | lr 6.4e-05 | gnorm 1.757 | clip 0 | train_wall 233 | gb_free 72.3 | wall 3849 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:21:01] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:21:17]    INFO >> epoch 015 | valid on 'valid' subset | loss 3.5 | wps 9143.4 | wpb 5412.5 | bsz 5412.5 | num_updates 23025 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:21:18]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:21:18]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 15 @ 23025 updates, score 3.5) (writing took 0.014762 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:21:18] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:21:22]    INFO >> epoch 016:     25 / 1539 loss=3.229, wps=1421.6, ups=2.02, wpb=703.6, bsz=703.6, num_updates=23050, lr=4.8e-05, gnorm=1.88, clip=0, train_wall=7, gb_free=73.3, wall=3870 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:21:29]    INFO >> epoch 016:     75 / 1539 loss=3.28, wps=4457.7, ups=6.5, wpb=686.3, bsz=686.3, num_updates=23100, lr=4.8e-05, gnorm=1.772, clip=0, train_wall=7, gb_free=71.5, wall=3878 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:21:39]    INFO >> epoch 016:    125 / 1539 loss=3.242, wps=3792.4, ups=6.3, wpb=602, bsz=602, num_updates=23150, lr=4.8e-05, gnorm=1.605, clip=0, train_wall=7, gb_free=76.3, wall=3886 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:21:46]    INFO >> epoch 016:    175 / 1539 loss=3.14, wps=3797.9, ups=6.31, wpb=602.2, bsz=602.2, num_updates=23200, lr=4.8e-05, gnorm=1.596, clip=0, train_wall=7, gb_free=70.8, wall=3893 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:21:55]    INFO >> epoch 016:    225 / 1539 loss=3.008, wps=4856.9, ups=6.16, wpb=788.5, bsz=788.5, num_updates=23250, lr=4.8e-05, gnorm=1.867, clip=0, train_wall=8, gb_free=65.6, wall=3902 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:22:03]    INFO >> epoch 016:    275 / 1539 loss=3.223, wps=4203.2, ups=5.68, wpb=739.8, bsz=739.8, num_updates=23300, lr=4.8e-05, gnorm=1.755, clip=0, train_wall=8, gb_free=71.7, wall=3910 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:22:12]    INFO >> epoch 016:    325 / 1539 loss=3.296, wps=4548.6, ups=6.54, wpb=695.3, bsz=695.3, num_updates=23350, lr=4.8e-05, gnorm=1.774, clip=0, train_wall=7, gb_free=75, wall=3918 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:22:20]    INFO >> epoch 016:    375 / 1539 loss=3.27, wps=4380.1, ups=6.31, wpb=693.6, bsz=693.6, num_updates=23400, lr=4.8e-05, gnorm=1.946, clip=0, train_wall=7, gb_free=70.1, wall=3926 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:22:24] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.46 GiB. GPU 2 has a total capacity of 79.14 GiB of which 471.25 MiB is free. Including non-PyTorch memory, this process has 78.66 GiB memory in use. Of the allocated memory 75.55 GiB is allocated by PyTorch, and 2.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:22:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:22:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:22:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 61           |        cudaMalloc retries: 104       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  66236 MiB |  77546 MiB | 718499 GiB | 718434 GiB |
|       from large pool |  66227 MiB |  77538 MiB | 714735 GiB | 714670 GiB |
|       from small pool |      8 MiB |     13 MiB |   3763 GiB |   3763 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  66236 MiB |  77546 MiB | 718499 GiB | 718434 GiB |
|       from large pool |  66227 MiB |  77538 MiB | 714735 GiB | 714670 GiB |
|       from small pool |      8 MiB |     13 MiB |   3763 GiB |   3763 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  66225 MiB |  77534 MiB | 717463 GiB | 717398 GiB |
|       from large pool |  66216 MiB |  77525 MiB | 713705 GiB | 713640 GiB |
|       from small pool |      8 MiB |     13 MiB |   3757 GiB |   3757 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80034 MiB |  80444 MiB |    804 GiB | 743890 MiB |
|       from large pool |  80012 MiB |  80012 MiB |    795 GiB | 734682 MiB |
|       from small pool |     22 MiB |    432 MiB |      9 GiB |   9208 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5707 MiB |   7356 MiB | 710433 GiB | 710428 GiB |
|       from large pool |   5694 MiB |   7343 MiB | 706181 GiB | 706176 GiB |
|       from small pool |     13 MiB |     17 MiB |   4252 GiB |   4252 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     545    |     586    |   44961 K  |   44960 K  |
|       from large pool |     260    |     301    |   20193 K  |   20193 K  |
|       from small pool |     285    |     356    |   24768 K  |   24767 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     545    |     586    |   44961 K  |   44960 K  |
|       from large pool |     260    |     301    |   20193 K  |   20193 K  |
|       from small pool |     285    |     356    |   24768 K  |   24767 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     133    |     338    |    8458    |    8325    |
|       from large pool |     122    |     122    |    3843    |    3721    |
|       from small pool |      11    |     216    |    4615    |    4604    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     139    |     140    |   25004 K  |   25004 K  |
|       from large pool |     117    |     118    |   12759 K  |   12759 K  |
|       from small pool |      22    |      46    |   12245 K  |   12245 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:22:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:22:24] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:22:29]    INFO >> epoch 016:    426 / 1539 loss=3.14, wps=3929.9, ups=5.46, wpb=719.4, bsz=719.4, num_updates=23450, lr=4.8e-05, gnorm=1.683, clip=0, train_wall=8, gb_free=72.5, wall=3935 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:22:36] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 57.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 76.40 GiB is allocated by PyTorch, and 2.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:22:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:22:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:22:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 62           |        cudaMalloc retries: 105       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78174 MiB |  78234 MiB | 720599 GiB | 720523 GiB |
|       from large pool |  77788 MiB |  77848 MiB | 716822 GiB | 716746 GiB |
|       from small pool |    385 MiB |    386 MiB |   3777 GiB |   3776 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78174 MiB |  78234 MiB | 720599 GiB | 720523 GiB |
|       from large pool |  77788 MiB |  77848 MiB | 716822 GiB | 716746 GiB |
|       from small pool |    385 MiB |    386 MiB |   3777 GiB |   3776 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78087 MiB |  78147 MiB | 719560 GiB | 719484 GiB |
|       from large pool |  77704 MiB |  77763 MiB | 715789 GiB | 715713 GiB |
|       from small pool |    383 MiB |    384 MiB |   3771 GiB |   3770 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80448 MiB |  80448 MiB |    812 GiB | 751980 MiB |
|       from large pool |  80022 MiB |  80022 MiB |    803 GiB | 742772 MiB |
|       from small pool |    426 MiB |    426 MiB |      9 GiB |   9208 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2213 MiB |   5585 MiB | 712582 GiB | 712580 GiB |
|       from large pool |   2173 MiB |   5580 MiB | 708314 GiB | 708312 GiB |
|       from small pool |     40 MiB |     41 MiB |   4267 GiB |   4267 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7212    |    7215    |   45111 K  |   45104 K  |
|       from large pool |     876    |     877    |   20254 K  |   20254 K  |
|       from small pool |    6336    |    6339    |   24856 K  |   24850 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7212    |    7215    |   45111 K  |   45104 K  |
|       from large pool |     876    |     877    |   20254 K  |   20254 K  |
|       from small pool |    6336    |    6339    |   24856 K  |   24850 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     467    |     467    |    8795    |    8328    |
|       from large pool |     254    |     254    |    3978    |    3724    |
|       from small pool |     213    |     213    |    4817    |    4604    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     493    |     493    |   25088 K  |   25088 K  |
|       from large pool |     109    |     109    |   12798 K  |   12798 K  |
|       from small pool |     384    |     384    |   12290 K  |   12290 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:22:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:22:36] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:22:38]    INFO >> epoch 016:    477 / 1539 loss=3.177, wps=4640.9, ups=5.63, wpb=824.9, bsz=824.9, num_updates=23500, lr=4.8e-05, gnorm=1.724, clip=0, train_wall=8, gb_free=69.3, wall=3944 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:22:48]    INFO >> epoch 016:    527 / 1539 loss=3.269, wps=4208.7, ups=6.38, wpb=659.3, bsz=659.3, num_updates=23550, lr=4.8e-05, gnorm=1.833, clip=0, train_wall=7, gb_free=73.6, wall=3952 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:22:56]    INFO >> epoch 016:    577 / 1539 loss=3.118, wps=4839, ups=6.16, wpb=785, bsz=785, num_updates=23600, lr=4.8e-05, gnorm=1.887, clip=0, train_wall=8, gb_free=75.9, wall=3960 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:23:03]    INFO >> epoch 016:    627 / 1539 loss=3.115, wps=4309.3, ups=6.65, wpb=647.6, bsz=647.6, num_updates=23650, lr=4.8e-05, gnorm=1.558, clip=0, train_wall=7, gb_free=73.1, wall=3967 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:23:12]    INFO >> epoch 016:    677 / 1539 loss=3.141, wps=3674, ups=5.58, wpb=657.9, bsz=657.9, num_updates=23700, lr=4.8e-05, gnorm=1.648, clip=0, train_wall=8, gb_free=71.7, wall=3976 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:23:22]    INFO >> epoch 016:    727 / 1539 loss=3.251, wps=4227.8, ups=6.16, wpb=686.5, bsz=686.5, num_updates=23750, lr=4.8e-05, gnorm=1.783, clip=0, train_wall=8, gb_free=72, wall=3985 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:23:30]    INFO >> epoch 016:    777 / 1539 loss=3.123, wps=4401.9, ups=5.78, wpb=761.6, bsz=761.6, num_updates=23800, lr=4.8e-05, gnorm=1.869, clip=0, train_wall=8, gb_free=61.2, wall=3993 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:23:39]    INFO >> epoch 016:    827 / 1539 loss=3.105, wps=4516.2, ups=6.03, wpb=749, bsz=749, num_updates=23850, lr=4.8e-05, gnorm=1.637, clip=0, train_wall=8, gb_free=71.4, wall=4001 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:23:47]    INFO >> epoch 016:    877 / 1539 loss=3.105, wps=3967.3, ups=5.85, wpb=678.3, bsz=678.3, num_updates=23900, lr=4.8e-05, gnorm=2.083, clip=0, train_wall=8, gb_free=65.9, wall=4010 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:23:56]    INFO >> epoch 016:    927 / 1539 loss=3.311, wps=4771.1, ups=6.62, wpb=720.8, bsz=720.8, num_updates=23950, lr=4.8e-05, gnorm=1.838, clip=0, train_wall=7, gb_free=74.4, wall=4018 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:24:04]    INFO >> epoch 016:    977 / 1539 loss=3.287, wps=4578.2, ups=6.11, wpb=748.9, bsz=748.9, num_updates=24000, lr=4.8e-05, gnorm=1.813, clip=0, train_wall=8, gb_free=71.5, wall=4026 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:24:12]    INFO >> epoch 016:   1027 / 1539 loss=3.216, wps=3890.2, ups=6.3, wpb=617.4, bsz=617.4, num_updates=24050, lr=4.8e-05, gnorm=1.612, clip=0, train_wall=7, gb_free=66.2, wall=4034 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:24:20]    INFO >> epoch 016:   1077 / 1539 loss=3.211, wps=4020.6, ups=6.28, wpb=639.8, bsz=639.8, num_updates=24100, lr=4.8e-05, gnorm=1.802, clip=0, train_wall=7, gb_free=70.6, wall=4042 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:24:27] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 76.99 GiB is allocated by PyTorch, and 1.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:24:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:24:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:24:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 63           |        cudaMalloc retries: 108       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78778 MiB |  78838 MiB | 738859 GiB | 738782 GiB |
|       from large pool |  78645 MiB |  78705 MiB | 734994 GiB | 734918 GiB |
|       from small pool |    132 MiB |    133 MiB |   3865 GiB |   3864 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78778 MiB |  78838 MiB | 738859 GiB | 738782 GiB |
|       from large pool |  78645 MiB |  78705 MiB | 734994 GiB | 734918 GiB |
|       from small pool |    132 MiB |    133 MiB |   3865 GiB |   3864 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78699 MiB |  78758 MiB | 737794 GiB | 737717 GiB |
|       from large pool |  78567 MiB |  78626 MiB | 733935 GiB | 733858 GiB |
|       from small pool |    132 MiB |    133 MiB |   3858 GiB |   3858 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80496 MiB |  80498 MiB |    834 GiB | 773582 MiB |
|       from large pool |  80360 MiB |  80360 MiB |    824 GiB | 763886 MiB |
|       from small pool |    136 MiB |    218 MiB |      9 GiB |   9696 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1657 MiB |   6579 MiB | 730558 GiB | 730556 GiB |
|       from large pool |   1654 MiB |   6572 MiB | 726189 GiB | 726187 GiB |
|       from small pool |      3 MiB |     21 MiB |   4369 GiB |   4369 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2713    |    2716    |   46232 K  |   46229 K  |
|       from large pool |     481    |     482    |   20795 K  |   20795 K  |
|       from small pool |    2232    |    2235    |   25436 K  |   25434 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2713    |    2716    |   46232 K  |   46229 K  |
|       from large pool |     481    |     482    |   20795 K  |   20795 K  |
|       from small pool |    2232    |    2235    |   25436 K  |   25434 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     321    |     360    |    9029    |    8708    |
|       from large pool |     253    |     253    |    4113    |    3860    |
|       from small pool |      68    |     109    |    4916    |    4848    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     178    |     179    |   25700 K  |   25700 K  |
|       from large pool |     113    |     115    |   13142 K  |   13141 K  |
|       from small pool |      65    |      66    |   12558 K  |   12558 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:24:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:24:27] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:24:30]    INFO >> epoch 016:   1128 / 1539 loss=3.162, wps=4846.8, ups=5.84, wpb=830.1, bsz=830.1, num_updates=24150, lr=4.8e-05, gnorm=1.814, clip=0, train_wall=7, gb_free=70.5, wall=4050 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:24:39]    INFO >> epoch 016:   1178 / 1539 loss=3.207, wps=5504.9, ups=5.54, wpb=994.5, bsz=994.5, num_updates=24200, lr=4.8e-05, gnorm=2.056, clip=0, train_wall=9, gb_free=71.9, wall=4059 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:24:47]    INFO >> epoch 016:   1228 / 1539 loss=3.104, wps=4459.2, ups=6.55, wpb=680.8, bsz=680.8, num_updates=24250, lr=4.8e-05, gnorm=1.832, clip=0, train_wall=7, gb_free=71.7, wall=4067 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:24:56]    INFO >> epoch 016:   1278 / 1539 loss=3.162, wps=3814.6, ups=6.46, wpb=590.4, bsz=590.4, num_updates=24300, lr=4.8e-05, gnorm=1.669, clip=0, train_wall=7, gb_free=76.4, wall=4075 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:25:04]    INFO >> epoch 016:   1328 / 1539 loss=3.104, wps=4148.2, ups=6.38, wpb=649.8, bsz=649.8, num_updates=24350, lr=4.8e-05, gnorm=1.613, clip=0, train_wall=7, gb_free=69.1, wall=4082 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:25:12]    INFO >> epoch 016:   1378 / 1539 loss=2.894, wps=4513.7, ups=6.02, wpb=749.3, bsz=749.3, num_updates=24400, lr=4.8e-05, gnorm=1.705, clip=0, train_wall=8, gb_free=71.9, wall=4091 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:25:19]    INFO >> epoch 016:   1428 / 1539 loss=3.197, wps=4860.8, ups=6.61, wpb=734.9, bsz=734.9, num_updates=24450, lr=4.8e-05, gnorm=1.809, clip=0, train_wall=7, gb_free=71.9, wall=4098 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:25:26] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.17 GiB. GPU 2 has a total capacity of 79.14 GiB of which 839.25 MiB is free. Including non-PyTorch memory, this process has 78.30 GiB memory in use. Of the allocated memory 74.62 GiB is allocated by PyTorch, and 3.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:25:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:25:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:25:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 64           |        cudaMalloc retries: 111       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  70625 MiB |  76890 MiB | 748505 GiB | 748436 GiB |
|       from large pool |  70614 MiB |  76879 MiB | 744590 GiB | 744521 GiB |
|       from small pool |     11 MiB |     25 MiB |   3915 GiB |   3915 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  70625 MiB |  76890 MiB | 748505 GiB | 748436 GiB |
|       from large pool |  70614 MiB |  76879 MiB | 744590 GiB | 744521 GiB |
|       from small pool |     11 MiB |     25 MiB |   3915 GiB |   3915 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  70612 MiB |  76876 MiB | 747425 GiB | 747356 GiB |
|       from large pool |  70600 MiB |  76865 MiB | 743515 GiB | 743447 GiB |
|       from small pool |     11 MiB |     25 MiB |   3909 GiB |   3909 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79666 MiB |  80078 MiB |    868 GiB |    791 GiB |
|       from large pool |  79642 MiB |  79840 MiB |    859 GiB |    781 GiB |
|       from small pool |     24 MiB |    238 MiB |      9 GiB |      9 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4732 MiB |   7568 MiB | 739264 GiB | 739259 GiB |
|       from large pool |   4719 MiB |   7555 MiB | 734835 GiB | 734831 GiB |
|       from small pool |     12 MiB |     21 MiB |   4428 GiB |   4428 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     541    |     559    |   46859 K  |   46859 K  |
|       from large pool |     256    |     274    |   21091 K  |   21091 K  |
|       from small pool |     285    |     356    |   25768 K  |   25768 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     541    |     559    |   46859 K  |   46859 K  |
|       from large pool |     256    |     274    |   21091 K  |   21091 K  |
|       from small pool |     285    |     356    |   25768 K  |   25768 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     128    |     423    |    9152    |    9024    |
|       from large pool |     116    |     304    |    4185    |    4069    |
|       from small pool |      12    |     119    |    4967    |    4955    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     130    |     130    |   26056 K  |   26056 K  |
|       from large pool |     104    |     104    |   13337 K  |   13337 K  |
|       from small pool |      26    |      49    |   12718 K  |   12718 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:25:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:25:26] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:25:30]    INFO >> epoch 016:   1479 / 1539 loss=3.099, wps=4006.3, ups=5.69, wpb=704.5, bsz=704.5, num_updates=24500, lr=4.8e-05, gnorm=1.623, clip=0, train_wall=7, gb_free=71.7, wall=4107 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:25:37]    INFO >> epoch 016:   1529 / 1539 loss=2.989, wps=4486.3, ups=6.34, wpb=708.1, bsz=708.1, num_updates=24550, lr=4.8e-05, gnorm=1.564, clip=0, train_wall=7, gb_free=68.5, wall=4115 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:25:39]    INFO >> epoch 016 | loss 3.166 | wps 4088.6 | ups 5.74 | wpb 712.7 | bsz 712.7 | num_updates 24560 | lr 4.8e-05 | gnorm 1.767 | clip 0 | train_wall 232 | gb_free 70.4 | wall 4117 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:25:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:25:55]    INFO >> epoch 016 | valid on 'valid' subset | loss 3.499 | wps 9536.4 | wpb 5412.5 | bsz 5412.5 | num_updates 24560 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:25:55]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:25:55]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 16 @ 24560 updates, score 3.499) (writing took 0.014993 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:25:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:26:01]    INFO >> epoch 017:     40 / 1539 loss=3.098, wps=1498.1, ups=2.08, wpb=718.6, bsz=718.6, num_updates=24600, lr=3.5e-05, gnorm=1.72, clip=0, train_wall=7, gb_free=72, wall=4139 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:26:10]    INFO >> epoch 017:     90 / 1539 loss=3.036, wps=4293.1, ups=6.69, wpb=642.2, bsz=642.2, num_updates=24650, lr=3.5e-05, gnorm=1.687, clip=0, train_wall=7, gb_free=70.9, wall=4146 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:26:20]    INFO >> epoch 017:    140 / 1539 loss=3.244, wps=4510.7, ups=5.28, wpb=854.2, bsz=854.2, num_updates=24700, lr=3.5e-05, gnorm=1.776, clip=0, train_wall=9, gb_free=74.8, wall=4156 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:26:27]    INFO >> epoch 017:    190 / 1539 loss=3.244, wps=4281.7, ups=6.6, wpb=648.4, bsz=648.4, num_updates=24750, lr=3.5e-05, gnorm=1.854, clip=0, train_wall=7, gb_free=72.2, wall=4164 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:26:39] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 938.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 579.25 MiB is free. Including non-PyTorch memory, this process has 78.55 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 683.92 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:26:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:26:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:26:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 65           |        cudaMalloc retries: 113       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73490 MiB |  79430 MiB | 761647 GiB | 761575 GiB |
|       from large pool |  73481 MiB |  79421 MiB | 757651 GiB | 757580 GiB |
|       from small pool |      8 MiB |     17 MiB |   3995 GiB |   3995 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73490 MiB |  79430 MiB | 761647 GiB | 761575 GiB |
|       from large pool |  73481 MiB |  79421 MiB | 757651 GiB | 757580 GiB |
|       from small pool |      8 MiB |     17 MiB |   3995 GiB |   3995 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73471 MiB |  79407 MiB | 760548 GiB | 760476 GiB |
|       from large pool |  73462 MiB |  79398 MiB | 756559 GiB | 756487 GiB |
|       from small pool |      8 MiB |     17 MiB |   3988 GiB |   3988 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79926 MiB |  80114 MiB |    941 GiB |    863 GiB |
|       from large pool |  79904 MiB |  80092 MiB |    931 GiB |    853 GiB |
|       from small pool |     22 MiB |    238 MiB |      9 GiB |      9 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2955 MiB |   3940 MiB | 750685 GiB | 750682 GiB |
|       from large pool |   2942 MiB |   3926 MiB | 746170 GiB | 746167 GiB |
|       from small pool |     13 MiB |     17 MiB |   4514 GiB |   4514 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     589    |   47684 K  |   47683 K  |
|       from large pool |     286    |     304    |   21394 K  |   21394 K  |
|       from small pool |     285    |     356    |   26289 K  |   26289 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     589    |   47684 K  |   47683 K  |
|       from large pool |     286    |     304    |   21394 K  |   21394 K  |
|       from small pool |     285    |     356    |   26289 K  |   26289 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     146    |     232    |    9359    |    9213    |
|       from large pool |     135    |     136    |    4285    |    4150    |
|       from small pool |      11    |     119    |    5074    |    5063    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     146    |     147    |   26538 K  |   26538 K  |
|       from large pool |     122    |     123    |   13530 K  |   13530 K  |
|       from small pool |      24    |      46    |   13008 K  |   13008 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:26:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:26:39] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:26:39]    INFO >> epoch 017:    241 / 1539 loss=3.184, wps=3298.9, ups=4.73, wpb=696.8, bsz=696.8, num_updates=24800, lr=3.5e-05, gnorm=1.65, clip=0, train_wall=8, gb_free=2.4, wall=4174 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:26:46]    INFO >> epoch 017:    291 / 1539 loss=3.248, wps=4345.9, ups=6.84, wpb=635.2, bsz=635.2, num_updates=24850, lr=3.5e-05, gnorm=1.675, clip=0, train_wall=7, gb_free=70.5, wall=4181 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:26:54]    INFO >> epoch 017:    341 / 1539 loss=3.172, wps=4210.4, ups=6.34, wpb=664.5, bsz=664.5, num_updates=24900, lr=3.5e-05, gnorm=1.614, clip=0, train_wall=7, gb_free=69.9, wall=4189 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:27:02]    INFO >> epoch 017:    391 / 1539 loss=3.145, wps=4362.5, ups=6.27, wpb=696, bsz=696, num_updates=24950, lr=3.5e-05, gnorm=1.604, clip=0, train_wall=8, gb_free=76.3, wall=4197 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:27:12]    INFO >> epoch 017:    441 / 1539 loss=3.305, wps=4461.4, ups=6.33, wpb=704.9, bsz=704.9, num_updates=25000, lr=3.5e-05, gnorm=1.691, clip=0, train_wall=7, gb_free=72.5, wall=4205 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:27:20]    INFO >> epoch 017:    491 / 1539 loss=3.228, wps=4462.9, ups=6.19, wpb=721.4, bsz=721.4, num_updates=25050, lr=3.5e-05, gnorm=1.826, clip=0, train_wall=8, gb_free=71.6, wall=4213 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:27:27]    INFO >> epoch 017:    541 / 1539 loss=3.236, wps=4084.8, ups=6.42, wpb=636.3, bsz=636.3, num_updates=25100, lr=3.5e-05, gnorm=1.811, clip=0, train_wall=7, gb_free=62.2, wall=4221 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:27:36]    INFO >> epoch 017:    591 / 1539 loss=3.181, wps=4248.5, ups=6.09, wpb=698.2, bsz=698.2, num_updates=25150, lr=3.5e-05, gnorm=1.607, clip=0, train_wall=8, gb_free=73.3, wall=4229 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:27:41] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.01 GiB is allocated by PyTorch, and 2.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:27:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:27:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:27:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 66           |        cudaMalloc retries: 117       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77772 MiB |  77832 MiB | 771978 GiB | 771902 GiB |
|       from large pool |  77391 MiB |  77451 MiB | 767932 GiB | 767857 GiB |
|       from small pool |    381 MiB |    383 MiB |   4045 GiB |   4045 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77772 MiB |  77832 MiB | 771978 GiB | 771902 GiB |
|       from large pool |  77391 MiB |  77451 MiB | 767932 GiB | 767857 GiB |
|       from small pool |    381 MiB |    383 MiB |   4045 GiB |   4045 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77727 MiB |  77786 MiB | 770864 GiB | 770788 GiB |
|       from large pool |  77347 MiB |  77407 MiB | 766825 GiB | 766749 GiB |
|       from small pool |    379 MiB |    380 MiB |   4039 GiB |   4039 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB |    945 GiB |    866 GiB |
|       from large pool |  80084 MiB |  80084 MiB |    935 GiB |    857 GiB |
|       from small pool |    420 MiB |    422 MiB |     10 GiB |      9 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2671 MiB |   7559 MiB | 761111 GiB | 761108 GiB |
|       from large pool |   2632 MiB |   7555 MiB | 756538 GiB | 756535 GiB |
|       from small pool |     38 MiB |     40 MiB |   4573 GiB |   4573 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7146    |    7149    |   48332 K  |   48325 K  |
|       from large pool |     870    |     871    |   21708 K  |   21707 K  |
|       from small pool |    6276    |    6279    |   26623 K  |   26617 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7146    |    7149    |   48332 K  |   48325 K  |
|       from large pool |     870    |     871    |   21708 K  |   21707 K  |
|       from small pool |    6276    |    6279    |   26623 K  |   26617 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     401    |     401    |    9622    |    9221    |
|       from large pool |     191    |     191    |    4347    |    4156    |
|       from small pool |     210    |     211    |    5275    |    5065    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     505    |     507    |   26890 K  |   26890 K  |
|       from large pool |     126    |     126    |   13729 K  |   13729 K  |
|       from small pool |     379    |     381    |   13161 K  |   13161 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:27:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:27:41] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:27:45]    INFO >> epoch 017:    642 / 1539 loss=3.162, wps=4172.2, ups=6.15, wpb=678.2, bsz=678.2, num_updates=25200, lr=3.5e-05, gnorm=1.67, clip=0, train_wall=7, gb_free=75.1, wall=4237 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:27:53]    INFO >> epoch 017:    692 / 1539 loss=3.369, wps=4347.7, ups=6.17, wpb=704.2, bsz=704.2, num_updates=25250, lr=3.5e-05, gnorm=1.94, clip=0, train_wall=8, gb_free=72.1, wall=4245 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:28:01]    INFO >> epoch 017:    742 / 1539 loss=3.017, wps=4572.3, ups=6.15, wpb=742.9, bsz=742.9, num_updates=25300, lr=3.5e-05, gnorm=1.945, clip=0, train_wall=8, gb_free=74.3, wall=4254 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:28:09]    INFO >> epoch 017:    792 / 1539 loss=3.092, wps=4050.3, ups=6.38, wpb=634.9, bsz=634.9, num_updates=25350, lr=3.5e-05, gnorm=1.597, clip=0, train_wall=7, gb_free=65.7, wall=4261 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:28:11] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 57.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 75.69 GiB is allocated by PyTorch, and 2.87 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:28:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:28:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:28:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 67           |        cudaMalloc retries: 119       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77449 MiB |  77509 MiB | 776778 GiB | 776703 GiB |
|       from large pool |  77329 MiB |  77389 MiB | 772709 GiB | 772634 GiB |
|       from small pool |    119 MiB |    121 MiB |   4068 GiB |   4068 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77449 MiB |  77509 MiB | 776778 GiB | 776703 GiB |
|       from large pool |  77329 MiB |  77389 MiB | 772709 GiB | 772634 GiB |
|       from small pool |    119 MiB |    121 MiB |   4068 GiB |   4068 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77437 MiB |  77496 MiB | 775656 GiB | 775581 GiB |
|       from large pool |  77317 MiB |  77377 MiB | 771594 GiB | 771518 GiB |
|       from small pool |    119 MiB |    120 MiB |   4062 GiB |   4062 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80448 MiB |  80504 MiB |    945 GiB |    867 GiB |
|       from large pool |  80324 MiB |  80324 MiB |    935 GiB |    857 GiB |
|       from small pool |    124 MiB |    420 MiB |     10 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2938 MiB |   9201 MiB | 765245 GiB | 765242 GiB |
|       from large pool |   2934 MiB |   9190 MiB | 760645 GiB | 760642 GiB |
|       from small pool |      4 MiB |     21 MiB |   4599 GiB |   4599 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2482    |    2485    |   48631 K  |   48628 K  |
|       from large pool |     460    |     461    |   21855 K  |   21854 K  |
|       from small pool |    2022    |    2025    |   26776 K  |   26774 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2482    |    2485    |   48631 K  |   48628 K  |
|       from large pool |     460    |     461    |   21855 K  |   21854 K  |
|       from small pool |    2022    |    2025    |   26776 K  |   26774 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     257    |     401    |    9629    |    9372    |
|       from large pool |     195    |     195    |    4352    |    4157    |
|       from small pool |      62    |     210    |    5277    |    5215    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     240    |     241    |   27058 K  |   27057 K  |
|       from large pool |     181    |     181    |   13826 K  |   13825 K  |
|       from small pool |      59    |      60    |   13231 K  |   13231 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:28:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:28:11] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:28:19]    INFO >> epoch 017:    843 / 1539 loss=3.097, wps=4012.4, ups=5.72, wpb=701.9, bsz=701.9, num_updates=25400, lr=3.5e-05, gnorm=1.757, clip=0, train_wall=8, gb_free=72.4, wall=4270 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:28:28]    INFO >> epoch 017:    893 / 1539 loss=3.054, wps=4352.8, ups=5.96, wpb=729.9, bsz=729.9, num_updates=25450, lr=3.5e-05, gnorm=1.648, clip=0, train_wall=8, gb_free=73.5, wall=4279 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:28:35]    INFO >> epoch 017:    943 / 1539 loss=3.137, wps=4429.7, ups=6.46, wpb=685.7, bsz=685.7, num_updates=25500, lr=3.5e-05, gnorm=1.779, clip=0, train_wall=7, gb_free=74.5, wall=4286 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:28:43]    INFO >> epoch 017:    993 / 1539 loss=3.312, wps=4162.9, ups=6.25, wpb=665.9, bsz=665.9, num_updates=25550, lr=3.5e-05, gnorm=1.908, clip=0, train_wall=8, gb_free=73.7, wall=4294 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:28:53]    INFO >> epoch 017:   1043 / 1539 loss=3.27, wps=3969.9, ups=5.81, wpb=683.1, bsz=683.1, num_updates=25600, lr=3.5e-05, gnorm=1.747, clip=0, train_wall=8, gb_free=67.1, wall=4303 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:29:01]    INFO >> epoch 017:   1093 / 1539 loss=3.399, wps=4541, ups=6.27, wpb=724.4, bsz=724.4, num_updates=25650, lr=3.5e-05, gnorm=1.774, clip=0, train_wall=7, gb_free=73, wall=4311 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:29:09]    INFO >> epoch 017:   1143 / 1539 loss=3.033, wps=4455.5, ups=6.35, wpb=702.1, bsz=702.1, num_updates=25700, lr=3.5e-05, gnorm=1.65, clip=0, train_wall=7, gb_free=67.1, wall=4319 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:29:17]    INFO >> epoch 017:   1193 / 1539 loss=2.803, wps=4878.9, ups=6.16, wpb=792.2, bsz=792.2, num_updates=25750, lr=3.5e-05, gnorm=1.748, clip=0, train_wall=8, gb_free=73.3, wall=4327 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:29:26]    INFO >> epoch 017:   1243 / 1539 loss=3.211, wps=4053.4, ups=6.54, wpb=619.5, bsz=619.5, num_updates=25800, lr=3.5e-05, gnorm=1.676, clip=0, train_wall=7, gb_free=71.7, wall=4335 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:29:36]    INFO >> epoch 017:   1293 / 1539 loss=3.152, wps=5053.6, ups=5.12, wpb=988, bsz=988, num_updates=25850, lr=3.5e-05, gnorm=2.025, clip=0, train_wall=9, gb_free=72, wall=4344 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:29:44]    INFO >> epoch 017:   1343 / 1539 loss=3.385, wps=4127.5, ups=6.05, wpb=681.8, bsz=681.8, num_updates=25900, lr=3.5e-05, gnorm=1.791, clip=0, train_wall=8, gb_free=58.4, wall=4353 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:29:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 807.25 MiB is free. Including non-PyTorch memory, this process has 78.33 GiB memory in use. Of the allocated memory 72.51 GiB is allocated by PyTorch, and 5.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:29:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:29:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:29:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 68           |        cudaMalloc retries: 121       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63159 MiB |  75128 MiB |    774 TiB |    774 TiB |
|       from large pool |  63148 MiB |  75117 MiB |    770 TiB |    770 TiB |
|       from small pool |     11 MiB |     20 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  63159 MiB |  75128 MiB |    774 TiB |    774 TiB |
|       from large pool |  63148 MiB |  75117 MiB |    770 TiB |    770 TiB |
|       from small pool |     11 MiB |     20 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63147 MiB |  75114 MiB |    773 TiB |    773 TiB |
|       from large pool |  63136 MiB |  75103 MiB |    769 TiB |    769 TiB |
|       from small pool |     11 MiB |     20 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79698 MiB |  80482 MiB |    967 GiB |    890 GiB |
|       from large pool |  79676 MiB |  80264 MiB |    957 GiB |    879 GiB |
|       from small pool |     22 MiB |    218 MiB |     10 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8422 MiB |   9997 MiB | 779364 GiB | 779355 GiB |
|       from large pool |   8411 MiB |   9986 MiB | 774671 GiB | 774663 GiB |
|       from small pool |     10 MiB |     19 MiB |   4692 GiB |   4692 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     515    |     557    |   49645 K  |   49644 K  |
|       from large pool |     230    |     272    |   22338 K  |   22337 K  |
|       from small pool |     285    |     356    |   27307 K  |   27306 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     515    |     557    |   49645 K  |   49644 K  |
|       from large pool |     230    |     272    |   22338 K  |   22337 K  |
|       from small pool |     285    |     356    |   27307 K  |   27306 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     303    |    9696    |    9600    |
|       from large pool |      85    |     194    |    4372    |    4287    |
|       from small pool |      11    |     109    |    5324    |    5313    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      96    |      98    |   27631 K  |   27631 K  |
|       from large pool |      74    |      76    |   14146 K  |   14146 K  |
|       from small pool |      22    |      44    |   13484 K  |   13484 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:29:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:29:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:29:54]    INFO >> epoch 017:   1394 / 1539 loss=3.085, wps=4039.6, ups=5.23, wpb=772.8, bsz=772.8, num_updates=25950, lr=3.5e-05, gnorm=1.861, clip=0, train_wall=8, gb_free=63.7, wall=4362 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:30:04]    INFO >> epoch 017:   1444 / 1539 loss=3.192, wps=4663.9, ups=5.83, wpb=800.2, bsz=800.2, num_updates=26000, lr=3.5e-05, gnorm=2.096, clip=0, train_wall=8, gb_free=70.8, wall=4371 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:30:12]    INFO >> epoch 017:   1494 / 1539 loss=3.035, wps=4975.8, ups=6.19, wpb=804.2, bsz=804.2, num_updates=26050, lr=3.5e-05, gnorm=1.842, clip=0, train_wall=8, gb_free=72.1, wall=4379 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:30:19]    INFO >> epoch 017 | loss 3.165 | wps 4059.5 | ups 5.7 | wpb 712.7 | bsz 712.7 | num_updates 26095 | lr 3.5e-05 | gnorm 1.761 | clip 0 | train_wall 233 | gb_free 76.4 | wall 4386 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:30:19] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:30:35]    INFO >> epoch 017 | valid on 'valid' subset | loss 3.499 | wps 10199.5 | wpb 5412.5 | bsz 5412.5 | num_updates 26095 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:30:36]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:30:36]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 17 @ 26095 updates, score 3.499) (writing took 0.014002 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 15:30:36] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:30:37]    INFO >> epoch 018:      5 / 1539 loss=3.029, wps=1421.5, ups=2.15, wpb=660.6, bsz=660.6, num_updates=26100, lr=2.6e-05, gnorm=1.666, clip=0, train_wall=7, gb_free=74.2, wall=4402 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:30:45]    INFO >> epoch 018:     55 / 1539 loss=3.34, wps=3966, ups=6.13, wpb=646.5, bsz=646.5, num_updates=26150, lr=2.6e-05, gnorm=1.855, clip=0, train_wall=8, gb_free=71.8, wall=4410 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:30:53]    INFO >> epoch 018:    105 / 1539 loss=3.115, wps=4605.6, ups=6.01, wpb=765.9, bsz=765.9, num_updates=26200, lr=2.6e-05, gnorm=1.814, clip=0, train_wall=8, gb_free=73.3, wall=4418 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:31:01]    INFO >> epoch 018:    155 / 1539 loss=3.01, wps=4287.7, ups=6.4, wpb=669.5, bsz=669.5, num_updates=26250, lr=2.6e-05, gnorm=1.74, clip=0, train_wall=7, gb_free=73.6, wall=4426 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:31:03] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.16 GiB is allocated by PyTorch, and 1.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:31:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:31:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:31:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 69           |        cudaMalloc retries: 122       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78955 MiB |  79015 MiB |    787 TiB |    787 TiB |
|       from large pool |  78821 MiB |  78881 MiB |    783 TiB |    783 TiB |
|       from small pool |    134 MiB |    135 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  78955 MiB |  79015 MiB |    787 TiB |    787 TiB |
|       from large pool |  78821 MiB |  78881 MiB |    783 TiB |    783 TiB |
|       from small pool |    134 MiB |    135 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78879 MiB |  78939 MiB |    786 TiB |    786 TiB |
|       from large pool |  78745 MiB |  78805 MiB |    782 TiB |    782 TiB |
|       from small pool |    133 MiB |    135 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80464 MiB |    976 GiB |    897 GiB |
|       from large pool |  80324 MiB |  80324 MiB |    966 GiB |    887 GiB |
|       from small pool |    138 MiB |    140 MiB |     10 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1446 MiB |   7464 MiB |    773 TiB |    773 TiB |
|       from large pool |   1442 MiB |   7455 MiB |    769 TiB |    769 TiB |
|       from small pool |      3 MiB |     19 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Allocations           |    2746    |    2749    |   50500 K  |   50497 K  |
|       from large pool |     484    |     485    |   22654 K  |   22653 K  |
|       from small pool |    2262    |    2265    |   27846 K  |   27843 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2746    |    2749    |   50500 K  |   50497 K  |
|       from large pool |     484    |     485    |   22654 K  |   22653 K  |
|       from small pool |    2262    |    2265    |   27846 K  |   27843 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     278    |     279    |    9887    |    9609    |
|       from large pool |     209    |     209    |    4504    |    4295    |
|       from small pool |      69    |      70    |    5383    |    5314    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     156    |     157    |   28124 K  |   28124 K  |
|       from large pool |      91    |      94    |   14345 K  |   14345 K  |
|       from small pool |      65    |      66    |   13779 K  |   13779 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:31:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:31:03] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:31:11]    INFO >> epoch 018:    206 / 1539 loss=3.338, wps=3771.7, ups=5.57, wpb=677.6, bsz=677.6, num_updates=26300, lr=2.6e-05, gnorm=1.701, clip=0, train_wall=8, gb_free=72.7, wall=4435 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:31:19]    INFO >> epoch 018:    256 / 1539 loss=3.017, wps=4538.7, ups=6.4, wpb=709, bsz=709, num_updates=26350, lr=2.6e-05, gnorm=1.684, clip=0, train_wall=7, gb_free=74.8, wall=4443 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:31:27]    INFO >> epoch 018:    306 / 1539 loss=3.034, wps=3870.4, ups=6.21, wpb=623.4, bsz=623.4, num_updates=26400, lr=2.6e-05, gnorm=1.535, clip=0, train_wall=8, gb_free=72.8, wall=4451 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:31:35]    INFO >> epoch 018:    356 / 1539 loss=3.103, wps=4695.7, ups=6.12, wpb=766.7, bsz=766.7, num_updates=26450, lr=2.6e-05, gnorm=1.651, clip=0, train_wall=8, gb_free=73.1, wall=4459 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:31:45]    INFO >> epoch 018:    406 / 1539 loss=3.096, wps=4309.3, ups=5.66, wpb=760.9, bsz=760.9, num_updates=26500, lr=2.6e-05, gnorm=1.671, clip=0, train_wall=8, gb_free=69.8, wall=4468 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:31:47] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.70 GiB is free. Including non-PyTorch memory, this process has 77.41 GiB memory in use. Of the allocated memory 72.51 GiB is allocated by PyTorch, and 4.40 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:31:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:31:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:31:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 70           |        cudaMalloc retries: 124       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63159 MiB |  75130 MiB |    794 TiB |    794 TiB |
|       from large pool |  63148 MiB |  75119 MiB |    790 TiB |    790 TiB |
|       from small pool |     11 MiB |     19 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  63159 MiB |  75130 MiB |    794 TiB |    794 TiB |
|       from large pool |  63148 MiB |  75119 MiB |    790 TiB |    790 TiB |
|       from small pool |     11 MiB |     19 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63147 MiB |  75114 MiB |    793 TiB |    793 TiB |
|       from large pool |  63136 MiB |  75103 MiB |    789 TiB |    789 TiB |
|       from small pool |     11 MiB |     19 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78760 MiB |  80402 MiB |    982 GiB |    905 GiB |
|       from large pool |  78738 MiB |  80264 MiB |    972 GiB |    895 GiB |
|       from small pool |     22 MiB |    138 MiB |     10 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9386 MiB |   9579 MiB |    780 TiB |    780 TiB |
|       from large pool |   9375 MiB |   9568 MiB |    775 TiB |    775 TiB |
|       from small pool |     10 MiB |     19 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     515    |     557    |   50917 K  |   50916 K  |
|       from large pool |     230    |     272    |   22861 K  |   22861 K  |
|       from small pool |     285    |     356    |   28055 K  |   28054 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     515    |     557    |   50917 K  |   50916 K  |
|       from large pool |     230    |     272    |   22861 K  |   22861 K  |
|       from small pool |     285    |     356    |   28055 K  |   28054 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      97    |     277    |    9894    |    9797    |
|       from large pool |      86    |     208    |    4511    |    4425    |
|       from small pool |      11    |      69    |    5383    |    5372    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     102    |     102    |   28355 K  |   28354 K  |
|       from large pool |      80    |      80    |   14482 K  |   14482 K  |
|       from small pool |      22    |      42    |   13872 K  |   13872 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:31:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:31:47] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:31:54]    INFO >> epoch 018:    457 / 1539 loss=3.004, wps=4132.9, ups=5.5, wpb=750.9, bsz=750.9, num_updates=26550, lr=2.6e-05, gnorm=1.679, clip=0, train_wall=8, gb_free=69.5, wall=4477 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:32:02]    INFO >> epoch 018:    507 / 1539 loss=3.268, wps=4502.6, ups=6.37, wpb=706.6, bsz=706.6, num_updates=26600, lr=2.6e-05, gnorm=2.003, clip=0, train_wall=7, gb_free=69.4, wall=4485 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:32:11]    INFO >> epoch 018:    557 / 1539 loss=3.251, wps=4743.9, ups=5.97, wpb=794.3, bsz=794.3, num_updates=26650, lr=2.6e-05, gnorm=1.676, clip=0, train_wall=8, gb_free=71.6, wall=4493 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:32:20]    INFO >> epoch 018:    607 / 1539 loss=3.286, wps=4577.3, ups=6.22, wpb=736.1, bsz=736.1, num_updates=26700, lr=2.6e-05, gnorm=1.762, clip=0, train_wall=8, gb_free=64.7, wall=4501 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:32:28]    INFO >> epoch 018:    657 / 1539 loss=3.06, wps=4043.4, ups=6.41, wpb=631.1, bsz=631.1, num_updates=26750, lr=2.6e-05, gnorm=1.631, clip=0, train_wall=7, gb_free=76.3, wall=4509 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:32:36]    INFO >> epoch 018:    707 / 1539 loss=3.035, wps=4035, ups=6.36, wpb=634.9, bsz=634.9, num_updates=26800, lr=2.6e-05, gnorm=1.648, clip=0, train_wall=7, gb_free=58.9, wall=4517 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:32:44]    INFO >> epoch 018:    757 / 1539 loss=3.139, wps=4598.1, ups=6.19, wpb=743.4, bsz=743.4, num_updates=26850, lr=2.6e-05, gnorm=1.75, clip=0, train_wall=8, gb_free=63.9, wall=4525 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:32:51] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 938.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 63.25 MiB is free. Including non-PyTorch memory, this process has 79.05 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 1.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:32:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:32:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:32:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 71           |        cudaMalloc retries: 126       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73483 MiB |  79421 MiB |    804 TiB |    804 TiB |
|       from large pool |  73474 MiB |  79412 MiB |    800 TiB |    800 TiB |
|       from small pool |      8 MiB |     20 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  73483 MiB |  79421 MiB |    804 TiB |    804 TiB |
|       from large pool |  73474 MiB |  79412 MiB |    800 TiB |    800 TiB |
|       from small pool |      8 MiB |     20 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73471 MiB |  79407 MiB |    803 TiB |    803 TiB |
|       from large pool |  73462 MiB |  79398 MiB |    799 TiB |    799 TiB |
|       from small pool |      8 MiB |     20 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80442 MiB |  80442 MiB |    990 GiB |    912 GiB |
|       from large pool |  80420 MiB |  80420 MiB |    979 GiB |    901 GiB |
|       from small pool |     22 MiB |    236 MiB |     10 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4042 MiB |   4728 MiB |    791 TiB |    791 TiB |
|       from large pool |   4029 MiB |   4715 MiB |    786 TiB |    786 TiB |
|       from small pool |     13 MiB |     19 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     571    |     589    |   51566 K  |   51566 K  |
|       from large pool |     286    |     304    |   23175 K  |   23175 K  |
|       from small pool |     285    |     356    |   28391 K  |   28390 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     571    |     589    |   51566 K  |   51566 K  |
|       from large pool |     286    |     304    |   23175 K  |   23175 K  |
|       from small pool |     285    |     356    |   28391 K  |   28390 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     202    |   10007    |    9911    |
|       from large pool |      85    |      85    |    4517    |    4432    |
|       from small pool |      11    |     118    |    5490    |    5479    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     108    |     108    |   28705 K  |   28705 K  |
|       from large pool |      87    |      87    |   14675 K  |   14675 K  |
|       from small pool |      21    |      45    |   14029 K  |   14029 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:32:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:32:51] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:32:54]    INFO >> epoch 018:    808 / 1539 loss=3.138, wps=4304.2, ups=5.88, wpb=731.9, bsz=731.9, num_updates=26900, lr=2.6e-05, gnorm=1.856, clip=0, train_wall=7, gb_free=73.2, wall=4534 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:33:01]    INFO >> epoch 018:    858 / 1539 loss=3.172, wps=4529.3, ups=6.48, wpb=699.1, bsz=699.1, num_updates=26950, lr=2.6e-05, gnorm=1.767, clip=0, train_wall=7, gb_free=66.4, wall=4541 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:33:09]    INFO >> epoch 018:    908 / 1539 loss=3.081, wps=4155.8, ups=6.41, wpb=648.8, bsz=648.8, num_updates=27000, lr=2.6e-05, gnorm=1.687, clip=0, train_wall=7, gb_free=73.4, wall=4549 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:33:17]    INFO >> epoch 018:    958 / 1539 loss=3.348, wps=4039.9, ups=6.31, wpb=640.3, bsz=640.3, num_updates=27050, lr=2.6e-05, gnorm=1.686, clip=0, train_wall=7, gb_free=73.7, wall=4557 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:33:26]    INFO >> epoch 018:   1008 / 1539 loss=3.243, wps=4368.3, ups=6.37, wpb=685.5, bsz=685.5, num_updates=27100, lr=2.6e-05, gnorm=1.732, clip=0, train_wall=7, gb_free=68.6, wall=4565 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:33:34]    INFO >> epoch 018:   1058 / 1539 loss=3.269, wps=4625.5, ups=6.51, wpb=710.7, bsz=710.7, num_updates=27150, lr=2.6e-05, gnorm=1.726, clip=0, train_wall=7, gb_free=69.9, wall=4573 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:33:42]    INFO >> epoch 018:   1108 / 1539 loss=3.358, wps=4666.9, ups=6.18, wpb=755.4, bsz=755.4, num_updates=27200, lr=2.6e-05, gnorm=2.093, clip=0, train_wall=8, gb_free=67.7, wall=4581 (progress_bar.py:258, log())[0m
[33m[2025-11-21 15:33:46] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 51.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.76 GiB is allocated by PyTorch, and 1.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 15:33:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:33:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:33:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 72           |        cudaMalloc retries: 127       |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78544 MiB |  78604 MiB |    813 TiB |    813 TiB |
|       from large pool |  78155 MiB |  78215 MiB |    809 TiB |    809 TiB |
|       from small pool |    389 MiB |    390 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Active memory         |  78544 MiB |  78604 MiB |    813 TiB |    813 TiB |
|       from large pool |  78155 MiB |  78215 MiB |    809 TiB |    809 TiB |
|       from small pool |    389 MiB |    390 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78508 MiB |  78567 MiB |    812 TiB |    812 TiB |
|       from large pool |  78120 MiB |  78179 MiB |    808 TiB |    808 TiB |
|       from small pool |    387 MiB |    388 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80454 MiB |  80454 MiB |    993 GiB |    914 GiB |
|       from large pool |  80024 MiB |  80024 MiB |    982 GiB |    904 GiB |
|       from small pool |    430 MiB |    430 MiB |     11 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1849 MiB |   4460 MiB |    801 TiB |    801 TiB |
|       from large pool |   1808 MiB |   4424 MiB |    797 TiB |    797 TiB |
|       from small pool |     40 MiB |     41 MiB |      4 TiB |      4 TiB |
|---------------------------------------------------------------------------|
| Allocations           |    7289    |    7292    |   52162 K  |   52155 K  |
|       from large pool |     883    |     884    |   23458 K  |   23457 K  |
|       from small pool |    6406    |    6409    |   28704 K  |   28697 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7289    |    7292    |   52162 K  |   52155 K  |
|       from large pool |     883    |     884    |   23458 K  |   23457 K  |
|       from small pool |    6406    |    6409    |   28704 K  |   28697 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     340    |     340    |   10253    |    9913    |
|       from large pool |     125    |     125    |    4559    |    4434    |
|       from small pool |     215    |     215    |    5694    |    5479    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     470    |     471    |   29024 K  |   29024 K  |
|       from large pool |      83    |      83    |   14848 K  |   14848 K  |
|       from small pool |     387    |     388    |   14176 K  |   14175 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:33:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 15:33:46] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 15:33:51]    INFO >> epoch 018:   1159 / 1539 loss=3.278, wps=4188.9, ups=5.82, wpb=719.9, bsz=719.9, num_updates=27250, lr=2.6e-05, gnorm=1.986, clip=0, train_wall=8, gb_free=74.3, wall=4589 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:34:00]    INFO >> epoch 018:   1209 / 1539 loss=3.2, wps=4580.4, ups=6.22, wpb=736.9, bsz=736.9, num_updates=27300, lr=2.6e-05, gnorm=1.807, clip=0, train_wall=8, gb_free=68.2, wall=4597 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:34:09]    INFO >> epoch 018:   1259 / 1539 loss=3.176, wps=4301.7, ups=5.41, wpb=795, bsz=795, num_updates=27350, lr=2.6e-05, gnorm=1.862, clip=0, train_wall=9, gb_free=68.2, wall=4607 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:34:17]    INFO >> epoch 018:   1309 / 1539 loss=3.047, wps=4783.4, ups=6.48, wpb=738.3, bsz=738.3, num_updates=27400, lr=2.6e-05, gnorm=1.834, clip=0, train_wall=7, gb_free=71.8, wall=4614 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:34:25]    INFO >> epoch 018:   1359 / 1539 loss=3.179, wps=4263.2, ups=6.15, wpb=693.2, bsz=693.2, num_updates=27450, lr=2.6e-05, gnorm=1.797, clip=0, train_wall=8, gb_free=71.5, wall=4623 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:34:35]    INFO >> epoch 018:   1409 / 1539 loss=3.178, wps=4649.2, ups=6.18, wpb=752.7, bsz=752.7, num_updates=27500, lr=2.6e-05, gnorm=1.752, clip=0, train_wall=8, gb_free=70.4, wall=4631 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:34:43]    INFO >> epoch 018:   1459 / 1539 loss=3.185, wps=4128.8, ups=5.87, wpb=703.6, bsz=703.6, num_updates=27550, lr=2.6e-05, gnorm=1.856, clip=0, train_wall=8, gb_free=72.4, wall=4639 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:34:51]    INFO >> epoch 018:   1509 / 1539 loss=3.258, wps=4070.3, ups=6.47, wpb=629.4, bsz=629.4, num_updates=27600, lr=2.6e-05, gnorm=1.735, clip=0, train_wall=7, gb_free=73, wall=4647 (progress_bar.py:258, log())[0m
[32m[2025-11-21 15:34:56]    INFO >> epoch 018 | loss 3.163 | wps 4106.5 | ups 5.76 | wpb 712.7 | bsz 712.7 | num_updates 27630 | lr 2.6e-05 | gnorm 1.766 | clip 0 | train_wall 233 | gb_free 70.8 | wall 4652 (progress_bar.py:267, print())[0m
[33m[2025-11-21 15:34:56] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 15:35:12]    INFO >> epoch 018 | valid on 'valid' subset | loss 3.498 | wps 10186.5 | wpb 5412.5 | bsz 5412.5 | num_updates 27630 | best_loss 3.524 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:35:13]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:35:13]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_last.pt (epoch 18 @ 27630 updates, score 3.498) (writing took 0.014742 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-21 15:35:13]    INFO >> æ—©åœ: éªŒè¯æ€§èƒ½å·²10è½®æœªæå‡ (train_enhanced.py:616, single_main())[0m
[32m[2025-11-21 15:35:13]    INFO >> è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ 4602.8 ç§’ (train_enhanced.py:626, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 15:35:13]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 15:35:13]    INFO >> æ‰€æœ‰æ—¥å¿—å·²ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs (train_enhanced.py:631, single_main())[0m
[32m[2025-11-21 15:35:13]    INFO >> 
================================================================================ (train_enhanced.py:634, single_main())[0m
[32m[2025-11-21 15:35:13]    INFO >> å¼€å§‹æµ‹è¯•... (train_enhanced.py:635, single_main())[0m
[32m[2025-11-21 15:35:13]    INFO >> ================================================================================ (train_enhanced.py:636, single_main())[0m
[32m[2025-11-21 15:35:13]    INFO >> åŠ è½½æœ€ä½³checkpoint: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/checkpoint_best.pt (train_enhanced.py:50, run_test_after_training())[0m
[32m[2025-11-21 15:35:13]    INFO >> æµ‹è¯•é›†: test (train_enhanced.py:51, run_test_after_training())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/checkpoint_utils.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(
[32m[2025-11-21 15:36:06]    INFO >> 
================================================================================ (train_enhanced.py:168, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> æµ‹è¯•ç»“æžœ: (train_enhanced.py:169, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> -------------------------------------------------------------------------------- (train_enhanced.py:170, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> å¹³å‡Loss:      7.9390 (train_enhanced.py:171, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> Acc@1:         0.78% (train_enhanced.py:172, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> Acc@5:         2.68% (train_enhanced.py:173, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> Acc@1 (å«any): 0.78% (train_enhanced.py:174, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> Acc@5 (å«any): 2.68% (train_enhanced.py:175, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> ================================================================================ (train_enhanced.py:176, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> æµ‹è¯•ç»“æžœå·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/checkpoints/res.txt (train_enhanced.py:187, run_test_after_training())[0m
[32m[2025-11-21 15:36:06]    INFO >> è®­ç»ƒæ—¥å¿—å·²æ›´æ–°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs (train_enhanced.py:222, run_test_after_training())[0m
[TrainingLogger] æ—¥å¿—ç›®å½•: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs
[TrainingLogger] åŽŸå§‹è¾“å‡ºå°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/training_output.log
[TrainingLogger] Epoch 1 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 2 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 3 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 4 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 5 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 6 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 7 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 8 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 9 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 10 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 11 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 12 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 13 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 14 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 15 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 16 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 17 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
[TrainingLogger] Epoch 18 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/dropout_0/logs/metrics.json
