[32m[2025-11-21 00:55:12]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/config.yml (train_enhanced.py:666, cli_main())[0m
[32m[2025-11-21 00:55:12]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:694, cli_main())[0m
[32m[2025-11-21 00:55:12]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs (train_enhanced.py:561, single_main())[0m
[32m[2025-11-21 00:55:12]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 00:55:12]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 00:55:12]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-21 00:55:23]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 96, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=96, out_features=96, bias=False)
      (2): Dropout(p=0.1, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=96, out_features=96, bias=False)
          (OCCURRENCE_OF): Linear(in_features=96, out_features=96, bias=False)
          (NEXT): Linear(in_features=96, out_features=96, bias=False)
          (SUBTOKEN_OF): Linear(in_features=96, out_features=96, bias=False)
          (COMPUTED_FROM): Linear(in_features=96, out_features=96, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=96, out_features=96, bias=False)
          (NEXT_USE): Linear(in_features=96, out_features=96, bias=False)
          (RETURNS_TO): Linear(in_features=96, out_features=96, bias=False)
          (_CHILD): Linear(in_features=96, out_features=96, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=96, out_features=96, bias=False)
          (_NEXT): Linear(in_features=96, out_features=96, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=96, out_features=96, bias=False)
          (_COMPUTED_FROM): Linear(in_features=96, out_features=96, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=96, out_features=96, bias=False)
          (_NEXT_USE): Linear(in_features=96, out_features=96, bias=False)
          (_RETURNS_TO): Linear(in_features=96, out_features=96, bias=False)
        )
        (rnn_cell): GRUCell(96, 96)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=96, out_features=96, bias=False)
          (OCCURRENCE_OF): Linear(in_features=96, out_features=96, bias=False)
          (NEXT): Linear(in_features=96, out_features=96, bias=False)
          (SUBTOKEN_OF): Linear(in_features=96, out_features=96, bias=False)
          (COMPUTED_FROM): Linear(in_features=96, out_features=96, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=96, out_features=96, bias=False)
          (NEXT_USE): Linear(in_features=96, out_features=96, bias=False)
          (RETURNS_TO): Linear(in_features=96, out_features=96, bias=False)
          (_CHILD): Linear(in_features=96, out_features=96, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=96, out_features=96, bias=False)
          (_NEXT): Linear(in_features=96, out_features=96, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=96, out_features=96, bias=False)
          (_COMPUTED_FROM): Linear(in_features=96, out_features=96, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=96, out_features=96, bias=False)
          (_NEXT_USE): Linear(in_features=96, out_features=96, bias=False)
          (_RETURNS_TO): Linear(in_features=96, out_features=96, bias=False)
        )
        (rnn_cell): GRUCell(192, 96)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=96, out_features=96, bias=False)
      (1): Dropout(p=0.1, inplace=False)
      (2): Linear(in_features=96, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:568, single_main())[0m
[32m[2025-11-21 00:55:23]    INFO >> æ¨¡åž‹: typilus, æŸå¤±å‡½æ•°: TypilusCriterion (train_enhanced.py:569, single_main())[0m
[32m[2025-11-21 00:55:23]    INFO >> æ¨¡åž‹å‚æ•°: 1422243 (å¯è®­ç»ƒ: 1422243) (train_enhanced.py:570, single_main())[0m
[32m[2025-11-21 00:55:23]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 00:55:23]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80579 MB ; used memory = 1340 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 00:55:23]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 00:55:23]    INFO >> ä½¿ç”¨ 1 ä¸ªGPUè®­ç»ƒ (train_enhanced.py:576, single_main())[0m
[32m[2025-11-21 00:55:23]    INFO >> no existing checkpoint found /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-21 00:55:23]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-21 00:56:40]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-21 00:56:40] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 00:56:51]    INFO >> epoch 001:     50 / 1539 loss=5.69, wps=3905.4, ups=5.4, wpb=720, bsz=720, num_updates=50, lr=0.0004, gnorm=6.657, clip=0, train_wall=9, gb_free=71.6, wall=85 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:56:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 23.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.46 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:56:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:56:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:56:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 3         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79255 MiB |  79315 MiB |   2476 GiB |   2398 GiB |
|       from large pool |  79074 MiB |  79134 MiB |   2465 GiB |   2387 GiB |
|       from small pool |    181 MiB |    182 MiB |     10 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79255 MiB |  79315 MiB |   2476 GiB |   2398 GiB |
|       from large pool |  79074 MiB |  79134 MiB |   2465 GiB |   2387 GiB |
|       from small pool |    181 MiB |    182 MiB |     10 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79094 MiB |  79153 MiB |   2470 GiB |   2393 GiB |
|       from large pool |  78913 MiB |  78973 MiB |   2459 GiB |   2382 GiB |
|       from small pool |    180 MiB |    181 MiB |     10 GiB |     10 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80484 MiB |  80484 MiB | 133948 MiB |  53464 MiB |
|       from large pool |  80284 MiB |  80284 MiB | 133636 MiB |  53352 MiB |
|       from small pool |    200 MiB |    200 MiB |    312 MiB |    112 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1168 MiB |   5387 MiB |   1230 GiB |   1228 GiB |
|       from large pool |   1149 MiB |   5381 MiB |   1217 GiB |   1216 GiB |
|       from small pool |     18 MiB |     25 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3378    |    3381    |  122658    |  119280    |
|       from large pool |     585    |     586    |   57467    |   56882    |
|       from small pool |    2793    |    2796    |   65191    |   62398    |
|---------------------------------------------------------------------------|
| Active allocs         |    3378    |    3381    |  122658    |  119280    |
|       from large pool |     585    |     586    |   57467    |   56882    |
|       from small pool |    2793    |    2796    |   65191    |   62398    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     497    |     497    |    1002    |     505    |
|       from large pool |     397    |     420    |     846    |     449    |
|       from small pool |     100    |     100    |     156    |      56    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     272    |     273    |   74230    |   73958    |
|       from large pool |      97    |      97    |   42388    |   42291    |
|       from small pool |     175    |     176    |   31842    |   31667    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:56:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:56:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:57:01]    INFO >> epoch 001:    101 / 1539 loss=5.921, wps=3270.3, ups=5.36, wpb=610.7, bsz=610.7, num_updates=100, lr=0.0004, gnorm=6.296, clip=0, train_wall=7, gb_free=73.5, wall=94 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:57:11]    INFO >> epoch 001:    151 / 1539 loss=6.039, wps=4206.7, ups=5.15, wpb=816.7, bsz=816.7, num_updates=150, lr=0.0004, gnorm=6.383, clip=0, train_wall=9, gb_free=71.4, wall=104 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:57:19]    INFO >> epoch 001:    201 / 1539 loss=6.003, wps=3780.6, ups=5.89, wpb=641.7, bsz=641.7, num_updates=200, lr=0.0004, gnorm=6.141, clip=0, train_wall=8, gb_free=72.3, wall=112 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:57:29]    INFO >> epoch 001:    251 / 1539 loss=5.967, wps=3688.9, ups=5.78, wpb=637.9, bsz=637.9, num_updates=250, lr=0.0004, gnorm=6.588, clip=0, train_wall=8, gb_free=67.3, wall=121 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:57:39]    INFO >> epoch 001:    301 / 1539 loss=5.728, wps=4056.2, ups=5.16, wpb=786.5, bsz=786.5, num_updates=300, lr=0.0004, gnorm=5.832, clip=0, train_wall=9, gb_free=70.7, wall=131 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:57:47]    INFO >> epoch 001:    351 / 1539 loss=5.87, wps=3829.1, ups=5.7, wpb=671.6, bsz=671.6, num_updates=350, lr=0.0004, gnorm=5.968, clip=0, train_wall=8, gb_free=68.8, wall=139 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:57:59]    INFO >> epoch 001:    401 / 1539 loss=5.598, wps=4367.5, ups=5.13, wpb=851.7, bsz=851.7, num_updates=400, lr=0.0004, gnorm=7.122, clip=2, train_wall=9, gb_free=69.9, wall=149 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:58:05] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.55 GiB is free. Including non-PyTorch memory, this process has 77.56 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:58:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:58:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:58:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 5         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77209 MiB |  78288 MiB |  17798 GiB |  17722 GiB |
|       from large pool |  77187 MiB |  78266 MiB |  17725 GiB |  17649 GiB |
|       from small pool |     22 MiB |     23 MiB |     72 GiB |     72 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77209 MiB |  78288 MiB |  17798 GiB |  17722 GiB |
|       from large pool |  77187 MiB |  78266 MiB |  17725 GiB |  17649 GiB |
|       from small pool |     22 MiB |     23 MiB |     72 GiB |     72 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB |  17767 GiB |  17692 GiB |
|       from large pool |  77170 MiB |  78247 MiB |  17695 GiB |  17619 GiB |
|       from small pool |     22 MiB |     23 MiB |     72 GiB |     72 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78916 MiB |  80472 MiB | 203698 MiB | 124782 MiB |
|       from large pool |  78888 MiB |  80224 MiB | 203338 MiB | 124450 MiB |
|       from small pool |     28 MiB |    248 MiB |    360 MiB |    332 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1706 MiB |   5120 MiB |  14116 GiB |  14114 GiB |
|       from large pool |   1700 MiB |   5113 MiB |  14033 GiB |  14031 GiB |
|       from small pool |      5 MiB |     26 MiB |     83 GiB |     83 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     564    |     572    |     848 K  |     848 K  |
|       from large pool |     270    |     278    |     427 K  |     427 K  |
|       from small pool |     294    |     354    |     421 K  |     421 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     564    |     572    |     848 K  |     848 K  |
|       from large pool |     270    |     278    |     427 K  |     427 K  |
|       from small pool |     294    |     354    |     421 K  |     421 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     117    |     520    |    1087    |     970    |
|       from large pool |     103    |     396    |     907    |     804    |
|       from small pool |      14    |     124    |     180    |     166    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     128    |     128    |  496831    |  496703    |
|       from large pool |      99    |      99    |  300973    |  300874    |
|       from small pool |      29    |      56    |  195858    |  195829    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:58:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:58:05] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:58:15]    INFO >> epoch 001:    452 / 1539 loss=5.804, wps=1924.1, ups=3.05, wpb=631.6, bsz=631.6, num_updates=450, lr=0.0004, gnorm=4.998, clip=0, train_wall=9, gb_free=68.1, wall=166 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:58:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.69 GiB is free. Including non-PyTorch memory, this process has 76.42 GiB memory in use. Of the allocated memory 73.71 GiB is allocated by PyTorch, and 2.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:58:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:58:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:58:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75044 MiB |  75474 MiB |  19592 GiB |  19518 GiB |
|       from large pool |  75018 MiB |  75448 MiB |  19513 GiB |  19439 GiB |
|       from small pool |     25 MiB |     26 MiB |     79 GiB |     79 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75044 MiB |  75474 MiB |  19592 GiB |  19518 GiB |
|       from large pool |  75018 MiB |  75448 MiB |  19513 GiB |  19439 GiB |
|       from small pool |     25 MiB |     26 MiB |     79 GiB |     79 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB |  19559 GiB |  19485 GiB |
|       from large pool |  74999 MiB |  75428 MiB |  19480 GiB |  19406 GiB |
|       from small pool |     25 MiB |     26 MiB |     78 GiB |     78 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77748 MiB |  78948 MiB | 274718 MiB | 196970 MiB |
|       from large pool |  77716 MiB |  78888 MiB | 274326 MiB | 196610 MiB |
|       from small pool |     32 MiB |     60 MiB |    392 MiB |    360 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2703 MiB |   5140 MiB |  15997 GiB |  15994 GiB |
|       from large pool |   2697 MiB |   5133 MiB |  15907 GiB |  15904 GiB |
|       from small pool |      6 MiB |     24 MiB |     89 GiB |     89 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     565    |     572    |     926 K  |     926 K  |
|       from large pool |     265    |     272    |     469 K  |     469 K  |
|       from small pool |     300    |     354    |     457 K  |     456 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     565    |     572    |     926 K  |     926 K  |
|       from large pool |     265    |     272    |     469 K  |     469 K  |
|       from small pool |     300    |     354    |     457 K  |     456 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      82    |     133    |    1132    |    1050    |
|       from large pool |      66    |     103    |     936    |     870    |
|       from small pool |      16    |      30    |     196    |     180    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      92    |      94    |  538319    |  538227    |
|       from large pool |      59    |      61    |  327675    |  327616    |
|       from small pool |      33    |      56    |  210644    |  210611    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:58:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:58:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:58:27]    INFO >> epoch 001:    503 / 1539 loss=5.572, wps=2912, ups=4.21, wpb=692.2, bsz=692.2, num_updates=500, lr=0.0004, gnorm=6.616, clip=2, train_wall=9, gb_free=70.9, wall=178 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:58:38]    INFO >> epoch 001:    553 / 1539 loss=5.681, wps=3349.5, ups=5.14, wpb=651.5, bsz=651.5, num_updates=550, lr=0.0004, gnorm=5.094, clip=0, train_wall=9, gb_free=67.6, wall=187 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:58:48]    INFO >> epoch 001:    603 / 1539 loss=5.584, wps=3426.8, ups=5.07, wpb=676.1, bsz=676.1, num_updates=600, lr=0.0004, gnorm=5.62, clip=0, train_wall=9, gb_free=65.3, wall=197 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:58:59]    INFO >> epoch 001:    653 / 1539 loss=5.434, wps=3225.6, ups=4.57, wpb=705.7, bsz=705.7, num_updates=650, lr=0.0004, gnorm=5.785, clip=0, train_wall=10, gb_free=72.2, wall=208 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:59:12]    INFO >> epoch 001:    703 / 1539 loss=5.372, wps=2829.4, ups=4.16, wpb=680.4, bsz=680.4, num_updates=700, lr=0.0004, gnorm=5.336, clip=0, train_wall=11, gb_free=69.1, wall=220 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:59:23]    INFO >> epoch 001:    753 / 1539 loss=5.04, wps=3554.3, ups=4.7, wpb=756.5, bsz=756.5, num_updates=750, lr=0.0004, gnorm=7.41, clip=4, train_wall=10, gb_free=70.1, wall=231 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:59:33]    INFO >> epoch 001:    803 / 1539 loss=5.21, wps=3517.4, ups=4.85, wpb=725.4, bsz=725.4, num_updates=800, lr=0.0004, gnorm=5.739, clip=0, train_wall=10, gb_free=55.3, wall=241 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:59:45]    INFO >> epoch 001:    853 / 1539 loss=4.895, wps=3014.9, ups=4.7, wpb=641.9, bsz=641.9, num_updates=850, lr=0.0004, gnorm=5.292, clip=0, train_wall=10, gb_free=68.9, wall=252 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:59:55]    INFO >> epoch 001:    903 / 1539 loss=4.79, wps=3337.8, ups=5.12, wpb=651.9, bsz=651.9, num_updates=900, lr=0.0004, gnorm=5.034, clip=0, train_wall=9, gb_free=71.4, wall=261 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:00:05]    INFO >> epoch 001:    953 / 1539 loss=4.552, wps=3458.7, ups=4.82, wpb=717.7, bsz=717.7, num_updates=950, lr=0.0004, gnorm=6.116, clip=0, train_wall=9, gb_free=68.7, wall=272 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:00:09] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 77.58 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:00:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:00:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:00:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77289 MiB |  77889 MiB |  40735 GiB |  40659 GiB |
|       from large pool |  77273 MiB |  77873 MiB |  40581 GiB |  40506 GiB |
|       from small pool |     16 MiB |     24 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77289 MiB |  77889 MiB |  40735 GiB |  40659 GiB |
|       from large pool |  77273 MiB |  77873 MiB |  40581 GiB |  40506 GiB |
|       from small pool |     16 MiB |     24 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77280 MiB |  77879 MiB |  40674 GiB |  40598 GiB |
|       from large pool |  77263 MiB |  77862 MiB |  40520 GiB |  40445 GiB |
|       from small pool |     16 MiB |     24 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78932 MiB |  80482 MiB | 344502 MiB | 265570 MiB |
|       from large pool |  78904 MiB |  80376 MiB | 343982 MiB | 265078 MiB |
|       from small pool |     28 MiB |    106 MiB |    520 MiB |    492 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1642 MiB |   5057 MiB |  40343 GiB |  40342 GiB |
|       from large pool |   1630 MiB |   5044 MiB |  40170 GiB |  40168 GiB |
|       from small pool |     11 MiB |     25 MiB |    173 GiB |    173 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     526    |     533    |    1855 K  |    1854 K  |
|       from large pool |     239    |     246    |     964 K  |     964 K  |
|       from small pool |     287    |     354    |     890 K  |     890 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     526    |     533    |    1855 K  |    1854 K  |
|       from large pool |     239    |     246    |     964 K  |     964 K  |
|       from small pool |     287    |     354    |     890 K  |     890 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |     118    |    1230    |    1156    |
|       from large pool |      60    |      65    |     970    |     910    |
|       from small pool |      14    |      53    |     260    |     246    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      89    |    1028 K  |    1028 K  |
|       from large pool |      61    |      61    |     630 K  |     630 K  |
|       from small pool |      28    |      55    |     397 K  |     397 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:00:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:00:09] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:00:19]    INFO >> epoch 001:   1004 / 1539 loss=4.47, wps=2618.3, ups=4.02, wpb=651.1, bsz=651.1, num_updates=1000, lr=0.0004, gnorm=6.153, clip=0, train_wall=9, gb_free=67.4, wall=284 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:00:30]    INFO >> epoch 001:   1054 / 1539 loss=4.45, wps=3663.5, ups=4.46, wpb=822.1, bsz=822.1, num_updates=1050, lr=0.0004, gnorm=7.48, clip=2, train_wall=10, gb_free=65.4, wall=296 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:00:41] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 722.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 83.25 MiB is free. Including non-PyTorch memory, this process has 79.04 GiB memory in use. Of the allocated memory 73.99 GiB is allocated by PyTorch, and 4.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:00:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:00:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:00:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72193 MiB |  76053 MiB |  46922 GiB |  46852 GiB |
|       from large pool |  72175 MiB |  76035 MiB |  46744 GiB |  46673 GiB |
|       from small pool |     18 MiB |     21 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72193 MiB |  76053 MiB |  46922 GiB |  46852 GiB |
|       from large pool |  72175 MiB |  76035 MiB |  46744 GiB |  46673 GiB |
|       from small pool |     18 MiB |     21 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72184 MiB |  76043 MiB |  46853 GiB |  46783 GiB |
|       from large pool |  72166 MiB |  76025 MiB |  46675 GiB |  46605 GiB |
|       from small pool |     18 MiB |     21 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80422 MiB |  80422 MiB | 348978 MiB | 268556 MiB |
|       from large pool |  80388 MiB |  80388 MiB | 348276 MiB | 267888 MiB |
|       from small pool |     34 MiB |    210 MiB |    702 MiB |    668 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5376 MiB |  10840 MiB |  47553 GiB |  47547 GiB |
|       from large pool |   5360 MiB |  10823 MiB |  47351 GiB |  47345 GiB |
|       from small pool |     15 MiB |     31 MiB |    202 GiB |    202 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     507    |     516    |    2140 K  |    2139 K  |
|       from large pool |     220    |     229    |    1104 K  |    1103 K  |
|       from small pool |     287    |     354    |    1035 K  |    1035 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     507    |     516    |    2140 K  |    2139 K  |
|       from large pool |     220    |     229    |    1104 K  |    1103 K  |
|       from small pool |     287    |     354    |    1035 K  |    1035 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |     165    |    1323    |    1249    |
|       from large pool |      57    |      60    |     972    |     915    |
|       from small pool |      17    |     105    |     351    |     334    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      83    |      83    |    1181 K  |    1181 K  |
|       from large pool |      53    |      53    |     712 K  |     712 K  |
|       from small pool |      30    |      67    |     468 K  |     468 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:00:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:00:41] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:00:43]    INFO >> epoch 001:   1105 / 1539 loss=4.715, wps=3631.4, ups=3.9, wpb=930.5, bsz=930.5, num_updates=1100, lr=0.0004, gnorm=6.031, clip=0, train_wall=11, gb_free=58.8, wall=308 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:00:55]    INFO >> epoch 001:   1155 / 1539 loss=4.364, wps=3347.8, ups=4.85, wpb=690.2, bsz=690.2, num_updates=1150, lr=0.0004, gnorm=6.36, clip=0, train_wall=9, gb_free=54.3, wall=319 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:01:05]    INFO >> epoch 001:   1205 / 1539 loss=4.398, wps=3295.8, ups=4.87, wpb=676.1, bsz=676.1, num_updates=1200, lr=0.0004, gnorm=6.425, clip=2, train_wall=9, gb_free=61.7, wall=329 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:01:16]    INFO >> epoch 001:   1255 / 1539 loss=4.378, wps=3465, ups=4.71, wpb=735.2, bsz=735.2, num_updates=1250, lr=0.0004, gnorm=6.236, clip=0, train_wall=10, gb_free=70.2, wall=339 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:01:28]    INFO >> epoch 001:   1305 / 1539 loss=4.224, wps=3351.9, ups=4.49, wpb=746.4, bsz=746.4, num_updates=1300, lr=0.0004, gnorm=5.92, clip=0, train_wall=10, gb_free=63.5, wall=351 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:01:38]    INFO >> epoch 001:   1355 / 1539 loss=4.38, wps=3205.6, ups=4.96, wpb=645.9, bsz=645.9, num_updates=1350, lr=0.0004, gnorm=5.634, clip=0, train_wall=9, gb_free=69.7, wall=361 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:01:49]    INFO >> epoch 001:   1405 / 1539 loss=4.152, wps=3200.5, ups=4.51, wpb=710.1, bsz=710.1, num_updates=1400, lr=0.0004, gnorm=5.879, clip=0, train_wall=10, gb_free=58.6, wall=372 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:02:01]    INFO >> epoch 001:   1455 / 1539 loss=4.192, wps=3428.3, ups=4.87, wpb=703.3, bsz=703.3, num_updates=1450, lr=0.0004, gnorm=7.031, clip=0, train_wall=9, gb_free=64.9, wall=382 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:02:12]    INFO >> epoch 001:   1505 / 1539 loss=4.105, wps=3134.9, ups=4.52, wpb=693.7, bsz=693.7, num_updates=1500, lr=0.0004, gnorm=6.672, clip=2, train_wall=10, gb_free=69.1, wall=393 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:02:19]    INFO >> epoch 001 | loss 5.054 | wps 3361.3 | ups 4.73 | wpb 711.1 | bsz 711.1 | num_updates 1534 | lr 0.0004 | gnorm 6.155 | clip 0.5 | train_wall 285 | gb_free 75 | wall 400 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:02:19] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:02:43]    INFO >> epoch 001 | valid on 'valid' subset | loss 4.043 | wps 6754.5 | wpb 5412.5 | bsz 5412.5 | num_updates 1534 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-21 01:02:43]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:02:43]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_best.pt (epoch 1 @ 1534 updates, score 4.043) (writing took 0.022064 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:02:43] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 01:02:46]    INFO >> epoch 002:     16 / 1539 loss=4.146, wps=1108.9, ups=1.51, wpb=735.1, bsz=735.1, num_updates=1550, lr=0.0004, gnorm=6.438, clip=0, train_wall=9, gb_free=70.4, wall=426 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:02:56]    INFO >> epoch 002:     66 / 1539 loss=4.043, wps=3594.8, ups=5.46, wpb=658.3, bsz=658.3, num_updates=1600, lr=0.0004, gnorm=5.98, clip=0, train_wall=9, gb_free=68, wall=435 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:03:07]    INFO >> epoch 002:    116 / 1539 loss=4.115, wps=3479.3, ups=4.87, wpb=713.9, bsz=713.9, num_updates=1650, lr=0.0004, gnorm=5.859, clip=0, train_wall=10, gb_free=68.7, wall=446 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:03:17]    INFO >> epoch 002:    166 / 1539 loss=3.79, wps=3690.3, ups=4.97, wpb=743.2, bsz=743.2, num_updates=1700, lr=0.0004, gnorm=5.998, clip=0, train_wall=10, gb_free=68.3, wall=456 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:03:28]    INFO >> epoch 002:    216 / 1539 loss=4.24, wps=3418.2, ups=4.85, wpb=704.6, bsz=704.6, num_updates=1750, lr=0.0004, gnorm=6.338, clip=0, train_wall=10, gb_free=69.2, wall=466 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:03:40]    INFO >> epoch 002:    266 / 1539 loss=3.781, wps=3816, ups=4.36, wpb=876, bsz=876, num_updates=1800, lr=0.0004, gnorm=6.806, clip=0, train_wall=11, gb_free=61.4, wall=478 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:03:50]    INFO >> epoch 002:    316 / 1539 loss=4.051, wps=3167.9, ups=4.94, wpb=640.7, bsz=640.7, num_updates=1850, lr=0.0004, gnorm=5.868, clip=2, train_wall=10, gb_free=65.7, wall=488 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:04:01]    INFO >> epoch 002:    366 / 1539 loss=3.969, wps=3233.2, ups=4.92, wpb=656.7, bsz=656.7, num_updates=1900, lr=0.0004, gnorm=6.068, clip=0, train_wall=10, gb_free=65.3, wall=498 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:04:13]    INFO >> epoch 002:    416 / 1539 loss=3.982, wps=3156.3, ups=4.43, wpb=712.6, bsz=712.6, num_updates=1950, lr=0.0004, gnorm=5.195, clip=0, train_wall=11, gb_free=68.1, wall=509 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:04:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.87 GiB is free. Including non-PyTorch memory, this process has 76.25 GiB memory in use. Of the allocated memory 72.07 GiB is allocated by PyTorch, and 3.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:04:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:04:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:04:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 13        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73200 MiB |  74277 MiB |  89318 GiB |  89246 GiB |
|       from large pool |  73184 MiB |  74260 MiB |  88966 GiB |  88894 GiB |
|       from small pool |     16 MiB |     27 MiB |    351 GiB |    351 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73200 MiB |  74277 MiB |  89318 GiB |  89246 GiB |
|       from large pool |  73184 MiB |  74260 MiB |  88966 GiB |  88894 GiB |
|       from small pool |     16 MiB |     27 MiB |    351 GiB |    351 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73189 MiB |  74265 MiB |  89199 GiB |  89127 GiB |
|       from large pool |  73173 MiB |  74248 MiB |  88848 GiB |  88776 GiB |
|       from small pool |     16 MiB |     27 MiB |    351 GiB |    351 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77566 MiB |  77746 MiB | 349154 MiB | 271588 MiB |
|       from large pool |  77536 MiB |  77536 MiB | 348276 MiB | 270740 MiB |
|       from small pool |     30 MiB |    210 MiB |    878 MiB |    848 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4365 MiB |   7738 MiB |  94371 GiB |  94367 GiB |
|       from large pool |   4351 MiB |   7724 MiB |  93979 GiB |  93974 GiB |
|       from small pool |     13 MiB |     27 MiB |    392 GiB |    392 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     524    |     532    |    4064 K  |    4064 K  |
|       from large pool |     238    |     246    |    2004 K  |    2003 K  |
|       from small pool |     286    |     356    |    2060 K  |    2060 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     524    |     532    |    4064 K  |    4064 K  |
|       from large pool |     238    |     246    |    2004 K  |    2003 K  |
|       from small pool |     286    |     356    |    2060 K  |    2060 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      71    |     161    |    1411    |    1340    |
|       from large pool |      56    |      56    |     972    |     916    |
|       from small pool |      15    |     105    |     439    |     424    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      82    |      82    |    2223 K  |    2223 K  |
|       from large pool |      52    |      52    |    1249 K  |    1249 K  |
|       from small pool |      30    |      60    |     974 K  |     974 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:04:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:04:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:04:25]    INFO >> epoch 002:    467 / 1539 loss=3.992, wps=3207.8, ups=4.43, wpb=724.9, bsz=724.9, num_updates=2000, lr=0.0004, gnorm=5.593, clip=0, train_wall=10, gb_free=67.4, wall=520 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:04:35]    INFO >> epoch 002:    517 / 1539 loss=4.054, wps=3383.2, ups=4.79, wpb=706.2, bsz=706.2, num_updates=2050, lr=0.0004, gnorm=5.93, clip=0, train_wall=10, gb_free=72.9, wall=531 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:04:46]    INFO >> epoch 002:    567 / 1539 loss=3.966, wps=3193.6, ups=5.02, wpb=636.1, bsz=636.1, num_updates=2100, lr=0.0004, gnorm=6.124, clip=0, train_wall=9, gb_free=59.6, wall=541 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:04:57]    INFO >> epoch 002:    617 / 1539 loss=4.039, wps=4061.5, ups=4.73, wpb=859.3, bsz=859.3, num_updates=2150, lr=0.0004, gnorm=6.298, clip=0, train_wall=10, gb_free=67.8, wall=551 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:05:07]    INFO >> epoch 002:    667 / 1539 loss=3.77, wps=3630.2, ups=4.7, wpb=773, bsz=773, num_updates=2200, lr=0.0004, gnorm=6.419, clip=2, train_wall=10, gb_free=69.3, wall=562 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:05:18]    INFO >> epoch 002:    717 / 1539 loss=3.909, wps=3708.1, ups=5.22, wpb=710.8, bsz=710.8, num_updates=2250, lr=0.0004, gnorm=5.844, clip=0, train_wall=9, gb_free=68.4, wall=572 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:05:29]    INFO >> epoch 002:    767 / 1539 loss=3.963, wps=3296.3, ups=4.9, wpb=673, bsz=673, num_updates=2300, lr=0.0004, gnorm=5.461, clip=0, train_wall=10, gb_free=67.6, wall=582 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:05:39]    INFO >> epoch 002:    817 / 1539 loss=4.112, wps=3106.2, ups=4.8, wpb=647.4, bsz=647.4, num_updates=2350, lr=0.0004, gnorm=5.34, clip=0, train_wall=10, gb_free=70.4, wall=592 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:05:51]    INFO >> epoch 002:    867 / 1539 loss=3.998, wps=3361.4, ups=4.63, wpb=726.4, bsz=726.4, num_updates=2400, lr=0.0004, gnorm=6.074, clip=0, train_wall=10, gb_free=66.5, wall=603 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:06:01]    INFO >> epoch 002:    917 / 1539 loss=3.971, wps=3253.2, ups=4.86, wpb=668.7, bsz=668.7, num_updates=2450, lr=0.0004, gnorm=5.241, clip=0, train_wall=10, gb_free=63.4, wall=613 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:06:11]    INFO >> epoch 002:    967 / 1539 loss=3.804, wps=3238.7, ups=5.13, wpb=631.4, bsz=631.4, num_updates=2500, lr=0.0004, gnorm=5.276, clip=0, train_wall=9, gb_free=72, wall=623 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:06:12] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.58 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:06:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:06:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:06:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 15        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79383 MiB |  79443 MiB | 112747 GiB | 112669 GiB |
|       from large pool |  79199 MiB |  79259 MiB | 112306 GiB | 112229 GiB |
|       from small pool |    183 MiB |    185 MiB |    440 GiB |    440 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79383 MiB |  79443 MiB | 112747 GiB | 112669 GiB |
|       from large pool |  79199 MiB |  79259 MiB | 112306 GiB | 112229 GiB |
|       from small pool |    183 MiB |    185 MiB |    440 GiB |    440 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79334 MiB |  79394 MiB | 112598 GiB | 112521 GiB |
|       from large pool |  79151 MiB |  79211 MiB | 112158 GiB | 112081 GiB |
|       from small pool |    183 MiB |    184 MiB |    439 GiB |    439 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 352134 MiB | 271636 MiB |
|       from large pool |  80296 MiB |  80296 MiB | 351036 MiB | 270740 MiB |
|       from small pool |    202 MiB |    248 MiB |   1098 MiB |    896 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1054 MiB |   5809 MiB | 122332 GiB | 122331 GiB |
|       from large pool |   1036 MiB |   5802 MiB | 121839 GiB | 121838 GiB |
|       from small pool |     18 MiB |     28 MiB |    492 GiB |    492 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3424    |    3427    |    5136 K  |    5133 K  |
|       from large pool |     589    |     590    |    2560 K  |    2560 K  |
|       from small pool |    2835    |    2838    |    2575 K  |    2573 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3424    |    3427    |    5136 K  |    5133 K  |
|       from large pool |     589    |     590    |    2560 K  |    2560 K  |
|       from small pool |    2835    |    2838    |    2575 K  |    2573 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     203    |     225    |    1567    |    1364    |
|       from large pool |     102    |     102    |    1018    |     916    |
|       from small pool |     101    |     124    |     549    |     448    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     239    |     240    |    2783 K  |    2783 K  |
|       from large pool |      59    |      61    |    1578 K  |    1578 K  |
|       from small pool |     180    |     181    |    1205 K  |    1205 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:06:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:06:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 01:06:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.05 GiB is free. Including non-PyTorch memory, this process has 78.07 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 1.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:06:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:06:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:06:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 17        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77208 MiB |  78284 MiB | 114057 GiB | 113982 GiB |
|       from large pool |  77186 MiB |  78262 MiB | 113612 GiB | 113537 GiB |
|       from small pool |     22 MiB |     27 MiB |    445 GiB |    445 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77208 MiB |  78284 MiB | 114057 GiB | 113982 GiB |
|       from large pool |  77186 MiB |  78262 MiB | 113612 GiB | 113537 GiB |
|       from small pool |     22 MiB |     27 MiB |    445 GiB |    445 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 113907 GiB | 113832 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 113463 GiB | 113387 GiB |
|       from small pool |     22 MiB |     27 MiB |    444 GiB |    444 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79434 MiB |  80438 MiB | 353222 MiB | 273788 MiB |
|       from large pool |  79404 MiB |  80236 MiB | 352124 MiB | 272720 MiB |
|       from small pool |     30 MiB |    202 MiB |   1098 MiB |   1068 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2225 MiB |   8258 MiB | 123721 GiB | 123719 GiB |
|       from large pool |   2217 MiB |   8249 MiB | 123223 GiB | 123221 GiB |
|       from small pool |      7 MiB |     31 MiB |    497 GiB |    497 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |    5193 K  |    5193 K  |
|       from large pool |     270    |     278    |    2590 K  |    2590 K  |
|       from small pool |     296    |     356    |    2603 K  |    2603 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |    5193 K  |    5193 K  |
|       from large pool |     270    |     278    |    2590 K  |    2590 K  |
|       from small pool |     296    |     356    |    2603 K  |    2603 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      86    |     202    |    1569    |    1483    |
|       from large pool |      71    |     101    |    1020    |     949    |
|       from small pool |      15    |     101    |     549    |     534    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     107    |     107    |    2816 K  |    2815 K  |
|       from large pool |      76    |      76    |    1597 K  |    1597 K  |
|       from small pool |      31    |      62    |    1218 K  |    1218 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:06:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:06:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:06:24]    INFO >> epoch 002:   1019 / 1539 loss=3.945, wps=3096.6, ups=4.5, wpb=687.7, bsz=687.7, num_updates=2550, lr=0.0004, gnorm=5.944, clip=0, train_wall=9, gb_free=70.8, wall=634 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:06:35]    INFO >> epoch 002:   1069 / 1539 loss=3.781, wps=3546.9, ups=4.61, wpb=769.7, bsz=769.7, num_updates=2600, lr=0.0004, gnorm=5.75, clip=0, train_wall=10, gb_free=71, wall=645 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:06:45]    INFO >> epoch 002:   1119 / 1539 loss=3.573, wps=3723.7, ups=4.69, wpb=794.6, bsz=794.6, num_updates=2650, lr=0.0004, gnorm=6.355, clip=0, train_wall=10, gb_free=63.2, wall=656 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:06:57]    INFO >> epoch 002:   1169 / 1539 loss=4.05, wps=3598.5, ups=4.69, wpb=767.5, bsz=767.5, num_updates=2700, lr=0.0004, gnorm=5.89, clip=2, train_wall=10, gb_free=66.6, wall=666 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:07:08]    INFO >> epoch 002:   1219 / 1539 loss=3.961, wps=3396.4, ups=4.81, wpb=706.2, bsz=706.2, num_updates=2750, lr=0.0004, gnorm=5.159, clip=2, train_wall=10, gb_free=72.9, wall=677 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:07:12] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.79 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.31 GiB is free. Including non-PyTorch memory, this process has 77.80 GiB memory in use. Of the allocated memory 68.26 GiB is allocated by PyTorch, and 9.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:07:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:07:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:07:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65551 MiB |  69899 MiB | 124430 GiB | 124366 GiB |
|       from large pool |  65534 MiB |  69881 MiB | 123944 GiB | 123880 GiB |
|       from small pool |     17 MiB |     20 MiB |    485 GiB |    485 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65551 MiB |  69899 MiB | 124430 GiB | 124366 GiB |
|       from large pool |  65534 MiB |  69881 MiB | 123944 GiB | 123880 GiB |
|       from small pool |     17 MiB |     20 MiB |    485 GiB |    485 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65539 MiB |  69886 MiB | 124266 GiB | 124202 GiB |
|       from large pool |  65521 MiB |  69869 MiB | 123781 GiB | 123717 GiB |
|       from small pool |     17 MiB |     20 MiB |    484 GiB |    484 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79162 MiB |  79630 MiB | 353418 MiB | 274256 MiB |
|       from large pool |  79132 MiB |  79404 MiB | 352124 MiB | 272992 MiB |
|       from small pool |     30 MiB |    226 MiB |   1294 MiB |   1264 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9236 MiB |  11545 MiB | 135402 GiB | 135393 GiB |
|       from large pool |   9223 MiB |  11533 MiB | 134859 GiB | 134850 GiB |
|       from small pool |     12 MiB |     27 MiB |    543 GiB |    543 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |    5670 K  |    5669 K  |
|       from large pool |     201    |     210    |    2830 K  |    2830 K  |
|       from small pool |     288    |     356    |    2840 K  |    2839 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |    5670 K  |    5669 K  |
|       from large pool |     201    |     210    |    2830 K  |    2830 K  |
|       from small pool |     288    |     356    |    2840 K  |    2839 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      85    |     184    |    1667    |    1582    |
|       from large pool |      70    |      71    |    1020    |     950    |
|       from small pool |      15    |     113    |     647    |     632    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      88    |      88    |    3072 K  |    3072 K  |
|       from large pool |      58    |      58    |    1743 K  |    1743 K  |
|       from small pool |      30    |      61    |    1329 K  |    1328 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:07:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:07:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:07:19]    INFO >> epoch 002:   1270 / 1539 loss=3.801, wps=3440.6, ups=4.48, wpb=767.2, bsz=767.2, num_updates=2800, lr=0.0004, gnorm=5.615, clip=0, train_wall=10, gb_free=65.1, wall=688 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:07:28]    INFO >> epoch 002:   1320 / 1539 loss=3.882, wps=3475.1, ups=5.32, wpb=653.8, bsz=653.8, num_updates=2850, lr=0.0004, gnorm=5.099, clip=0, train_wall=9, gb_free=70.3, wall=697 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:07:40]    INFO >> epoch 002:   1370 / 1539 loss=3.896, wps=3624.7, ups=4.88, wpb=742.9, bsz=742.9, num_updates=2900, lr=0.0004, gnorm=5.267, clip=0, train_wall=10, gb_free=58.4, wall=708 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:07:50]    INFO >> epoch 002:   1420 / 1539 loss=3.818, wps=3234.4, ups=5.04, wpb=641.3, bsz=641.3, num_updates=2950, lr=0.0004, gnorm=5.422, clip=0, train_wall=9, gb_free=72.7, wall=717 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:07:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.72 GiB is free. Including non-PyTorch memory, this process has 77.39 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 3.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:07:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:07:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:07:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 19        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75039 MiB |  75469 MiB | 132770 GiB | 132697 GiB |
|       from large pool |  75013 MiB |  75443 MiB | 132253 GiB | 132180 GiB |
|       from small pool |     25 MiB |     31 MiB |    516 GiB |    516 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75039 MiB |  75469 MiB | 132770 GiB | 132697 GiB |
|       from large pool |  75013 MiB |  75443 MiB | 132253 GiB | 132180 GiB |
|       from small pool |     25 MiB |     31 MiB |    516 GiB |    516 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 132596 GiB | 132522 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 132080 GiB | 132007 GiB |
|       from small pool |     25 MiB |     31 MiB |    515 GiB |    515 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78740 MiB |  78842 MiB | 357472 MiB | 278732 MiB |
|       from large pool |  78708 MiB |  78708 MiB | 356074 MiB | 277366 MiB |
|       from small pool |     32 MiB |    134 MiB |   1398 MiB |   1366 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3700 MiB |  10469 MiB | 144898 GiB | 144894 GiB |
|       from large pool |   3694 MiB |  10458 MiB | 144320 GiB | 144317 GiB |
|       from small pool |      6 MiB |     27 MiB |    577 GiB |    577 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |    6047 K  |    6046 K  |
|       from large pool |     265    |     272    |    3030 K  |    3029 K  |
|       from small pool |     302    |     356    |    3017 K  |    3016 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |    6047 K  |    6046 K  |
|       from large pool |     265    |     272    |    3030 K  |    3029 K  |
|       from small pool |     302    |     356    |    3017 K  |    3016 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      85    |     136    |    1720    |    1635    |
|       from large pool |      69    |      69    |    1021    |     952    |
|       from small pool |      16    |      67    |     699    |     683    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      92    |    3271 K  |    3271 K  |
|       from large pool |      57    |      60    |    1865 K  |    1865 K  |
|       from small pool |      32    |      56    |    1405 K  |    1405 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:07:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:07:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:08:00]    INFO >> epoch 002:   1471 / 1539 loss=3.847, wps=3116.1, ups=4.72, wpb=660, bsz=660, num_updates=3000, lr=0.0004, gnorm=5.181, clip=0, train_wall=9, gb_free=67.6, wall=728 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:08:05] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.74 GiB is free. Including non-PyTorch memory, this process has 77.38 GiB memory in use. Of the allocated memory 71.67 GiB is allocated by PyTorch, and 5.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:08:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:08:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:08:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 20        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69620 MiB |  73386 MiB | 134858 GiB | 134790 GiB |
|       from large pool |  69600 MiB |  73366 MiB | 134334 GiB | 134267 GiB |
|       from small pool |     19 MiB |     29 MiB |    523 GiB |    523 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69620 MiB |  73386 MiB | 134858 GiB | 134790 GiB |
|       from large pool |  69600 MiB |  73366 MiB | 134334 GiB | 134267 GiB |
|       from small pool |     19 MiB |     29 MiB |    523 GiB |    523 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69602 MiB |  73368 MiB | 134681 GiB | 134613 GiB |
|       from large pool |  69582 MiB |  73348 MiB | 134158 GiB | 134090 GiB |
|       from small pool |     19 MiB |     29 MiB |    522 GiB |    522 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78726 MiB |  78764 MiB | 357496 MiB | 278770 MiB |
|       from large pool |  78692 MiB |  78708 MiB | 356074 MiB | 277382 MiB |
|       from small pool |     34 MiB |     56 MiB |   1422 MiB |   1388 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5013 MiB |   8691 MiB | 147257 GiB | 147252 GiB |
|       from large pool |   4999 MiB |   8676 MiB | 146671 GiB | 146667 GiB |
|       from small pool |     14 MiB |     26 MiB |    585 GiB |    585 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |    6135 K  |    6135 K  |
|       from large pool |     336    |     344    |    3078 K  |    3078 K  |
|       from small pool |     305    |     356    |    3057 K  |    3057 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |    6135 K  |    6135 K  |
|       from large pool |     336    |     344    |    3078 K  |    3078 K  |
|       from small pool |     305    |     356    |    3057 K  |    3057 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      85    |      97    |    1732    |    1647    |
|       from large pool |      68    |      69    |    1021    |     953    |
|       from small pool |      17    |      28    |     711    |     694    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     100    |     105    |    3317 K  |    3317 K  |
|       from large pool |      70    |      75    |    1894 K  |    1894 K  |
|       from small pool |      30    |      57    |    1422 K  |    1422 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:08:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:08:05] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:08:12]    INFO >> epoch 002:   1522 / 1539 loss=3.757, wps=3189.7, ups=4.87, wpb=655.6, bsz=655.6, num_updates=3050, lr=0.0004, gnorm=5.936, clip=2, train_wall=9, gb_free=70, wall=738 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:08:16]    INFO >> epoch 002 | loss 3.927 | wps 3188.3 | ups 4.48 | wpb 711 | bsz 711 | num_updates 3067 | lr 0.0004 | gnorm 5.782 | clip 0.3 | train_wall 297 | gb_free 68.7 | wall 742 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:08:16] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:08:38]    INFO >> epoch 002 | valid on 'valid' subset | loss 3.725 | wps 7058 | wpb 5412.5 | bsz 5412.5 | num_updates 3067 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:08:39]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:08:39]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 2 @ 3067 updates, score 3.725) (writing took 0.018611 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:08:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:08:45]    INFO >> epoch 003:     33 / 1539 loss=3.828, wps=1106, ups=1.56, wpb=708.8, bsz=708.8, num_updates=3100, lr=0.000392, gnorm=6.328, clip=0, train_wall=10, gb_free=68.2, wall=770 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:08:56]    INFO >> epoch 003:     83 / 1539 loss=3.741, wps=3522.2, ups=4.72, wpb=745.9, bsz=745.9, num_updates=3150, lr=0.000392, gnorm=6.276, clip=0, train_wall=10, gb_free=68.8, wall=781 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:08:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.05 GiB is free. Including non-PyTorch memory, this process has 78.06 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:08:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:08:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:08:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77290 MiB |  77889 MiB | 147470 GiB | 147395 GiB |
|       from large pool |  77273 MiB |  77873 MiB | 146887 GiB | 146812 GiB |
|       from small pool |     16 MiB |     25 MiB |    583 GiB |    583 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77290 MiB |  77889 MiB | 147470 GiB | 147395 GiB |
|       from large pool |  77273 MiB |  77873 MiB | 146887 GiB | 146812 GiB |
|       from small pool |     16 MiB |     25 MiB |    583 GiB |    583 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77280 MiB |  77879 MiB | 147279 GiB | 147203 GiB |
|       from large pool |  77263 MiB |  77862 MiB | 146697 GiB | 146621 GiB |
|       from small pool |     16 MiB |     25 MiB |    582 GiB |    582 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79426 MiB |  79898 MiB | 362760 MiB | 283334 MiB |
|       from large pool |  79396 MiB |  79816 MiB | 361290 MiB | 281894 MiB |
|       from small pool |     30 MiB |     82 MiB |   1470 MiB |   1440 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2075 MiB |   7190 MiB | 157603 GiB | 157601 GiB |
|       from large pool |   2062 MiB |   7176 MiB | 156955 GiB | 156953 GiB |
|       from small pool |     13 MiB |     29 MiB |    647 GiB |    647 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     528    |     535    |    6691 K  |    6690 K  |
|       from large pool |     239    |     246    |    3263 K  |    3263 K  |
|       from small pool |     289    |     356    |    3427 K  |    3426 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     528    |     535    |    6691 K  |    6690 K  |
|       from large pool |     239    |     246    |    3263 K  |    3263 K  |
|       from small pool |     289    |     356    |    3427 K  |    3426 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      78    |     111    |    1759    |    1681    |
|       from large pool |      63    |      70    |    1024    |     961    |
|       from small pool |      15    |      41    |     735    |     720    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      79    |      81    |    3639 K  |    3639 K  |
|       from large pool |      50    |      52    |    2011 K  |    2011 K  |
|       from small pool |      29    |      65    |    1627 K  |    1627 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:08:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:08:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:09:07]    INFO >> epoch 003:    134 / 1539 loss=3.942, wps=3802.2, ups=4.51, wpb=842.2, bsz=842.2, num_updates=3200, lr=0.000392, gnorm=5.376, clip=0, train_wall=10, gb_free=70.4, wall=792 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:09:18]    INFO >> epoch 003:    184 / 1539 loss=3.94, wps=3249.8, ups=4.91, wpb=661.4, bsz=661.4, num_updates=3250, lr=0.000392, gnorm=5.465, clip=0, train_wall=10, gb_free=63.3, wall=802 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:09:25] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.79 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.76 GiB is free. Including non-PyTorch memory, this process has 76.36 GiB memory in use. Of the allocated memory 68.26 GiB is allocated by PyTorch, and 7.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:09:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:09:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:09:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 22        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65550 MiB |  69897 MiB | 152467 GiB | 152403 GiB |
|       from large pool |  65532 MiB |  69879 MiB | 151862 GiB | 151798 GiB |
|       from small pool |     17 MiB |     27 MiB |    604 GiB |    604 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65550 MiB |  69897 MiB | 152467 GiB | 152403 GiB |
|       from large pool |  65532 MiB |  69879 MiB | 151862 GiB | 151798 GiB |
|       from small pool |     17 MiB |     27 MiB |    604 GiB |    604 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65539 MiB |  69886 MiB | 152269 GiB | 152205 GiB |
|       from large pool |  65521 MiB |  69869 MiB | 151666 GiB | 151602 GiB |
|       from small pool |     17 MiB |     26 MiB |    603 GiB |    603 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77680 MiB |  79546 MiB | 362940 MiB | 285260 MiB |
|       from large pool |  77650 MiB |  79336 MiB | 361290 MiB | 283640 MiB |
|       from small pool |     30 MiB |    210 MiB |   1650 MiB |   1620 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8037 MiB |  10281 MiB | 163444 GiB | 163436 GiB |
|       from large pool |   8025 MiB |  10268 MiB | 162772 GiB | 162764 GiB |
|       from small pool |     12 MiB |     33 MiB |    671 GiB |    671 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |    6930 K  |    6929 K  |
|       from large pool |     201    |     210    |    3380 K  |    3379 K  |
|       from small pool |     288    |     356    |    3550 K  |    3549 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |    6930 K  |    6929 K  |
|       from large pool |     201    |     210    |    3380 K  |    3379 K  |
|       from small pool |     288    |     356    |    3550 K  |    3549 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |     167    |    1849    |    1775    |
|       from large pool |      59    |      62    |    1024    |     965    |
|       from small pool |      15    |     105    |     825    |     810    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      84    |      84    |    3769 K  |    3769 K  |
|       from large pool |      55    |      55    |    2081 K  |    2081 K  |
|       from small pool |      29    |      61    |    1687 K  |    1687 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:09:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:09:25] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:09:30]    INFO >> epoch 003:    235 / 1539 loss=3.756, wps=3324.8, ups=4.25, wpb=783.1, bsz=783.1, num_updates=3300, lr=0.000392, gnorm=5.016, clip=0, train_wall=10, gb_free=68.9, wall=814 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:09:41]    INFO >> epoch 003:    285 / 1539 loss=3.764, wps=3530.8, ups=4.74, wpb=744.5, bsz=744.5, num_updates=3350, lr=0.000392, gnorm=5.349, clip=0, train_wall=10, gb_free=71, wall=825 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:09:53]    INFO >> epoch 003:    335 / 1539 loss=3.804, wps=3461, ups=4.42, wpb=782.2, bsz=782.2, num_updates=3400, lr=0.000392, gnorm=5.628, clip=0, train_wall=11, gb_free=66.8, wall=836 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:10:05]    INFO >> epoch 003:    385 / 1539 loss=3.782, wps=3032.2, ups=4.43, wpb=683.8, bsz=683.8, num_updates=3450, lr=0.000392, gnorm=5.069, clip=0, train_wall=11, gb_free=64.5, wall=847 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:10:15]    INFO >> epoch 003:    435 / 1539 loss=3.778, wps=3375.2, ups=4.94, wpb=682.8, bsz=682.8, num_updates=3500, lr=0.000392, gnorm=5.834, clip=0, train_wall=10, gb_free=60.5, wall=857 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:10:26]    INFO >> epoch 003:    485 / 1539 loss=3.798, wps=3511.7, ups=4.87, wpb=721.3, bsz=721.3, num_updates=3550, lr=0.000392, gnorm=5.185, clip=0, train_wall=10, gb_free=71.8, wall=868 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:10:37]    INFO >> epoch 003:    535 / 1539 loss=3.771, wps=3795.6, ups=4.87, wpb=778.7, bsz=778.7, num_updates=3600, lr=0.000392, gnorm=5.67, clip=0, train_wall=10, gb_free=65.2, wall=878 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:10:47]    INFO >> epoch 003:    585 / 1539 loss=3.726, wps=3473.8, ups=4.84, wpb=717.5, bsz=717.5, num_updates=3650, lr=0.000392, gnorm=5.492, clip=2, train_wall=10, gb_free=72.5, wall=888 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:10:58]    INFO >> epoch 003:    635 / 1539 loss=3.773, wps=3442.2, ups=5.02, wpb=686.3, bsz=686.3, num_updates=3700, lr=0.000392, gnorm=6.235, clip=0, train_wall=9, gb_free=62.7, wall=898 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:11:09]    INFO >> epoch 003:    685 / 1539 loss=3.871, wps=3547.2, ups=4.69, wpb=755.6, bsz=755.6, num_updates=3750, lr=0.000392, gnorm=5.318, clip=0, train_wall=10, gb_free=70.2, wall=909 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:11:20]    INFO >> epoch 003:    735 / 1539 loss=3.68, wps=3550.7, ups=4.34, wpb=817.3, bsz=817.3, num_updates=3800, lr=0.000392, gnorm=4.848, clip=0, train_wall=11, gb_free=63.2, wall=920 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:11:33] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.16 GiB is free. Including non-PyTorch memory, this process has 77.96 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:11:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:11:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:11:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 23        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77209 MiB |  78285 MiB | 177740 GiB | 177665 GiB |
|       from large pool |  77187 MiB |  78263 MiB | 177038 GiB | 176963 GiB |
|       from small pool |     22 MiB |     24 MiB |    701 GiB |    701 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77209 MiB |  78285 MiB | 177740 GiB | 177665 GiB |
|       from large pool |  77187 MiB |  78263 MiB | 177038 GiB | 176963 GiB |
|       from small pool |     22 MiB |     24 MiB |    701 GiB |    701 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 177509 GiB | 177434 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 176809 GiB | 176733 GiB |
|       from small pool |     22 MiB |     24 MiB |    700 GiB |    700 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79320 MiB |  80138 MiB | 369490 MiB | 290170 MiB |
|       from large pool |  79290 MiB |  79890 MiB | 367622 MiB | 288332 MiB |
|       from small pool |     30 MiB |    248 MiB |   1868 MiB |   1838 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2050 MiB |   8193 MiB | 191704 GiB | 191702 GiB |
|       from large pool |   2042 MiB |   8184 MiB | 190923 GiB | 190921 GiB |
|       from small pool |      7 MiB |     25 MiB |    781 GiB |    781 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |    8084 K  |    8083 K  |
|       from large pool |     270    |     278    |    3968 K  |    3968 K  |
|       from small pool |     296    |     356    |    4115 K  |    4115 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |    8084 K  |    8083 K  |
|       from large pool |     270    |     278    |    3968 K  |    3968 K  |
|       from small pool |     296    |     356    |    4115 K  |    4115 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     210    |    1986    |    1895    |
|       from large pool |      76    |      86    |    1052    |     976    |
|       from small pool |      15    |     124    |     934    |     919    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     109    |     109    |    4399 K  |    4399 K  |
|       from large pool |      80    |      80    |    2448 K  |    2448 K  |
|       from small pool |      29    |      59    |    1950 K  |    1950 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:11:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:11:33] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:11:33]    INFO >> epoch 003:    786 / 1539 loss=3.329, wps=3457.3, ups=4.46, wpb=774.9, bsz=774.9, num_updates=3850, lr=0.000392, gnorm=5.879, clip=2, train_wall=10, gb_free=3.5, wall=931 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:11:43]    INFO >> epoch 003:    836 / 1539 loss=3.865, wps=3778.2, ups=4.95, wpb=763.4, bsz=763.4, num_updates=3900, lr=0.000392, gnorm=6.188, clip=0, train_wall=10, gb_free=63.3, wall=942 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:11:53]    INFO >> epoch 003:    886 / 1539 loss=3.791, wps=3421.9, ups=4.94, wpb=692.7, bsz=692.7, num_updates=3950, lr=0.000392, gnorm=6.114, clip=0, train_wall=10, gb_free=67, wall=952 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:12:06]    INFO >> epoch 003:    936 / 1539 loss=3.648, wps=3292.7, ups=4.47, wpb=737, bsz=737, num_updates=4000, lr=0.000392, gnorm=5.389, clip=0, train_wall=11, gb_free=67.9, wall=963 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:12:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 77.90 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 3.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:12:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 24        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75041 MiB |  75471 MiB | 184710 GiB | 184637 GiB |
|       from large pool |  75015 MiB |  75445 MiB | 183984 GiB | 183911 GiB |
|       from small pool |     25 MiB |     26 MiB |    726 GiB |    726 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75041 MiB |  75471 MiB | 184710 GiB | 184637 GiB |
|       from large pool |  75015 MiB |  75445 MiB | 183984 GiB | 183911 GiB |
|       from small pool |     25 MiB |     26 MiB |    726 GiB |    726 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 184471 GiB | 184397 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 183745 GiB | 183672 GiB |
|       from small pool |     25 MiB |     26 MiB |    725 GiB |    725 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79262 MiB |  79294 MiB | 369524 MiB | 290262 MiB |
|       from large pool |  79230 MiB |  79230 MiB | 367622 MiB | 288392 MiB |
|       from small pool |     32 MiB |     64 MiB |   1902 MiB |   1870 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4220 MiB |   8716 MiB | 199422 GiB | 199418 GiB |
|       from large pool |   4214 MiB |   8708 MiB | 198613 GiB | 198609 GiB |
|       from small pool |      6 MiB |     23 MiB |    809 GiB |    809 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |    8388 K  |    8388 K  |
|       from large pool |     265    |     272    |    4129 K  |    4129 K  |
|       from small pool |     302    |     356    |    4258 K  |    4258 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |    8388 K  |    8388 K  |
|       from large pool |     265    |     272    |    4129 K  |    4129 K  |
|       from small pool |     302    |     356    |    4258 K  |    4258 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     107    |    2003    |    1912    |
|       from large pool |      75    |      75    |    1052    |     977    |
|       from small pool |      16    |      32    |     951    |     935    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      89    |    4561 K  |    4561 K  |
|       from large pool |      57    |      57    |    2547 K  |    2547 K  |
|       from small pool |      32    |      51    |    2014 K  |    2014 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 01:12:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.21 GiB is free. Including non-PyTorch memory, this process has 77.90 GiB memory in use. Of the allocated memory 72.13 GiB is allocated by PyTorch, and 5.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:12:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 25        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69479 MiB |  76766 MiB | 185678 GiB | 185610 GiB |
|       from large pool |  69459 MiB |  76746 MiB | 184949 GiB | 184881 GiB |
|       from small pool |     20 MiB |     29 MiB |    729 GiB |    729 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69479 MiB |  76766 MiB | 185678 GiB | 185610 GiB |
|       from large pool |  69459 MiB |  76746 MiB | 184949 GiB | 184881 GiB |
|       from small pool |     20 MiB |     29 MiB |    729 GiB |    729 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69460 MiB |  76747 MiB | 185437 GiB | 185369 GiB |
|       from large pool |  69439 MiB |  76727 MiB | 184709 GiB | 184641 GiB |
|       from small pool |     20 MiB |     29 MiB |    728 GiB |    728 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79262 MiB |  79286 MiB | 369548 MiB | 290286 MiB |
|       from large pool |  79230 MiB |  79230 MiB | 367622 MiB | 288392 MiB |
|       from small pool |     32 MiB |     56 MiB |   1926 MiB |   1894 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5690 MiB |   8026 MiB | 200461 GiB | 200456 GiB |
|       from large pool |   5678 MiB |   8015 MiB | 199649 GiB | 199643 GiB |
|       from small pool |     11 MiB |     31 MiB |    812 GiB |    812 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |     721    |    8427 K  |    8426 K  |
|       from large pool |     331    |     355    |    4151 K  |    4150 K  |
|       from small pool |     315    |     376    |    4276 K  |    4276 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |     721    |    8427 K  |    8426 K  |
|       from large pool |     331    |     355    |    4151 K  |    4150 K  |
|       from small pool |     315    |     376    |    4276 K  |    4276 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     103    |    2015    |    1924    |
|       from large pool |      75    |      75    |    1052    |     977    |
|       from small pool |      16    |      28    |     963    |     947    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     101    |     111    |    4582 K  |    4582 K  |
|       from large pool |      73    |      77    |    2560 K  |    2560 K  |
|       from small pool |      28    |      60    |    2021 K  |    2021 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:12:18]    INFO >> epoch 003:    988 / 1539 loss=3.769, wps=2660.2, ups=4.24, wpb=626.8, bsz=626.8, num_updates=4050, lr=0.000392, gnorm=5.308, clip=0, train_wall=9, gb_free=61.7, wall=975 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:12:28]    INFO >> epoch 003:   1038 / 1539 loss=3.781, wps=3191.1, ups=5.04, wpb=633.3, bsz=633.3, num_updates=4100, lr=0.000392, gnorm=5.179, clip=0, train_wall=9, gb_free=72.3, wall=985 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:12:39]    INFO >> epoch 003:   1088 / 1539 loss=3.764, wps=3356.9, ups=4.95, wpb=678.6, bsz=678.6, num_updates=4150, lr=0.000392, gnorm=5.79, clip=0, train_wall=10, gb_free=66.5, wall=995 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:12:47] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 5.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.54 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:12:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79342 MiB |  79402 MiB | 192202 GiB | 192125 GiB |
|       from large pool |  79158 MiB |  79218 MiB | 191447 GiB | 191370 GiB |
|       from small pool |    183 MiB |    184 MiB |    754 GiB |    754 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79342 MiB |  79402 MiB | 192202 GiB | 192125 GiB |
|       from large pool |  79158 MiB |  79218 MiB | 191447 GiB | 191370 GiB |
|       from small pool |    183 MiB |    184 MiB |    754 GiB |    754 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79274 MiB |  79334 MiB | 191952 GiB | 191875 GiB |
|       from large pool |  79092 MiB |  79151 MiB | 191198 GiB | 191121 GiB |
|       from small pool |    182 MiB |    183 MiB |    753 GiB |    753 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80500 MiB |  80500 MiB | 374878 MiB | 294378 MiB |
|       from large pool |  80298 MiB |  80298 MiB | 372782 MiB | 292484 MiB |
|       from small pool |    202 MiB |    202 MiB |   2096 MiB |   1894 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1097 MiB |   6815 MiB | 207757 GiB | 207756 GiB |
|       from large pool |   1079 MiB |   6811 MiB | 206916 GiB | 206915 GiB |
|       from small pool |     18 MiB |     22 MiB |    841 GiB |    841 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3413    |    3416    |    8735 K  |    8731 K  |
|       from large pool |     588    |     589    |    4311 K  |    4310 K  |
|       from small pool |    2825    |    2828    |    4424 K  |    4421 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3413    |    3416    |    8735 K  |    8731 K  |
|       from large pool |     588    |     589    |    4311 K  |    4310 K  |
|       from small pool |    2825    |    2828    |    4424 K  |    4421 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     261    |     261    |    2186    |    1925    |
|       from large pool |     160    |     160    |    1138    |     978    |
|       from small pool |     101    |     101    |    1048    |     947    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     243    |     243    |    4744 K  |    4744 K  |
|       from large pool |      69    |      72    |    2658 K  |    2658 K  |
|       from small pool |     174    |     174    |    2086 K  |    2085 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:12:47] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:12:49]    INFO >> epoch 003:   1139 / 1539 loss=3.725, wps=3407.3, ups=4.92, wpb=692.4, bsz=692.4, num_updates=4200, lr=0.000392, gnorm=4.839, clip=0, train_wall=9, gb_free=64.9, wall=1005 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:13:00]    INFO >> epoch 003:   1189 / 1539 loss=3.697, wps=3036.6, ups=4.63, wpb=655.7, bsz=655.7, num_updates=4250, lr=0.000392, gnorm=4.956, clip=0, train_wall=10, gb_free=64, wall=1016 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:13:12]    INFO >> epoch 003:   1239 / 1539 loss=3.701, wps=3371.5, ups=4.75, wpb=710.2, bsz=710.2, num_updates=4300, lr=0.000392, gnorm=6.257, clip=6, train_wall=10, gb_free=70.7, wall=1026 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:13:21]    INFO >> epoch 003:   1289 / 1539 loss=3.612, wps=3269.7, ups=5.23, wpb=625.7, bsz=625.7, num_updates=4350, lr=0.000392, gnorm=5.057, clip=0, train_wall=9, gb_free=74.4, wall=1036 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:13:32]    INFO >> epoch 003:   1339 / 1539 loss=3.701, wps=3665.6, ups=4.92, wpb=744.7, bsz=744.7, num_updates=4400, lr=0.000392, gnorm=4.835, clip=0, train_wall=10, gb_free=67.4, wall=1046 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:13:42]    INFO >> epoch 003:   1389 / 1539 loss=3.633, wps=3463.5, ups=5.25, wpb=660.3, bsz=660.3, num_updates=4450, lr=0.000392, gnorm=4.931, clip=0, train_wall=9, gb_free=69.1, wall=1055 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:13:52]    INFO >> epoch 003:   1439 / 1539 loss=3.757, wps=3371.7, ups=5.05, wpb=667.7, bsz=667.7, num_updates=4500, lr=0.000392, gnorm=4.66, clip=0, train_wall=9, gb_free=69.7, wall=1065 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:14:03]    INFO >> epoch 003:   1489 / 1539 loss=3.687, wps=3118.7, ups=4.57, wpb=682.6, bsz=682.6, num_updates=4550, lr=0.000392, gnorm=5.632, clip=2, train_wall=10, gb_free=66.1, wall=1076 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:14:15]    INFO >> epoch 003:   1539 / 1539 loss=3.738, wps=3160.7, ups=4.91, wpb=644.1, bsz=644.1, num_updates=4600, lr=0.000392, gnorm=5.459, clip=0, train_wall=10, gb_free=71.7, wall=1086 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:14:15]    INFO >> epoch 003 | loss 3.746 | wps 3164.7 | ups 4.45 | wpb 711 | bsz 711 | num_updates 4600 | lr 0.000392 | gnorm 5.449 | clip 0.4 | train_wall 300 | gb_free 71.7 | wall 1086 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:14:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:14:36]    INFO >> epoch 003 | valid on 'valid' subset | loss 3.843 | wps 7057.1 | wpb 5412.5 | bsz 5412.5 | num_updates 4600 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:14:37]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:14:37]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 3 @ 4600 updates, score 3.843) (writing took 0.033478 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:14:37] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:14:49]    INFO >> epoch 004:     50 / 1539 loss=3.676, wps=1121.7, ups=1.53, wpb=734.4, bsz=734.4, num_updates=4650, lr=0.000376, gnorm=5.737, clip=0, train_wall=10, gb_free=66.9, wall=1119 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:15:01]    INFO >> epoch 004:    100 / 1539 loss=3.582, wps=3817.4, ups=4.28, wpb=892.1, bsz=892.1, num_updates=4700, lr=0.000376, gnorm=5.298, clip=0, train_wall=11, gb_free=62.5, wall=1131 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:15:13]    INFO >> epoch 004:    150 / 1539 loss=3.503, wps=3611.4, ups=3.93, wpb=920.1, bsz=920.1, num_updates=4750, lr=0.000376, gnorm=5.518, clip=0, train_wall=12, gb_free=66.2, wall=1144 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:15:24]    INFO >> epoch 004:    200 / 1539 loss=3.736, wps=3679.7, ups=5.19, wpb=708.5, bsz=708.5, num_updates=4800, lr=0.000376, gnorm=4.531, clip=0, train_wall=9, gb_free=61.9, wall=1153 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:15:35]    INFO >> epoch 004:    250 / 1539 loss=3.654, wps=3329.6, ups=4.58, wpb=726.8, bsz=726.8, num_updates=4850, lr=0.000376, gnorm=5.018, clip=0, train_wall=10, gb_free=70.2, wall=1164 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:15:45]    INFO >> epoch 004:    300 / 1539 loss=3.658, wps=3267.9, ups=5.04, wpb=649, bsz=649, num_updates=4900, lr=0.000376, gnorm=4.815, clip=0, train_wall=9, gb_free=63.4, wall=1174 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:15:57]    INFO >> epoch 004:    350 / 1539 loss=3.653, wps=3772.5, ups=4.87, wpb=774.4, bsz=774.4, num_updates=4950, lr=0.000376, gnorm=5.41, clip=0, train_wall=10, gb_free=31.6, wall=1184 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:16:07]    INFO >> epoch 004:    400 / 1539 loss=3.537, wps=3257.6, ups=4.88, wpb=667.8, bsz=667.8, num_updates=5000, lr=0.000376, gnorm=5.077, clip=0, train_wall=10, gb_free=70.1, wall=1195 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:16:12] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 2 has a total capacity of 79.14 GiB of which 597.25 MiB is free. Including non-PyTorch memory, this process has 78.53 GiB memory in use. Of the allocated memory 71.67 GiB is allocated by PyTorch, and 6.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:16:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:16:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:16:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 27        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69626 MiB |  73392 MiB | 233801 GiB | 233733 GiB |
|       from large pool |  69606 MiB |  73372 MiB | 232874 GiB | 232806 GiB |
|       from small pool |     19 MiB |     28 MiB |    926 GiB |    926 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69626 MiB |  73392 MiB | 233801 GiB | 233733 GiB |
|       from large pool |  69606 MiB |  73372 MiB | 232874 GiB | 232806 GiB |
|       from small pool |     19 MiB |     28 MiB |    926 GiB |    926 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69602 MiB |  73368 MiB | 233497 GiB | 233429 GiB |
|       from large pool |  69582 MiB |  73348 MiB | 232571 GiB | 232503 GiB |
|       from small pool |     19 MiB |     28 MiB |    925 GiB |    925 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79908 MiB |  80464 MiB | 374902 MiB | 294994 MiB |
|       from large pool |  79878 MiB |  80238 MiB | 372782 MiB | 292904 MiB |
|       from small pool |     30 MiB |    226 MiB |   2120 MiB |   2090 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6069 MiB |  11416 MiB | 247510 GiB | 247504 GiB |
|       from large pool |   6059 MiB |  11405 MiB | 246480 GiB | 246474 GiB |
|       from small pool |     10 MiB |     27 MiB |   1030 GiB |   1030 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   10636 K  |   10636 K  |
|       from large pool |     336    |     344    |    5190 K  |    5189 K  |
|       from small pool |     305    |     356    |    5446 K  |    5446 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   10636 K  |   10636 K  |
|       from large pool |     336    |     344    |    5190 K  |    5189 K  |
|       from small pool |     305    |     356    |    5446 K  |    5446 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     168    |     272    |    2198    |    2030    |
|       from large pool |     153    |     159    |    1138    |     985    |
|       from small pool |      15    |     113    |    1060    |    1045    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     151    |     153    |    5829 K  |    5829 K  |
|       from large pool |     122    |     124    |    3233 K  |    3232 K  |
|       from small pool |      29    |      53    |    2596 K  |    2596 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:16:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:16:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:16:18]    INFO >> epoch 004:    451 / 1539 loss=3.761, wps=2880.2, ups=4.43, wpb=650.1, bsz=650.1, num_updates=5050, lr=0.000376, gnorm=4.93, clip=0, train_wall=10, gb_free=70.1, wall=1206 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:16:30]    INFO >> epoch 004:    501 / 1539 loss=3.613, wps=3339.2, ups=4.65, wpb=717.9, bsz=717.9, num_updates=5100, lr=0.000376, gnorm=4.907, clip=0, train_wall=10, gb_free=70.6, wall=1217 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:16:41]    INFO >> epoch 004:    551 / 1539 loss=3.724, wps=3210.5, ups=4.69, wpb=684.3, bsz=684.3, num_updates=5150, lr=0.000376, gnorm=5.346, clip=0, train_wall=10, gb_free=69.6, wall=1227 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:16:51] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.07 GiB is free. Including non-PyTorch memory, this process has 78.04 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 1.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:16:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:16:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:16:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 29        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77207 MiB |  78283 MiB | 241162 GiB | 241086 GiB |
|       from large pool |  77185 MiB |  78261 MiB | 240210 GiB | 240134 GiB |
|       from small pool |     22 MiB |     26 MiB |    952 GiB |    952 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77207 MiB |  78283 MiB | 241162 GiB | 241086 GiB |
|       from large pool |  77185 MiB |  78261 MiB | 240210 GiB | 240134 GiB |
|       from small pool |     22 MiB |     26 MiB |    952 GiB |    952 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 240847 GiB | 240771 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 239896 GiB | 239821 GiB |
|       from small pool |     22 MiB |     26 MiB |    950 GiB |    950 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79408 MiB |  79408 MiB | 383330 MiB | 303922 MiB |
|       from large pool |  79378 MiB |  79378 MiB | 381174 MiB | 301796 MiB |
|       from small pool |     30 MiB |     66 MiB |   2156 MiB |   2126 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2200 MiB |   8992 MiB | 254910 GiB | 254908 GiB |
|       from large pool |   2192 MiB |   8983 MiB | 253851 GiB | 253849 GiB |
|       from small pool |      7 MiB |     29 MiB |   1059 GiB |   1059 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   10954 K  |   10953 K  |
|       from large pool |     270    |     278    |    5362 K  |    5362 K  |
|       from small pool |     296    |     356    |    5591 K  |    5591 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   10954 K  |   10953 K  |
|       from large pool |     270    |     278    |    5362 K  |    5362 K  |
|       from small pool |     296    |     356    |    5591 K  |    5591 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      94    |     184    |    2223    |    2129    |
|       from large pool |      79    |     151    |    1145    |    1066    |
|       from small pool |      15    |      33    |    1078    |    1063    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     109    |     111    |    6002 K  |    6001 K  |
|       from large pool |      80    |      82    |    3344 K  |    3344 K  |
|       from small pool |      29    |      61    |    2657 K  |    2657 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:16:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:16:51] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:16:52]    INFO >> epoch 004:    602 / 1539 loss=3.575, wps=2846.3, ups=4.55, wpb=625.7, bsz=625.7, num_updates=5200, lr=0.000376, gnorm=4.825, clip=0, train_wall=10, gb_free=70.3, wall=1238 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:17:03]    INFO >> epoch 004:    652 / 1539 loss=3.612, wps=3507.8, ups=5.19, wpb=676.2, bsz=676.2, num_updates=5250, lr=0.000376, gnorm=4.741, clip=0, train_wall=9, gb_free=73.2, wall=1248 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:17:11] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 GiB is free. Including non-PyTorch memory, this process has 75.87 GiB memory in use. Of the allocated memory 72.07 GiB is allocated by PyTorch, and 3.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:17:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:17:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:17:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73199 MiB |  74276 MiB | 244882 GiB | 244810 GiB |
|       from large pool |  73182 MiB |  74259 MiB | 243916 GiB | 243845 GiB |
|       from small pool |     16 MiB |     30 MiB |    965 GiB |    965 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73199 MiB |  74276 MiB | 244882 GiB | 244810 GiB |
|       from large pool |  73182 MiB |  74259 MiB | 243916 GiB | 243845 GiB |
|       from small pool |     16 MiB |     30 MiB |    965 GiB |    965 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73189 MiB |  74265 MiB | 244562 GiB | 244490 GiB |
|       from large pool |  73173 MiB |  74248 MiB | 243597 GiB | 243526 GiB |
|       from small pool |     16 MiB |     30 MiB |    964 GiB |    964 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77178 MiB |  79436 MiB | 383358 MiB | 306180 MiB |
|       from large pool |  77146 MiB |  79378 MiB | 381174 MiB | 304028 MiB |
|       from small pool |     32 MiB |     58 MiB |   2184 MiB |   2152 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3918 MiB |   6687 MiB | 258963 GiB | 258959 GiB |
|       from large pool |   3903 MiB |   6672 MiB | 257888 GiB | 257884 GiB |
|       from small pool |     15 MiB |     31 MiB |   1074 GiB |   1074 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     527    |     535    |   11123 K  |   11123 K  |
|       from large pool |     238    |     246    |    5453 K  |    5453 K  |
|       from small pool |     289    |     356    |    5669 K  |    5669 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     527    |     535    |   11123 K  |   11123 K  |
|       from large pool |     238    |     246    |    5453 K  |    5453 K  |
|       from small pool |     289    |     356    |    5669 K  |    5669 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      82    |     108    |    2237    |    2155    |
|       from large pool |      66    |      79    |    1145    |    1079    |
|       from small pool |      16    |      29    |    1092    |    1076    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      89    |    6090 K  |    6090 K  |
|       from large pool |      60    |      60    |    3400 K  |    3400 K  |
|       from small pool |      29    |      59    |    2690 K  |    2690 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:17:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:17:11] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:17:14]    INFO >> epoch 004:    703 / 1539 loss=3.643, wps=2936.1, ups=4.63, wpb=634, bsz=634, num_updates=5300, lr=0.000376, gnorm=4.868, clip=0, train_wall=9, gb_free=58.4, wall=1259 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:17:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 3.31 GiB is free. Including non-PyTorch memory, this process has 75.81 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 1.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:17:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:17:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:17:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 31        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75038 MiB |  75468 MiB | 246895 GiB | 246822 GiB |
|       from large pool |  75012 MiB |  75442 MiB | 245922 GiB | 245849 GiB |
|       from small pool |     25 MiB |     26 MiB |    972 GiB |    972 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75038 MiB |  75468 MiB | 246895 GiB | 246822 GiB |
|       from large pool |  75012 MiB |  75442 MiB | 245922 GiB | 245849 GiB |
|       from small pool |     25 MiB |     26 MiB |    972 GiB |    972 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 246573 GiB | 246499 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 245602 GiB | 245528 GiB |
|       from small pool |     25 MiB |     26 MiB |    971 GiB |    971 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77116 MiB |  77142 MiB | 383382 MiB | 306266 MiB |
|       from large pool |  77086 MiB |  77086 MiB | 381174 MiB | 304088 MiB |
|       from small pool |     30 MiB |     56 MiB |   2208 MiB |   2178 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2077 MiB |   9083 MiB | 261223 GiB | 261221 GiB |
|       from large pool |   2073 MiB |   9077 MiB | 260141 GiB | 260139 GiB |
|       from small pool |      4 MiB |     19 MiB |   1082 GiB |   1082 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   11207 K  |   11206 K  |
|       from large pool |     265    |     272    |    5499 K  |    5499 K  |
|       from small pool |     302    |     348    |    5707 K  |    5707 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   11207 K  |   11206 K  |
|       from large pool |     265    |     272    |    5499 K  |    5499 K  |
|       from small pool |     302    |     348    |    5707 K  |    5707 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      80    |      93    |    2249    |    2169    |
|       from large pool |      65    |      65    |    1145    |    1080    |
|       from small pool |      15    |      28    |    1104    |    1089    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      84    |      86    |    6133 K  |    6133 K  |
|       from large pool |      56    |      58    |    3427 K  |    3427 K  |
|       from small pool |      28    |      49    |    2706 K  |    2706 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:17:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:17:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:17:25]    INFO >> epoch 004:    754 / 1539 loss=3.547, wps=2863, ups=4.43, wpb=645.8, bsz=645.8, num_updates=5350, lr=0.000376, gnorm=4.313, clip=0, train_wall=10, gb_free=68.8, wall=1270 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:17:37]    INFO >> epoch 004:    804 / 1539 loss=3.595, wps=3434.1, ups=4.91, wpb=699.4, bsz=699.4, num_updates=5400, lr=0.000376, gnorm=4.675, clip=0, train_wall=10, gb_free=69.9, wall=1280 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:17:47]    INFO >> epoch 004:    854 / 1539 loss=3.54, wps=3624.1, ups=4.89, wpb=741.8, bsz=741.8, num_updates=5450, lr=0.000376, gnorm=5.283, clip=0, train_wall=10, gb_free=69.7, wall=1290 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:17:59]    INFO >> epoch 004:    904 / 1539 loss=3.634, wps=3280.2, ups=4.25, wpb=771.9, bsz=771.9, num_updates=5500, lr=0.000376, gnorm=5.656, clip=0, train_wall=11, gb_free=67.5, wall=1302 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:18:10]    INFO >> epoch 004:    954 / 1539 loss=3.591, wps=3263.6, ups=5.06, wpb=645.3, bsz=645.3, num_updates=5550, lr=0.000376, gnorm=4.521, clip=0, train_wall=9, gb_free=69.8, wall=1312 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:18:21]    INFO >> epoch 004:   1004 / 1539 loss=3.579, wps=3357.8, ups=4.32, wpb=777.9, bsz=777.9, num_updates=5600, lr=0.000376, gnorm=5.564, clip=0, train_wall=11, gb_free=57.3, wall=1324 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:18:32]    INFO >> epoch 004:   1054 / 1539 loss=3.78, wps=3135.5, ups=4.9, wpb=639.5, bsz=639.5, num_updates=5650, lr=0.000376, gnorm=4.513, clip=0, train_wall=10, gb_free=68.8, wall=1334 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:18:44]    INFO >> epoch 004:   1104 / 1539 loss=3.654, wps=3319.1, ups=4.73, wpb=701.1, bsz=701.1, num_updates=5700, lr=0.000376, gnorm=4.481, clip=0, train_wall=10, gb_free=68.8, wall=1344 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:18:54]    INFO >> epoch 004:   1154 / 1539 loss=3.589, wps=3436, ups=4.65, wpb=738.7, bsz=738.7, num_updates=5750, lr=0.000376, gnorm=4.978, clip=2, train_wall=10, gb_free=60.2, wall=1355 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:19:04]    INFO >> epoch 004:   1204 / 1539 loss=3.661, wps=3480.8, ups=4.99, wpb=697.7, bsz=697.7, num_updates=5800, lr=0.000376, gnorm=5.847, clip=0, train_wall=9, gb_free=66.7, wall=1365 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:19:14]    INFO >> epoch 004:   1254 / 1539 loss=3.56, wps=3237.6, ups=5.08, wpb=637, bsz=637, num_updates=5850, lr=0.000376, gnorm=4.786, clip=0, train_wall=9, gb_free=70.9, wall=1375 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:19:26]    INFO >> epoch 004:   1304 / 1539 loss=3.628, wps=3046.9, ups=4.6, wpb=662.8, bsz=662.8, num_updates=5900, lr=0.000376, gnorm=4.223, clip=0, train_wall=10, gb_free=69.5, wall=1386 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:19:37]    INFO >> epoch 004:   1354 / 1539 loss=3.521, wps=3769.8, ups=4.68, wpb=806.1, bsz=806.1, num_updates=5950, lr=0.000376, gnorm=5.215, clip=0, train_wall=10, gb_free=70.7, wall=1397 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:19:46] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.79 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.72 GiB is free. Including non-PyTorch memory, this process has 76.39 GiB memory in use. Of the allocated memory 68.26 GiB is allocated by PyTorch, and 7.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:19:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:19:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:19:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65549 MiB |  69896 MiB | 274575 GiB | 274510 GiB |
|       from large pool |  65531 MiB |  69879 MiB | 273498 GiB | 273434 GiB |
|       from small pool |     17 MiB |     19 MiB |   1076 GiB |   1076 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65549 MiB |  69896 MiB | 274575 GiB | 274510 GiB |
|       from large pool |  65531 MiB |  69879 MiB | 273498 GiB | 273434 GiB |
|       from small pool |     17 MiB |     19 MiB |   1076 GiB |   1076 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65539 MiB |  69886 MiB | 274217 GiB | 274153 GiB |
|       from large pool |  65521 MiB |  69869 MiB | 273142 GiB | 273078 GiB |
|       from small pool |     17 MiB |     19 MiB |   1075 GiB |   1075 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77718 MiB |  80186 MiB | 386452 MiB | 308734 MiB |
|       from large pool |  77690 MiB |  79938 MiB | 384026 MiB | 306336 MiB |
|       from small pool |     28 MiB |    248 MiB |   2426 MiB |   2398 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9316 MiB |  13924 MiB | 293000 GiB | 292991 GiB |
|       from large pool |   9306 MiB |  13913 MiB | 291800 GiB | 291791 GiB |
|       from small pool |     10 MiB |     18 MiB |   1199 GiB |   1199 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   12468 K  |   12468 K  |
|       from large pool |     201    |     210    |    6157 K  |    6157 K  |
|       from small pool |     288    |     336    |    6311 K  |    6311 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   12468 K  |   12468 K  |
|       from large pool |     201    |     210    |    6157 K  |    6157 K  |
|       from small pool |     288    |     336    |    6311 K  |    6311 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      76    |     190    |    2359    |    2283    |
|       from large pool |      62    |      66    |    1146    |    1084    |
|       from small pool |      14    |     124    |    1213    |    1199    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      85    |      85    |    6799 K  |    6799 K  |
|       from large pool |      57    |      57    |    3823 K  |    3823 K  |
|       from small pool |      28    |      46    |    2976 K  |    2975 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:19:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:19:46] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:19:50]    INFO >> epoch 004:   1405 / 1539 loss=3.709, wps=3316.3, ups=4.29, wpb=772.2, bsz=772.2, num_updates=6000, lr=0.000376, gnorm=4.975, clip=0, train_wall=10, gb_free=72.6, wall=1408 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:20:00]    INFO >> epoch 004:   1455 / 1539 loss=3.633, wps=3401.2, ups=4.88, wpb=697.1, bsz=697.1, num_updates=6050, lr=0.000376, gnorm=4.982, clip=0, train_wall=10, gb_free=67.5, wall=1419 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:20:11]    INFO >> epoch 004:   1505 / 1539 loss=3.701, wps=3441.2, ups=4.86, wpb=708.2, bsz=708.2, num_updates=6100, lr=0.000376, gnorm=4.84, clip=0, train_wall=10, gb_free=70.4, wall=1429 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:20:12] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 5.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.54 GiB is allocated by PyTorch, and 1.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:20:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:20:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:20:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 33        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79342 MiB |  79402 MiB | 279532 GiB | 279454 GiB |
|       from large pool |  79159 MiB |  79219 MiB | 278435 GiB | 278358 GiB |
|       from small pool |    183 MiB |    184 MiB |   1096 GiB |   1096 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79342 MiB |  79402 MiB | 279532 GiB | 279454 GiB |
|       from large pool |  79159 MiB |  79219 MiB | 278435 GiB | 278358 GiB |
|       from small pool |    183 MiB |    184 MiB |   1096 GiB |   1096 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79274 MiB |  79334 MiB | 279168 GiB | 279090 GiB |
|       from large pool |  79092 MiB |  79151 MiB | 278073 GiB | 277995 GiB |
|       from small pool |    182 MiB |    183 MiB |   1095 GiB |   1094 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80500 MiB |  80502 MiB | 392088 MiB | 311588 MiB |
|       from large pool |  80298 MiB |  80298 MiB | 389486 MiB | 309188 MiB |
|       from small pool |    202 MiB |    204 MiB |   2602 MiB |   2400 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1097 MiB |   6556 MiB | 298767 GiB | 298766 GiB |
|       from large pool |   1078 MiB |   6551 MiB | 297545 GiB | 297544 GiB |
|       from small pool |     18 MiB |     20 MiB |   1222 GiB |   1221 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3413    |    3416    |   12701 K  |   12698 K  |
|       from large pool |     588    |     589    |    6275 K  |    6274 K  |
|       from small pool |    2825    |    2828    |    6426 K  |    6423 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3413    |    3416    |   12701 K  |   12698 K  |
|       from large pool |     588    |     589    |    6275 K  |    6274 K  |
|       from small pool |    2825    |    2828    |    6426 K  |    6423 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     253    |     254    |    2538    |    2285    |
|       from large pool |     152    |     152    |    1237    |    1085    |
|       from small pool |     101    |     102    |    1301    |    1200    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     240    |     240    |    6923 K  |    6922 K  |
|       from large pool |      60    |      63    |    3894 K  |    3894 K  |
|       from small pool |     180    |     180    |    3028 K  |    3028 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:20:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:20:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:20:17]    INFO >> epoch 004 | loss 3.625 | wps 3121.7 | ups 4.39 | wpb 711 | bsz 711 | num_updates 6133 | lr 0.000376 | gnorm 5.005 | clip 0.1 | train_wall 304 | gb_free 65.2 | wall 1436 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:20:17] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:20:41]    INFO >> epoch 004 | valid on 'valid' subset | loss 3.774 | wps 6872.9 | wpb 5412.5 | bsz 5412.5 | num_updates 6133 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:20:41]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:20:41]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 4 @ 6133 updates, score 3.774) (writing took 0.026904 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:20:41] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:20:45]    INFO >> epoch 005:     17 / 1539 loss=3.641, wps=945.5, ups=1.53, wpb=618.7, bsz=618.7, num_updates=6150, lr=0.000354, gnorm=4.908, clip=0, train_wall=9, gb_free=74, wall=1462 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:20:47] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.73 GiB is free. Including non-PyTorch memory, this process has 77.39 GiB memory in use. Of the allocated memory 69.84 GiB is allocated by PyTorch, and 7.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:20:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:20:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:20:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 34        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71090 MiB |  72154 MiB | 288040 GiB | 287970 GiB |
|       from large pool |  71064 MiB |  72130 MiB | 286898 GiB | 286829 GiB |
|       from small pool |     25 MiB |     26 MiB |   1141 GiB |   1141 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71090 MiB |  72154 MiB | 288040 GiB | 287970 GiB |
|       from large pool |  71064 MiB |  72130 MiB | 286898 GiB | 286829 GiB |
|       from small pool |     25 MiB |     26 MiB |   1141 GiB |   1141 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  71075 MiB |  72140 MiB | 287666 GiB | 287597 GiB |
|       from large pool |  71049 MiB |  72116 MiB | 286526 GiB | 286457 GiB |
|       from small pool |     25 MiB |     26 MiB |   1139 GiB |   1139 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78736 MiB |  80440 MiB | 392088 MiB | 313352 MiB |
|       from large pool |  78706 MiB |  80238 MiB | 389486 MiB | 310780 MiB |
|       from small pool |     30 MiB |    202 MiB |   2602 MiB |   2572 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7525 MiB |   7525 MiB | 304274 GiB | 304266 GiB |
|       from large pool |   7521 MiB |   7521 MiB | 303006 GiB | 302998 GiB |
|       from small pool |      4 MiB |     25 MiB |   1267 GiB |   1267 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   13081 K  |   13080 K  |
|       from large pool |     264    |     272    |    6368 K  |    6368 K  |
|       from small pool |     302    |     356    |    6712 K  |    6712 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   13081 K  |   13080 K  |
|       from large pool |     264    |     272    |    6368 K  |    6368 K  |
|       from small pool |     302    |     356    |    6712 K  |    6712 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     144    |     252    |    2538    |    2394    |
|       from large pool |     129    |     151    |    1237    |    1108    |
|       from small pool |      15    |     101    |    1301    |    1286    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     132    |     133    |    7166 K  |    7166 K  |
|       from large pool |     102    |     103    |    3958 K  |    3957 K  |
|       from small pool |      30    |      55    |    3208 K  |    3208 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:20:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:20:47] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:20:55]    INFO >> epoch 005:     68 / 1539 loss=3.619, wps=3454.8, ups=4.96, wpb=696.6, bsz=696.6, num_updates=6200, lr=0.000354, gnorm=4.223, clip=0, train_wall=9, gb_free=68.2, wall=1472 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:21:06]    INFO >> epoch 005:    118 / 1539 loss=3.607, wps=3357, ups=4.93, wpb=681.6, bsz=681.6, num_updates=6250, lr=0.000354, gnorm=4.327, clip=0, train_wall=10, gb_free=68, wall=1482 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:21:16]    INFO >> epoch 005:    168 / 1539 loss=3.624, wps=3269.3, ups=5.24, wpb=623.6, bsz=623.6, num_updates=6300, lr=0.000354, gnorm=4.207, clip=0, train_wall=9, gb_free=67.8, wall=1491 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:21:26]    INFO >> epoch 005:    218 / 1539 loss=3.425, wps=3263.8, ups=4.82, wpb=676.8, bsz=676.8, num_updates=6350, lr=0.000354, gnorm=5.108, clip=0, train_wall=10, gb_free=71.3, wall=1502 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:21:39]    INFO >> epoch 005:    268 / 1539 loss=3.559, wps=3842.3, ups=4.19, wpb=918.1, bsz=918.1, num_updates=6400, lr=0.000354, gnorm=5.338, clip=2, train_wall=11, gb_free=58.5, wall=1514 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:21:49]    INFO >> epoch 005:    318 / 1539 loss=3.564, wps=3379.3, ups=5.06, wpb=667.8, bsz=667.8, num_updates=6450, lr=0.000354, gnorm=4.499, clip=0, train_wall=9, gb_free=68.8, wall=1524 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:22:00]    INFO >> epoch 005:    368 / 1539 loss=3.398, wps=3851.8, ups=4.86, wpb=792.1, bsz=792.1, num_updates=6500, lr=0.000354, gnorm=4.623, clip=0, train_wall=10, gb_free=70.9, wall=1534 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:22:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 39.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.28 GiB is allocated by PyTorch, and 1.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:22:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:22:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:22:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 36        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79070 MiB |  79130 MiB | 303678 GiB | 303601 GiB |
|       from large pool |  78889 MiB |  78949 MiB | 302473 GiB | 302396 GiB |
|       from small pool |    180 MiB |    182 MiB |   1205 GiB |   1204 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79070 MiB |  79130 MiB | 303678 GiB | 303601 GiB |
|       from large pool |  78889 MiB |  78949 MiB | 302473 GiB | 302396 GiB |
|       from small pool |    180 MiB |    182 MiB |   1205 GiB |   1204 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79034 MiB |  79093 MiB | 303282 GiB | 303205 GiB |
|       from large pool |  78854 MiB |  78913 MiB | 302079 GiB | 302002 GiB |
|       from small pool |    180 MiB |    181 MiB |   1203 GiB |   1203 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80466 MiB |  80466 MiB | 393988 MiB | 313522 MiB |
|       from large pool |  80266 MiB |  80266 MiB | 391166 MiB | 310900 MiB |
|       from small pool |    200 MiB |    248 MiB |   2822 MiB |   2622 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1335 MiB |   8746 MiB | 320558 GiB | 320557 GiB |
|       from large pool |   1316 MiB |   8741 MiB | 319217 GiB | 319216 GiB |
|       from small pool |     19 MiB |     22 MiB |   1340 GiB |   1340 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3369    |    3372    |   13833 K  |   13830 K  |
|       from large pool |     584    |     585    |    6748 K  |    6747 K  |
|       from small pool |    2785    |    2788    |    7085 K  |    7082 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3369    |    3372    |   13833 K  |   13830 K  |
|       from large pool |     584    |     585    |    6748 K  |    6747 K  |
|       from small pool |    2785    |    2788    |    7085 K  |    7082 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     255    |     278    |    2676    |    2421    |
|       from large pool |     155    |     155    |    1265    |    1110    |
|       from small pool |     100    |     124    |    1411    |    1311    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     284    |     284    |    7587 K  |    7587 K  |
|       from large pool |     109    |     109    |    4204 K  |    4204 K  |
|       from small pool |     175    |     175    |    3382 K  |    3382 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:22:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:22:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:22:11]    INFO >> epoch 005:    419 / 1539 loss=3.605, wps=3183.1, ups=4.82, wpb=660.8, bsz=660.8, num_updates=6550, lr=0.000354, gnorm=4.568, clip=0, train_wall=9, gb_free=65.1, wall=1544 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:22:21]    INFO >> epoch 005:    469 / 1539 loss=3.621, wps=3118.5, ups=4.93, wpb=632.8, bsz=632.8, num_updates=6600, lr=0.000354, gnorm=4.464, clip=0, train_wall=10, gb_free=54, wall=1554 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:22:32]    INFO >> epoch 005:    519 / 1539 loss=3.45, wps=3328.7, ups=4.71, wpb=706, bsz=706, num_updates=6650, lr=0.000354, gnorm=4.419, clip=0, train_wall=10, gb_free=65.4, wall=1565 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:22:43]    INFO >> epoch 005:    569 / 1539 loss=3.663, wps=3418.7, ups=5.18, wpb=660.3, bsz=660.3, num_updates=6700, lr=0.000354, gnorm=4.944, clip=0, train_wall=9, gb_free=68.7, wall=1575 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:22:47] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 843.25 MiB is free. Including non-PyTorch memory, this process has 78.29 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 1.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:22:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:22:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:22:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 38        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77203 MiB |  78281 MiB | 311472 GiB | 311397 GiB |
|       from large pool |  77181 MiB |  78259 MiB | 310240 GiB | 310164 GiB |
|       from small pool |     22 MiB |     23 MiB |   1232 GiB |   1232 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77203 MiB |  78281 MiB | 311472 GiB | 311397 GiB |
|       from large pool |  77181 MiB |  78259 MiB | 310240 GiB | 310164 GiB |
|       from small pool |     22 MiB |     23 MiB |   1232 GiB |   1232 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 311065 GiB | 310990 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 309834 GiB | 309759 GiB |
|       from small pool |     22 MiB |     23 MiB |   1230 GiB |   1230 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79662 MiB |  80406 MiB | 398456 MiB | 318794 MiB |
|       from large pool |  79634 MiB |  80206 MiB | 395634 MiB | 316000 MiB |
|       from small pool |     28 MiB |    200 MiB |   2822 MiB |   2794 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2398 MiB |   9199 MiB | 328538 GiB | 328536 GiB |
|       from large pool |   2392 MiB |   9192 MiB | 327166 GiB | 327164 GiB |
|       from small pool |      5 MiB |     27 MiB |   1371 GiB |   1371 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   14176 K  |   14176 K  |
|       from large pool |     270    |     278    |    6933 K  |    6933 K  |
|       from small pool |     296    |     356    |    7243 K  |    7242 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   14176 K  |   14176 K  |
|       from large pool |     270    |     278    |    6933 K  |    6933 K  |
|       from small pool |     296    |     356    |    7243 K  |    7242 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     254    |    2680    |    2592    |
|       from large pool |      74    |     154    |    1269    |    1195    |
|       from small pool |      14    |     100    |    1411    |    1397    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     107    |     108    |    7774 K  |    7774 K  |
|       from large pool |      79    |      80    |    4324 K  |    4324 K  |
|       from small pool |      28    |      59    |    3449 K  |    3449 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:22:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:22:47] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:22:54]    INFO >> epoch 005:    620 / 1539 loss=3.545, wps=3162.7, ups=4.42, wpb=715.3, bsz=715.3, num_updates=6750, lr=0.000354, gnorm=4.772, clip=0, train_wall=10, gb_free=67.3, wall=1586 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:23:10]    INFO >> epoch 005:    670 / 1539 loss=3.69, wps=2648, ups=3.28, wpb=808.5, bsz=808.5, num_updates=6800, lr=0.000354, gnorm=5.381, clip=0, train_wall=15, gb_free=69, wall=1601 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:23:21]    INFO >> epoch 005:    720 / 1539 loss=3.573, wps=3458.5, ups=5.08, wpb=681.1, bsz=681.1, num_updates=6850, lr=0.000354, gnorm=5.406, clip=0, train_wall=9, gb_free=67.4, wall=1611 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:23:31]    INFO >> epoch 005:    770 / 1539 loss=3.505, wps=3677.8, ups=5.04, wpb=729.6, bsz=729.6, num_updates=6900, lr=0.000354, gnorm=4.53, clip=0, train_wall=9, gb_free=65.7, wall=1621 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:23:35] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 2 has a total capacity of 79.14 GiB of which 899.25 MiB is free. Including non-PyTorch memory, this process has 78.24 GiB memory in use. Of the allocated memory 72.14 GiB is allocated by PyTorch, and 5.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:23:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:23:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:23:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 39        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69479 MiB |  76767 MiB | 319958 GiB | 319890 GiB |
|       from large pool |  69459 MiB |  76746 MiB | 318694 GiB | 318626 GiB |
|       from small pool |     20 MiB |     27 MiB |   1264 GiB |   1264 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69479 MiB |  76767 MiB | 319958 GiB | 319890 GiB |
|       from large pool |  69459 MiB |  76746 MiB | 318694 GiB | 318626 GiB |
|       from small pool |     20 MiB |     27 MiB |   1264 GiB |   1264 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69460 MiB |  76747 MiB | 319539 GiB | 319472 GiB |
|       from large pool |  69439 MiB |  76727 MiB | 318277 GiB | 318209 GiB |
|       from small pool |     20 MiB |     26 MiB |   1262 GiB |   1262 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79606 MiB |  79646 MiB | 398500 MiB | 318894 MiB |
|       from large pool |  79574 MiB |  79574 MiB | 395634 MiB | 316060 MiB |
|       from small pool |     32 MiB |     72 MiB |   2866 MiB |   2834 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1942 MiB |   8675 MiB | 338046 GiB | 338044 GiB |
|       from large pool |   1930 MiB |   8664 MiB | 336638 GiB | 336637 GiB |
|       from small pool |     11 MiB |     31 MiB |   1407 GiB |   1407 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |     721    |   14558 K  |   14557 K  |
|       from large pool |     331    |     355    |    7132 K  |    7132 K  |
|       from small pool |     315    |     376    |    7425 K  |    7424 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |     721    |   14558 K  |   14557 K  |
|       from large pool |     331    |     355    |    7132 K  |    7132 K  |
|       from small pool |     315    |     376    |    7425 K  |    7424 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      89    |     109    |    2702    |    2613    |
|       from large pool |      73    |      73    |    1269    |    1196    |
|       from small pool |      16    |      36    |    1433    |    1417    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     104    |     116    |    7976 K  |    7976 K  |
|       from large pool |      73    |      79    |    4446 K  |    4446 K  |
|       from small pool |      31    |      62    |    3530 K  |    3530 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:23:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:23:35] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:23:42]    INFO >> epoch 005:    821 / 1539 loss=3.588, wps=2935.6, ups=4.6, wpb=637.8, bsz=637.8, num_updates=6950, lr=0.000354, gnorm=3.965, clip=0, train_wall=9, gb_free=69.7, wall=1632 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:23:53]    INFO >> epoch 005:    871 / 1539 loss=3.391, wps=3421.9, ups=4.84, wpb=707, bsz=707, num_updates=7000, lr=0.000354, gnorm=4.574, clip=0, train_wall=10, gb_free=69.1, wall=1642 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:24:04]    INFO >> epoch 005:    921 / 1539 loss=3.56, wps=3013.7, ups=4.71, wpb=640.5, bsz=640.5, num_updates=7050, lr=0.000354, gnorm=4.31, clip=0, train_wall=10, gb_free=69.9, wall=1653 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:24:10] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 317.25 MiB is free. Including non-PyTorch memory, this process has 78.81 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 2.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:24:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:24:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:24:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 41        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77289 MiB |  77889 MiB | 326772 GiB | 326696 GiB |
|       from large pool |  77272 MiB |  77872 MiB | 325484 GiB | 325408 GiB |
|       from small pool |     16 MiB |     30 MiB |   1288 GiB |   1288 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77289 MiB |  77889 MiB | 326772 GiB | 326696 GiB |
|       from large pool |  77272 MiB |  77872 MiB | 325484 GiB | 325408 GiB |
|       from small pool |     16 MiB |     30 MiB |   1288 GiB |   1288 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77280 MiB |  77879 MiB | 326344 GiB | 326269 GiB |
|       from large pool |  77263 MiB |  77862 MiB | 325058 GiB | 324983 GiB |
|       from small pool |     16 MiB |     30 MiB |   1286 GiB |   1286 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80188 MiB |  80188 MiB | 407846 MiB | 327658 MiB |
|       from large pool |  80158 MiB |  80158 MiB | 404942 MiB | 324784 MiB |
|       from small pool |     30 MiB |     70 MiB |   2904 MiB |   2874 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2838 MiB |   6449 MiB | 345633 GiB | 345630 GiB |
|       from large pool |   2825 MiB |   6435 MiB | 344199 GiB | 344196 GiB |
|       from small pool |     13 MiB |     25 MiB |   1434 GiB |   1434 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     528    |     535    |   14859 K  |   14858 K  |
|       from large pool |     239    |     246    |    7295 K  |    7295 K  |
|       from small pool |     289    |     356    |    7563 K  |    7563 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     528    |     535    |   14859 K  |   14858 K  |
|       from large pool |     239    |     246    |    7295 K  |    7295 K  |
|       from small pool |     289    |     356    |    7563 K  |    7563 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      81    |     109    |    2725    |    2644    |
|       from large pool |      66    |      74    |    1273    |    1207    |
|       from small pool |      15    |      35    |    1452    |    1437    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      90    |      90    |    8135 K  |    8134 K  |
|       from large pool |      63    |      63    |    4546 K  |    4545 K  |
|       from small pool |      27    |      50    |    3589 K  |    3588 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:24:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:24:10] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:24:15]    INFO >> epoch 005:    972 / 1539 loss=3.556, wps=3187.2, ups=4.56, wpb=698.3, bsz=698.3, num_updates=7100, lr=0.000354, gnorm=4.658, clip=0, train_wall=10, gb_free=51, wall=1664 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:24:28]    INFO >> epoch 005:   1022 / 1539 loss=3.278, wps=3916.9, ups=4.16, wpb=941.7, bsz=941.7, num_updates=7150, lr=0.000354, gnorm=4.862, clip=0, train_wall=11, gb_free=67.7, wall=1676 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:24:39]    INFO >> epoch 005:   1072 / 1539 loss=3.477, wps=3469.9, ups=4.49, wpb=773.6, bsz=773.6, num_updates=7200, lr=0.000354, gnorm=3.871, clip=0, train_wall=11, gb_free=72.2, wall=1687 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:24:49]    INFO >> epoch 005:   1122 / 1539 loss=3.484, wps=3186.9, ups=5.04, wpb=632.1, bsz=632.1, num_updates=7250, lr=0.000354, gnorm=4.799, clip=0, train_wall=9, gb_free=70.8, wall=1697 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:24:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.81 GiB. GPU 2 has a total capacity of 79.14 GiB of which 899.25 MiB is free. Including non-PyTorch memory, this process has 78.24 GiB memory in use. Of the allocated memory 67.00 GiB is allocated by PyTorch, and 10.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:24:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:24:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:24:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 43        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  51065 MiB |  72747 MiB | 334660 GiB | 334611 GiB |
|       from large pool |  51047 MiB |  72730 MiB | 333341 GiB | 333291 GiB |
|       from small pool |     17 MiB |     27 MiB |   1319 GiB |   1319 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  51065 MiB |  72747 MiB | 334660 GiB | 334611 GiB |
|       from large pool |  51047 MiB |  72730 MiB | 333341 GiB | 333291 GiB |
|       from small pool |     17 MiB |     27 MiB |   1319 GiB |   1319 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  51057 MiB |  72738 MiB | 334223 GiB | 334173 GiB |
|       from large pool |  51040 MiB |  72720 MiB | 332905 GiB | 332856 GiB |
|       from small pool |     17 MiB |     27 MiB |   1317 GiB |   1317 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79606 MiB |  80324 MiB | 410894 MiB | 331288 MiB |
|       from large pool |  79578 MiB |  80098 MiB | 407794 MiB | 328216 MiB |
|       from small pool |     28 MiB |    226 MiB |   3100 MiB |   3072 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  16420 MiB |  19680 MiB | 354755 GiB | 354739 GiB |
|       from large pool |  16410 MiB |  19670 MiB | 353285 GiB | 353269 GiB |
|       from small pool |     10 MiB |     33 MiB |   1470 GiB |   1470 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     454    |     504    |   15221 K  |   15220 K  |
|       from large pool |     166    |     215    |    7474 K  |    7474 K  |
|       from small pool |     288    |     356    |    7746 K  |    7746 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     454    |     504    |   15221 K  |   15220 K  |
|       from large pool |     166    |     215    |    7474 K  |    7474 K  |
|       from small pool |     288    |     356    |    7746 K  |    7746 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |     178    |    2824    |    2750    |
|       from large pool |      60    |      65    |    1274    |    1214    |
|       from small pool |      14    |     113    |    1550    |    1536    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      91    |      93    |    8330 K  |    8330 K  |
|       from large pool |      63    |      65    |    4652 K  |    4652 K  |
|       from small pool |      28    |      59    |    3677 K  |    3677 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:24:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:24:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:25:03]    INFO >> epoch 005:   1173 / 1539 loss=3.538, wps=3291.1, ups=4.17, wpb=788.8, bsz=788.8, num_updates=7300, lr=0.000354, gnorm=4.972, clip=0, train_wall=10, gb_free=71.2, wall=1709 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:25:13]    INFO >> epoch 005:   1223 / 1539 loss=3.618, wps=3062.6, ups=4.84, wpb=632.5, bsz=632.5, num_updates=7350, lr=0.000354, gnorm=4.077, clip=0, train_wall=10, gb_free=66.2, wall=1719 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:25:22]    INFO >> epoch 005:   1273 / 1539 loss=3.573, wps=3758.4, ups=5.62, wpb=669.3, bsz=669.3, num_updates=7400, lr=0.000354, gnorm=4.114, clip=0, train_wall=8, gb_free=72, wall=1728 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:25:34]    INFO >> epoch 005:   1323 / 1539 loss=3.473, wps=3182.5, ups=4.81, wpb=661.6, bsz=661.6, num_updates=7450, lr=0.000354, gnorm=4.377, clip=0, train_wall=10, gb_free=71, wall=1738 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:25:43]    INFO >> epoch 005:   1373 / 1539 loss=3.555, wps=3497.6, ups=5.11, wpb=684.4, bsz=684.4, num_updates=7500, lr=0.000354, gnorm=4.736, clip=0, train_wall=9, gb_free=70.3, wall=1748 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:25:53]    INFO >> epoch 005:   1423 / 1539 loss=3.658, wps=3272.8, ups=4.99, wpb=655.3, bsz=655.3, num_updates=7550, lr=0.000354, gnorm=4.542, clip=0, train_wall=9, gb_free=63.9, wall=1758 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:26:06]    INFO >> epoch 005:   1473 / 1539 loss=3.465, wps=3773.1, ups=4.61, wpb=819, bsz=819, num_updates=7600, lr=0.000354, gnorm=5.29, clip=0, train_wall=10, gb_free=71.4, wall=1769 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:26:17]    INFO >> epoch 005:   1523 / 1539 loss=3.636, wps=3322.9, ups=4.41, wpb=754.3, bsz=754.3, num_updates=7650, lr=0.000354, gnorm=4.524, clip=2, train_wall=11, gb_free=66, wall=1780 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:26:21]    INFO >> epoch 005 | loss 3.54 | wps 3127.6 | ups 4.4 | wpb 711 | bsz 711 | num_updates 7666 | lr 0.000354 | gnorm 4.609 | clip 0.1 | train_wall 304 | gb_free 56 | wall 1784 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:26:21] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:26:44]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.718 | wps 6831.4 | wpb 5412.5 | bsz 5412.5 | num_updates 7666 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:26:45]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:26:45]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 5 @ 7666 updates, score 3.718) (writing took 0.026611 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:26:45] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:26:51]    INFO >> epoch 006:     34 / 1539 loss=3.492, wps=1101.4, ups=1.51, wpb=727.7, bsz=727.7, num_updates=7700, lr=0.000327, gnorm=4.561, clip=0, train_wall=10, gb_free=64.6, wall=1813 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:27:03]    INFO >> epoch 006:     84 / 1539 loss=3.66, wps=3615.4, ups=4.45, wpb=813, bsz=813, num_updates=7750, lr=0.000327, gnorm=5.209, clip=2, train_wall=11, gb_free=66.7, wall=1825 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:27:14]    INFO >> epoch 006:    134 / 1539 loss=3.6, wps=3564.4, ups=5.05, wpb=706.4, bsz=706.4, num_updates=7800, lr=0.000327, gnorm=4.79, clip=0, train_wall=9, gb_free=65.1, wall=1835 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:27:25]    INFO >> epoch 006:    184 / 1539 loss=3.49, wps=3073.1, ups=4.56, wpb=674.2, bsz=674.2, num_updates=7850, lr=0.000327, gnorm=4.801, clip=0, train_wall=10, gb_free=67.5, wall=1846 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:27:26] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 722.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 141.25 MiB is free. Including non-PyTorch memory, this process has 78.98 GiB memory in use. Of the allocated memory 73.99 GiB is allocated by PyTorch, and 4.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:27:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:27:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:27:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 44        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72193 MiB |  76054 MiB | 366624 GiB | 366553 GiB |
|       from large pool |  72175 MiB |  76036 MiB | 365172 GiB | 365102 GiB |
|       from small pool |     18 MiB |     21 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72193 MiB |  76054 MiB | 366624 GiB | 366553 GiB |
|       from large pool |  72175 MiB |  76036 MiB | 365172 GiB | 365102 GiB |
|       from small pool |     18 MiB |     21 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72184 MiB |  76043 MiB | 366151 GiB | 366081 GiB |
|       from large pool |  72166 MiB |  76025 MiB | 364702 GiB | 364631 GiB |
|       from small pool |     18 MiB |     21 MiB |   1449 GiB |   1449 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80364 MiB |  80438 MiB | 423846 MiB | 343482 MiB |
|       from large pool |  80332 MiB |  80332 MiB | 420668 MiB | 340336 MiB |
|       from small pool |     32 MiB |    106 MiB |   3178 MiB |   3146 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5318 MiB |  14599 MiB | 388940 GiB | 388935 GiB |
|       from large pool |   5304 MiB |  14584 MiB | 387326 GiB | 387321 GiB |
|       from small pool |     13 MiB |     31 MiB |   1614 GiB |   1614 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     509    |     518    |   16649 K  |   16649 K  |
|       from large pool |     220    |     229    |    8116 K  |    8116 K  |
|       from small pool |     289    |     356    |    8533 K  |    8533 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     509    |     518    |   16649 K  |   16649 K  |
|       from large pool |     220    |     229    |    8116 K  |    8116 K  |
|       from small pool |     289    |     356    |    8533 K  |    8533 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      76    |     113    |    2869    |    2793    |
|       from large pool |      60    |      60    |    1280    |    1220    |
|       from small pool |      16    |      53    |    1589    |    1573    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      91    |    9116 K  |    9116 K  |
|       from large pool |      58    |      60    |    5038 K  |    5038 K  |
|       from small pool |      31    |      68    |    4078 K  |    4078 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:27:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:27:26] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 01:27:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.33 GiB is free. Including non-PyTorch memory, this process has 77.79 GiB memory in use. Of the allocated memory 75.97 GiB is allocated by PyTorch, and 1.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:27:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:27:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:27:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 45        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77203 MiB |  78280 MiB | 368250 GiB | 368174 GiB |
|       from large pool |  77181 MiB |  78257 MiB | 366793 GiB | 366718 GiB |
|       from small pool |     22 MiB |     23 MiB |   1456 GiB |   1456 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77203 MiB |  78280 MiB | 368250 GiB | 368174 GiB |
|       from large pool |  77181 MiB |  78257 MiB | 366793 GiB | 366718 GiB |
|       from small pool |     22 MiB |     23 MiB |   1456 GiB |   1456 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 367775 GiB | 367700 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 366321 GiB | 366245 GiB |
|       from small pool |     22 MiB |     23 MiB |   1454 GiB |   1454 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79144 MiB |  79172 MiB | 425506 MiB | 346362 MiB |
|       from large pool |  79112 MiB |  79112 MiB | 422300 MiB | 343188 MiB |
|       from small pool |     32 MiB |     60 MiB |   3206 MiB |   3174 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1940 MiB |   6463 MiB | 390819 GiB | 390817 GiB |
|       from large pool |   1930 MiB |   6453 MiB | 389199 GiB | 389197 GiB |
|       from small pool |      9 MiB |     31 MiB |   1619 GiB |   1619 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   16713 K  |   16712 K  |
|       from large pool |     270    |     278    |    8150 K  |    8149 K  |
|       from small pool |     296    |     356    |    8563 K  |    8562 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   16713 K  |   16712 K  |
|       from large pool |     270    |     278    |    8150 K  |    8149 K  |
|       from small pool |     296    |     356    |    8563 K  |    8562 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      78    |      92    |    2886    |    2808    |
|       from large pool |      62    |      62    |    1283    |    1221    |
|       from small pool |      16    |      30    |    1603    |    1587    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     101    |     101    |    9150 K  |    9150 K  |
|       from large pool |      69    |      69    |    5058 K  |    5058 K  |
|       from small pool |      32    |      69    |    4092 K  |    4092 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:27:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:27:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:27:38]    INFO >> epoch 006:    236 / 1539 loss=3.564, wps=2748.2, ups=3.84, wpb=716.2, bsz=716.2, num_updates=7900, lr=0.000327, gnorm=4.184, clip=0, train_wall=11, gb_free=72.2, wall=1859 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:27:49]    INFO >> epoch 006:    286 / 1539 loss=3.454, wps=3530.9, ups=5.17, wpb=683.2, bsz=683.2, num_updates=7950, lr=0.000327, gnorm=4.353, clip=0, train_wall=9, gb_free=69.8, wall=1868 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:28:00]    INFO >> epoch 006:    336 / 1539 loss=3.337, wps=3559.4, ups=4.49, wpb=792.7, bsz=792.7, num_updates=8000, lr=0.000327, gnorm=5.388, clip=0, train_wall=10, gb_free=67, wall=1879 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:28:10]    INFO >> epoch 006:    386 / 1539 loss=3.6, wps=3104.7, ups=4.76, wpb=651.9, bsz=651.9, num_updates=8050, lr=0.000327, gnorm=3.992, clip=0, train_wall=10, gb_free=73.3, wall=1890 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:28:22]    INFO >> epoch 006:    436 / 1539 loss=3.579, wps=3804.8, ups=5.01, wpb=759.3, bsz=759.3, num_updates=8100, lr=0.000327, gnorm=3.939, clip=0, train_wall=9, gb_free=67.2, wall=1900 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:28:23] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 3.51 GiB is free. Including non-PyTorch memory, this process has 75.60 GiB memory in use. Of the allocated memory 72.07 GiB is allocated by PyTorch, and 3.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:28:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:28:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:28:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 46        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73198 MiB |  74274 MiB | 377481 GiB | 377409 GiB |
|       from large pool |  73181 MiB |  74258 MiB | 375988 GiB | 375916 GiB |
|       from small pool |     16 MiB |     27 MiB |   1492 GiB |   1492 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73198 MiB |  74274 MiB | 377481 GiB | 377409 GiB |
|       from large pool |  73181 MiB |  74258 MiB | 375988 GiB | 375916 GiB |
|       from small pool |     16 MiB |     27 MiB |   1492 GiB |   1492 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73189 MiB |  74265 MiB | 376995 GiB | 376923 GiB |
|       from large pool |  73173 MiB |  74248 MiB | 375504 GiB | 375432 GiB |
|       from small pool |     16 MiB |     27 MiB |   1490 GiB |   1490 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  76906 MiB |  79322 MiB | 425684 MiB | 348778 MiB |
|       from large pool |  76876 MiB |  79112 MiB | 422300 MiB | 345424 MiB |
|       from small pool |     30 MiB |    210 MiB |   3384 MiB |   3354 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3707 MiB |   6243 MiB | 401658 GiB | 401654 GiB |
|       from large pool |   3694 MiB |   6229 MiB | 399997 GiB | 399994 GiB |
|       from small pool |     13 MiB |     35 MiB |   1660 GiB |   1660 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     527    |     535    |   17145 K  |   17145 K  |
|       from large pool |     238    |     246    |    8370 K  |    8370 K  |
|       from small pool |     289    |     356    |    8774 K  |    8774 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     527    |     535    |   17145 K  |   17145 K  |
|       from large pool |     238    |     246    |    8370 K  |    8370 K  |
|       from small pool |     289    |     356    |    8774 K  |    8774 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      72    |     167    |    2975    |    2903    |
|       from large pool |      57    |      62    |    1283    |    1226    |
|       from small pool |      15    |     105    |    1692    |    1677    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      90    |      90    |    9379 K  |    9379 K  |
|       from large pool |      63    |      63    |    5189 K  |    5189 K  |
|       from small pool |      27    |      60    |    4189 K  |    4189 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:28:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:28:23] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:28:33]    INFO >> epoch 006:    487 / 1539 loss=3.358, wps=3469.8, ups=4.43, wpb=783.9, bsz=783.9, num_updates=8150, lr=0.000327, gnorm=4.766, clip=0, train_wall=10, gb_free=60.2, wall=1911 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:28:44]    INFO >> epoch 006:    537 / 1539 loss=3.548, wps=2966.9, ups=4.49, wpb=660.1, bsz=660.1, num_updates=8200, lr=0.000327, gnorm=4.84, clip=0, train_wall=11, gb_free=66.4, wall=1922 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:28:56]    INFO >> epoch 006:    587 / 1539 loss=3.655, wps=3647.8, ups=4.83, wpb=754.9, bsz=754.9, num_updates=8250, lr=0.000327, gnorm=3.979, clip=0, train_wall=10, gb_free=68.5, wall=1933 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:29:06]    INFO >> epoch 006:    637 / 1539 loss=3.449, wps=3370.8, ups=4.94, wpb=682.4, bsz=682.4, num_updates=8300, lr=0.000327, gnorm=4.605, clip=0, train_wall=10, gb_free=63.2, wall=1943 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:29:16]    INFO >> epoch 006:    687 / 1539 loss=3.504, wps=3458.3, ups=4.91, wpb=704.8, bsz=704.8, num_updates=8350, lr=0.000327, gnorm=4.112, clip=0, train_wall=10, gb_free=67.5, wall=1953 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:29:27] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.52 GiB is allocated by PyTorch, and 1.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:29:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:29:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:29:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 33           |        cudaMalloc retries: 48        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79323 MiB |  79383 MiB | 389681 GiB | 389604 GiB |
|       from large pool |  79139 MiB |  79199 MiB | 388139 GiB | 388062 GiB |
|       from small pool |    183 MiB |    184 MiB |   1542 GiB |   1542 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79323 MiB |  79383 MiB | 389681 GiB | 389604 GiB |
|       from large pool |  79139 MiB |  79199 MiB | 388139 GiB | 388062 GiB |
|       from small pool |    183 MiB |    184 MiB |   1542 GiB |   1542 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79274 MiB |  79334 MiB | 389179 GiB | 389102 GiB |
|       from large pool |  79092 MiB |  79151 MiB | 387639 GiB | 387562 GiB |
|       from small pool |    182 MiB |    183 MiB |   1539 GiB |   1539 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 429324 MiB | 348826 MiB |
|       from large pool |  80296 MiB |  80296 MiB | 425720 MiB | 345424 MiB |
|       from small pool |    202 MiB |    248 MiB |   3604 MiB |   3402 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1114 MiB |   6546 MiB | 416293 GiB | 416292 GiB |
|       from large pool |   1096 MiB |   6542 MiB | 414577 GiB | 414576 GiB |
|       from small pool |     18 MiB |     27 MiB |   1716 GiB |   1716 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3413    |    3416    |   17724 K  |   17721 K  |
|       from large pool |     588    |     589    |    8664 K  |    8664 K  |
|       from small pool |    2825    |    2828    |    9059 K  |    9056 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3413    |    3416    |   17724 K  |   17721 K  |
|       from large pool |     588    |     589    |    8664 K  |    8664 K  |
|       from small pool |    2825    |    2828    |    9059 K  |    9056 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     215    |     237    |    3142    |    2927    |
|       from large pool |     114    |     114    |    1340    |    1226    |
|       from small pool |     101    |     124    |    1802    |    1701    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     243    |     243    |    9684 K  |    9684 K  |
|       from large pool |      63    |      65    |    5364 K  |    5364 K  |
|       from small pool |     180    |     180    |    4320 K  |    4320 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:29:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:29:27] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:29:28]    INFO >> epoch 006:    738 / 1539 loss=3.464, wps=2831.4, ups=4.72, wpb=599.7, bsz=599.7, num_updates=8400, lr=0.000327, gnorm=4.304, clip=0, train_wall=9, gb_free=68.8, wall=1964 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:29:38]    INFO >> epoch 006:    788 / 1539 loss=3.515, wps=3510.4, ups=5.31, wpb=661.1, bsz=661.1, num_updates=8450, lr=0.000327, gnorm=4.096, clip=0, train_wall=9, gb_free=71.7, wall=1973 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:29:48]    INFO >> epoch 006:    838 / 1539 loss=3.482, wps=3308.1, ups=4.82, wpb=687, bsz=687, num_updates=8500, lr=0.000327, gnorm=4.081, clip=0, train_wall=10, gb_free=67.2, wall=1983 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:30:00]    INFO >> epoch 006:    888 / 1539 loss=3.567, wps=3364.5, ups=4.77, wpb=705.5, bsz=705.5, num_updates=8550, lr=0.000327, gnorm=4.2, clip=0, train_wall=10, gb_free=66.2, wall=1994 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:30:12]    INFO >> epoch 006:    938 / 1539 loss=3.418, wps=3811.9, ups=4.07, wpb=936.3, bsz=936.3, num_updates=8600, lr=0.000327, gnorm=5.354, clip=0, train_wall=12, gb_free=54.2, wall=2006 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:30:22]    INFO >> epoch 006:    988 / 1539 loss=3.56, wps=2893.2, ups=5.05, wpb=572.7, bsz=572.7, num_updates=8650, lr=0.000327, gnorm=3.949, clip=0, train_wall=9, gb_free=71, wall=2016 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:30:33] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 239.25 MiB is free. Including non-PyTorch memory, this process has 78.88 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 4.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:30:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:30:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:30:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 34           |        cudaMalloc retries: 49        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75039 MiB |  75469 MiB | 402362 GiB | 402289 GiB |
|       from large pool |  75013 MiB |  75443 MiB | 400771 GiB | 400698 GiB |
|       from small pool |     25 MiB |     29 MiB |   1590 GiB |   1590 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75039 MiB |  75469 MiB | 402362 GiB | 402289 GiB |
|       from large pool |  75013 MiB |  75443 MiB | 400771 GiB | 400698 GiB |
|       from small pool |     25 MiB |     29 MiB |   1590 GiB |   1590 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 401841 GiB | 401768 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 400253 GiB | 400180 GiB |
|       from small pool |     25 MiB |     29 MiB |   1588 GiB |   1588 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80266 MiB |  80462 MiB | 429348 MiB | 349082 MiB |
|       from large pool |  80236 MiB |  80236 MiB | 425720 MiB | 345484 MiB |
|       from small pool |     30 MiB |    226 MiB |   3628 MiB |   3598 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5226 MiB |   7149 MiB | 429696 GiB | 429691 GiB |
|       from large pool |   5222 MiB |   7141 MiB | 427925 GiB | 427920 GiB |
|       from small pool |      4 MiB |     27 MiB |   1770 GiB |   1770 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   18303 K  |   18303 K  |
|       from large pool |     265    |     272    |    8964 K  |    8964 K  |
|       from small pool |     302    |     356    |    9339 K  |    9339 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   18303 K  |   18303 K  |
|       from large pool |     265    |     272    |    8964 K  |    8964 K  |
|       from small pool |     302    |     356    |    9339 K  |    9339 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     128    |     226    |    3154    |    3026    |
|       from large pool |     113    |     113    |    1340    |    1227    |
|       from small pool |      15    |     113    |    1814    |    1799    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     117    |     121    |   10006 K  |   10006 K  |
|       from large pool |      88    |      92    |    5558 K  |    5558 K  |
|       from small pool |      29    |      58    |    4447 K  |    4447 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:30:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:30:33] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:30:35]    INFO >> epoch 006:   1039 / 1539 loss=3.547, wps=3203.4, ups=4.23, wpb=758, bsz=758, num_updates=8700, lr=0.000327, gnorm=4.468, clip=0, train_wall=10, gb_free=61.7, wall=2028 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:30:46]    INFO >> epoch 006:   1089 / 1539 loss=3.54, wps=3319.7, ups=4.69, wpb=707.3, bsz=707.3, num_updates=8750, lr=0.000327, gnorm=4.446, clip=0, train_wall=10, gb_free=64.1, wall=2039 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:30:57]    INFO >> epoch 006:   1139 / 1539 loss=3.376, wps=3326.5, ups=4.3, wpb=774.5, bsz=774.5, num_updates=8800, lr=0.000327, gnorm=4.172, clip=0, train_wall=11, gb_free=59.9, wall=2050 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:31:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 2 has a total capacity of 79.14 GiB of which 239.25 MiB is free. Including non-PyTorch memory, this process has 78.88 GiB memory in use. Of the allocated memory 72.14 GiB is allocated by PyTorch, and 6.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:31:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:31:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:31:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 35           |        cudaMalloc retries: 50        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69480 MiB |  76767 MiB | 409215 GiB | 409147 GiB |
|       from large pool |  69459 MiB |  76747 MiB | 407600 GiB | 407532 GiB |
|       from small pool |     20 MiB |     26 MiB |   1615 GiB |   1615 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69480 MiB |  76767 MiB | 409215 GiB | 409147 GiB |
|       from large pool |  69459 MiB |  76747 MiB | 407600 GiB | 407532 GiB |
|       from small pool |     20 MiB |     26 MiB |   1615 GiB |   1615 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69460 MiB |  76747 MiB | 408686 GiB | 408618 GiB |
|       from large pool |  69439 MiB |  76727 MiB | 407073 GiB | 407005 GiB |
|       from small pool |     20 MiB |     25 MiB |   1612 GiB |   1612 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80266 MiB |  80370 MiB | 429452 MiB | 349186 MiB |
|       from large pool |  80236 MiB |  80236 MiB | 425720 MiB | 345484 MiB |
|       from small pool |     30 MiB |    134 MiB |   3732 MiB |   3702 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6693 MiB |   9030 MiB | 436955 GiB | 436948 GiB |
|       from large pool |   6684 MiB |   9020 MiB | 435156 GiB | 435150 GiB |
|       from small pool |      9 MiB |     20 MiB |   1798 GiB |   1798 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |     721    |   18601 K  |   18601 K  |
|       from large pool |     331    |     355    |    9118 K  |    9118 K  |
|       from small pool |     315    |     376    |    9483 K  |    9483 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |     721    |   18601 K  |   18601 K  |
|       from large pool |     331    |     355    |    9118 K  |    9118 K  |
|       from small pool |     315    |     376    |    9483 K  |    9483 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     128    |     180    |    3206    |    3078    |
|       from large pool |     113    |     113    |    1340    |    1227    |
|       from small pool |      15    |      67    |    1866    |    1851    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     133    |     144    |   10170 K  |   10170 K  |
|       from large pool |     106    |     111    |    5656 K  |    5656 K  |
|       from small pool |      27    |      48    |    4513 K  |    4513 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:31:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:31:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:31:10]    INFO >> epoch 006:   1190 / 1539 loss=3.561, wps=3239.8, ups=4.42, wpb=732.7, bsz=732.7, num_updates=8850, lr=0.000327, gnorm=5.114, clip=0, train_wall=10, gb_free=69.3, wall=2061 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:31:20]    INFO >> epoch 006:   1240 / 1539 loss=3.466, wps=3569.5, ups=5.15, wpb=692.9, bsz=692.9, num_updates=8900, lr=0.000327, gnorm=4.709, clip=0, train_wall=9, gb_free=63.8, wall=2071 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:31:30]    INFO >> epoch 006:   1290 / 1539 loss=3.6, wps=2944.6, ups=4.76, wpb=618.3, bsz=618.3, num_updates=8950, lr=0.000327, gnorm=4.146, clip=0, train_wall=10, gb_free=46.9, wall=2082 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:31:42]    INFO >> epoch 006:   1340 / 1539 loss=3.455, wps=3442.4, ups=4.82, wpb=714.5, bsz=714.5, num_updates=9000, lr=0.000327, gnorm=4.423, clip=0, train_wall=10, gb_free=74.5, wall=2092 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:31:52]    INFO >> epoch 006:   1390 / 1539 loss=3.535, wps=3296, ups=4.96, wpb=665, bsz=665, num_updates=9050, lr=0.000327, gnorm=3.818, clip=0, train_wall=10, gb_free=70.7, wall=2102 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:32:03]    INFO >> epoch 006:   1440 / 1539 loss=3.399, wps=3413.9, ups=4.7, wpb=726.4, bsz=726.4, num_updates=9100, lr=0.000327, gnorm=5.045, clip=0, train_wall=10, gb_free=66.7, wall=2113 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:32:15]    INFO >> epoch 006:   1490 / 1539 loss=3.403, wps=3134.9, ups=4.39, wpb=714.4, bsz=714.4, num_updates=9150, lr=0.000327, gnorm=4.449, clip=0, train_wall=11, gb_free=67.3, wall=2124 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:32:26]    INFO >> epoch 006 | loss 3.503 | wps 3112.4 | ups 4.38 | wpb 711 | bsz 711 | num_updates 9199 | lr 0.000327 | gnorm 4.452 | clip 0.1 | train_wall 305 | gb_free 68.5 | wall 2134 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:32:26] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:32:48]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.719 | wps 7116.7 | wpb 5412.5 | bsz 5412.5 | num_updates 9199 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:32:49]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:32:49]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 6 @ 9199 updates, score 3.719) (writing took 0.019999 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:32:49] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:32:49]    INFO >> epoch 007:      1 / 1539 loss=3.452, wps=1040.3, ups=1.55, wpb=671.1, bsz=671.1, num_updates=9200, lr=0.000295, gnorm=3.728, clip=0, train_wall=10, gb_free=63.9, wall=2156 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:32:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 381.25 MiB is free. Including non-PyTorch memory, this process has 78.74 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 4.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:32:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:32:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:32:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 36           |        cudaMalloc retries: 51        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75039 MiB |  75470 MiB | 431340 GiB | 431266 GiB |
|       from large pool |  75013 MiB |  75444 MiB | 429630 GiB | 429557 GiB |
|       from small pool |     25 MiB |     27 MiB |   1709 GiB |   1709 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75039 MiB |  75470 MiB | 431340 GiB | 431266 GiB |
|       from large pool |  75013 MiB |  75444 MiB | 429630 GiB | 429557 GiB |
|       from small pool |     25 MiB |     27 MiB |   1709 GiB |   1709 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 430783 GiB | 430710 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 429076 GiB | 429003 GiB |
|       from small pool |     25 MiB |     27 MiB |   1706 GiB |   1706 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80124 MiB |  80166 MiB | 433444 MiB | 353320 MiB |
|       from large pool |  80094 MiB |  80094 MiB | 429670 MiB | 349576 MiB |
|       from small pool |     30 MiB |     72 MiB |   3774 MiB |   3744 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5084 MiB |   7149 MiB | 456969 GiB | 456964 GiB |
|       from large pool |   5080 MiB |   7141 MiB | 455070 GiB | 455065 GiB |
|       from small pool |      4 MiB |     33 MiB |   1899 GiB |   1899 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   19588 K  |   19588 K  |
|       from large pool |     265    |     272    |    9536 K  |    9535 K  |
|       from small pool |     302    |     356    |   10052 K  |   10052 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   19588 K  |   19588 K  |
|       from large pool |     265    |     272    |    9536 K  |    9535 K  |
|       from small pool |     302    |     356    |   10052 K  |   10052 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     128    |     149    |    3228    |    3100    |
|       from large pool |     113    |     113    |    1341    |    1228    |
|       from small pool |      15    |      36    |    1887    |    1872    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     116    |     120    |   10743 K  |   10743 K  |
|       from large pool |      87    |      91    |    5929 K  |    5929 K  |
|       from small pool |      29    |      64    |    4813 K  |    4813 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:32:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:32:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 01:33:00] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 3.99 GiB is free. Including non-PyTorch memory, this process has 75.13 GiB memory in use. Of the allocated memory 72.07 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:33:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:33:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:33:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 37           |        cudaMalloc retries: 53        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73198 MiB |  74274 MiB | 432472 GiB | 432400 GiB |
|       from large pool |  73181 MiB |  74258 MiB | 430759 GiB | 430687 GiB |
|       from small pool |     16 MiB |     31 MiB |   1713 GiB |   1713 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73198 MiB |  74274 MiB | 432472 GiB | 432400 GiB |
|       from large pool |  73181 MiB |  74258 MiB | 430759 GiB | 430687 GiB |
|       from small pool |     16 MiB |     31 MiB |   1713 GiB |   1713 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73189 MiB |  74265 MiB | 431914 GiB | 431842 GiB |
|       from large pool |  73173 MiB |  74248 MiB | 430203 GiB | 430132 GiB |
|       from small pool |     16 MiB |     31 MiB |   1710 GiB |   1710 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  76424 MiB |  80152 MiB | 437564 MiB | 361140 MiB |
|       from large pool |  76392 MiB |  80094 MiB | 433762 MiB | 357370 MiB |
|       from small pool |     32 MiB |     58 MiB |   3802 MiB |   3770 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3225 MiB |   6365 MiB | 458153 GiB | 458150 GiB |
|       from large pool |   3210 MiB |   6349 MiB | 456249 GiB | 456246 GiB |
|       from small pool |     15 MiB |     26 MiB |   1903 GiB |   1903 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     527    |     535    |   19636 K  |   19636 K  |
|       from large pool |     238    |     246    |    9561 K  |    9561 K  |
|       from small pool |     289    |     356    |   10075 K  |   10075 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     527    |     535    |   19636 K  |   19636 K  |
|       from large pool |     238    |     246    |    9561 K  |    9561 K  |
|       from small pool |     289    |     356    |   10075 K  |   10075 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      73    |     142    |    3243    |    3170    |
|       from large pool |      57    |     113    |    1342    |    1285    |
|       from small pool |      16    |      29    |    1901    |    1885    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      93    |      93    |   10769 K  |   10769 K  |
|       from large pool |      64    |      64    |    5946 K  |    5945 K  |
|       from small pool |      29    |      55    |    4823 K  |    4823 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:33:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:33:00] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:33:00]    INFO >> epoch 007:     53 / 1539 loss=3.464, wps=2928.2, ups=4.41, wpb=664.4, bsz=664.4, num_updates=9250, lr=0.000295, gnorm=4.334, clip=0, train_wall=9, gb_free=71.1, wall=2168 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:33:10]    INFO >> epoch 007:    103 / 1539 loss=3.535, wps=3491.1, ups=5.09, wpb=686.3, bsz=686.3, num_updates=9300, lr=0.000295, gnorm=4.207, clip=0, train_wall=9, gb_free=46.9, wall=2178 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:33:24]    INFO >> epoch 007:    153 / 1539 loss=3.462, wps=3343.2, ups=4.18, wpb=799.7, bsz=799.7, num_updates=9350, lr=0.000295, gnorm=4.433, clip=0, train_wall=11, gb_free=69.4, wall=2190 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:33:34]    INFO >> epoch 007:    203 / 1539 loss=3.499, wps=3463.7, ups=4.58, wpb=756.8, bsz=756.8, num_updates=9400, lr=0.000295, gnorm=4.158, clip=0, train_wall=10, gb_free=66.1, wall=2200 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:33:46]    INFO >> epoch 007:    253 / 1539 loss=3.455, wps=3507.6, ups=4.37, wpb=802.1, bsz=802.1, num_updates=9450, lr=0.000295, gnorm=4.26, clip=0, train_wall=11, gb_free=64, wall=2212 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:33:57]    INFO >> epoch 007:    303 / 1539 loss=3.407, wps=3637.1, ups=5.32, wpb=683.1, bsz=683.1, num_updates=9500, lr=0.000295, gnorm=4.308, clip=0, train_wall=9, gb_free=67.4, wall=2221 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:34:02] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.22 GiB is free. Including non-PyTorch memory, this process has 77.90 GiB memory in use. Of the allocated memory 75.97 GiB is allocated by PyTorch, and 1.43 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:34:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 38           |        cudaMalloc retries: 54        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77203 MiB |  78280 MiB | 444295 GiB | 444220 GiB |
|       from large pool |  77181 MiB |  78257 MiB | 442537 GiB | 442462 GiB |
|       from small pool |     22 MiB |     23 MiB |   1758 GiB |   1758 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77203 MiB |  78280 MiB | 444295 GiB | 444220 GiB |
|       from large pool |  77181 MiB |  78257 MiB | 442537 GiB | 442462 GiB |
|       from small pool |     22 MiB |     23 MiB |   1758 GiB |   1758 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 443722 GiB | 443647 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 441966 GiB | 441891 GiB |
|       from small pool |     22 MiB |     23 MiB |   1755 GiB |   1755 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79258 MiB |  79454 MiB | 440594 MiB | 361336 MiB |
|       from large pool |  79228 MiB |  79228 MiB | 436598 MiB | 357370 MiB |
|       from small pool |     30 MiB |    226 MiB |   3996 MiB |   3966 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2054 MiB |  10490 MiB | 472289 GiB | 472287 GiB |
|       from large pool |   2046 MiB |  10481 MiB | 470335 GiB | 470333 GiB |
|       from small pool |      7 MiB |     26 MiB |   1954 GiB |   1954 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   20176 K  |   20176 K  |
|       from large pool |     270    |     278    |    9839 K  |    9839 K  |
|       from small pool |     296    |     342    |   10337 K  |   10336 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   20176 K  |   20176 K  |
|       from large pool |     270    |     278    |    9839 K  |    9839 K  |
|       from small pool |     296    |     342    |   10337 K  |   10336 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      73    |     171    |    3341    |    3268    |
|       from large pool |      58    |      58    |    1343    |    1285    |
|       from small pool |      15    |     113    |    1998    |    1983    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      93    |      95    |   11053 K  |   11053 K  |
|       from large pool |      63    |      65    |    6111 K  |    6111 K  |
|       from small pool |      30    |      52    |    4942 K  |    4942 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:02] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 01:34:04] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.22 GiB is free. Including non-PyTorch memory, this process has 77.90 GiB memory in use. Of the allocated memory 72.14 GiB is allocated by PyTorch, and 5.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:34:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 39           |        cudaMalloc retries: 55        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69480 MiB |  76767 MiB | 444787 GiB | 444719 GiB |
|       from large pool |  69459 MiB |  76747 MiB | 443027 GiB | 442959 GiB |
|       from small pool |     20 MiB |     28 MiB |   1759 GiB |   1759 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69480 MiB |  76767 MiB | 444787 GiB | 444719 GiB |
|       from large pool |  69459 MiB |  76747 MiB | 443027 GiB | 442959 GiB |
|       from small pool |     20 MiB |     28 MiB |   1759 GiB |   1759 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69460 MiB |  76747 MiB | 444213 GiB | 444145 GiB |
|       from large pool |  69439 MiB |  76727 MiB | 442456 GiB | 442388 GiB |
|       from small pool |     20 MiB |     28 MiB |   1756 GiB |   1756 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79260 MiB |  79284 MiB | 440620 MiB | 361360 MiB |
|       from large pool |  79228 MiB |  79228 MiB | 436598 MiB | 357370 MiB |
|       from small pool |     32 MiB |     56 MiB |   4022 MiB |   3990 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5687 MiB |   8024 MiB | 472847 GiB | 472842 GiB |
|       from large pool |   5676 MiB |   8012 MiB | 470891 GiB | 470886 GiB |
|       from small pool |     11 MiB |     29 MiB |   1955 GiB |   1955 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |     721    |   20192 K  |   20191 K  |
|       from large pool |     331    |     355    |    9847 K  |    9847 K  |
|       from small pool |     315    |     376    |   10344 K  |   10344 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |     721    |   20192 K  |   20191 K  |
|       from large pool |     331    |     355    |    9847 K  |    9847 K  |
|       from small pool |     315    |     376    |   10344 K  |   10344 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |      86    |    3354    |    3280    |
|       from large pool |      58    |      58    |    1343    |    1285    |
|       from small pool |      16    |      28    |    2011    |    1995    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      99    |     109    |   11062 K  |   11061 K  |
|       from large pool |      69    |      74    |    6116 K  |    6116 K  |
|       from small pool |      30    |      59    |    4945 K  |    4945 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:04] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:34:09]    INFO >> epoch 007:    355 / 1539 loss=3.524, wps=2610.7, ups=4.05, wpb=644.9, bsz=644.9, num_updates=9550, lr=0.000295, gnorm=3.86, clip=0, train_wall=10, gb_free=68.4, wall=2234 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:34:19]    INFO >> epoch 007:    405 / 1539 loss=3.562, wps=2969.6, ups=4.89, wpb=607.6, bsz=607.6, num_updates=9600, lr=0.000295, gnorm=4.111, clip=0, train_wall=10, gb_free=60.1, wall=2244 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:34:31]    INFO >> epoch 007:    455 / 1539 loss=3.449, wps=3367.4, ups=4.76, wpb=707.3, bsz=707.3, num_updates=9650, lr=0.000295, gnorm=3.566, clip=0, train_wall=10, gb_free=70.3, wall=2254 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:34:35] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.81 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.43 GiB is free. Including non-PyTorch memory, this process has 76.69 GiB memory in use. Of the allocated memory 67.00 GiB is allocated by PyTorch, and 9.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:34:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 40           |        cudaMalloc retries: 56        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  51066 MiB |  72749 MiB | 450547 GiB | 450497 GiB |
|       from large pool |  51049 MiB |  72732 MiB | 448767 GiB | 448717 GiB |
|       from small pool |     17 MiB |     24 MiB |   1779 GiB |   1779 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  51066 MiB |  72749 MiB | 450547 GiB | 450497 GiB |
|       from large pool |  51049 MiB |  72732 MiB | 448767 GiB | 448717 GiB |
|       from small pool |     17 MiB |     24 MiB |   1779 GiB |   1779 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  51057 MiB |  72738 MiB | 449966 GiB | 449916 GiB |
|       from large pool |  51040 MiB |  72720 MiB | 448189 GiB | 448139 GiB |
|       from small pool |     17 MiB |     23 MiB |   1776 GiB |   1776 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78018 MiB |  78066 MiB | 443518 MiB | 365500 MiB |
|       from large pool |  77988 MiB |  77988 MiB | 439450 MiB | 361462 MiB |
|       from small pool |     30 MiB |     78 MiB |   4068 MiB |   4038 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  15915 MiB |  19147 MiB | 479741 GiB | 479725 GiB |
|       from large pool |  15902 MiB |  19134 MiB | 477763 GiB | 477747 GiB |
|       from small pool |     12 MiB |     25 MiB |   1977 GiB |   1977 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     454    |     504    |   20445 K  |   20445 K  |
|       from large pool |     166    |     215    |    9984 K  |    9984 K  |
|       from small pool |     288    |     356    |   10461 K  |   10460 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     454    |     504    |   20445 K  |   20445 K  |
|       from large pool |     166    |     215    |    9984 K  |    9984 K  |
|       from small pool |     288    |     356    |   10461 K  |   10460 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      73    |      97    |    3378    |    3305    |
|       from large pool |      58    |      58    |    1344    |    1286    |
|       from small pool |      15    |      39    |    2034    |    2019    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      86    |      91    |   11193 K  |   11193 K  |
|       from large pool |      57    |      62    |    6198 K  |    6198 K  |
|       from small pool |      29    |      61    |    4995 K  |    4995 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:34:35] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:34:42]    INFO >> epoch 007:    506 / 1539 loss=3.426, wps=3250.1, ups=4.39, wpb=740, bsz=740, num_updates=9700, lr=0.000295, gnorm=4.312, clip=0, train_wall=10, gb_free=67.5, wall=2266 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:34:52]    INFO >> epoch 007:    556 / 1539 loss=3.479, wps=3371.2, ups=5.06, wpb=666, bsz=666, num_updates=9750, lr=0.000295, gnorm=4.251, clip=0, train_wall=9, gb_free=69.8, wall=2276 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:35:04]    INFO >> epoch 007:    606 / 1539 loss=3.535, wps=3116.6, ups=4.87, wpb=640.3, bsz=640.3, num_updates=9800, lr=0.000295, gnorm=3.905, clip=0, train_wall=10, gb_free=70.7, wall=2286 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:35:15]    INFO >> epoch 007:    656 / 1539 loss=3.36, wps=3852.1, ups=4.75, wpb=811.4, bsz=811.4, num_updates=9850, lr=0.000295, gnorm=5.081, clip=0, train_wall=10, gb_free=67.6, wall=2296 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:35:25]    INFO >> epoch 007:    706 / 1539 loss=3.35, wps=3571.3, ups=4.8, wpb=744.1, bsz=744.1, num_updates=9900, lr=0.000295, gnorm=3.893, clip=0, train_wall=10, gb_free=71.7, wall=2307 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:35:37]    INFO >> epoch 007:    756 / 1539 loss=3.55, wps=3198.4, ups=4.81, wpb=665.4, bsz=665.4, num_updates=9950, lr=0.000295, gnorm=3.927, clip=0, train_wall=10, gb_free=73.5, wall=2317 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:35:47]    INFO >> epoch 007:    806 / 1539 loss=3.554, wps=3184.8, ups=4.93, wpb=646, bsz=646, num_updates=10000, lr=0.000295, gnorm=3.766, clip=0, train_wall=10, gb_free=72.4, wall=2327 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:35:58]    INFO >> epoch 007:    856 / 1539 loss=3.454, wps=3462, ups=4.57, wpb=756.9, bsz=756.9, num_updates=10050, lr=0.000295, gnorm=4.121, clip=0, train_wall=10, gb_free=68.7, wall=2338 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:36:09]    INFO >> epoch 007:    906 / 1539 loss=3.439, wps=3318.6, ups=4.96, wpb=668.4, bsz=668.4, num_updates=10100, lr=0.000295, gnorm=4.258, clip=0, train_wall=10, gb_free=71.8, wall=2348 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:36:20]    INFO >> epoch 007:    956 / 1539 loss=3.513, wps=3487.1, ups=4.62, wpb=754.1, bsz=754.1, num_updates=10150, lr=0.000295, gnorm=4.266, clip=0, train_wall=10, gb_free=55.4, wall=2359 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:36:30]    INFO >> epoch 007:   1006 / 1539 loss=3.401, wps=3324.3, ups=4.88, wpb=681.8, bsz=681.8, num_updates=10200, lr=0.000295, gnorm=3.922, clip=0, train_wall=10, gb_free=70, wall=2370 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:36:43]    INFO >> epoch 007:   1056 / 1539 loss=3.502, wps=3299.7, ups=4.37, wpb=754.4, bsz=754.4, num_updates=10250, lr=0.000295, gnorm=4.403, clip=0, train_wall=11, gb_free=64.4, wall=2381 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:36:53]    INFO >> epoch 007:   1106 / 1539 loss=3.404, wps=3452.6, ups=5.06, wpb=682.8, bsz=682.8, num_updates=10300, lr=0.000295, gnorm=3.668, clip=0, train_wall=9, gb_free=74.1, wall=2391 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:37:05]    INFO >> epoch 007:   1156 / 1539 loss=3.338, wps=3463.5, ups=4.26, wpb=813, bsz=813, num_updates=10350, lr=0.000295, gnorm=4.555, clip=0, train_wall=11, gb_free=46.6, wall=2403 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:37:16]    INFO >> epoch 007:   1206 / 1539 loss=3.366, wps=3606.6, ups=4.8, wpb=750.9, bsz=750.9, num_updates=10400, lr=0.000295, gnorm=4.501, clip=0, train_wall=10, gb_free=68.2, wall=2413 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:37:27]    INFO >> epoch 007:   1256 / 1539 loss=3.384, wps=3465.1, ups=4.75, wpb=728.9, bsz=728.9, num_updates=10450, lr=0.000295, gnorm=4.917, clip=0, train_wall=10, gb_free=68.1, wall=2423 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:37:36]    INFO >> epoch 007:   1306 / 1539 loss=3.402, wps=3324.4, ups=5.21, wpb=637.5, bsz=637.5, num_updates=10500, lr=0.000295, gnorm=4.323, clip=0, train_wall=9, gb_free=68.5, wall=2433 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:37:48]    INFO >> epoch 007:   1356 / 1539 loss=3.561, wps=3357.6, ups=5.12, wpb=656.2, bsz=656.2, num_updates=10550, lr=0.000295, gnorm=4.221, clip=0, train_wall=9, gb_free=68.9, wall=2443 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:37:58]    INFO >> epoch 007:   1406 / 1539 loss=3.503, wps=3443.8, ups=4.75, wpb=725.3, bsz=725.3, num_updates=10600, lr=0.000295, gnorm=3.781, clip=0, train_wall=10, gb_free=69.9, wall=2453 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:38:03] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 35.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.27 GiB is allocated by PyTorch, and 1.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:38:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:38:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:38:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 41           |        cudaMalloc retries: 58        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79065 MiB |  79125 MiB | 490895 GiB | 490818 GiB |
|       from large pool |  78885 MiB |  78945 MiB | 488961 GiB | 488884 GiB |
|       from small pool |    180 MiB |    181 MiB |   1934 GiB |   1933 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79065 MiB |  79125 MiB | 490895 GiB | 490818 GiB |
|       from large pool |  78885 MiB |  78945 MiB | 488961 GiB | 488884 GiB |
|       from small pool |    180 MiB |    181 MiB |   1934 GiB |   1933 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78974 MiB |  79033 MiB | 490262 GiB | 490185 GiB |
|       from large pool |  78794 MiB |  78854 MiB | 488330 GiB | 488253 GiB |
|       from small pool |    179 MiB |    180 MiB |   1931 GiB |   1931 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80470 MiB |  80472 MiB | 457058 MiB | 376588 MiB |
|       from large pool |  80272 MiB |  80272 MiB | 452770 MiB | 372498 MiB |
|       from small pool |    198 MiB |    248 MiB |   4288 MiB |   4090 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1344 MiB |   8920 MiB | 525956 GiB | 525954 GiB |
|       from large pool |   1326 MiB |   8915 MiB | 523804 GiB | 523803 GiB |
|       from small pool |     17 MiB |     29 MiB |   2151 GiB |   2151 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3358    |    3361    |   22300 K  |   22296 K  |
|       from large pool |     583    |     584    |   10946 K  |   10945 K  |
|       from small pool |    2775    |    2778    |   11353 K  |   11351 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3358    |    3361    |   22300 K  |   22296 K  |
|       from large pool |     583    |     584    |   10946 K  |   10945 K  |
|       from small pool |    2775    |    2778    |   11353 K  |   11351 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     376    |     400    |    3710    |    3334    |
|       from large pool |     277    |     277    |    1566    |    1289    |
|       from small pool |      99    |     124    |    2144    |    2045    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     295    |     296    |   12181 K  |   12181 K  |
|       from large pool |     123    |     124    |    6787 K  |    6786 K  |
|       from small pool |     172    |     173    |    5394 K  |    5394 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:38:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:38:03] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:38:09]    INFO >> epoch 007:   1457 / 1539 loss=3.394, wps=3170.3, ups=4.43, wpb=715.8, bsz=715.8, num_updates=10650, lr=0.000295, gnorm=4.719, clip=0, train_wall=10, gb_free=62.9, wall=2465 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:38:21]    INFO >> epoch 007:   1507 / 1539 loss=3.471, wps=3430.5, ups=4.83, wpb=710.2, bsz=710.2, num_updates=10700, lr=0.000295, gnorm=4.376, clip=0, train_wall=10, gb_free=68.4, wall=2475 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:38:28]    INFO >> epoch 007 | loss 3.451 | wps 3132.8 | ups 4.41 | wpb 711 | bsz 711 | num_updates 10732 | lr 0.000295 | gnorm 4.193 | clip 0 | train_wall 303 | gb_free 65.3 | wall 2482 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:38:28] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:38:50]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.756 | wps 6898.3 | wpb 5412.5 | bsz 5412.5 | num_updates 10732 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:38:51]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:38:51]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 7 @ 10732 updates, score 3.756) (writing took 0.029432 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:38:51] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:38:56]    INFO >> epoch 008:     18 / 1539 loss=3.254, wps=1218.6, ups=1.49, wpb=818.3, bsz=818.3, num_updates=10750, lr=0.000262, gnorm=3.501, clip=0, train_wall=11, gb_free=71.4, wall=2509 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:39:06]    INFO >> epoch 008:     68 / 1539 loss=3.425, wps=3391.6, ups=4.83, wpb=702.3, bsz=702.3, num_updates=10800, lr=0.000262, gnorm=4.17, clip=0, train_wall=10, gb_free=68.9, wall=2519 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:39:17]    INFO >> epoch 008:    118 / 1539 loss=3.436, wps=3163.2, ups=4.71, wpb=671.2, bsz=671.2, num_updates=10850, lr=0.000262, gnorm=3.432, clip=0, train_wall=10, gb_free=71.3, wall=2530 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:39:29]    INFO >> epoch 008:    168 / 1539 loss=3.336, wps=3183.5, ups=4.71, wpb=675.6, bsz=675.6, num_updates=10900, lr=0.000262, gnorm=4.16, clip=0, train_wall=10, gb_free=66.5, wall=2540 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:39:41]    INFO >> epoch 008:    218 / 1539 loss=3.382, wps=3554.6, ups=4.28, wpb=830.8, bsz=830.8, num_updates=10950, lr=0.000262, gnorm=4.243, clip=0, train_wall=11, gb_free=68.3, wall=2552 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:39:51]    INFO >> epoch 008:    268 / 1539 loss=3.471, wps=3540.6, ups=4.78, wpb=740, bsz=740, num_updates=11000, lr=0.000262, gnorm=4.586, clip=0, train_wall=10, gb_free=67.5, wall=2562 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:40:03]    INFO >> epoch 008:    318 / 1539 loss=3.597, wps=3196.9, ups=4.62, wpb=692.2, bsz=692.2, num_updates=11050, lr=0.000262, gnorm=4.249, clip=0, train_wall=10, gb_free=70.5, wall=2573 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:40:15]    INFO >> epoch 008:    368 / 1539 loss=3.351, wps=3043.4, ups=4.32, wpb=705, bsz=705, num_updates=11100, lr=0.000262, gnorm=4.411, clip=0, train_wall=11, gb_free=69.6, wall=2585 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:40:26]    INFO >> epoch 008:    418 / 1539 loss=3.505, wps=3256.5, ups=4.51, wpb=722.1, bsz=722.1, num_updates=11150, lr=0.000262, gnorm=4.684, clip=0, train_wall=11, gb_free=73.3, wall=2596 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:40:39]    INFO >> epoch 008:    468 / 1539 loss=3.495, wps=3913.6, ups=4.45, wpb=880, bsz=880, num_updates=11200, lr=0.000262, gnorm=4.044, clip=0, train_wall=11, gb_free=42.2, wall=2607 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:40:49]    INFO >> epoch 008:    518 / 1539 loss=3.521, wps=3307.8, ups=4.83, wpb=684.3, bsz=684.3, num_updates=11250, lr=0.000262, gnorm=3.983, clip=0, train_wall=10, gb_free=62.6, wall=2617 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:41:00]    INFO >> epoch 008:    568 / 1539 loss=3.457, wps=3155.6, ups=4.68, wpb=674.5, bsz=674.5, num_updates=11300, lr=0.000262, gnorm=4.968, clip=0, train_wall=10, gb_free=69.1, wall=2628 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:41:11]    INFO >> epoch 008:    618 / 1539 loss=3.488, wps=3264.4, ups=4.95, wpb=659, bsz=659, num_updates=11350, lr=0.000262, gnorm=4.237, clip=0, train_wall=10, gb_free=73.2, wall=2638 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:41:21]    INFO >> epoch 008:    668 / 1539 loss=3.406, wps=3338.4, ups=5.17, wpb=645.6, bsz=645.6, num_updates=11400, lr=0.000262, gnorm=3.402, clip=0, train_wall=9, gb_free=69.2, wall=2648 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:41:31]    INFO >> epoch 008:    718 / 1539 loss=3.523, wps=3378.9, ups=4.8, wpb=704.5, bsz=704.5, num_updates=11450, lr=0.000262, gnorm=4.926, clip=0, train_wall=10, gb_free=69.6, wall=2658 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:41:43]    INFO >> epoch 008:    768 / 1539 loss=3.44, wps=3331.8, ups=4.74, wpb=702.4, bsz=702.4, num_updates=11500, lr=0.000262, gnorm=4.149, clip=0, train_wall=10, gb_free=64.1, wall=2669 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:41:54]    INFO >> epoch 008:    818 / 1539 loss=3.311, wps=3336.2, ups=4.63, wpb=720.8, bsz=720.8, num_updates=11550, lr=0.000262, gnorm=4.112, clip=0, train_wall=10, gb_free=66.4, wall=2680 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:42:03]    INFO >> epoch 008:    868 / 1539 loss=3.473, wps=3167.6, ups=5.46, wpb=580.1, bsz=580.1, num_updates=11600, lr=0.000262, gnorm=3.202, clip=0, train_wall=9, gb_free=65.3, wall=2689 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:42:16]    INFO >> epoch 008:    918 / 1539 loss=3.313, wps=3602.6, ups=4.45, wpb=809.8, bsz=809.8, num_updates=11650, lr=0.000262, gnorm=3.846, clip=0, train_wall=11, gb_free=58.8, wall=2700 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:42:25]    INFO >> epoch 008:    968 / 1539 loss=3.394, wps=3462.9, ups=5.08, wpb=681.2, bsz=681.2, num_updates=11700, lr=0.000262, gnorm=4.008, clip=0, train_wall=9, gb_free=64.5, wall=2710 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:42:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.27 GiB is free. Including non-PyTorch memory, this process has 77.85 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 1.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:42:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:42:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:42:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 42           |        cudaMalloc retries: 60        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77204 MiB |  78282 MiB | 543496 GiB | 543421 GiB |
|       from large pool |  77182 MiB |  78260 MiB | 541345 GiB | 541270 GiB |
|       from small pool |     22 MiB |     26 MiB |   2150 GiB |   2150 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77204 MiB |  78282 MiB | 543496 GiB | 543421 GiB |
|       from large pool |  77182 MiB |  78260 MiB | 541345 GiB | 541270 GiB |
|       from small pool |     22 MiB |     26 MiB |   2150 GiB |   2150 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 542791 GiB | 542716 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 540644 GiB | 540568 GiB |
|       from small pool |     22 MiB |     26 MiB |   2147 GiB |   2147 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79204 MiB |  80460 MiB | 468608 MiB | 389404 MiB |
|       from large pool |  79172 MiB |  80212 MiB | 464270 MiB | 385098 MiB |
|       from small pool |     32 MiB |    248 MiB |   4338 MiB |   4306 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1939 MiB |   8870 MiB | 577831 GiB | 577829 GiB |
|       from large pool |   1929 MiB |   8859 MiB | 575440 GiB | 575438 GiB |
|       from small pool |      9 MiB |     27 MiB |   2390 GiB |   2390 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   24737 K  |   24736 K  |
|       from large pool |     270    |     278    |   12096 K  |   12096 K  |
|       from small pool |     296    |     356    |   12640 K  |   12640 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   24737 K  |   24736 K  |
|       from large pool |     270    |     278    |   12096 K  |   12096 K  |
|       from small pool |     296    |     356    |   12640 K  |   12640 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      92    |     400    |    3744    |    3652    |
|       from large pool |      76    |     276    |    1575    |    1499    |
|       from small pool |      16    |     124    |    2169    |    2153    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     109    |     110    |   13560 K  |   13560 K  |
|       from large pool |      78    |      79    |    7535 K  |    7535 K  |
|       from small pool |      31    |      56    |    6025 K  |    6025 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:42:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:42:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-21 01:42:36] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.01 GiB is free. Including non-PyTorch memory, this process has 78.11 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 3.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:42:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:42:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:42:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 43           |        cudaMalloc retries: 62        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75039 MiB |  75470 MiB | 544600 GiB | 544526 GiB |
|       from large pool |  75014 MiB |  75444 MiB | 542445 GiB | 542372 GiB |
|       from small pool |     25 MiB |     31 MiB |   2154 GiB |   2154 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75039 MiB |  75470 MiB | 544600 GiB | 544526 GiB |
|       from large pool |  75014 MiB |  75444 MiB | 542445 GiB | 542372 GiB |
|       from small pool |     25 MiB |     31 MiB |   2154 GiB |   2154 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 543894 GiB | 543820 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 541742 GiB | 541669 GiB |
|       from small pool |     25 MiB |     31 MiB |   2151 GiB |   2151 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79470 MiB |  79470 MiB | 500236 MiB | 420766 MiB |
|       from large pool |  79438 MiB |  79438 MiB | 495870 MiB | 416432 MiB |
|       from small pool |     32 MiB |     60 MiB |   4366 MiB |   4334 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4430 MiB |   6436 MiB | 579037 GiB | 579033 GiB |
|       from large pool |   4423 MiB |   6426 MiB | 576643 GiB | 576638 GiB |
|       from small pool |      6 MiB |     25 MiB |   2394 GiB |   2394 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   24782 K  |   24781 K  |
|       from large pool |     265    |     272    |   12121 K  |   12120 K  |
|       from small pool |     302    |     356    |   12660 K  |   12660 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   24782 K  |   24781 K  |
|       from large pool |     265    |     272    |   12121 K  |   12120 K  |
|       from small pool |     302    |     356    |   12660 K  |   12660 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     105    |    3766    |    3678    |
|       from large pool |      72    |      75    |    1583    |    1511    |
|       from small pool |      16    |      30    |    2183    |    2167    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      90    |   13583 K  |   13583 K  |
|       from large pool |      58    |      59    |    7550 K  |    7550 K  |
|       from small pool |      31    |      50    |    6033 K  |    6033 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:42:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:42:36] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:42:38]    INFO >> epoch 008:   1020 / 1539 loss=3.358, wps=2595.9, ups=4.05, wpb=641.3, bsz=641.3, num_updates=11750, lr=0.000262, gnorm=4.076, clip=0, train_wall=9, gb_free=61.2, wall=2722 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:42:49]    INFO >> epoch 008:   1070 / 1539 loss=3.256, wps=3478.1, ups=4.88, wpb=712.9, bsz=712.9, num_updates=11800, lr=0.000262, gnorm=4.844, clip=0, train_wall=10, gb_free=68.9, wall=2732 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:43:00]    INFO >> epoch 008:   1120 / 1539 loss=3.478, wps=3114.2, ups=4.68, wpb=666, bsz=666, num_updates=11850, lr=0.000262, gnorm=4.067, clip=0, train_wall=10, gb_free=65, wall=2743 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:43:09]    INFO >> epoch 008:   1170 / 1539 loss=3.572, wps=3220.4, ups=5.36, wpb=600.9, bsz=600.9, num_updates=11900, lr=0.000262, gnorm=4.059, clip=0, train_wall=9, gb_free=63.6, wall=2752 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:43:10] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.79 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.54 GiB is free. Including non-PyTorch memory, this process has 77.58 GiB memory in use. Of the allocated memory 68.26 GiB is allocated by PyTorch, and 8.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:43:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 44           |        cudaMalloc retries: 63        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65550 MiB |  69898 MiB | 551083 GiB | 551019 GiB |
|       from large pool |  65533 MiB |  69880 MiB | 548904 GiB | 548840 GiB |
|       from small pool |     17 MiB |     24 MiB |   2178 GiB |   2178 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65550 MiB |  69898 MiB | 551083 GiB | 551019 GiB |
|       from large pool |  65533 MiB |  69880 MiB | 548904 GiB | 548840 GiB |
|       from small pool |     17 MiB |     24 MiB |   2178 GiB |   2178 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65539 MiB |  69886 MiB | 550368 GiB | 550304 GiB |
|       from large pool |  65521 MiB |  69869 MiB | 548193 GiB | 548129 GiB |
|       from small pool |     17 MiB |     24 MiB |   2175 GiB |   2175 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78928 MiB |  79508 MiB | 500274 MiB | 421346 MiB |
|       from large pool |  78894 MiB |  79438 MiB | 495870 MiB | 416976 MiB |
|       from small pool |     34 MiB |     70 MiB |   4404 MiB |   4370 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9285 MiB |  13621 MiB | 586320 GiB | 586311 GiB |
|       from large pool |   9268 MiB |  13604 MiB | 583898 GiB | 583889 GiB |
|       from small pool |     16 MiB |     24 MiB |   2421 GiB |   2421 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   25077 K  |   25076 K  |
|       from large pool |     201    |     210    |   12278 K  |   12278 K  |
|       from small pool |     288    |     348    |   12798 K  |   12798 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   25077 K  |   25076 K  |
|       from large pool |     201    |     210    |   12278 K  |   12278 K  |
|       from small pool |     288    |     348    |   12798 K  |   12798 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     107    |    3785    |    3697    |
|       from large pool |      71    |      72    |    1583    |    1512    |
|       from small pool |      17    |      35    |    2202    |    2185    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      91    |      92    |   13739 K  |   13738 K  |
|       from large pool |      60    |      61    |    7646 K  |    7646 K  |
|       from small pool |      31    |      49    |    6092 K  |    6092 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:10] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:43:23]    INFO >> epoch 008:   1221 / 1539 loss=3.454, wps=2937.8, ups=4.06, wpb=724.2, bsz=724.2, num_updates=11950, lr=0.000262, gnorm=3.74, clip=0, train_wall=11, gb_free=54, wall=2765 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:43:28] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.37 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:43:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 45           |        cudaMalloc retries: 64        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79165 MiB |  79225 MiB | 554331 GiB | 554253 GiB |
|       from large pool |  78983 MiB |  79043 MiB | 552140 GiB | 552063 GiB |
|       from small pool |    181 MiB |    182 MiB |   2190 GiB |   2190 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79165 MiB |  79225 MiB | 554331 GiB | 554253 GiB |
|       from large pool |  78983 MiB |  79043 MiB | 552140 GiB | 552063 GiB |
|       from small pool |    181 MiB |    182 MiB |   2190 GiB |   2190 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79094 MiB |  79153 MiB | 553612 GiB | 553535 GiB |
|       from large pool |  78913 MiB |  78973 MiB | 551425 GiB | 551348 GiB |
|       from small pool |    180 MiB |    181 MiB |   2187 GiB |   2187 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80462 MiB | 505900 MiB | 425438 MiB |
|       from large pool |  80262 MiB |  80262 MiB | 501330 MiB | 421068 MiB |
|       from small pool |    200 MiB |    200 MiB |   4570 MiB |   4370 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1236 MiB |   6833 MiB | 589952 GiB | 589951 GiB |
|       from large pool |   1218 MiB |   6826 MiB | 587517 GiB | 587515 GiB |
|       from small pool |     18 MiB |     22 MiB |   2435 GiB |   2435 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3380    |    3383    |   25216 K  |   25212 K  |
|       from large pool |     585    |     586    |   12346 K  |   12346 K  |
|       from small pool |    2795    |    2798    |   12869 K  |   12866 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3380    |    3383    |   25216 K  |   25212 K  |
|       from large pool |     585    |     586    |   12346 K  |   12346 K  |
|       from small pool |    2795    |    2798    |   12869 K  |   12866 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     261    |     261    |    3959    |    3698    |
|       from large pool |     161    |     161    |    1674    |    1513    |
|       from small pool |     100    |     100    |    2285    |    2185    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     236    |     236    |   13814 K  |   13814 K  |
|       from large pool |      63    |      64    |    7688 K  |    7688 K  |
|       from small pool |     173    |     173    |    6126 K  |    6125 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:28] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:43:35]    INFO >> epoch 008:   1272 / 1539 loss=3.492, wps=3523, ups=4.16, wpb=847.8, bsz=847.8, num_updates=12000, lr=0.000262, gnorm=4.759, clip=0, train_wall=11, gb_free=62, wall=2777 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:43:45]    INFO >> epoch 008:   1322 / 1539 loss=3.488, wps=3465.4, ups=5.07, wpb=684.1, bsz=684.1, num_updates=12050, lr=0.000262, gnorm=3.792, clip=0, train_wall=9, gb_free=67.7, wall=2787 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:43:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.35 GiB is free. Including non-PyTorch memory, this process has 77.77 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 1.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:43:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 46           |        cudaMalloc retries: 66        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77291 MiB |  77890 MiB | 559563 GiB | 559488 GiB |
|       from large pool |  77274 MiB |  77873 MiB | 557354 GiB | 557278 GiB |
|       from small pool |     16 MiB |     24 MiB |   2209 GiB |   2209 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77291 MiB |  77890 MiB | 559563 GiB | 559488 GiB |
|       from large pool |  77274 MiB |  77873 MiB | 557354 GiB | 557278 GiB |
|       from small pool |     16 MiB |     24 MiB |   2209 GiB |   2209 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77280 MiB |  77879 MiB | 558837 GiB | 558762 GiB |
|       from large pool |  77263 MiB |  77862 MiB | 556631 GiB | 556556 GiB |
|       from small pool |     16 MiB |     24 MiB |   2206 GiB |   2206 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79122 MiB |  80402 MiB | 547406 MiB | 468284 MiB |
|       from large pool |  79092 MiB |  80202 MiB | 542836 MiB | 463744 MiB |
|       from small pool |     30 MiB |    200 MiB |   4570 MiB |   4540 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1830 MiB |   5537 MiB | 595394 GiB | 595392 GiB |
|       from large pool |   1817 MiB |   5522 MiB | 592937 GiB | 592936 GiB |
|       from small pool |     13 MiB |     20 MiB |   2456 GiB |   2456 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     528    |     535    |   25442 K  |   25441 K  |
|       from large pool |     239    |     246    |   12466 K  |   12466 K  |
|       from small pool |     289    |     348    |   12975 K  |   12975 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     528    |     535    |   25442 K  |   25441 K  |
|       from large pool |     239    |     246    |   12466 K  |   12466 K  |
|       from small pool |     289    |     348    |   12975 K  |   12975 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      73    |     260    |    3976    |    3903    |
|       from large pool |      58    |     160    |    1691    |    1633    |
|       from small pool |      15    |     100    |    2285    |    2270    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      90    |      90    |   13939 K  |   13939 K  |
|       from large pool |      61    |      61    |    7766 K  |    7766 K  |
|       from small pool |      29    |      49    |    6173 K  |    6173 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:43:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:43:59]    INFO >> epoch 008:   1373 / 1539 loss=3.311, wps=2771.5, ups=3.97, wpb=698.4, bsz=698.4, num_updates=12100, lr=0.000262, gnorm=3.793, clip=0, train_wall=10, gb_free=50.7, wall=2799 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:44:09]    INFO >> epoch 008:   1423 / 1539 loss=3.448, wps=3646.5, ups=4.69, wpb=777.5, bsz=777.5, num_updates=12150, lr=0.000262, gnorm=3.793, clip=0, train_wall=10, gb_free=67.2, wall=2810 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:44:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.35 GiB is free. Including non-PyTorch memory, this process has 77.77 GiB memory in use. Of the allocated memory 72.13 GiB is allocated by PyTorch, and 5.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:44:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:44:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:44:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 47           |        cudaMalloc retries: 67        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69477 MiB |  76764 MiB | 563556 GiB | 563488 GiB |
|       from large pool |  69457 MiB |  76744 MiB | 561331 GiB | 561263 GiB |
|       from small pool |     20 MiB |     32 MiB |   2224 GiB |   2224 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69477 MiB |  76764 MiB | 563556 GiB | 563488 GiB |
|       from large pool |  69457 MiB |  76744 MiB | 561331 GiB | 561263 GiB |
|       from small pool |     20 MiB |     32 MiB |   2224 GiB |   2224 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69460 MiB |  76747 MiB | 562825 GiB | 562757 GiB |
|       from large pool |  69439 MiB |  76727 MiB | 560604 GiB | 560536 GiB |
|       from small pool |     20 MiB |     32 MiB |   2221 GiB |   2221 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79124 MiB |  79186 MiB | 547470 MiB | 468346 MiB |
|       from large pool |  79092 MiB |  79092 MiB | 542836 MiB | 463744 MiB |
|       from small pool |     32 MiB |     94 MiB |   4634 MiB |   4602 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5554 MiB |   7965 MiB | 600058 GiB | 600053 GiB |
|       from large pool |   5542 MiB |   7952 MiB | 597585 GiB | 597579 GiB |
|       from small pool |     11 MiB |     23 MiB |   2473 GiB |   2473 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |     721    |   25621 K  |   25620 K  |
|       from large pool |     331    |     355    |   12557 K  |   12557 K  |
|       from small pool |     315    |     376    |   13063 K  |   13063 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |     721    |   25621 K  |   25620 K  |
|       from large pool |     331    |     355    |   12557 K  |   12557 K  |
|       from small pool |     315    |     376    |   13063 K  |   13063 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |     105    |    4008    |    3934    |
|       from large pool |      58    |      58    |    1691    |    1633    |
|       from small pool |      16    |      47    |    2317    |    2301    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     101    |     112    |   14033 K  |   14033 K  |
|       from large pool |      73    |      76    |    7820 K  |    7819 K  |
|       from small pool |      28    |      54    |    6213 K  |    6213 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:44:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:44:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:44:21]    INFO >> epoch 008:   1474 / 1539 loss=3.429, wps=3292.2, ups=4.45, wpb=740.2, bsz=740.2, num_updates=12200, lr=0.000262, gnorm=4.305, clip=0, train_wall=10, gb_free=68.9, wall=2821 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:44:32]    INFO >> epoch 008:   1524 / 1539 loss=3.386, wps=3223.5, ups=4.91, wpb=656.8, bsz=656.8, num_updates=12250, lr=0.000262, gnorm=3.672, clip=0, train_wall=10, gb_free=68.3, wall=2831 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:44:36]    INFO >> epoch 008 | loss 3.429 | wps 3093 | ups 4.35 | wpb 711 | bsz 711 | num_updates 12265 | lr 0.000262 | gnorm 4.117 | clip 0 | train_wall 306 | gb_free 72.3 | wall 2835 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:44:36] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:44:58]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.766 | wps 6784.1 | wpb 5412.5 | bsz 5412.5 | num_updates 12265 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:44:58]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:44:58]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 8 @ 12265 updates, score 3.766) (writing took 0.022156 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:44:58] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:45:06]    INFO >> epoch 009:     35 / 1539 loss=3.465, wps=982, ups=1.52, wpb=644.9, bsz=644.9, num_updates=12300, lr=0.000227, gnorm=4.034, clip=0, train_wall=10, gb_free=69.9, wall=2864 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:45:17]    INFO >> epoch 009:     85 / 1539 loss=3.4, wps=3399, ups=4.76, wpb=714.8, bsz=714.8, num_updates=12350, lr=0.000227, gnorm=4.004, clip=0, train_wall=10, gb_free=67, wall=2875 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:45:28]    INFO >> epoch 009:    135 / 1539 loss=3.39, wps=3601.2, ups=4.68, wpb=769.2, bsz=769.2, num_updates=12400, lr=0.000227, gnorm=3.553, clip=0, train_wall=10, gb_free=68.7, wall=2885 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:45:40]    INFO >> epoch 009:    185 / 1539 loss=3.463, wps=3338, ups=4.7, wpb=710, bsz=710, num_updates=12450, lr=0.000227, gnorm=4.38, clip=2, train_wall=10, gb_free=70, wall=2896 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:45:50]    INFO >> epoch 009:    235 / 1539 loss=3.429, wps=3663.1, ups=4.97, wpb=737.7, bsz=737.7, num_updates=12500, lr=0.000227, gnorm=3.83, clip=0, train_wall=9, gb_free=69.4, wall=2906 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:46:00]    INFO >> epoch 009:    285 / 1539 loss=3.349, wps=3438.5, ups=5.05, wpb=681.6, bsz=681.6, num_updates=12550, lr=0.000227, gnorm=4.231, clip=0, train_wall=9, gb_free=72.6, wall=2916 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:46:12]    INFO >> epoch 009:    335 / 1539 loss=3.292, wps=3700.1, ups=4.43, wpb=836, bsz=836, num_updates=12600, lr=0.000227, gnorm=5.084, clip=0, train_wall=11, gb_free=67.7, wall=2927 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:46:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.79 GiB. GPU 2 has a total capacity of 79.14 GiB of which 59.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 68.26 GiB is allocated by PyTorch, and 10.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:46:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:46:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:46:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 48           |        cudaMalloc retries: 69        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65551 MiB |  69899 MiB | 589702 GiB | 589638 GiB |
|       from large pool |  65534 MiB |  69881 MiB | 587368 GiB | 587304 GiB |
|       from small pool |     17 MiB |     22 MiB |   2333 GiB |   2333 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65551 MiB |  69899 MiB | 589702 GiB | 589638 GiB |
|       from large pool |  65534 MiB |  69881 MiB | 587368 GiB | 587304 GiB |
|       from small pool |     17 MiB |     22 MiB |   2333 GiB |   2333 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65539 MiB |  69886 MiB | 588942 GiB | 588878 GiB |
|       from large pool |  65521 MiB |  69869 MiB | 586611 GiB | 586547 GiB |
|       from small pool |     17 MiB |     22 MiB |   2330 GiB |   2330 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80446 MiB |  80476 MiB | 553018 MiB | 472572 MiB |
|       from large pool |  80416 MiB |  80416 MiB | 548252 MiB | 467836 MiB |
|       from small pool |     30 MiB |     60 MiB |   4766 MiB |   4736 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  10802 MiB |  14997 MiB | 626948 GiB | 626937 GiB |
|       from large pool |  10789 MiB |  14985 MiB | 624357 GiB | 624346 GiB |
|       from small pool |     12 MiB |     22 MiB |   2591 GiB |   2591 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   26781 K  |   26781 K  |
|       from large pool |     201    |     210    |   13060 K  |   13060 K  |
|       from small pool |     288    |     342    |   13721 K  |   13720 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   26781 K  |   26781 K  |
|       from large pool |     201    |     210    |   13060 K  |   13060 K  |
|       from small pool |     288    |     342    |   13721 K  |   13720 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |      89    |    4076    |    4002    |
|       from large pool |      59    |      59    |    1693    |    1634    |
|       from small pool |      15    |      30    |    2383    |    2368    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      85    |      86    |   14669 K  |   14669 K  |
|       from large pool |      55    |      56    |    8125 K  |    8125 K  |
|       from small pool |      30    |      48    |    6544 K  |    6544 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:46:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:46:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:46:25]    INFO >> epoch 009:    386 / 1539 loss=3.519, wps=2571.9, ups=3.82, wpb=672.9, bsz=672.9, num_updates=12650, lr=0.000227, gnorm=4.206, clip=0, train_wall=12, gb_free=58.8, wall=2940 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:46:36]    INFO >> epoch 009:    436 / 1539 loss=3.442, wps=3215.3, ups=4.62, wpb=695.7, bsz=695.7, num_updates=12700, lr=0.000227, gnorm=3.916, clip=0, train_wall=10, gb_free=61.1, wall=2951 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:46:47]    INFO >> epoch 009:    486 / 1539 loss=3.311, wps=3605, ups=5.23, wpb=689.1, bsz=689.1, num_updates=12750, lr=0.000227, gnorm=3.595, clip=0, train_wall=9, gb_free=68, wall=2961 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:46:59]    INFO >> epoch 009:    536 / 1539 loss=3.302, wps=3523, ups=4.25, wpb=828.7, bsz=828.7, num_updates=12800, lr=0.000227, gnorm=4.304, clip=0, train_wall=11, gb_free=63.2, wall=2973 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:47:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 77.83 GiB memory in use. Of the allocated memory 75.97 GiB is allocated by PyTorch, and 1.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:47:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:47:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:47:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 49           |        cudaMalloc retries: 70        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77200 MiB |  78277 MiB | 598472 GiB | 598396 GiB |
|       from large pool |  77178 MiB |  78255 MiB | 596107 GiB | 596031 GiB |
|       from small pool |     22 MiB |     23 MiB |   2365 GiB |   2365 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77200 MiB |  78277 MiB | 598472 GiB | 598396 GiB |
|       from large pool |  77178 MiB |  78255 MiB | 596107 GiB | 596031 GiB |
|       from small pool |     22 MiB |     23 MiB |   2365 GiB |   2365 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 597700 GiB | 597625 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 595338 GiB | 595263 GiB |
|       from small pool |     22 MiB |     23 MiB |   2361 GiB |   2361 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79190 MiB |  79232 MiB | 555896 MiB | 476706 MiB |
|       from large pool |  79160 MiB |  79160 MiB | 551088 MiB | 471928 MiB |
|       from small pool |     30 MiB |     72 MiB |   4808 MiB |   4778 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1989 MiB |   9400 MiB | 637234 GiB | 637232 GiB |
|       from large pool |   1981 MiB |   9391 MiB | 634608 GiB | 634606 GiB |
|       from small pool |      7 MiB |     25 MiB |   2626 GiB |   2626 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   27165 K  |   27165 K  |
|       from large pool |     270    |     278    |   13263 K  |   13263 K  |
|       from small pool |     296    |     356    |   13902 K  |   13901 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   27165 K  |   27165 K  |
|       from large pool |     270    |     278    |   13263 K  |   13263 K  |
|       from small pool |     296    |     356    |   13902 K  |   13901 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |      95    |    4098    |    4024    |
|       from large pool |      59    |      59    |    1694    |    1635    |
|       from small pool |      15    |      36    |    2404    |    2389    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      95    |      97    |   14871 K  |   14870 K  |
|       from large pool |      65    |      67    |    8246 K  |    8246 K  |
|       from small pool |      30    |      53    |    6624 K  |    6624 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:47:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:47:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:47:10]    INFO >> epoch 009:    587 / 1539 loss=3.534, wps=2837.3, ups=4.38, wpb=648.3, bsz=648.3, num_updates=12850, lr=0.000227, gnorm=3.949, clip=0, train_wall=10, gb_free=69.1, wall=2984 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:47:23]    INFO >> epoch 009:    637 / 1539 loss=3.342, wps=3355.7, ups=4.56, wpb=735.3, bsz=735.3, num_updates=12900, lr=0.000227, gnorm=4.289, clip=0, train_wall=10, gb_free=43.7, wall=2995 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:47:33]    INFO >> epoch 009:    687 / 1539 loss=3.441, wps=3299.6, ups=4.79, wpb=689.1, bsz=689.1, num_updates=12950, lr=0.000227, gnorm=4.364, clip=0, train_wall=10, gb_free=70.4, wall=3005 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:47:43]    INFO >> epoch 009:    737 / 1539 loss=3.42, wps=2888.4, ups=4.79, wpb=602.9, bsz=602.9, num_updates=13000, lr=0.000227, gnorm=3.805, clip=0, train_wall=10, gb_free=59.2, wall=3016 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:47:56]    INFO >> epoch 009:    787 / 1539 loss=3.42, wps=3582.3, ups=4.57, wpb=783.9, bsz=783.9, num_updates=13050, lr=0.000227, gnorm=4.525, clip=0, train_wall=10, gb_free=65.5, wall=3027 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:48:03] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 77.83 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 1.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:48:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:48:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:48:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 50           |        cudaMalloc retries: 71        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77288 MiB |  77887 MiB | 608792 GiB | 608717 GiB |
|       from large pool |  77272 MiB |  77871 MiB | 606389 GiB | 606314 GiB |
|       from small pool |     16 MiB |     24 MiB |   2403 GiB |   2402 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77288 MiB |  77887 MiB | 608792 GiB | 608717 GiB |
|       from large pool |  77272 MiB |  77871 MiB | 606389 GiB | 606314 GiB |
|       from small pool |     16 MiB |     24 MiB |   2403 GiB |   2402 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77280 MiB |  77879 MiB | 608008 GiB | 607933 GiB |
|       from large pool |  77263 MiB |  77862 MiB | 605608 GiB | 605533 GiB |
|       from small pool |     16 MiB |     24 MiB |   2399 GiB |   2399 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79192 MiB |  79242 MiB | 555948 MiB | 476756 MiB |
|       from large pool |  79160 MiB |  79160 MiB | 551088 MiB | 471928 MiB |
|       from small pool |     32 MiB |     82 MiB |   4860 MiB |   4828 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1903 MiB |   5528 MiB | 649374 GiB | 649372 GiB |
|       from large pool |   1887 MiB |   5511 MiB | 646705 GiB | 646703 GiB |
|       from small pool |     15 MiB |     22 MiB |   2668 GiB |   2668 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     528    |     535    |   27625 K  |   27624 K  |
|       from large pool |     239    |     246    |   13507 K  |   13506 K  |
|       from small pool |     289    |     348    |   14118 K  |   14117 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     528    |     535    |   27625 K  |   27624 K  |
|       from large pool |     239    |     246    |   13507 K  |   13506 K  |
|       from small pool |     289    |     348    |   14118 K  |   14117 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      75    |     100    |    4124    |    4049    |
|       from large pool |      59    |      59    |    1694    |    1635    |
|       from small pool |      16    |      41    |    2430    |    2414    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      90    |   15109 K  |   15109 K  |
|       from large pool |      60    |      61    |    8391 K  |    8391 K  |
|       from small pool |      29    |      52    |    6718 K  |    6718 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:48:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:48:03] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:48:06]    INFO >> epoch 009:    838 / 1539 loss=3.386, wps=3086.8, ups=4.88, wpb=633, bsz=633, num_updates=13100, lr=0.000227, gnorm=3.592, clip=0, train_wall=9, gb_free=71.1, wall=3037 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:48:16]    INFO >> epoch 009:    888 / 1539 loss=3.464, wps=3306.7, ups=4.88, wpb=678.2, bsz=678.2, num_updates=13150, lr=0.000227, gnorm=3.663, clip=0, train_wall=10, gb_free=73.4, wall=3047 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:48:27]    INFO >> epoch 009:    938 / 1539 loss=3.496, wps=3251, ups=5.26, wpb=618.1, bsz=618.1, num_updates=13200, lr=0.000227, gnorm=3.615, clip=0, train_wall=9, gb_free=69.3, wall=3057 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:48:28] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.28 GiB is free. Including non-PyTorch memory, this process has 77.83 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 3.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:48:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:48:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:48:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 51           |        cudaMalloc retries: 72        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75037 MiB |  75468 MiB | 613412 GiB | 613338 GiB |
|       from large pool |  75011 MiB |  75442 MiB | 610992 GiB | 610919 GiB |
|       from small pool |     25 MiB |     29 MiB |   2419 GiB |   2419 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75037 MiB |  75468 MiB | 613412 GiB | 613338 GiB |
|       from large pool |  75011 MiB |  75442 MiB | 610992 GiB | 610919 GiB |
|       from small pool |     25 MiB |     29 MiB |   2419 GiB |   2419 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 612621 GiB | 612548 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 610205 GiB | 610131 GiB |
|       from small pool |     25 MiB |     29 MiB |   2416 GiB |   2416 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79190 MiB |  79224 MiB | 555980 MiB | 476790 MiB |
|       from large pool |  79160 MiB |  79160 MiB | 551088 MiB | 471928 MiB |
|       from small pool |     30 MiB |     64 MiB |   4892 MiB |   4862 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4152 MiB |   8033 MiB | 654795 GiB | 654790 GiB |
|       from large pool |   4148 MiB |   8025 MiB | 652107 GiB | 652103 GiB |
|       from small pool |      4 MiB |     29 MiB |   2687 GiB |   2687 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   27839 K  |   27838 K  |
|       from large pool |     265    |     272    |   13623 K  |   13623 K  |
|       from small pool |     302    |     355    |   14216 K  |   14215 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   27839 K  |   27838 K  |
|       from large pool |     265    |     272    |   13623 K  |   13623 K  |
|       from small pool |     302    |     355    |   14216 K  |   14215 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      74    |      91    |    4140    |    4066    |
|       from large pool |      59    |      59    |    1694    |    1635    |
|       from small pool |      15    |      32    |    2446    |    2431    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      84    |      84    |   15219 K  |   15219 K  |
|       from large pool |      56    |      56    |    8461 K  |    8461 K  |
|       from small pool |      28    |      55    |    6758 K  |    6758 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:48:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:48:28] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:48:40]    INFO >> epoch 009:    989 / 1539 loss=3.293, wps=3305.3, ups=3.97, wpb=832.6, bsz=832.6, num_updates=13250, lr=0.000227, gnorm=4.277, clip=0, train_wall=11, gb_free=72.1, wall=3069 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:48:50]    INFO >> epoch 009:   1039 / 1539 loss=3.401, wps=3393.2, ups=4.71, wpb=720.7, bsz=720.7, num_updates=13300, lr=0.000227, gnorm=3.615, clip=0, train_wall=10, gb_free=68.7, wall=3080 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:49:03]    INFO >> epoch 009:   1089 / 1539 loss=3.359, wps=3089.1, ups=4.39, wpb=703.2, bsz=703.2, num_updates=13350, lr=0.000227, gnorm=3.794, clip=0, train_wall=11, gb_free=61.3, wall=3091 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:49:14]    INFO >> epoch 009:   1139 / 1539 loss=3.362, wps=3603, ups=4.67, wpb=770.8, bsz=770.8, num_updates=13400, lr=0.000227, gnorm=4.088, clip=0, train_wall=10, gb_free=71, wall=3102 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:49:24]    INFO >> epoch 009:   1189 / 1539 loss=3.434, wps=3174.8, ups=4.79, wpb=662.5, bsz=662.5, num_updates=13450, lr=0.000227, gnorm=3.943, clip=0, train_wall=10, gb_free=70.1, wall=3113 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:49:37]    INFO >> epoch 009:   1239 / 1539 loss=3.202, wps=3797.7, ups=4.44, wpb=856.2, bsz=856.2, num_updates=13500, lr=0.000227, gnorm=4.331, clip=0, train_wall=11, gb_free=63.8, wall=3124 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:49:48]    INFO >> epoch 009:   1289 / 1539 loss=3.46, wps=3560.9, ups=4.47, wpb=796.8, bsz=796.8, num_updates=13550, lr=0.000227, gnorm=4.422, clip=0, train_wall=11, gb_free=75.3, wall=3135 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:49:59]    INFO >> epoch 009:   1339 / 1539 loss=3.373, wps=3188.3, ups=4.6, wpb=692.4, bsz=692.4, num_updates=13600, lr=0.000227, gnorm=4.883, clip=0, train_wall=10, gb_free=70.1, wall=3146 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:50:11]    INFO >> epoch 009:   1389 / 1539 loss=3.28, wps=3563.3, ups=4.75, wpb=749.6, bsz=749.6, num_updates=13650, lr=0.000227, gnorm=3.568, clip=0, train_wall=10, gb_free=68.1, wall=3156 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:50:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.56 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:50:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:50:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:50:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 52           |        cudaMalloc retries: 74        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79360 MiB |  79420 MiB | 634136 GiB | 634058 GiB |
|       from large pool |  79176 MiB |  79236 MiB | 631631 GiB | 631554 GiB |
|       from small pool |    183 MiB |    185 MiB |   2504 GiB |   2504 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79360 MiB |  79420 MiB | 634136 GiB | 634058 GiB |
|       from large pool |  79176 MiB |  79236 MiB | 631631 GiB | 631554 GiB |
|       from small pool |    183 MiB |    185 MiB |   2504 GiB |   2504 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79334 MiB |  79394 MiB | 633319 GiB | 633241 GiB |
|       from large pool |  79151 MiB |  79211 MiB | 630818 GiB | 630740 GiB |
|       from small pool |    183 MiB |    184 MiB |   2501 GiB |   2500 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB | 557340 MiB | 476838 MiB |
|       from large pool |  80300 MiB |  80300 MiB | 552228 MiB | 471928 MiB |
|       from small pool |    202 MiB |    248 MiB |   5112 MiB |   4910 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1081 MiB |   6749 MiB | 679371 GiB | 679370 GiB |
|       from large pool |   1063 MiB |   6745 MiB | 676588 GiB | 676587 GiB |
|       from small pool |     18 MiB |     29 MiB |   2783 GiB |   2783 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3424    |    3427    |   28822 K  |   28818 K  |
|       from large pool |     589    |     590    |   14110 K  |   14110 K  |
|       from small pool |    2835    |    2838    |   14711 K  |   14708 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3424    |    3427    |   28822 K  |   28818 K  |
|       from large pool |     589    |     590    |   14110 K  |   14110 K  |
|       from small pool |    2835    |    2838    |   14711 K  |   14708 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     179    |     201    |    4269    |    4090    |
|       from large pool |      78    |      78    |    1713    |    1635    |
|       from small pool |     101    |     124    |    2556    |    2455    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     244    |     245    |   15748 K  |   15748 K  |
|       from large pool |      65    |      66    |    8752 K  |    8752 K  |
|       from small pool |     179    |     180    |    6996 K  |    6996 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:50:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:50:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:50:22]    INFO >> epoch 009:   1440 / 1539 loss=3.347, wps=3046.8, ups=4.35, wpb=700, bsz=700, num_updates=13700, lr=0.000227, gnorm=4.146, clip=0, train_wall=10, gb_free=69, wall=3168 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:50:32]    INFO >> epoch 009:   1490 / 1539 loss=3.5, wps=3165.4, ups=5.34, wpb=592.3, bsz=592.3, num_updates=13750, lr=0.000227, gnorm=3.91, clip=0, train_wall=9, gb_free=69.5, wall=3177 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:50:43]    INFO >> epoch 009 | loss 3.393 | wps 3094.3 | ups 4.35 | wpb 711.1 | bsz 711.1 | num_updates 13799 | lr 0.000227 | gnorm 4.059 | clip 0.1 | train_wall 308 | gb_free 71.3 | wall 3187 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:50:43] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:51:04]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.703 | wps 7102 | wpb 5412.5 | bsz 5412.5 | num_updates 13799 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:51:05]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:51:05]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 9 @ 13799 updates, score 3.703) (writing took 0.027560 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:51:05] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:51:05]    INFO >> epoch 010:      1 / 1539 loss=3.459, wps=963.8, ups=1.56, wpb=616.7, bsz=616.7, num_updates=13800, lr=0.000193, gnorm=3.75, clip=0, train_wall=10, gb_free=63.9, wall=3209 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:51:09] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 235.25 MiB is free. Including non-PyTorch memory, this process has 78.89 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 4.69 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:51:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:51:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:51:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 53           |        cudaMalloc retries: 75        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75041 MiB |  75471 MiB | 646461 GiB | 646387 GiB |
|       from large pool |  75015 MiB |  75446 MiB | 643898 GiB | 643825 GiB |
|       from small pool |     25 MiB |     26 MiB |   2562 GiB |   2562 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75041 MiB |  75471 MiB | 646461 GiB | 646387 GiB |
|       from large pool |  75015 MiB |  75446 MiB | 643898 GiB | 643825 GiB |
|       from small pool |     25 MiB |     26 MiB |   2562 GiB |   2562 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 645630 GiB | 645557 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 643072 GiB | 642998 GiB |
|       from small pool |     25 MiB |     26 MiB |   2558 GiB |   2558 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80270 MiB |  80442 MiB | 557340 MiB | 477070 MiB |
|       from large pool |  80240 MiB |  80240 MiB | 552228 MiB | 471988 MiB |
|       from small pool |     30 MiB |    202 MiB |   5112 MiB |   5082 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5228 MiB |   9019 MiB | 689286 GiB | 689281 GiB |
|       from large pool |   5224 MiB |   9012 MiB | 686443 GiB | 686438 GiB |
|       from small pool |      4 MiB |     22 MiB |   2843 GiB |   2843 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   29362 K  |   29361 K  |
|       from large pool |     265    |     272    |   14291 K  |   14291 K  |
|       from small pool |     302    |     342    |   15070 K  |   15070 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   29362 K  |   29361 K  |
|       from large pool |     265    |     272    |   14291 K  |   14291 K  |
|       from small pool |     302    |     342    |   15070 K  |   15070 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      92    |     178    |    4269    |    4177    |
|       from large pool |      77    |      77    |    1713    |    1636    |
|       from small pool |      15    |     101    |    2556    |    2541    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      91    |      91    |   16071 K  |   16071 K  |
|       from large pool |      60    |      60    |    8866 K  |    8866 K  |
|       from small pool |      31    |      52    |    7205 K  |    7205 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:51:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:51:09] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:51:17]    INFO >> epoch 010:     52 / 1539 loss=3.391, wps=3200.2, ups=4.69, wpb=681.6, bsz=681.6, num_updates=13850, lr=0.000193, gnorm=3.301, clip=0, train_wall=9, gb_free=71.1, wall=3220 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:51:27]    INFO >> epoch 010:    102 / 1539 loss=3.311, wps=3444.6, ups=4.99, wpb=690.5, bsz=690.5, num_updates=13900, lr=0.000193, gnorm=4.242, clip=0, train_wall=10, gb_free=68, wall=3230 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:51:37]    INFO >> epoch 010:    152 / 1539 loss=3.289, wps=3476.4, ups=5.13, wpb=678.2, bsz=678.2, num_updates=13950, lr=0.000193, gnorm=3.869, clip=0, train_wall=9, gb_free=60.4, wall=3240 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:51:50]    INFO >> epoch 010:    202 / 1539 loss=3.348, wps=3325.3, ups=4.19, wpb=793.6, bsz=793.6, num_updates=14000, lr=0.000193, gnorm=4.442, clip=0, train_wall=11, gb_free=68.7, wall=3252 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:52:02]    INFO >> epoch 010:    252 / 1539 loss=3.252, wps=3310.8, ups=4.29, wpb=772.3, bsz=772.3, num_updates=14050, lr=0.000193, gnorm=3.936, clip=0, train_wall=11, gb_free=62.7, wall=3263 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:52:11]    INFO >> epoch 010:    302 / 1539 loss=3.503, wps=3604.8, ups=5.1, wpb=706.8, bsz=706.8, num_updates=14100, lr=0.000193, gnorm=3.62, clip=0, train_wall=9, gb_free=67.9, wall=3273 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:52:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.11 GiB is free. Including non-PyTorch memory, this process has 78.01 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 1.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:52:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:52:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:52:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 54           |        cudaMalloc retries: 76        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77289 MiB |  77889 MiB | 659624 GiB | 659548 GiB |
|       from large pool |  77273 MiB |  77872 MiB | 657012 GiB | 656936 GiB |
|       from small pool |     16 MiB |     19 MiB |   2612 GiB |   2612 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77289 MiB |  77889 MiB | 659624 GiB | 659548 GiB |
|       from large pool |  77273 MiB |  77872 MiB | 657012 GiB | 656936 GiB |
|       from small pool |     16 MiB |     19 MiB |   2612 GiB |   2612 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77280 MiB |  77879 MiB | 658777 GiB | 658701 GiB |
|       from large pool |  77263 MiB |  77862 MiB | 656168 GiB | 656093 GiB |
|       from small pool |     16 MiB |     19 MiB |   2608 GiB |   2608 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79368 MiB |  80466 MiB | 557536 MiB | 478168 MiB |
|       from large pool |  79340 MiB |  80240 MiB | 552228 MiB | 472888 MiB |
|       from small pool |     28 MiB |    226 MiB |   5308 MiB |   5280 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2018 MiB |   5703 MiB | 703979 GiB | 703977 GiB |
|       from large pool |   2006 MiB |   5691 MiB | 701080 GiB | 701078 GiB |
|       from small pool |     11 MiB |     18 MiB |   2899 GiB |   2899 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     528    |     535    |   29955 K  |   29955 K  |
|       from large pool |     239    |     246    |   14597 K  |   14597 K  |
|       from small pool |     289    |     342    |   15358 K  |   15357 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     528    |     535    |   29955 K  |   29955 K  |
|       from large pool |     239    |     246    |   14597 K  |   14597 K  |
|       from small pool |     289    |     342    |   15358 K  |   15357 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      76    |     190    |    4367    |    4291    |
|       from large pool |      62    |      77    |    1713    |    1651    |
|       from small pool |      14    |     113    |    2654    |    2640    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      86    |      86    |   16391 K  |   16391 K  |
|       from large pool |      57    |      57    |    9055 K  |    9055 K  |
|       from small pool |      29    |      53    |    7336 K  |    7336 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:52:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:52:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:52:24]    INFO >> epoch 010:    353 / 1539 loss=3.283, wps=3383.1, ups=4.35, wpb=777, bsz=777, num_updates=14150, lr=0.000193, gnorm=3.729, clip=0, train_wall=10, gb_free=72.5, wall=3285 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:52:34]    INFO >> epoch 010:    403 / 1539 loss=3.524, wps=3168.9, ups=5.03, wpb=630.2, bsz=630.2, num_updates=14200, lr=0.000193, gnorm=3.861, clip=0, train_wall=9, gb_free=69.1, wall=3294 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:52:44]    INFO >> epoch 010:    453 / 1539 loss=3.346, wps=3065.7, ups=5.01, wpb=612.4, bsz=612.4, num_updates=14250, lr=0.000193, gnorm=3.686, clip=0, train_wall=9, gb_free=67.6, wall=3304 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:52:56]    INFO >> epoch 010:    503 / 1539 loss=3.372, wps=3183.4, ups=4.8, wpb=662.8, bsz=662.8, num_updates=14300, lr=0.000193, gnorm=3.828, clip=0, train_wall=10, gb_free=72, wall=3315 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:53:05]    INFO >> epoch 010:    553 / 1539 loss=3.353, wps=3618.7, ups=5.22, wpb=693.7, bsz=693.7, num_updates=14350, lr=0.000193, gnorm=3.763, clip=0, train_wall=9, gb_free=70.5, wall=3324 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:53:16]    INFO >> epoch 010:    603 / 1539 loss=3.422, wps=3215.9, ups=4.82, wpb=667, bsz=667, num_updates=14400, lr=0.000193, gnorm=4.133, clip=0, train_wall=10, gb_free=67.2, wall=3335 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:53:27]    INFO >> epoch 010:    653 / 1539 loss=3.399, wps=3485.2, ups=5.15, wpb=676.2, bsz=676.2, num_updates=14450, lr=0.000193, gnorm=3.312, clip=0, train_wall=9, gb_free=71.2, wall=3345 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:53:37]    INFO >> epoch 010:    703 / 1539 loss=3.324, wps=3723, ups=4.95, wpb=751.6, bsz=751.6, num_updates=14500, lr=0.000193, gnorm=4.421, clip=0, train_wall=9, gb_free=65.3, wall=3355 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:53:38] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 5.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.44 GiB is allocated by PyTorch, and 1.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:53:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:53:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:53:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 55           |        cudaMalloc retries: 77        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79242 MiB |  79302 MiB | 675646 GiB | 675568 GiB |
|       from large pool |  79059 MiB |  79119 MiB | 672974 GiB | 672897 GiB |
|       from small pool |    182 MiB |    183 MiB |   2671 GiB |   2671 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79242 MiB |  79302 MiB | 675646 GiB | 675568 GiB |
|       from large pool |  79059 MiB |  79119 MiB | 672974 GiB | 672897 GiB |
|       from small pool |    182 MiB |    183 MiB |   2671 GiB |   2671 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79214 MiB |  79274 MiB | 674778 GiB | 674701 GiB |
|       from large pool |  79032 MiB |  79092 MiB | 672110 GiB | 672033 GiB |
|       from small pool |    181 MiB |    183 MiB |   2667 GiB |   2667 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80500 MiB |  80502 MiB | 558730 MiB | 478230 MiB |
|       from large pool |  80300 MiB |  80300 MiB | 553248 MiB | 472948 MiB |
|       from small pool |    200 MiB |    202 MiB |   5482 MiB |   5282 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1197 MiB |   6867 MiB | 722845 GiB | 722844 GiB |
|       from large pool |   1180 MiB |   6863 MiB | 719878 GiB | 719877 GiB |
|       from small pool |     17 MiB |     25 MiB |   2966 GiB |   2966 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3402    |    3405    |   30685 K  |   30681 K  |
|       from large pool |     587    |     588    |   14982 K  |   14982 K  |
|       from small pool |    2815    |    2818    |   15702 K  |   15699 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3402    |    3405    |   30685 K  |   30681 K  |
|       from large pool |     587    |     588    |   14982 K  |   14982 K  |
|       from small pool |    2815    |    2818    |   15702 K  |   15699 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     178    |     179    |    4471    |    4293    |
|       from large pool |      78    |      78    |    1730    |    1652    |
|       from small pool |     100    |     101    |    2741    |    2641    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     245    |     246    |   16775 K  |   16775 K  |
|       from large pool |      68    |      70    |    9286 K  |    9286 K  |
|       from small pool |     177    |     178    |    7489 K  |    7488 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:53:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:53:38] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:53:47]    INFO >> epoch 010:    754 / 1539 loss=3.456, wps=3149, ups=5, wpb=629.6, bsz=629.6, num_updates=14550, lr=0.000193, gnorm=3.24, clip=0, train_wall=9, gb_free=68, wall=3365 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:54:00]    INFO >> epoch 010:    804 / 1539 loss=3.437, wps=3428.7, ups=4.34, wpb=789.2, bsz=789.2, num_updates=14600, lr=0.000193, gnorm=4.094, clip=0, train_wall=11, gb_free=68, wall=3376 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:54:10]    INFO >> epoch 010:    854 / 1539 loss=3.383, wps=2974.1, ups=4.79, wpb=620.9, bsz=620.9, num_updates=14650, lr=0.000193, gnorm=3.076, clip=0, train_wall=10, gb_free=67.2, wall=3387 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:54:22]    INFO >> epoch 010:    904 / 1539 loss=3.206, wps=3853.7, ups=4.14, wpb=931.5, bsz=931.5, num_updates=14700, lr=0.000193, gnorm=5.087, clip=0, train_wall=11, gb_free=70.7, wall=3399 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:54:32]    INFO >> epoch 010:    954 / 1539 loss=3.487, wps=3448.1, ups=5.3, wpb=650.5, bsz=650.5, num_updates=14750, lr=0.000193, gnorm=3.628, clip=0, train_wall=9, gb_free=68.9, wall=3408 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:54:43]    INFO >> epoch 010:   1004 / 1539 loss=3.44, wps=3265.3, ups=5.03, wpb=649.8, bsz=649.8, num_updates=14800, lr=0.000193, gnorm=4.246, clip=0, train_wall=9, gb_free=66, wall=3418 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:54:54]    INFO >> epoch 010:   1054 / 1539 loss=3.39, wps=3463.2, ups=4.66, wpb=742.6, bsz=742.6, num_updates=14850, lr=0.000193, gnorm=4.517, clip=0, train_wall=10, gb_free=60.4, wall=3429 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:55:04]    INFO >> epoch 010:   1104 / 1539 loss=3.429, wps=3534.4, ups=4.85, wpb=728.5, bsz=728.5, num_updates=14900, lr=0.000193, gnorm=3.356, clip=0, train_wall=10, gb_free=66.6, wall=3439 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:55:16]    INFO >> epoch 010:   1154 / 1539 loss=3.362, wps=3102.2, ups=4.99, wpb=622, bsz=622, num_updates=14950, lr=0.000193, gnorm=3.454, clip=0, train_wall=9, gb_free=71.3, wall=3449 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:55:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 475.25 MiB is free. Including non-PyTorch memory, this process has 78.65 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 2.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:55:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:55:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:55:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 56           |        cudaMalloc retries: 78        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77206 MiB |  78283 MiB | 695600 GiB | 695524 GiB |
|       from large pool |  77184 MiB |  78261 MiB | 692852 GiB | 692776 GiB |
|       from small pool |     22 MiB |     23 MiB |   2747 GiB |   2747 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77206 MiB |  78283 MiB | 695600 GiB | 695524 GiB |
|       from large pool |  77184 MiB |  78261 MiB | 692852 GiB | 692776 GiB |
|       from small pool |     22 MiB |     23 MiB |   2747 GiB |   2747 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 694705 GiB | 694630 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 691961 GiB | 691886 GiB |
|       from small pool |     22 MiB |     23 MiB |   2743 GiB |   2743 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80030 MiB |  80488 MiB | 558778 MiB | 478748 MiB |
|       from large pool |  80000 MiB |  80240 MiB | 553248 MiB | 473248 MiB |
|       from small pool |     30 MiB |    248 MiB |   5530 MiB |   5500 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2763 MiB |   8799 MiB | 745200 GiB | 745197 GiB |
|       from large pool |   2755 MiB |   8791 MiB | 742147 GiB | 742144 GiB |
|       from small pool |      7 MiB |     20 MiB |   3053 GiB |   3053 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   31600 K  |   31600 K  |
|       from large pool |     270    |     278    |   15455 K  |   15455 K  |
|       from small pool |     296    |     342    |   16145 K  |   16144 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   31600 K  |   31600 K  |
|       from large pool |     270    |     278    |   15455 K  |   15455 K  |
|       from small pool |     296    |     342    |   16145 K  |   16144 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     201    |    4495    |    4407    |
|       from large pool |      73    |      77    |    1730    |    1657    |
|       from small pool |      15    |     124    |    2765    |    2750    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      99    |      99    |   17269 K  |   17269 K  |
|       from large pool |      68    |      68    |    9579 K  |    9578 K  |
|       from small pool |      31    |      50    |    7690 K  |    7690 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:55:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:55:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:55:28]    INFO >> epoch 010:   1205 / 1539 loss=3.387, wps=3682.2, ups=3.97, wpb=927, bsz=927, num_updates=15000, lr=0.000193, gnorm=4.287, clip=0, train_wall=11, gb_free=63.6, wall=3462 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:55:39]    INFO >> epoch 010:   1255 / 1539 loss=3.426, wps=3134.9, ups=4.8, wpb=652.6, bsz=652.6, num_updates=15050, lr=0.000193, gnorm=4.411, clip=0, train_wall=10, gb_free=73, wall=3472 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:55:51]    INFO >> epoch 010:   1305 / 1539 loss=3.343, wps=3445.7, ups=4.52, wpb=762.6, bsz=762.6, num_updates=15100, lr=0.000193, gnorm=3.947, clip=0, train_wall=10, gb_free=72.1, wall=3483 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:56:01]    INFO >> epoch 010:   1355 / 1539 loss=3.463, wps=3453.1, ups=4.86, wpb=710.9, bsz=710.9, num_updates=15150, lr=0.000193, gnorm=3.562, clip=0, train_wall=10, gb_free=69.4, wall=3493 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:56:12]    INFO >> epoch 010:   1405 / 1539 loss=3.358, wps=3589.7, ups=4.75, wpb=755.9, bsz=755.9, num_updates=15200, lr=0.000193, gnorm=4.328, clip=0, train_wall=10, gb_free=72.6, wall=3504 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:56:23]    INFO >> epoch 010:   1455 / 1539 loss=3.472, wps=3113.8, ups=4.84, wpb=643.9, bsz=643.9, num_updates=15250, lr=0.000193, gnorm=3.306, clip=0, train_wall=10, gb_free=70.8, wall=3514 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:56:27] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.79 GiB. GPU 2 has a total capacity of 79.14 GiB of which 535.25 MiB is free. Including non-PyTorch memory, this process has 78.59 GiB memory in use. Of the allocated memory 68.26 GiB is allocated by PyTorch, and 9.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:56:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:56:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:56:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 57           |        cudaMalloc retries: 79        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65551 MiB |  69899 MiB | 708173 GiB | 708109 GiB |
|       from large pool |  65534 MiB |  69881 MiB | 705377 GiB | 705313 GiB |
|       from small pool |     17 MiB |     29 MiB |   2795 GiB |   2795 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65551 MiB |  69899 MiB | 708173 GiB | 708109 GiB |
|       from large pool |  65534 MiB |  69881 MiB | 705377 GiB | 705313 GiB |
|       from small pool |     17 MiB |     29 MiB |   2795 GiB |   2795 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65539 MiB |  69886 MiB | 707261 GiB | 707197 GiB |
|       from large pool |  65521 MiB |  69869 MiB | 704469 GiB | 704405 GiB |
|       from small pool |     17 MiB |     29 MiB |   2791 GiB |   2791 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79970 MiB |  80150 MiB | 558958 MiB | 478988 MiB |
|       from large pool |  79940 MiB |  79940 MiB | 553248 MiB | 473308 MiB |
|       from small pool |     30 MiB |    210 MiB |   5710 MiB |   5680 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  10326 MiB |  14178 MiB | 759462 GiB | 759452 GiB |
|       from large pool |  10313 MiB |  14166 MiB | 756354 GiB | 756344 GiB |
|       from small pool |     12 MiB |     25 MiB |   3107 GiB |   3107 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   32176 K  |   32176 K  |
|       from large pool |     201    |     210    |   15752 K  |   15752 K  |
|       from small pool |     288    |     356    |   16424 K  |   16423 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   32176 K  |   32176 K  |
|       from large pool |     201    |     210    |   15752 K  |   15752 K  |
|       from small pool |     288    |     356    |   16424 K  |   16423 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      87    |     177    |    4585    |    4498    |
|       from large pool |      72    |      72    |    1730    |    1658    |
|       from small pool |      15    |     105    |    2855    |    2840    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      95    |      95    |   17579 K  |   17579 K  |
|       from large pool |      64    |      64    |    9761 K  |    9761 K  |
|       from small pool |      31    |      57    |    7818 K  |    7818 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:56:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:56:27] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:56:34]    INFO >> epoch 010:   1506 / 1539 loss=3.317, wps=3336.3, ups=4.87, wpb=685.6, bsz=685.6, num_updates=15300, lr=0.000193, gnorm=4.003, clip=0, train_wall=9, gb_free=64, wall=3525 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:56:41]    INFO >> epoch 010 | loss 3.377 | wps 3163.5 | ups 4.45 | wpb 711.1 | bsz 711.1 | num_updates 15333 | lr 0.000193 | gnorm 3.907 | clip 0 | train_wall 301 | gb_free 66.2 | wall 3532 (progress_bar.py:267, print())[0m
[33m[2025-11-21 01:56:41] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:57:04]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.698 | wps 6978.8 | wpb 5412.5 | bsz 5412.5 | num_updates 15333 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 01:57:05]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 01:57:05]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 10 @ 15333 updates, score 3.698) (writing took 0.018886 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 01:57:05] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 01:57:08]    INFO >> epoch 011:     17 / 1539 loss=3.333, wps=1070.1, ups=1.5, wpb=713.7, bsz=713.7, num_updates=15350, lr=0.000161, gnorm=4.348, clip=0, train_wall=11, gb_free=70.2, wall=3558 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:57:18]    INFO >> epoch 011:     67 / 1539 loss=3.414, wps=3446.2, ups=5.05, wpb=682, bsz=682, num_updates=15400, lr=0.000161, gnorm=3.583, clip=0, train_wall=9, gb_free=65.9, wall=3568 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:57:30]    INFO >> epoch 011:    117 / 1539 loss=3.439, wps=3440.8, ups=4.86, wpb=708.5, bsz=708.5, num_updates=15450, lr=0.000161, gnorm=4.035, clip=0, train_wall=10, gb_free=70.3, wall=3578 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:57:39]    INFO >> epoch 011:    167 / 1539 loss=3.323, wps=3612.2, ups=5.3, wpb=681.5, bsz=681.5, num_updates=15500, lr=0.000161, gnorm=3.57, clip=0, train_wall=9, gb_free=66.8, wall=3588 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:57:50]    INFO >> epoch 011:    217 / 1539 loss=3.392, wps=3724.5, ups=4.76, wpb=782.3, bsz=782.3, num_updates=15550, lr=0.000161, gnorm=3.808, clip=0, train_wall=10, gb_free=73.2, wall=3598 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:58:00]    INFO >> epoch 011:    267 / 1539 loss=3.296, wps=3561.5, ups=5.04, wpb=706.9, bsz=706.9, num_updates=15600, lr=0.000161, gnorm=4.211, clip=2, train_wall=9, gb_free=65, wall=3608 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:58:10]    INFO >> epoch 011:    317 / 1539 loss=3.358, wps=3401.1, ups=4.69, wpb=725.5, bsz=725.5, num_updates=15650, lr=0.000161, gnorm=3.437, clip=0, train_wall=10, gb_free=62.2, wall=3619 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:58:20]    INFO >> epoch 011:    367 / 1539 loss=3.347, wps=3352.9, ups=5.05, wpb=664, bsz=664, num_updates=15700, lr=0.000161, gnorm=3.458, clip=0, train_wall=9, gb_free=68.1, wall=3629 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:58:32]    INFO >> epoch 011:    417 / 1539 loss=3.4, wps=3461.5, ups=5.32, wpb=650.2, bsz=650.2, num_updates=15750, lr=0.000161, gnorm=4.27, clip=0, train_wall=9, gb_free=72.1, wall=3638 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:58:42]    INFO >> epoch 011:    467 / 1539 loss=3.529, wps=3263.4, ups=5.38, wpb=607, bsz=607, num_updates=15800, lr=0.000161, gnorm=3.375, clip=0, train_wall=9, gb_free=71.9, wall=3647 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:58:43] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.64 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.87 GiB is free. Including non-PyTorch memory, this process has 77.24 GiB memory in use. Of the allocated memory 72.14 GiB is allocated by PyTorch, and 4.60 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:58:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:58:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:58:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 58           |        cudaMalloc retries: 80        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69483 MiB |  76771 MiB | 736554 GiB | 736486 GiB |
|       from large pool |  69462 MiB |  76750 MiB | 733638 GiB | 733570 GiB |
|       from small pool |     20 MiB |     26 MiB |   2915 GiB |   2915 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69483 MiB |  76771 MiB | 736554 GiB | 736486 GiB |
|       from large pool |  69462 MiB |  76750 MiB | 733638 GiB | 733570 GiB |
|       from small pool |     20 MiB |     26 MiB |   2915 GiB |   2915 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69460 MiB |  76747 MiB | 735607 GiB | 735539 GiB |
|       from large pool |  69439 MiB |  76727 MiB | 732696 GiB | 732628 GiB |
|       from small pool |     20 MiB |     25 MiB |   2911 GiB |   2911 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78586 MiB |  78638 MiB | 561718 MiB | 483132 MiB |
|       from large pool |  78556 MiB |  78556 MiB | 555956 MiB | 477400 MiB |
|       from small pool |     30 MiB |     82 MiB |   5762 MiB |   5732 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2302 MiB |   8557 MiB |    769 TiB |    769 TiB |
|       from large pool |   2293 MiB |   8546 MiB |    766 TiB |    766 TiB |
|       from small pool |      9 MiB |     24 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |     721    |   33467 K  |   33466 K  |
|       from large pool |     331    |     355    |   16326 K  |   16326 K  |
|       from small pool |     315    |     376    |   17140 K  |   17140 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |     721    |   33467 K  |   33466 K  |
|       from large pool |     331    |     355    |   16326 K  |   16326 K  |
|       from small pool |     315    |     376    |   17140 K  |   17140 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      87    |     113    |    4612    |    4525    |
|       from large pool |      72    |      72    |    1731    |    1659    |
|       from small pool |      15    |      41    |    2881    |    2866    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     104    |     115    |   18291 K  |   18291 K  |
|       from large pool |      74    |      78    |   10116 K  |   10116 K  |
|       from small pool |      30    |      57    |    8175 K  |    8175 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:58:43] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:58:43] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:58:53]    INFO >> epoch 011:    518 / 1539 loss=3.511, wps=2760.4, ups=4.34, wpb=636.4, bsz=636.4, num_updates=15850, lr=0.000161, gnorm=3.44, clip=0, train_wall=10, gb_free=72.5, wall=3659 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:59:02] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 27.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.57 GiB is allocated by PyTorch, and 1.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:59:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:59:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:59:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 59           |        cudaMalloc retries: 81        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79374 MiB |  79434 MiB | 740204 GiB | 740126 GiB |
|       from large pool |  79191 MiB |  79251 MiB | 737275 GiB | 737198 GiB |
|       from small pool |    183 MiB |    184 MiB |   2928 GiB |   2928 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79374 MiB |  79434 MiB | 740204 GiB | 740126 GiB |
|       from large pool |  79191 MiB |  79251 MiB | 737275 GiB | 737198 GiB |
|       from small pool |    183 MiB |    184 MiB |   2928 GiB |   2928 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79274 MiB |  79334 MiB | 739252 GiB | 739175 GiB |
|       from large pool |  79092 MiB |  79151 MiB | 736328 GiB | 736251 GiB |
|       from small pool |    182 MiB |    183 MiB |   2924 GiB |   2924 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80478 MiB |  80478 MiB | 570410 MiB | 489932 MiB |
|       from large pool |  80276 MiB |  80276 MiB | 564476 MiB | 484200 MiB |
|       from small pool |    202 MiB |    202 MiB |   5934 MiB |   5732 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1043 MiB |   7522 MiB |    773 TiB |    773 TiB |
|       from large pool |   1024 MiB |   7519 MiB |    770 TiB |    770 TiB |
|       from small pool |     18 MiB |     20 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |    3413    |    3416    |   33629 K  |   33626 K  |
|       from large pool |     588    |     589    |   16410 K  |   16410 K  |
|       from small pool |    2825    |    2828    |   17219 K  |   17216 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3413    |    3416    |   33629 K  |   33626 K  |
|       from large pool |     588    |     589    |   16410 K  |   16410 K  |
|       from small pool |    2825    |    2828    |   17219 K  |   17216 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     313    |     313    |    4840    |    4527    |
|       from large pool |     212    |     212    |    1873    |    1661    |
|       from small pool |     101    |     101    |    2967    |    2866    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     239    |     239    |   18378 K  |   18378 K  |
|       from large pool |      64    |      64    |   10168 K  |   10167 K  |
|       from small pool |     175    |     175    |    8210 K  |    8210 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:59:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:59:02] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 01:59:05]    INFO >> epoch 011:    569 / 1539 loss=3.323, wps=2944.4, ups=4.76, wpb=618.5, bsz=618.5, num_updates=15900, lr=0.000161, gnorm=4.226, clip=0, train_wall=9, gb_free=68.9, wall=3669 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:59:16]    INFO >> epoch 011:    619 / 1539 loss=3.317, wps=3813.7, ups=4.51, wpb=845.6, bsz=845.6, num_updates=15950, lr=0.000161, gnorm=3.885, clip=0, train_wall=11, gb_free=68.4, wall=3680 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:59:26]    INFO >> epoch 011:    669 / 1539 loss=3.382, wps=3388.7, ups=4.92, wpb=689.1, bsz=689.1, num_updates=16000, lr=0.000161, gnorm=4.017, clip=0, train_wall=10, gb_free=63, wall=3691 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:59:38]    INFO >> epoch 011:    719 / 1539 loss=3.406, wps=3321, ups=4.64, wpb=716, bsz=716, num_updates=16050, lr=0.000161, gnorm=3.004, clip=0, train_wall=10, gb_free=55.6, wall=3701 (progress_bar.py:258, log())[0m
[32m[2025-11-21 01:59:49]    INFO >> epoch 011:    769 / 1539 loss=3.391, wps=3069.5, ups=4.69, wpb=654.3, bsz=654.3, num_updates=16100, lr=0.000161, gnorm=3.609, clip=0, train_wall=10, gb_free=68, wall=3712 (progress_bar.py:258, log())[0m
[33m[2025-11-21 01:59:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.79 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.76 GiB is free. Including non-PyTorch memory, this process has 76.35 GiB memory in use. Of the allocated memory 68.26 GiB is allocated by PyTorch, and 7.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 01:59:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:59:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:59:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 60           |        cudaMalloc retries: 83        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  65552 MiB |  69900 MiB | 750267 GiB | 750203 GiB |
|       from large pool |  65534 MiB |  69882 MiB | 747301 GiB | 747237 GiB |
|       from small pool |     17 MiB |     27 MiB |   2966 GiB |   2966 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  65552 MiB |  69900 MiB | 750267 GiB | 750203 GiB |
|       from large pool |  65534 MiB |  69882 MiB | 747301 GiB | 747237 GiB |
|       from small pool |     17 MiB |     27 MiB |   2966 GiB |   2966 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  65539 MiB |  69886 MiB | 749302 GiB | 749238 GiB |
|       from large pool |  65521 MiB |  69869 MiB | 746339 GiB | 746275 GiB |
|       from small pool |     17 MiB |     27 MiB |   2962 GiB |   2962 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77674 MiB |  80464 MiB | 576160 MiB | 498486 MiB |
|       from large pool |  77640 MiB |  80216 MiB | 570180 MiB | 492540 MiB |
|       from small pool |     34 MiB |    248 MiB |   5980 MiB |   5946 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9209 MiB |  14242 MiB |    783 TiB |    783 TiB |
|       from large pool |   9193 MiB |  14226 MiB |    780 TiB |    780 TiB |
|       from small pool |     16 MiB |     29 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     489    |     498    |   34088 K  |   34087 K  |
|       from large pool |     201    |     210    |   16646 K  |   16646 K  |
|       from small pool |     288    |     356    |   17441 K  |   17441 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     489    |     498    |   34088 K  |   34087 K  |
|       from large pool |     201    |     210    |   16646 K  |   16646 K  |
|       from small pool |     288    |     356    |   17441 K  |   17441 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      92    |     335    |    4865    |    4773    |
|       from large pool |      75    |     211    |    1875    |    1800    |
|       from small pool |      17    |     124    |    2990    |    2973    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      97    |      98    |   18635 K  |   18634 K  |
|       from large pool |      64    |      65    |   10322 K  |   10322 K  |
|       from small pool |      33    |      57    |    8312 K  |    8312 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:59:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 01:59:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:00:00]    INFO >> epoch 011:    820 / 1539 loss=3.352, wps=2926.8, ups=4.7, wpb=622.1, bsz=622.1, num_updates=16150, lr=0.000161, gnorm=3.864, clip=0, train_wall=9, gb_free=64.5, wall=3723 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:00:11]    INFO >> epoch 011:    870 / 1539 loss=3.361, wps=3457.3, ups=5, wpb=691, bsz=691, num_updates=16200, lr=0.000161, gnorm=4.628, clip=0, train_wall=9, gb_free=71, wall=3733 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:00:21]    INFO >> epoch 011:    920 / 1539 loss=3.434, wps=3807.9, ups=4.97, wpb=766.7, bsz=766.7, num_updates=16250, lr=0.000161, gnorm=3.369, clip=0, train_wall=9, gb_free=68.3, wall=3743 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:00:32]    INFO >> epoch 011:    970 / 1539 loss=3.143, wps=3394.9, ups=4.54, wpb=747, bsz=747, num_updates=16300, lr=0.000161, gnorm=3.776, clip=0, train_wall=10, gb_free=73.5, wall=3754 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:00:44]    INFO >> epoch 011:   1020 / 1539 loss=3.395, wps=3355.4, ups=4.9, wpb=685.3, bsz=685.3, num_updates=16350, lr=0.000161, gnorm=3.692, clip=0, train_wall=10, gb_free=69.8, wall=3764 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:00:56]    INFO >> epoch 011:   1070 / 1539 loss=3.163, wps=3544.1, ups=4.26, wpb=832.4, bsz=832.4, num_updates=16400, lr=0.000161, gnorm=3.365, clip=0, train_wall=11, gb_free=59.9, wall=3776 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:00:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.77 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 78.02 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 1.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:00:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:00:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:00:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 61           |        cudaMalloc retries: 84        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77205 MiB |  78281 MiB | 762630 GiB | 762554 GiB |
|       from large pool |  77183 MiB |  78259 MiB | 759617 GiB | 759541 GiB |
|       from small pool |     22 MiB |     23 MiB |   3012 GiB |   3012 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77205 MiB |  78281 MiB | 762630 GiB | 762554 GiB |
|       from large pool |  77183 MiB |  78259 MiB | 759617 GiB | 759541 GiB |
|       from small pool |     22 MiB |     23 MiB |   3012 GiB |   3012 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77192 MiB |  78269 MiB | 761648 GiB | 761572 GiB |
|       from large pool |  77170 MiB |  78247 MiB | 758639 GiB | 758564 GiB |
|       from small pool |     22 MiB |     23 MiB |   3008 GiB |   3008 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79380 MiB |  79602 MiB | 581000 MiB | 501620 MiB |
|       from large pool |  79348 MiB |  79468 MiB | 574920 MiB | 495572 MiB |
|       from small pool |     32 MiB |    134 MiB |   6080 MiB |   6048 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2114 MiB |   9689 MiB |    796 TiB |    796 TiB |
|       from large pool |   2104 MiB |   9678 MiB |    793 TiB |    793 TiB |
|       from small pool |      9 MiB |     22 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   34645 K  |   34645 K  |
|       from large pool |     270    |     278    |   16937 K  |   16937 K  |
|       from small pool |     296    |     342    |   17708 K  |   17708 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   34645 K  |   34645 K  |
|       from large pool |     270    |     278    |   16937 K  |   16937 K  |
|       from small pool |     296    |     342    |   17708 K  |   17708 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      92    |     145    |    4920    |    4828    |
|       from large pool |      76    |      78    |    1880    |    1804    |
|       from small pool |      16    |      67    |    3040    |    3024    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     105    |     105    |   18933 K  |   18933 K  |
|       from large pool |      75    |      75    |   10501 K  |   10501 K  |
|       from small pool |      30    |      57    |    8431 K  |    8431 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:00:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:00:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:01:07]    INFO >> epoch 011:   1121 / 1539 loss=3.319, wps=3432, ups=4.44, wpb=772.4, bsz=772.4, num_updates=16450, lr=0.000161, gnorm=3.941, clip=0, train_wall=10, gb_free=63.6, wall=3787 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:01:19]    INFO >> epoch 011:   1171 / 1539 loss=3.199, wps=3548.5, ups=4.64, wpb=764.3, bsz=764.3, num_updates=16500, lr=0.000161, gnorm=4.371, clip=0, train_wall=10, gb_free=69.6, wall=3798 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:01:29]    INFO >> epoch 011:   1221 / 1539 loss=3.254, wps=3381.5, ups=4.93, wpb=686.3, bsz=686.3, num_updates=16550, lr=0.000161, gnorm=3.594, clip=0, train_wall=10, gb_free=69.6, wall=3808 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:01:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 3.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.16 GiB is free. Including non-PyTorch memory, this process has 77.96 GiB memory in use. Of the allocated memory 73.70 GiB is allocated by PyTorch, and 3.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:01:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:01:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:01:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 62           |        cudaMalloc retries: 85        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75036 MiB |  75467 MiB | 769040 GiB | 768966 GiB |
|       from large pool |  75010 MiB |  75441 MiB | 766001 GiB | 765928 GiB |
|       from small pool |     25 MiB |     26 MiB |   3038 GiB |   3038 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75036 MiB |  75467 MiB | 769040 GiB | 768966 GiB |
|       from large pool |  75010 MiB |  75441 MiB | 766001 GiB | 765928 GiB |
|       from small pool |     25 MiB |     26 MiB |   3038 GiB |   3038 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75024 MiB |  75454 MiB | 768049 GiB | 767976 GiB |
|       from large pool |  74999 MiB |  75428 MiB | 765015 GiB | 764942 GiB |
|       from small pool |     25 MiB |     26 MiB |   3034 GiB |   3034 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79320 MiB |  79498 MiB | 581178 MiB | 501858 MiB |
|       from large pool |  79288 MiB |  79288 MiB | 574920 MiB | 495632 MiB |
|       from small pool |     32 MiB |    210 MiB |   6258 MiB |   6226 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4283 MiB |  10429 MiB |    804 TiB |    804 TiB |
|       from large pool |   4277 MiB |  10419 MiB |    800 TiB |    800 TiB |
|       from small pool |      6 MiB |     22 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   34944 K  |   34943 K  |
|       from large pool |     265    |     272    |   17086 K  |   17086 K  |
|       from small pool |     302    |     342    |   17857 K  |   17857 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   34944 K  |   34943 K  |
|       from large pool |     265    |     272    |   17086 K  |   17086 K  |
|       from small pool |     302    |     342    |   17857 K  |   17857 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     180    |    5009    |    4918    |
|       from large pool |      75    |      75    |    1880    |    1805    |
|       from small pool |      16    |     105    |    3129    |    3113    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      91    |      91    |   19095 K  |   19095 K  |
|       from large pool |      61    |      61    |   10593 K  |   10593 K  |
|       from small pool |      30    |      49    |    8502 K  |    8502 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:01:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:01:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:01:39]    INFO >> epoch 011:   1272 / 1539 loss=3.445, wps=3672.4, ups=4.82, wpb=762.2, bsz=762.2, num_updates=16600, lr=0.000161, gnorm=3.929, clip=0, train_wall=9, gb_free=72.6, wall=3818 (progress_bar.py:258, log())[0m
[33m[2025-11-21 02:01:48] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.68 GiB is free. Including non-PyTorch memory, this process has 77.43 GiB memory in use. Of the allocated memory 72.07 GiB is allocated by PyTorch, and 4.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 02:01:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:01:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:01:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 63           |        cudaMalloc retries: 86        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73200 MiB |  74276 MiB | 772607 GiB | 772535 GiB |
|       from large pool |  73183 MiB |  74260 MiB | 769555 GiB | 769483 GiB |
|       from small pool |     16 MiB |     28 MiB |   3052 GiB |   3052 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73200 MiB |  74276 MiB | 772607 GiB | 772535 GiB |
|       from large pool |  73183 MiB |  74260 MiB | 769555 GiB | 769483 GiB |
|       from small pool |     16 MiB |     28 MiB |   3052 GiB |   3052 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73189 MiB |  74265 MiB | 771612 GiB | 771541 GiB |
|       from large pool |  73173 MiB |  74248 MiB | 768564 GiB | 768493 GiB |
|       from small pool |     16 MiB |     28 MiB |   3047 GiB |   3047 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78782 MiB |  79372 MiB | 581230 MiB | 502448 MiB |
|       from large pool |  78748 MiB |  79288 MiB | 574920 MiB | 496172 MiB |
|       from small pool |     34 MiB |     84 MiB |   6310 MiB |   6276 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5521 MiB |   6017 MiB |    807 TiB |    807 TiB |
|       from large pool |   5504 MiB |   5998 MiB |    804 TiB |    804 TiB |
|       from small pool |     17 MiB |     29 MiB |      3 TiB |      3 TiB |
|---------------------------------------------------------------------------|
| Allocations           |     527    |     535    |   35104 K  |   35103 K  |
|       from large pool |     238    |     246    |   17170 K  |   17170 K  |
|       from small pool |     289    |     356    |   17934 K  |   17933 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     527    |     535    |   35104 K  |   35103 K  |
|       from large pool |     238    |     246    |   17170 K  |   17170 K  |
|       from small pool |     289    |     356    |   17934 K  |   17933 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      83    |     117    |    5035    |    4952    |
|       from large pool |      66    |      75    |    1880    |    1814    |
|       from small pool |      17    |      42    |    3155    |    3138    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      88    |      89    |   19180 K  |   19180 K  |
|       from large pool |      57    |      58    |   10644 K  |   10644 K  |
|       from small pool |      31    |      64    |    8536 K  |    8536 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:01:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 02:01:48] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 02:01:52]    INFO >> epoch 011:   1323 / 1539 loss=3.249, wps=3157.5, ups=4.56, wpb=692.5, bsz=692.5, num_updates=16650, lr=0.000161, gnorm=3.938, clip=0, train_wall=10, gb_free=55.4, wall=3829 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:02:02]    INFO >> epoch 011:   1373 / 1539 loss=3.439, wps=3445.5, ups=4.81, wpb=716.2, bsz=716.2, num_updates=16700, lr=0.000161, gnorm=3.849, clip=0, train_wall=10, gb_free=66.7, wall=3840 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:02:13]    INFO >> epoch 011:   1423 / 1539 loss=3.487, wps=3653.8, ups=4.63, wpb=789.8, bsz=789.8, num_updates=16750, lr=0.000161, gnorm=4.551, clip=0, train_wall=10, gb_free=63.2, wall=3850 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:02:25]    INFO >> epoch 011:   1473 / 1539 loss=3.259, wps=3693.7, ups=4.8, wpb=770.2, bsz=770.2, num_updates=16800, lr=0.000161, gnorm=4.144, clip=0, train_wall=10, gb_free=65.7, wall=3861 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:02:36]    INFO >> epoch 011:   1523 / 1539 loss=3.364, wps=3337, ups=4.56, wpb=731.3, bsz=731.3, num_updates=16850, lr=0.000161, gnorm=3.731, clip=0, train_wall=10, gb_free=69.5, wall=3872 (progress_bar.py:258, log())[0m
[32m[2025-11-21 02:02:39]    INFO >> epoch 011 | loss 3.354 | wps 3181.2 | ups 4.47 | wpb 711 | bsz 711 | num_updates 16866 | lr 0.000161 | gnorm 3.815 | clip 0.1 | train_wall 299 | gb_free 70.9 | wall 3875 (progress_bar.py:267, print())[0m
[33m[2025-11-21 02:02:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 02:03:01]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.71 | wps 7130.6 | wpb 5412.5 | bsz 5412.5 | num_updates 16866 | best_loss 4.043 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 02:03:02]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 02:03:02]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_last.pt (epoch 11 @ 16866 updates, score 3.71) (writing took 0.031879 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-21 02:03:02]    INFO >> æ—©åœ: éªŒè¯æ€§èƒ½å·²10è½®æœªæå‡ (train_enhanced.py:616, single_main())[0m
[32m[2025-11-21 02:03:02]    INFO >> è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ 3821.2 ç§’ (train_enhanced.py:626, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 02:03:02]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 02:03:02]    INFO >> æ‰€æœ‰æ—¥å¿—å·²ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs (train_enhanced.py:631, single_main())[0m
[32m[2025-11-21 02:03:02]    INFO >> 
================================================================================ (train_enhanced.py:634, single_main())[0m
[32m[2025-11-21 02:03:02]    INFO >> å¼€å§‹æµ‹è¯•... (train_enhanced.py:635, single_main())[0m
[32m[2025-11-21 02:03:02]    INFO >> ================================================================================ (train_enhanced.py:636, single_main())[0m
[32m[2025-11-21 02:03:02]    INFO >> åŠ è½½æœ€ä½³checkpoint: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/checkpoint_best.pt (train_enhanced.py:50, run_test_after_training())[0m
[32m[2025-11-21 02:03:02]    INFO >> æµ‹è¯•é›†: test (train_enhanced.py:51, run_test_after_training())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/checkpoint_utils.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(
[32m[2025-11-21 02:04:06]    INFO >> 
================================================================================ (train_enhanced.py:168, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> æµ‹è¯•ç»“æžœ: (train_enhanced.py:169, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> -------------------------------------------------------------------------------- (train_enhanced.py:170, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> å¹³å‡Loss:      4.0728 (train_enhanced.py:171, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> Acc@1:         20.80% (train_enhanced.py:172, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> Acc@5:         41.85% (train_enhanced.py:173, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> Acc@1 (å«any): 20.80% (train_enhanced.py:174, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> Acc@5 (å«any): 41.85% (train_enhanced.py:175, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> ================================================================================ (train_enhanced.py:176, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> æµ‹è¯•ç»“æžœå·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/checkpoints/res.txt (train_enhanced.py:187, run_test_after_training())[0m
[32m[2025-11-21 02:04:06]    INFO >> è®­ç»ƒæ—¥å¿—å·²æ›´æ–°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs (train_enhanced.py:222, run_test_after_training())[0m
[TrainingLogger] æ—¥å¿—ç›®å½•: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs
[TrainingLogger] åŽŸå§‹è¾“å‡ºå°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/training_output.log
[TrainingLogger] Epoch 1 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 2 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 3 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 4 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 5 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 6 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 7 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 8 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 9 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 10 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
[TrainingLogger] Epoch 11 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_96/logs/metrics.json
