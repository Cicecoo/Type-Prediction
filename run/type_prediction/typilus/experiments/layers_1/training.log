[32m[2025-11-21 06:31:32]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/config.yml (train_enhanced.py:666, cli_main())[0m
[32m[2025-11-21 06:31:32]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:694, cli_main())[0m
[32m[2025-11-21 06:31:32]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs (train_enhanced.py:561, single_main())[0m
[32m[2025-11-21 06:31:32]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 06:31:32]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 06:31:32]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-21 06:31:42]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 64, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=64, out_features=64, bias=False)
      (2): Dropout(p=0.1, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(64, 64)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=False)
      (1): Dropout(p=0.1, inplace=False)
      (2): Linear(in_features=64, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:568, single_main())[0m
[32m[2025-11-21 06:31:42]    INFO >> æ¨¡å‹: typilus, æŸå¤±å‡½æ•°: TypilusCriterion (train_enhanced.py:569, single_main())[0m
[32m[2025-11-21 06:31:42]    INFO >> æ¨¡å‹å‚æ•°: 745059 (å¯è®­ç»ƒ: 745059) (train_enhanced.py:570, single_main())[0m
[32m[2025-11-21 06:31:42]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 06:31:42]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80579 MB ; used memory = 1340 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 06:31:42]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 06:31:42]    INFO >> ä½¿ç”¨ 1 ä¸ªGPUè®­ç»ƒ (train_enhanced.py:576, single_main())[0m
[32m[2025-11-21 06:31:42]    INFO >> no existing checkpoint found /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-21 06:31:42]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-21 06:32:57]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-21 06:32:57] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 06:33:06]    INFO >> epoch 001:     50 / 1539 loss=5.505, wps=5426.6, ups=7.51, wpb=720, bsz=720, num_updates=50, lr=0.0004, gnorm=7.913, clip=0, train_wall=6, gb_free=74.9, wall=79 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:33:13]    INFO >> epoch 001:    100 / 1539 loss=5.602, wps=5648.8, ups=7.07, wpb=798.5, bsz=798.5, num_updates=100, lr=0.0004, gnorm=9.328, clip=0, train_wall=6, gb_free=72.5, wall=86 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:33:20]    INFO >> epoch 001:    150 / 1539 loss=5.969, wps=5757.3, ups=7.03, wpb=818.6, bsz=818.6, num_updates=150, lr=0.0004, gnorm=9.289, clip=0, train_wall=6, gb_free=73.9, wall=93 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:33:26]    INFO >> epoch 001:    200 / 1539 loss=5.953, wps=5065.3, ups=7.97, wpb=635.8, bsz=635.8, num_updates=200, lr=0.0004, gnorm=9.315, clip=0, train_wall=5, gb_free=74, wall=100 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:33:34]    INFO >> epoch 001:    250 / 1539 loss=6.039, wps=5102.8, ups=8, wpb=638, bsz=638, num_updates=250, lr=0.0004, gnorm=7.43, clip=0, train_wall=5, gb_free=74.9, wall=106 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:33:41]    INFO >> epoch 001:    300 / 1539 loss=5.789, wps=5427, ups=6.88, wpb=788.8, bsz=788.8, num_updates=300, lr=0.0004, gnorm=8.666, clip=0, train_wall=6, gb_free=70.2, wall=113 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:33:48]    INFO >> epoch 001:    350 / 1539 loss=5.893, wps=5063.4, ups=7.61, wpb=665.5, bsz=665.5, num_updates=350, lr=0.0004, gnorm=8.31, clip=0, train_wall=6, gb_free=71.3, wall=120 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:33:55]    INFO >> epoch 001:    400 / 1539 loss=5.695, wps=6117.2, ups=7.12, wpb=859.7, bsz=859.7, num_updates=400, lr=0.0004, gnorm=8.374, clip=0, train_wall=6, gb_free=71.4, wall=127 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:34:04] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.94 GiB is allocated by PyTorch, and 688.09 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:34:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:34:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:34:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 3         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79753 MiB |  79813 MiB |  11167 GiB |  11089 GiB |
|       from large pool |  79570 MiB |  79630 MiB |  11094 GiB |  11016 GiB |
|       from small pool |    183 MiB |    184 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79753 MiB |  79813 MiB |  11167 GiB |  11089 GiB |
|       from large pool |  79570 MiB |  79630 MiB |  11094 GiB |  11016 GiB |
|       from small pool |    183 MiB |    184 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79617 MiB |  79676 MiB |  11131 GiB |  11053 GiB |
|       from large pool |  79434 MiB |  79494 MiB |  11058 GiB |  10980 GiB |
|       from small pool |    182 MiB |    183 MiB |     73 GiB |     72 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB | 165174 MiB |  84672 MiB |
|       from large pool |  80314 MiB |  80314 MiB | 164552 MiB |  84238 MiB |
|       from small pool |    188 MiB |    440 MiB |    622 MiB |    434 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory | 704607 KiB |   3180 MiB |   5558 GiB |   5558 GiB |
|       from large pool | 699620 KiB |   3177 MiB |   5473 GiB |   5472 GiB |
|       from small pool |   4987 KiB |     15 MiB |     85 GiB |     85 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3555    |    3558    |     819 K  |     815 K  |
|       from large pool |     591    |     592    |     378 K  |     377 K  |
|       from small pool |    2964    |    2967    |     440 K  |     437 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3555    |    3558    |     819 K  |     815 K  |
|       from large pool |     591    |     592    |     378 K  |     377 K  |
|       from small pool |    2964    |    2967    |     440 K  |     437 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     432    |     857    |    1459    |    1027    |
|       from large pool |     338    |     637    |    1148    |     810    |
|       from small pool |      94    |     220    |     311    |     217    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     167    |     169    |  506198    |  506031    |
|       from large pool |      83    |      83    |  290453    |  290370    |
|       from small pool |      84    |      86    |  215745    |  215661    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:34:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:34:04] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:34:09]    INFO >> epoch 001:    451 / 1539 loss=5.843, wps=2263.4, ups=3.68, wpb=614.5, bsz=614.5, num_updates=450, lr=0.0004, gnorm=7.904, clip=0, train_wall=6, gb_free=75.4, wall=140 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:34:19]    INFO >> epoch 001:    501 / 1539 loss=5.692, wps=4109.7, ups=5.39, wpb=762.8, bsz=762.8, num_updates=500, lr=0.0004, gnorm=7.906, clip=0, train_wall=8, gb_free=73.3, wall=150 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:34:25]    INFO >> epoch 001:    551 / 1539 loss=5.759, wps=4991.6, ups=7.64, wpb=653.1, bsz=653.1, num_updates=550, lr=0.0004, gnorm=7.101, clip=0, train_wall=6, gb_free=68.1, wall=156 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:34:32]    INFO >> epoch 001:    601 / 1539 loss=5.659, wps=5042.2, ups=7.54, wpb=668.9, bsz=668.9, num_updates=600, lr=0.0004, gnorm=8.174, clip=2, train_wall=6, gb_free=75.7, wall=163 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:34:39]    INFO >> epoch 001:    651 / 1539 loss=5.649, wps=5016.6, ups=7.06, wpb=710.3, bsz=710.3, num_updates=650, lr=0.0004, gnorm=7.427, clip=0, train_wall=6, gb_free=72, wall=170 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:34:48]    INFO >> epoch 001:    701 / 1539 loss=5.589, wps=4408.5, ups=6.55, wpb=673.4, bsz=673.4, num_updates=700, lr=0.0004, gnorm=7.076, clip=0, train_wall=7, gb_free=72.3, wall=178 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:34:55]    INFO >> epoch 001:    751 / 1539 loss=5.504, wps=5434.5, ups=7.14, wpb=760.6, bsz=760.6, num_updates=750, lr=0.0004, gnorm=6.979, clip=0, train_wall=6, gb_free=56, wall=185 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:35:02]    INFO >> epoch 001:    801 / 1539 loss=5.546, wps=5270.3, ups=7.34, wpb=717.9, bsz=717.9, num_updates=800, lr=0.0004, gnorm=7.764, clip=0, train_wall=6, gb_free=73.2, wall=191 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:35:09]    INFO >> epoch 001:    851 / 1539 loss=5.502, wps=4422.3, ups=6.88, wpb=642.6, bsz=642.6, num_updates=850, lr=0.0004, gnorm=7.815, clip=0, train_wall=6, gb_free=75.3, wall=199 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:35:17]    INFO >> epoch 001:    901 / 1539 loss=5.536, wps=4939.2, ups=7.48, wpb=660.3, bsz=660.3, num_updates=900, lr=0.0004, gnorm=6.28, clip=0, train_wall=6, gb_free=75.9, wall=205 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:35:24]    INFO >> epoch 001:    951 / 1539 loss=5.459, wps=5080.7, ups=7.11, wpb=714.2, bsz=714.2, num_updates=950, lr=0.0004, gnorm=7.445, clip=0, train_wall=6, gb_free=71.8, wall=212 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:35:33]    INFO >> epoch 001:   1001 / 1539 loss=5.421, wps=3981.6, ups=5.47, wpb=727.3, bsz=727.3, num_updates=1000, lr=0.0004, gnorm=8.218, clip=2, train_wall=8, gb_free=69.3, wall=221 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:35:41]    INFO >> epoch 001:   1051 / 1539 loss=5.455, wps=5563, ups=6.88, wpb=808.7, bsz=808.7, num_updates=1050, lr=0.0004, gnorm=7.603, clip=2, train_wall=6, gb_free=74.3, wall=229 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:35:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.82 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 2.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:35:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:35:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:35:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75049 MiB |  75992 MiB |  29281 GiB |  29208 GiB |
|       from large pool |  75038 MiB |  75982 MiB |  29108 GiB |  29035 GiB |
|       from small pool |     10 MiB |     13 MiB |    173 GiB |    173 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75049 MiB |  75992 MiB |  29281 GiB |  29208 GiB |
|       from large pool |  75038 MiB |  75982 MiB |  29108 GiB |  29035 GiB |
|       from small pool |     10 MiB |     13 MiB |    173 GiB |    173 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75032 MiB |  75975 MiB |  29208 GiB |  29135 GiB |
|       from large pool |  75022 MiB |  75964 MiB |  29035 GiB |  28962 GiB |
|       from small pool |     10 MiB |     13 MiB |    173 GiB |    173 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78638 MiB |  80224 MiB | 281698 MiB | 203060 MiB |
|       from large pool |  78620 MiB |  80018 MiB | 280812 MiB | 202192 MiB |
|       from small pool |     18 MiB |    206 MiB |    886 MiB |    868 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3588 MiB |   9196 MiB |  25839 GiB |  25836 GiB |
|       from large pool |   3581 MiB |   9189 MiB |  25638 GiB |  25634 GiB |
|       from small pool |      7 MiB |     17 MiB |    201 GiB |    201 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     499    |     507    |    2001 K  |    2001 K  |
|       from large pool |     314    |     322    |     967 K  |     967 K  |
|       from small pool |     185    |     238    |    1033 K  |    1033 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     499    |     507    |    2001 K  |    2001 K  |
|       from large pool |     314    |     322    |     967 K  |     967 K  |
|       from small pool |     185    |     238    |    1033 K  |    1033 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      81    |     179    |    1657    |    1576    |
|       from large pool |      72    |      76    |    1214    |    1142    |
|       from small pool |       9    |     103    |     443    |     434    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      86    |      86    |    1151 K  |    1151 K  |
|       from large pool |      67    |      67    |     669 K  |     668 K  |
|       from small pool |      19    |      41    |     482 K  |     482 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:35:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:35:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:35:51]    INFO >> epoch 001:   1102 / 1539 loss=5.21, wps=5052.6, ups=5.69, wpb=888.7, bsz=888.7, num_updates=1100, lr=0.0004, gnorm=9.783, clip=2, train_wall=7, gb_free=73.2, wall=238 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:35:58]    INFO >> epoch 001:   1152 / 1539 loss=5.439, wps=5214.5, ups=7.07, wpb=737.9, bsz=737.9, num_updates=1150, lr=0.0004, gnorm=7.158, clip=0, train_wall=6, gb_free=74.8, wall=245 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:05]    INFO >> epoch 001:   1202 / 1539 loss=5.329, wps=4820.3, ups=7.16, wpb=672.9, bsz=672.9, num_updates=1200, lr=0.0004, gnorm=7.654, clip=2, train_wall=6, gb_free=73.3, wall=252 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:12]    INFO >> epoch 001:   1252 / 1539 loss=5.223, wps=5104.4, ups=6.99, wpb=730.1, bsz=730.1, num_updates=1250, lr=0.0004, gnorm=8.117, clip=0, train_wall=6, gb_free=72.2, wall=259 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:19]    INFO >> epoch 001:   1302 / 1539 loss=5.211, wps=5251.2, ups=7.03, wpb=746.9, bsz=746.9, num_updates=1300, lr=0.0004, gnorm=6.328, clip=0, train_wall=6, gb_free=70, wall=266 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:27]    INFO >> epoch 001:   1352 / 1539 loss=5.171, wps=4542.9, ups=7.29, wpb=622.9, bsz=622.9, num_updates=1350, lr=0.0004, gnorm=6.636, clip=0, train_wall=6, gb_free=76.3, wall=273 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:35]    INFO >> epoch 001:   1402 / 1539 loss=5.206, wps=5050.4, ups=6.81, wpb=741.7, bsz=741.7, num_updates=1400, lr=0.0004, gnorm=6.464, clip=0, train_wall=7, gb_free=72.5, wall=280 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:42]    INFO >> epoch 001:   1452 / 1539 loss=4.904, wps=4909.8, ups=6.97, wpb=704.9, bsz=704.9, num_updates=1450, lr=0.0004, gnorm=7.81, clip=2, train_wall=6, gb_free=71.5, wall=287 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:49]    INFO >> epoch 001:   1502 / 1539 loss=5.012, wps=4945, ups=7.04, wpb=702.3, bsz=702.3, num_updates=1500, lr=0.0004, gnorm=6.604, clip=0, train_wall=6, gb_free=74.2, wall=294 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:36:55]    INFO >> epoch 001 | loss 5.505 | wps 4883.3 | ups 6.77 | wpb 720.8 | bsz 720.8 | num_updates 1537 | lr 0.0004 | gnorm 7.774 | clip 0.5 | train_wall 194 | gb_free 77 | wall 299 (progress_bar.py:267, print())[0m
[33m[2025-11-21 06:36:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:37:09]    INFO >> epoch 001 | valid on 'valid' subset | loss 4.97 | wps 10998.2 | wpb 5412.5 | bsz 5412.5 | num_updates 1537 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-21 06:37:10]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 06:37:10]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_best.pt (epoch 1 @ 1537 updates, score 4.97) (writing took 0.012757 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 06:37:10] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 06:37:12]    INFO >> epoch 002:     13 / 1539 loss=4.894, wps=1671.8, ups=2.34, wpb=714.8, bsz=714.8, num_updates=1550, lr=0.0004, gnorm=7.583, clip=2, train_wall=6, gb_free=74.8, wall=316 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:37:18]    INFO >> epoch 002:     63 / 1539 loss=4.718, wps=5083.8, ups=7.56, wpb=672.6, bsz=672.6, num_updates=1600, lr=0.0004, gnorm=7.624, clip=0, train_wall=6, gb_free=74.3, wall=322 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:37:25]    INFO >> epoch 002:    113 / 1539 loss=4.851, wps=5091.7, ups=7.1, wpb=717.1, bsz=717.1, num_updates=1650, lr=0.0004, gnorm=6.999, clip=0, train_wall=7, gb_free=73.5, wall=329 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:37:34]    INFO >> epoch 002:    163 / 1539 loss=4.613, wps=5058.4, ups=7, wpb=722.3, bsz=722.3, num_updates=1700, lr=0.0004, gnorm=7.212, clip=2, train_wall=7, gb_free=72.8, wall=336 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:37:41]    INFO >> epoch 002:    213 / 1539 loss=4.841, wps=4907.3, ups=6.88, wpb=713.4, bsz=713.4, num_updates=1750, lr=0.0004, gnorm=7.846, clip=2, train_wall=7, gb_free=74.3, wall=344 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:37:49]    INFO >> epoch 002:    263 / 1539 loss=4.623, wps=5599.5, ups=6.5, wpb=861.4, bsz=861.4, num_updates=1800, lr=0.0004, gnorm=7.548, clip=2, train_wall=7, gb_free=74.9, wall=351 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:37:55]    INFO >> epoch 002:    313 / 1539 loss=4.732, wps=4844.2, ups=7.39, wpb=655.5, bsz=655.5, num_updates=1850, lr=0.0004, gnorm=6.446, clip=0, train_wall=6, gb_free=73.6, wall=358 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:38:03]    INFO >> epoch 002:    363 / 1539 loss=4.564, wps=4994.1, ups=7.53, wpb=663.3, bsz=663.3, num_updates=1900, lr=0.0004, gnorm=7.579, clip=0, train_wall=6, gb_free=67, wall=365 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:38:11]    INFO >> epoch 002:    413 / 1539 loss=4.465, wps=4788.5, ups=6.74, wpb=710.4, bsz=710.4, num_updates=1950, lr=0.0004, gnorm=7.344, clip=2, train_wall=7, gb_free=64, wall=372 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:38:12] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.82 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 74.09 GiB is allocated by PyTorch, and 2.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:38:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:38:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:38:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 8         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72059 MiB |  75869 MiB |  55097 GiB |  55026 GiB |
|       from large pool |  72044 MiB |  75854 MiB |  54762 GiB |  54691 GiB |
|       from small pool |     15 MiB |     19 MiB |    334 GiB |    334 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72059 MiB |  75869 MiB |  55097 GiB |  55026 GiB |
|       from large pool |  72044 MiB |  75854 MiB |  54762 GiB |  54691 GiB |
|       from small pool |     15 MiB |     19 MiB |    334 GiB |    334 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  75850 MiB |  54975 GiB |  54904 GiB |
|       from large pool |  72027 MiB |  75835 MiB |  54640 GiB |  54570 GiB |
|       from small pool |     15 MiB |     19 MiB |    334 GiB |    334 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78640 MiB |  78828 MiB | 281888 MiB | 203248 MiB |
|       from large pool |  78620 MiB |  78620 MiB | 280812 MiB | 202192 MiB |
|       from small pool |     20 MiB |    208 MiB |   1076 MiB |   1056 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3852 MiB |   4791 MiB |  53896 GiB |  53892 GiB |
|       from large pool |   3847 MiB |   4781 MiB |  53512 GiB |  53508 GiB |
|       from small pool |      4 MiB |     21 MiB |    383 GiB |    383 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     511    |    3766 K  |    3766 K  |
|       from large pool |     308    |     317    |    1751 K  |    1751 K  |
|       from small pool |     194    |     240    |    2015 K  |    2014 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     511    |    3766 K  |    3766 K  |
|       from large pool |     308    |     317    |    1751 K  |    1751 K  |
|       from small pool |     194    |     240    |    2015 K  |    2014 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      82    |     176    |    1752    |    1670    |
|       from large pool |      72    |      72    |    1214    |    1142    |
|       from small pool |      10    |     104    |     538    |     528    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      90    |      90    |    2131 K  |    2131 K  |
|       from large pool |      70    |      70    |    1172 K  |    1172 K  |
|       from small pool |      20    |      47    |     958 K  |     958 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:38:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:38:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:38:18]    INFO >> epoch 002:    464 / 1539 loss=4.626, wps=4745, ups=6.6, wpb=719.5, bsz=719.5, num_updates=2000, lr=0.0004, gnorm=6.934, clip=0, train_wall=6, gb_free=69.8, wall=380 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:38:26]    INFO >> epoch 002:    514 / 1539 loss=4.574, wps=4968.3, ups=6.94, wpb=716.1, bsz=716.1, num_updates=2050, lr=0.0004, gnorm=7.876, clip=0, train_wall=7, gb_free=72.9, wall=387 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:38:33]    INFO >> epoch 002:    564 / 1539 loss=4.51, wps=4459.7, ups=7.17, wpb=621.7, bsz=621.7, num_updates=2100, lr=0.0004, gnorm=6.955, clip=2, train_wall=6, gb_free=73.7, wall=394 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:38:41]    INFO >> epoch 002:    614 / 1539 loss=4.431, wps=5970.5, ups=6.89, wpb=866.5, bsz=866.5, num_updates=2150, lr=0.0004, gnorm=7.247, clip=0, train_wall=7, gb_free=74.1, wall=401 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:38:49]    INFO >> epoch 002:    664 / 1539 loss=4.438, wps=5215.3, ups=6.79, wpb=767.7, bsz=767.7, num_updates=2200, lr=0.0004, gnorm=7.827, clip=4, train_wall=7, gb_free=72, wall=409 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:38:55]    INFO >> epoch 002:    714 / 1539 loss=4.571, wps=4926.4, ups=7.25, wpb=679.3, bsz=679.3, num_updates=2250, lr=0.0004, gnorm=6.329, clip=0, train_wall=6, gb_free=75.4, wall=416 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:39:03]    INFO >> epoch 002:    764 / 1539 loss=4.435, wps=4852.5, ups=6.9, wpb=703.3, bsz=703.3, num_updates=2300, lr=0.0004, gnorm=6.334, clip=0, train_wall=7, gb_free=69.8, wall=423 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:39:11]    INFO >> epoch 002:    814 / 1539 loss=4.44, wps=4522.7, ups=7.16, wpb=631.7, bsz=631.7, num_updates=2350, lr=0.0004, gnorm=6.339, clip=0, train_wall=6, gb_free=73.1, wall=430 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:39:18]    INFO >> epoch 002:    864 / 1539 loss=4.308, wps=5040.4, ups=6.74, wpb=747.5, bsz=747.5, num_updates=2400, lr=0.0004, gnorm=7.864, clip=0, train_wall=7, gb_free=76, wall=437 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:39:26]    INFO >> epoch 002:    914 / 1539 loss=4.321, wps=4613, ups=6.89, wpb=669.2, bsz=669.2, num_updates=2450, lr=0.0004, gnorm=6.19, clip=0, train_wall=7, gb_free=75.5, wall=444 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:39:33]    INFO >> epoch 002:    964 / 1539 loss=4.141, wps=4502, ups=7.07, wpb=637.1, bsz=637.1, num_updates=2500, lr=0.0004, gnorm=6.679, clip=0, train_wall=7, gb_free=74.9, wall=452 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:39:39] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 49.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.51 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:39:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:39:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:39:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79305 MiB |  79365 MiB |  70418 GiB |  70341 GiB |
|       from large pool |  79126 MiB |  79186 MiB |  69990 GiB |  69913 GiB |
|       from small pool |    179 MiB |    180 MiB |    427 GiB |    427 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79305 MiB |  79365 MiB |  70418 GiB |  70341 GiB |
|       from large pool |  79126 MiB |  79186 MiB |  69990 GiB |  69913 GiB |
|       from small pool |    179 MiB |    180 MiB |    427 GiB |    427 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79256 MiB |  79315 MiB |  70266 GiB |  70189 GiB |
|       from large pool |  79077 MiB |  79137 MiB |  69839 GiB |  69762 GiB |
|       from small pool |    178 MiB |    179 MiB |    427 GiB |    427 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80456 MiB |  80472 MiB | 286692 MiB | 206236 MiB |
|       from large pool |  80272 MiB |  80272 MiB | 285192 MiB | 204920 MiB |
|       from small pool |    184 MiB |    440 MiB |   1500 MiB |   1316 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1090 MiB |   7837 MiB |  71569 GiB |  71568 GiB |
|       from large pool |   1085 MiB |   7830 MiB |  71077 GiB |  71076 GiB |
|       from small pool |      4 MiB |     21 MiB |    492 GiB |    492 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3491    |    3494    |    4836 K  |    4833 K  |
|       from large pool |     585    |     586    |    2266 K  |    2265 K  |
|       from small pool |    2906    |    2909    |    2570 K  |    2567 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3491    |    3494    |    4836 K  |    4833 K  |
|       from large pool |     585    |     586    |    2266 K  |    2265 K  |
|       from small pool |    2906    |    2909    |    2570 K  |    2567 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     236    |     360    |    2037    |    1801    |
|       from large pool |     144    |     144    |    1287    |    1143    |
|       from small pool |      92    |     220    |     750    |     658    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     164    |     165    |    2719 K  |    2719 K  |
|       from large pool |      80    |      81    |    1497 K  |    1497 K  |
|       from small pool |      84    |      85    |    1221 K  |    1221 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:39:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:39:39] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:39:41]    INFO >> epoch 002:   1015 / 1539 loss=3.619, wps=5170.8, ups=5.92, wpb=873.2, bsz=873.2, num_updates=2550, lr=0.0004, gnorm=8.294, clip=4, train_wall=7, gb_free=73.1, wall=460 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:39:50]    INFO >> epoch 002:   1065 / 1539 loss=3.926, wps=5214.8, ups=6.73, wpb=774.3, bsz=774.3, num_updates=2600, lr=0.0004, gnorm=8.141, clip=2, train_wall=7, gb_free=73.4, wall=467 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:39:57]    INFO >> epoch 002:   1115 / 1539 loss=3.843, wps=5329.1, ups=6.93, wpb=768.6, bsz=768.6, num_updates=2650, lr=0.0004, gnorm=6.974, clip=2, train_wall=7, gb_free=74.5, wall=475 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:40:05]    INFO >> epoch 002:   1165 / 1539 loss=4.215, wps=5390.4, ups=6.79, wpb=794.2, bsz=794.2, num_updates=2700, lr=0.0004, gnorm=7.291, clip=0, train_wall=7, gb_free=72.5, wall=482 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:40:12]    INFO >> epoch 002:   1215 / 1539 loss=4.095, wps=4848.7, ups=6.96, wpb=696.2, bsz=696.2, num_updates=2750, lr=0.0004, gnorm=6.859, clip=0, train_wall=7, gb_free=73.4, wall=489 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:40:17] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.31 GiB is free. Including non-PyTorch memory, this process has 77.80 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 3.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:40:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:40:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:40:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75050 MiB |  75994 MiB |  76835 GiB |  76762 GiB |
|       from large pool |  75040 MiB |  75983 MiB |  76369 GiB |  76296 GiB |
|       from small pool |     10 MiB |     12 MiB |    465 GiB |    465 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75050 MiB |  75994 MiB |  76835 GiB |  76762 GiB |
|       from large pool |  75040 MiB |  75983 MiB |  76369 GiB |  76296 GiB |
|       from small pool |     10 MiB |     12 MiB |    465 GiB |    465 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75032 MiB |  75975 MiB |  76669 GiB |  76596 GiB |
|       from large pool |  75022 MiB |  75964 MiB |  76204 GiB |  76131 GiB |
|       from small pool |     10 MiB |     11 MiB |    465 GiB |    465 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79160 MiB |  80434 MiB | 289140 MiB | 209980 MiB |
|       from large pool |  79142 MiB |  80212 MiB | 287602 MiB | 208460 MiB |
|       from small pool |     18 MiB |    222 MiB |   1538 MiB |   1520 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4109 MiB |   8051 MiB |  77655 GiB |  77651 GiB |
|       from large pool |   4101 MiB |   8043 MiB |  77118 GiB |  77114 GiB |
|       from small pool |      7 MiB |     17 MiB |    536 GiB |    536 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     501    |     509    |    5270 K  |    5270 K  |
|       from large pool |     314    |     322    |    2474 K  |    2474 K  |
|       from small pool |     187    |     240    |    2795 K  |    2795 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     501    |     509    |    5270 K  |    5270 K  |
|       from large pool |     314    |     322    |    2474 K  |    2474 K  |
|       from small pool |     187    |     240    |    2795 K  |    2795 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      99    |     254    |    2061    |    1962    |
|       from large pool |      90    |     143    |    1292    |    1202    |
|       from small pool |       9    |     111    |     769    |     760    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      90    |      90    |    2966 K  |    2966 K  |
|       from large pool |      72    |      72    |    1637 K  |    1637 K  |
|       from small pool |      18    |      43    |    1329 K  |    1329 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:40:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:40:17] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:40:21]    INFO >> epoch 002:   1266 / 1539 loss=3.933, wps=4780.1, ups=6.26, wpb=763.3, bsz=763.3, num_updates=2800, lr=0.0004, gnorm=7.202, clip=2, train_wall=7, gb_free=76.2, wall=497 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:40:28]    INFO >> epoch 002:   1316 / 1539 loss=3.985, wps=4801.6, ups=7.19, wpb=668, bsz=668, num_updates=2850, lr=0.0004, gnorm=7.13, clip=0, train_wall=6, gb_free=73.2, wall=504 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:40:35]    INFO >> epoch 002:   1366 / 1539 loss=4.134, wps=5005.3, ups=7.02, wpb=712.8, bsz=712.8, num_updates=2900, lr=0.0004, gnorm=6.407, clip=0, train_wall=7, gb_free=71, wall=511 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:40:42]    INFO >> epoch 002:   1416 / 1539 loss=3.868, wps=4658.3, ups=6.98, wpb=667, bsz=667, num_updates=2950, lr=0.0004, gnorm=6.735, clip=0, train_wall=7, gb_free=73.8, wall=518 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:40:52]    INFO >> epoch 002:   1466 / 1539 loss=4.082, wps=4262.6, ups=6.04, wpb=705.6, bsz=705.6, num_updates=3000, lr=0.0004, gnorm=6.647, clip=0, train_wall=8, gb_free=71.7, wall=527 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:40:59]    INFO >> epoch 002:   1516 / 1539 loss=4.008, wps=4382.9, ups=6.63, wpb=661.3, bsz=661.3, num_updates=3050, lr=0.0004, gnorm=6.343, clip=0, train_wall=7, gb_free=74.8, wall=534 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:41:03]    INFO >> epoch 002 | loss 4.323 | wps 4630.8 | ups 6.44 | wpb 718.6 | bsz 718.6 | num_updates 3073 | lr 0.0004 | gnorm 7.104 | clip 0.8 | train_wall 206 | gb_free 73.3 | wall 538 (progress_bar.py:267, print())[0m
[33m[2025-11-21 06:41:03] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:41:16]    INFO >> epoch 002 | valid on 'valid' subset | loss 3.932 | wps 11700.9 | wpb 5412.5 | bsz 5412.5 | num_updates 3073 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 06:41:17]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 06:41:17]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 2 @ 3073 updates, score 3.932) (writing took 0.013510 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 06:41:17] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:41:20]    INFO >> epoch 003:     27 / 1539 loss=3.959, wps=1660.4, ups=2.41, wpb=689.6, bsz=689.6, num_updates=3100, lr=0.000392, gnorm=7.371, clip=0, train_wall=7, gb_free=72.5, wall=555 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:41:29]    INFO >> epoch 003:     77 / 1539 loss=3.992, wps=5238.6, ups=6.77, wpb=773.7, bsz=773.7, num_updates=3150, lr=0.000392, gnorm=6.398, clip=0, train_wall=7, gb_free=74.2, wall=562 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:41:32] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.31 GiB is free. Including non-PyTorch memory, this process has 77.80 GiB memory in use. Of the allocated memory 74.09 GiB is allocated by PyTorch, and 3.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:41:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:41:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:41:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 13        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72060 MiB |  75870 MiB |  91083 GiB |  91012 GiB |
|       from large pool |  72045 MiB |  75854 MiB |  90524 GiB |  90454 GiB |
|       from small pool |     15 MiB |     17 MiB |    558 GiB |    558 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72060 MiB |  75870 MiB |  91083 GiB |  91012 GiB |
|       from large pool |  72045 MiB |  75854 MiB |  90524 GiB |  90454 GiB |
|       from small pool |     15 MiB |     17 MiB |    558 GiB |    558 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  75850 MiB |  90890 GiB |  90820 GiB |
|       from large pool |  72027 MiB |  75835 MiB |  90332 GiB |  90262 GiB |
|       from small pool |     15 MiB |     17 MiB |    557 GiB |    557 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79160 MiB |  79262 MiB | 289242 MiB | 210082 MiB |
|       from large pool |  79142 MiB |  79142 MiB | 287602 MiB | 208460 MiB |
|       from small pool |     18 MiB |    120 MiB |   1640 MiB |   1622 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1643 MiB |   5224 MiB |  91127 GiB |  91125 GiB |
|       from large pool |   1640 MiB |   5215 MiB |  90488 GiB |  90487 GiB |
|       from small pool |      2 MiB |     17 MiB |    638 GiB |    638 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     511    |    6222 K  |    6222 K  |
|       from large pool |     308    |     317    |    2855 K  |    2855 K  |
|       from small pool |     194    |     240    |    3367 K  |    3366 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     511    |    6222 K  |    6222 K  |
|       from large pool |     308    |     317    |    2855 K  |    2855 K  |
|       from small pool |     194    |     240    |    3367 K  |    3366 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      99    |     150    |    2112    |    2013    |
|       from large pool |      90    |      90    |    1292    |    1202    |
|       from small pool |       9    |      60    |     820    |     811    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      83    |      83    |    3514 K  |    3514 K  |
|       from large pool |      64    |      64    |    1887 K  |    1887 K  |
|       from small pool |      19    |      41    |    1626 K  |    1626 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:41:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:41:32] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:41:37]    INFO >> epoch 003:    128 / 1539 loss=3.926, wps=5144.4, ups=6.23, wpb=825.8, bsz=825.8, num_updates=3200, lr=0.000392, gnorm=6.806, clip=0, train_wall=7, gb_free=64.7, wall=570 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:41:44]    INFO >> epoch 003:    178 / 1539 loss=4.117, wps=4526.2, ups=7.17, wpb=630.9, bsz=630.9, num_updates=3250, lr=0.000392, gnorm=5.963, clip=0, train_wall=6, gb_free=73.3, wall=577 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:41:49] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 591.25 MiB is free. Including non-PyTorch memory, this process has 78.54 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 4.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:41:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:41:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:41:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 14        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75051 MiB |  75994 MiB |  94231 GiB |  94158 GiB |
|       from large pool |  75040 MiB |  75983 MiB |  93653 GiB |  93579 GiB |
|       from small pool |     10 MiB |     18 MiB |    578 GiB |    578 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75051 MiB |  75994 MiB |  94231 GiB |  94158 GiB |
|       from large pool |  75040 MiB |  75983 MiB |  93653 GiB |  93579 GiB |
|       from small pool |     10 MiB |     18 MiB |    578 GiB |    578 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75032 MiB |  75975 MiB |  94032 GiB |  93958 GiB |
|       from large pool |  75022 MiB |  75964 MiB |  93454 GiB |  93381 GiB |
|       from small pool |     10 MiB |     18 MiB |    577 GiB |    577 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79914 MiB |  80102 MiB | 295640 MiB | 215726 MiB |
|       from large pool |  79894 MiB |  79894 MiB | 293810 MiB | 213916 MiB |
|       from small pool |     20 MiB |    208 MiB |   1830 MiB |   1810 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4862 MiB |  10522 MiB |  94537 GiB |  94532 GiB |
|       from large pool |   4853 MiB |  10512 MiB |  93876 GiB |  93871 GiB |
|       from small pool |      9 MiB |     25 MiB |    661 GiB |    661 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     501    |     509    |    6441 K  |    6441 K  |
|       from large pool |     314    |     322    |    2957 K  |    2956 K  |
|       from small pool |     187    |     240    |    3484 K  |    3484 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     501    |     509    |    6441 K  |    6441 K  |
|       from large pool |     314    |     322    |    2957 K  |    2956 K  |
|       from small pool |     187    |     240    |    3484 K  |    3484 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     102    |     196    |    2211    |    2109    |
|       from large pool |      92    |      92    |    1296    |    1204    |
|       from small pool |      10    |     104    |     915    |     905    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      93    |      93    |    3638 K  |    3638 K  |
|       from large pool |      73    |      73    |    1953 K  |    1953 K  |
|       from small pool |      20    |      48    |    1684 K  |    1684 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:41:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:41:49] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:41:52]    INFO >> epoch 003:    229 / 1539 loss=3.928, wps=4704.4, ups=5.97, wpb=788.4, bsz=788.4, num_updates=3300, lr=0.000392, gnorm=6.054, clip=0, train_wall=7, gb_free=72, wall=586 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:42:01]    INFO >> epoch 003:    279 / 1539 loss=3.868, wps=5251.8, ups=7.05, wpb=745.4, bsz=745.4, num_updates=3350, lr=0.000392, gnorm=6.108, clip=0, train_wall=7, gb_free=67.8, wall=593 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:42:08]    INFO >> epoch 003:    329 / 1539 loss=3.919, wps=5429, ups=6.64, wpb=817.7, bsz=817.7, num_updates=3400, lr=0.000392, gnorm=6.803, clip=2, train_wall=7, gb_free=73.1, wall=600 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:42:16]    INFO >> epoch 003:    379 / 1539 loss=3.886, wps=4567.1, ups=6.84, wpb=667.7, bsz=667.7, num_updates=3450, lr=0.000392, gnorm=6.143, clip=0, train_wall=7, gb_free=73.9, wall=608 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:42:23]    INFO >> epoch 003:    429 / 1539 loss=4.004, wps=4393.1, ups=6.8, wpb=645.9, bsz=645.9, num_updates=3500, lr=0.000392, gnorm=6.316, clip=0, train_wall=7, gb_free=73.1, wall=615 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:42:32]    INFO >> epoch 003:    479 / 1539 loss=3.908, wps=5266.3, ups=6.8, wpb=774, bsz=774, num_updates=3550, lr=0.000392, gnorm=5.893, clip=0, train_wall=7, gb_free=68.7, wall=622 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:42:39]    INFO >> epoch 003:    529 / 1539 loss=3.89, wps=5106.6, ups=7.1, wpb=718.9, bsz=718.9, num_updates=3600, lr=0.000392, gnorm=7.202, clip=0, train_wall=7, gb_free=73, wall=629 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:42:46]    INFO >> epoch 003:    579 / 1539 loss=3.801, wps=5097.5, ups=6.61, wpb=771.6, bsz=771.6, num_updates=3650, lr=0.000392, gnorm=6.286, clip=2, train_wall=7, gb_free=74.4, wall=637 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:42:53]    INFO >> epoch 003:    629 / 1539 loss=3.944, wps=4879, ups=7.44, wpb=655.6, bsz=655.6, num_updates=3700, lr=0.000392, gnorm=5.825, clip=0, train_wall=6, gb_free=73.5, wall=644 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:43:00]    INFO >> epoch 003:    679 / 1539 loss=3.885, wps=5330.5, ups=6.8, wpb=783.8, bsz=783.8, num_updates=3750, lr=0.000392, gnorm=6.529, clip=0, train_wall=7, gb_free=71.4, wall=651 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:43:10]    INFO >> epoch 003:    729 / 1539 loss=3.821, wps=4880, ups=6.22, wpb=784.1, bsz=784.1, num_updates=3800, lr=0.000392, gnorm=5.382, clip=0, train_wall=7, gb_free=74.3, wall=659 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:43:17]    INFO >> epoch 003:    779 / 1539 loss=3.509, wps=5484.9, ups=6.7, wpb=819.1, bsz=819.1, num_updates=3850, lr=0.000392, gnorm=6.832, clip=0, train_wall=7, gb_free=74, wall=667 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:43:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 21.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.54 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:43:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:43:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:43:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 16        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79340 MiB |  79400 MiB | 109727 GiB | 109649 GiB |
|       from large pool |  79160 MiB |  79220 MiB | 109056 GiB | 108979 GiB |
|       from small pool |    180 MiB |    181 MiB |    670 GiB |    670 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79340 MiB |  79400 MiB | 109727 GiB | 109649 GiB |
|       from large pool |  79160 MiB |  79220 MiB | 109056 GiB | 108979 GiB |
|       from small pool |    180 MiB |    181 MiB |    670 GiB |    670 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79316 MiB |  79376 MiB | 109495 GiB | 109418 GiB |
|       from large pool |  79137 MiB |  79196 MiB | 108826 GiB | 108748 GiB |
|       from small pool |    179 MiB |    180 MiB |    669 GiB |    669 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80484 MiB |  80496 MiB | 296284 MiB | 215800 MiB |
|       from large pool |  80298 MiB |  80298 MiB | 294230 MiB | 213932 MiB |
|       from small pool |    186 MiB |    242 MiB |   2054 MiB |   1868 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1083 MiB |   5955 MiB | 111232 GiB | 111231 GiB |
|       from large pool |   1077 MiB |   5950 MiB | 110463 GiB | 110462 GiB |
|       from small pool |      5 MiB |     23 MiB |    769 GiB |    769 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3502    |    3505    |    7503 K  |    7499 K  |
|       from large pool |     586    |     587    |    3468 K  |    3468 K  |
|       from small pool |    2916    |    2919    |    4034 K  |    4031 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3502    |    3505    |    7503 K  |    7499 K  |
|       from large pool |     586    |     587    |    3468 K  |    3468 K  |
|       from small pool |    2916    |    2919    |    4034 K  |    4031 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     191    |     219    |    2330    |    2139    |
|       from large pool |      98    |      98    |    1303    |    1205    |
|       from small pool |      93    |     121    |    1027    |     934    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     163    |     163    |    4229 K  |    4229 K  |
|       from large pool |      79    |      79    |    2284 K  |    2284 K  |
|       from small pool |      84    |      84    |    1944 K  |    1944 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:43:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:43:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:43:25]    INFO >> epoch 003:    830 / 1539 loss=3.84, wps=4650, ups=6.45, wpb=721, bsz=721, num_updates=3900, lr=0.000392, gnorm=6.826, clip=0, train_wall=7, gb_free=63, wall=674 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:43:32]    INFO >> epoch 003:    880 / 1539 loss=3.931, wps=4993.3, ups=6.75, wpb=740, bsz=740, num_updates=3950, lr=0.000392, gnorm=6.087, clip=0, train_wall=7, gb_free=73.3, wall=682 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:43:41]    INFO >> epoch 003:    930 / 1539 loss=3.765, wps=4723.4, ups=6.75, wpb=699.4, bsz=699.4, num_updates=4000, lr=0.000392, gnorm=6.132, clip=0, train_wall=7, gb_free=60.6, wall=689 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:43:50]    INFO >> epoch 003:    980 / 1539 loss=3.875, wps=4109, ups=5.78, wpb=711.2, bsz=711.2, num_updates=4050, lr=0.000392, gnorm=6.205, clip=0, train_wall=8, gb_free=75, wall=698 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:43:57]    INFO >> epoch 003:   1030 / 1539 loss=3.916, wps=4400.3, ups=7.12, wpb=618.1, bsz=618.1, num_updates=4100, lr=0.000392, gnorm=5.382, clip=0, train_wall=7, gb_free=71.5, wall=705 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:44:04]    INFO >> epoch 003:   1080 / 1539 loss=3.856, wps=4910, ups=7.25, wpb=677.3, bsz=677.3, num_updates=4150, lr=0.000392, gnorm=6.315, clip=0, train_wall=6, gb_free=74.3, wall=712 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:44:13]    INFO >> epoch 003:   1130 / 1539 loss=3.688, wps=5543.2, ups=6.27, wpb=884.7, bsz=884.7, num_updates=4200, lr=0.000392, gnorm=5.95, clip=0, train_wall=7, gb_free=72.8, wall=720 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:44:20]    INFO >> epoch 003:   1180 / 1539 loss=3.769, wps=4325.5, ups=6.68, wpb=647.2, bsz=647.2, num_updates=4250, lr=0.000392, gnorm=5.479, clip=0, train_wall=7, gb_free=68.8, wall=727 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:44:28]    INFO >> epoch 003:   1230 / 1539 loss=3.907, wps=4898.8, ups=6.58, wpb=744.2, bsz=744.2, num_updates=4300, lr=0.000392, gnorm=6.186, clip=0, train_wall=7, gb_free=74, wall=735 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:44:35]    INFO >> epoch 003:   1280 / 1539 loss=3.777, wps=4603.9, ups=7.34, wpb=627.3, bsz=627.3, num_updates=4350, lr=0.000392, gnorm=5.319, clip=0, train_wall=6, gb_free=71.8, wall=742 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:44:42]    INFO >> epoch 003:   1330 / 1539 loss=3.758, wps=4986.6, ups=7.32, wpb=680.9, bsz=680.9, num_updates=4400, lr=0.000392, gnorm=5.502, clip=0, train_wall=6, gb_free=72.6, wall=748 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:44:50]    INFO >> epoch 003:   1380 / 1539 loss=3.74, wps=4919.5, ups=7.14, wpb=689.4, bsz=689.4, num_updates=4450, lr=0.000392, gnorm=6.001, clip=0, train_wall=6, gb_free=75.8, wall=755 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:44:57]    INFO >> epoch 003:   1430 / 1539 loss=3.822, wps=4827, ups=7.17, wpb=673.6, bsz=673.6, num_updates=4500, lr=0.000392, gnorm=5.762, clip=0, train_wall=7, gb_free=74, wall=762 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:45:04]    INFO >> epoch 003:   1480 / 1539 loss=3.834, wps=4580.5, ups=7.1, wpb=645.1, bsz=645.1, num_updates=4550, lr=0.000392, gnorm=5.509, clip=0, train_wall=7, gb_free=71.4, wall=769 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:45:11]    INFO >> epoch 003:   1530 / 1539 loss=3.877, wps=4813.7, ups=7.32, wpb=657.9, bsz=657.9, num_updates=4600, lr=0.000392, gnorm=6.569, clip=0, train_wall=6, gb_free=74.4, wall=776 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:45:12]    INFO >> epoch 003 | loss 3.855 | wps 4605.3 | ups 6.41 | wpb 718.6 | bsz 718.6 | num_updates 4609 | lr 0.000392 | gnorm 6.147 | clip 0.2 | train_wall 208 | gb_free 75.1 | wall 777 (progress_bar.py:267, print())[0m
[33m[2025-11-21 06:45:12] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:45:26]    INFO >> epoch 003 | valid on 'valid' subset | loss 3.878 | wps 11739.7 | wpb 5412.5 | bsz 5412.5 | num_updates 4609 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 06:45:27]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 06:45:27]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 3 @ 4609 updates, score 3.878) (writing took 0.011074 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 06:45:27] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:45:33]    INFO >> epoch 004:     41 / 1539 loss=3.574, wps=1880.4, ups=2.41, wpb=780.3, bsz=780.3, num_updates=4650, lr=0.000376, gnorm=7.267, clip=4, train_wall=7, gb_free=75.7, wall=797 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:45:40]    INFO >> epoch 004:     91 / 1539 loss=3.821, wps=4696.4, ups=6.76, wpb=694.9, bsz=694.9, num_updates=4700, lr=0.000376, gnorm=5.121, clip=0, train_wall=7, gb_free=72.4, wall=804 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:45:50]    INFO >> epoch 004:    141 / 1539 loss=3.603, wps=5673, ups=5.47, wpb=1036.7, bsz=1036.7, num_updates=4750, lr=0.000376, gnorm=7.792, clip=2, train_wall=9, gb_free=75.3, wall=814 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:45:58]    INFO >> epoch 004:    191 / 1539 loss=3.737, wps=5317.6, ups=6.91, wpb=769, bsz=769, num_updates=4800, lr=0.000376, gnorm=5.322, clip=0, train_wall=7, gb_free=71.8, wall=821 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:46:05]    INFO >> epoch 004:    241 / 1539 loss=3.669, wps=5135.4, ups=6.97, wpb=737, bsz=737, num_updates=4850, lr=0.000376, gnorm=7.561, clip=2, train_wall=7, gb_free=71.3, wall=828 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:46:12]    INFO >> epoch 004:    291 / 1539 loss=3.85, wps=4681.1, ups=7.17, wpb=653, bsz=653, num_updates=4900, lr=0.000376, gnorm=5.525, clip=0, train_wall=6, gb_free=72.3, wall=835 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:46:19]    INFO >> epoch 004:    341 / 1539 loss=3.823, wps=4908.4, ups=7.29, wpb=673.2, bsz=673.2, num_updates=4950, lr=0.000376, gnorm=5.163, clip=0, train_wall=6, gb_free=73, wall=842 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:46:28]    INFO >> epoch 004:    391 / 1539 loss=3.671, wps=5405, ups=7.05, wpb=766.3, bsz=766.3, num_updates=5000, lr=0.000376, gnorm=5.852, clip=0, train_wall=7, gb_free=73.5, wall=849 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:46:35]    INFO >> epoch 004:    441 / 1539 loss=3.815, wps=4182.2, ups=6.6, wpb=633.4, bsz=633.4, num_updates=5050, lr=0.000376, gnorm=6.127, clip=0, train_wall=7, gb_free=60.6, wall=856 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:46:43]    INFO >> epoch 004:    491 / 1539 loss=3.783, wps=4867.9, ups=6.79, wpb=717, bsz=717, num_updates=5100, lr=0.000376, gnorm=5.184, clip=0, train_wall=7, gb_free=73.9, wall=864 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:46:50]    INFO >> epoch 004:    541 / 1539 loss=3.775, wps=4893.8, ups=6.95, wpb=704.1, bsz=704.1, num_updates=5150, lr=0.000376, gnorm=6.172, clip=0, train_wall=7, gb_free=65.6, wall=871 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:46:57]    INFO >> epoch 004:    591 / 1539 loss=3.76, wps=4334.9, ups=7.18, wpb=603.5, bsz=603.5, num_updates=5200, lr=0.000376, gnorm=5.045, clip=0, train_wall=6, gb_free=75.4, wall=878 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:46:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 11.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 76.85 GiB is allocated by PyTorch, and 1.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:46:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:46:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:46:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 19        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78638 MiB |  78698 MiB | 148824 GiB | 148747 GiB |
|       from large pool |  78466 MiB |  78526 MiB | 147913 GiB | 147836 GiB |
|       from small pool |    172 MiB |    173 MiB |    911 GiB |    911 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78638 MiB |  78698 MiB | 148824 GiB | 148747 GiB |
|       from large pool |  78466 MiB |  78526 MiB | 147913 GiB | 147836 GiB |
|       from small pool |    172 MiB |    173 MiB |    911 GiB |    911 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78535 MiB |  78594 MiB | 148509 GiB | 148432 GiB |
|       from large pool |  78363 MiB |  78423 MiB | 147598 GiB | 147522 GiB |
|       from small pool |    171 MiB |    172 MiB |    910 GiB |    909 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80494 MiB |  80494 MiB | 326542 MiB | 246048 MiB |
|       from large pool |  80316 MiB |  80316 MiB | 324074 MiB | 243758 MiB |
|       from small pool |    178 MiB |    440 MiB |   2468 MiB |   2290 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1795 MiB |   6899 MiB | 146586 GiB | 146584 GiB |
|       from large pool |   1789 MiB |   6895 MiB | 145543 GiB | 145541 GiB |
|       from small pool |      5 MiB |     23 MiB |   1043 GiB |   1043 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3359    |    3362    |   10177 K  |   10173 K  |
|       from large pool |     573    |     574    |    4688 K  |    4688 K  |
|       from small pool |    2786    |    2789    |    5488 K  |    5485 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3359    |    3362    |   10177 K  |   10173 K  |
|       from large pool |     573    |     574    |    4688 K  |    4688 K  |
|       from small pool |    2786    |    2789    |    5488 K  |    5485 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     406    |     555    |    2936    |    2530    |
|       from large pool |     317    |     335    |    1702    |    1385    |
|       from small pool |      89    |     220    |    1234    |    1145    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     209    |     209    |    5753 K  |    5753 K  |
|       from large pool |     126    |     132    |    3097 K  |    3097 K  |
|       from small pool |      83    |      83    |    2655 K  |    2655 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:46:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:46:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:47:06]    INFO >> epoch 004:    642 / 1539 loss=3.715, wps=4416.2, ups=6.42, wpb=687.7, bsz=687.7, num_updates=5250, lr=0.000376, gnorm=5.724, clip=0, train_wall=6, gb_free=73.1, wall=886 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:47:13] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 329.25 MiB is free. Including non-PyTorch memory, this process has 78.79 GiB memory in use. Of the allocated memory 74.09 GiB is allocated by PyTorch, and 4.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:47:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:47:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:47:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72059 MiB |  75868 MiB | 151135 GiB | 151064 GiB |
|       from large pool |  72043 MiB |  75852 MiB | 150211 GiB | 150141 GiB |
|       from small pool |     15 MiB |     22 MiB |    923 GiB |    923 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72059 MiB |  75868 MiB | 151135 GiB | 151064 GiB |
|       from large pool |  72043 MiB |  75852 MiB | 150211 GiB | 150141 GiB |
|       from small pool |     15 MiB |     22 MiB |    923 GiB |    923 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  75850 MiB | 150814 GiB | 150744 GiB |
|       from large pool |  72027 MiB |  75835 MiB | 149892 GiB | 149822 GiB |
|       from small pool |     15 MiB |     22 MiB |    922 GiB |    922 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80176 MiB |  80434 MiB | 337456 MiB | 257280 MiB |
|       from large pool |  80158 MiB |  80256 MiB | 334986 MiB | 254828 MiB |
|       from small pool |     18 MiB |    178 MiB |   2470 MiB |   2452 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5388 MiB |   7349 MiB | 148677 GiB | 148672 GiB |
|       from large pool |   5386 MiB |   7345 MiB | 147620 GiB | 147615 GiB |
|       from small pool |      2 MiB |     19 MiB |   1057 GiB |   1057 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     511    |   10329 K  |   10329 K  |
|       from large pool |     308    |     317    |    4768 K  |    4767 K  |
|       from small pool |     194    |     240    |    5561 K  |    5561 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     511    |   10329 K  |   10329 K  |
|       from large pool |     308    |     317    |    4768 K  |    4767 K  |
|       from small pool |     194    |     240    |    5561 K  |    5561 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     149    |     405    |    2941    |    2792    |
|       from large pool |     140    |     316    |    1706    |    1566    |
|       from small pool |       9    |      89    |    1235    |    1226    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     138    |     138    |    5838 K  |    5837 K  |
|       from large pool |     118    |     118    |    3150 K  |    3150 K  |
|       from small pool |      20    |      44    |    2687 K  |    2687 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:47:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:47:13] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:47:14]    INFO >> epoch 004:    693 / 1539 loss=3.731, wps=4086.2, ups=6.34, wpb=644.3, bsz=644.3, num_updates=5300, lr=0.000376, gnorm=5.049, clip=0, train_wall=6, gb_free=73.8, wall=894 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:47:22]    INFO >> epoch 004:    743 / 1539 loss=3.693, wps=4219.9, ups=6.29, wpb=671.3, bsz=671.3, num_updates=5350, lr=0.000376, gnorm=5.453, clip=0, train_wall=7, gb_free=74.1, wall=902 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:47:29]    INFO >> epoch 004:    793 / 1539 loss=3.606, wps=4535.8, ups=6.53, wpb=694.1, bsz=694.1, num_updates=5400, lr=0.000376, gnorm=6.227, clip=0, train_wall=7, gb_free=58.7, wall=909 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:47:37]    INFO >> epoch 004:    843 / 1539 loss=3.607, wps=5535.3, ups=7.34, wpb=754.2, bsz=754.2, num_updates=5450, lr=0.000376, gnorm=5.967, clip=0, train_wall=6, gb_free=65.2, wall=916 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:47:45]    INFO >> epoch 004:    893 / 1539 loss=3.913, wps=5047.9, ups=6.39, wpb=789.4, bsz=789.4, num_updates=5500, lr=0.000376, gnorm=6.018, clip=0, train_wall=7, gb_free=73.2, wall=924 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:47:52]    INFO >> epoch 004:    943 / 1539 loss=3.699, wps=4670, ups=7.28, wpb=641.3, bsz=641.3, num_updates=5550, lr=0.000376, gnorm=5.285, clip=0, train_wall=6, gb_free=74.7, wall=931 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:48:00]    INFO >> epoch 004:    993 / 1539 loss=3.753, wps=5086.7, ups=6.75, wpb=753.8, bsz=753.8, num_updates=5600, lr=0.000376, gnorm=5.306, clip=0, train_wall=7, gb_free=73.3, wall=938 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:48:08]    INFO >> epoch 004:   1043 / 1539 loss=3.874, wps=4473.9, ups=7.25, wpb=617.4, bsz=617.4, num_updates=5650, lr=0.000376, gnorm=5.145, clip=0, train_wall=6, gb_free=73.5, wall=945 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:48:15]    INFO >> epoch 004:   1093 / 1539 loss=3.824, wps=5408, ups=7.1, wpb=762.2, bsz=762.2, num_updates=5700, lr=0.000376, gnorm=5.006, clip=0, train_wall=7, gb_free=74, wall=952 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:48:22]    INFO >> epoch 004:   1143 / 1539 loss=3.633, wps=4552.3, ups=7.28, wpb=625.3, bsz=625.3, num_updates=5750, lr=0.000376, gnorm=5.228, clip=0, train_wall=6, gb_free=72.4, wall=959 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:48:29]    INFO >> epoch 004:   1193 / 1539 loss=3.776, wps=5252.4, ups=6.57, wpb=799.5, bsz=799.5, num_updates=5800, lr=0.000376, gnorm=6.912, clip=2, train_wall=7, gb_free=64.3, wall=967 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:48:36]    INFO >> epoch 004:   1243 / 1539 loss=3.686, wps=4625.7, ups=7.39, wpb=625.9, bsz=625.9, num_updates=5850, lr=0.000376, gnorm=4.809, clip=0, train_wall=6, gb_free=73.7, wall=973 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:48:45]    INFO >> epoch 004:   1293 / 1539 loss=3.725, wps=4241.3, ups=6.66, wpb=637.1, bsz=637.1, num_updates=5900, lr=0.000376, gnorm=5.094, clip=0, train_wall=7, gb_free=73.5, wall=981 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:48:52]    INFO >> epoch 004:   1343 / 1539 loss=3.65, wps=5339.9, ups=6.81, wpb=784, bsz=784, num_updates=5950, lr=0.000376, gnorm=6.01, clip=0, train_wall=7, gb_free=68.1, wall=988 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:49:00] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.41 GiB is free. Including non-PyTorch memory, this process has 77.71 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 3.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:49:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:49:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:49:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 23        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75050 MiB |  75994 MiB | 169414 GiB | 169340 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 168387 GiB | 168314 GiB |
|       from small pool |     10 MiB |     11 MiB |   1026 GiB |   1026 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75050 MiB |  75994 MiB | 169414 GiB | 169340 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 168387 GiB | 168314 GiB |
|       from small pool |     10 MiB |     11 MiB |   1026 GiB |   1026 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75032 MiB |  75975 MiB | 169052 GiB | 168979 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 168027 GiB | 167954 GiB |
|       from small pool |     10 MiB |     11 MiB |   1025 GiB |   1025 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79062 MiB |  80056 MiB | 342474 MiB | 263412 MiB |
|       from large pool |  79044 MiB |  79814 MiB | 339780 MiB | 260736 MiB |
|       from small pool |     18 MiB |    242 MiB |   2694 MiB |   2676 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4011 MiB |  10661 MiB | 165814 GiB | 165810 GiB |
|       from large pool |   4003 MiB |  10653 MiB | 164636 GiB | 164632 GiB |
|       from small pool |      7 MiB |     15 MiB |   1177 GiB |   1177 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     501    |     509    |   11552 K  |   11551 K  |
|       from large pool |     314    |     322    |    5379 K  |    5379 K  |
|       from small pool |     187    |     220    |    6172 K  |    6172 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     501    |     509    |   11552 K  |   11551 K  |
|       from large pool |     314    |     322    |    5379 K  |    5379 K  |
|       from small pool |     187    |     220    |    6172 K  |    6172 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     102    |     262    |    3060    |    2958    |
|       from large pool |      93    |     141    |    1713    |    1620    |
|       from small pool |       9    |     121    |    1347    |    1338    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      99    |      99    |    6520 K  |    6520 K  |
|       from large pool |      81    |      81    |    3559 K  |    3559 K  |
|       from small pool |      18    |      32    |    2961 K  |    2961 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:49:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:49:00] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:49:01]    INFO >> epoch 004:   1394 / 1539 loss=3.705, wps=4970.5, ups=5.96, wpb=833.5, bsz=833.5, num_updates=6000, lr=0.000376, gnorm=6.314, clip=2, train_wall=7, gb_free=5.8, wall=997 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:49:08]    INFO >> epoch 004:   1444 / 1539 loss=3.758, wps=4956, ups=7.21, wpb=687.8, bsz=687.8, num_updates=6050, lr=0.000376, gnorm=6.018, clip=0, train_wall=6, gb_free=72.1, wall=1004 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:49:16]    INFO >> epoch 004:   1494 / 1539 loss=3.784, wps=5004.4, ups=7.19, wpb=695.7, bsz=695.7, num_updates=6100, lr=0.000376, gnorm=5.959, clip=0, train_wall=6, gb_free=73.8, wall=1010 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:49:23]    INFO >> epoch 004 | loss 3.727 | wps 4600.8 | ups 6.4 | wpb 718.6 | bsz 718.6 | num_updates 6145 | lr 0.000376 | gnorm 5.776 | clip 0.3 | train_wall 208 | gb_free 71.3 | wall 1017 (progress_bar.py:267, print())[0m
[33m[2025-11-21 06:49:23] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:49:36]    INFO >> epoch 004 | valid on 'valid' subset | loss 3.819 | wps 11900.4 | wpb 5412.5 | bsz 5412.5 | num_updates 6145 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 06:49:36]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 06:49:36]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 4 @ 6145 updates, score 3.819) (writing took 0.011094 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 06:49:36] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:49:37]    INFO >> epoch 005:      5 / 1539 loss=3.626, wps=1936.9, ups=2.4, wpb=807.4, bsz=807.4, num_updates=6150, lr=0.000354, gnorm=5.523, clip=0, train_wall=7, gb_free=72.2, wall=1031 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:49:45]    INFO >> epoch 005:     55 / 1539 loss=3.814, wps=4565.9, ups=6.39, wpb=714.8, bsz=714.8, num_updates=6200, lr=0.000354, gnorm=5.106, clip=0, train_wall=7, gb_free=71.5, wall=1039 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:49:53]    INFO >> epoch 005:    105 / 1539 loss=3.702, wps=4989.9, ups=7.24, wpb=688.8, bsz=688.8, num_updates=6250, lr=0.000354, gnorm=5.565, clip=0, train_wall=6, gb_free=67.9, wall=1046 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:50:00]    INFO >> epoch 005:    155 / 1539 loss=3.737, wps=4809.3, ups=7.24, wpb=664.2, bsz=664.2, num_updates=6300, lr=0.000354, gnorm=4.629, clip=0, train_wall=6, gb_free=74, wall=1053 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:50:07]    INFO >> epoch 005:    205 / 1539 loss=3.718, wps=4671.3, ups=6.92, wpb=675, bsz=675, num_updates=6350, lr=0.000354, gnorm=5.559, clip=0, train_wall=7, gb_free=71.7, wall=1060 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:50:15]    INFO >> epoch 005:    255 / 1539 loss=3.594, wps=5070.5, ups=6.42, wpb=790, bsz=790, num_updates=6400, lr=0.000354, gnorm=5.449, clip=2, train_wall=7, gb_free=75, wall=1068 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:50:23]    INFO >> epoch 005:    305 / 1539 loss=3.737, wps=5445.6, ups=6.91, wpb=787.6, bsz=787.6, num_updates=6450, lr=0.000354, gnorm=5.482, clip=0, train_wall=7, gb_free=75.1, wall=1075 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:50:31]    INFO >> epoch 005:    355 / 1539 loss=3.515, wps=5270.1, ups=6.92, wpb=761.7, bsz=761.7, num_updates=6500, lr=0.000354, gnorm=5.216, clip=0, train_wall=7, gb_free=73.5, wall=1082 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:50:38]    INFO >> epoch 005:    405 / 1539 loss=3.587, wps=5589.8, ups=6.31, wpb=885.5, bsz=885.5, num_updates=6550, lr=0.000354, gnorm=4.631, clip=0, train_wall=7, gb_free=73.6, wall=1090 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:50:46]    INFO >> epoch 005:    455 / 1539 loss=3.662, wps=4214.7, ups=7.08, wpb=595.1, bsz=595.1, num_updates=6600, lr=0.000354, gnorm=4.925, clip=0, train_wall=7, gb_free=75.5, wall=1097 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:50:53]    INFO >> epoch 005:    505 / 1539 loss=3.632, wps=4742.9, ups=6.61, wpb=718, bsz=718, num_updates=6650, lr=0.000354, gnorm=5.242, clip=0, train_wall=7, gb_free=73.3, wall=1105 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:51:01]    INFO >> epoch 005:    555 / 1539 loss=3.619, wps=4690.9, ups=7.14, wpb=656.7, bsz=656.7, num_updates=6700, lr=0.000354, gnorm=5.698, clip=0, train_wall=7, gb_free=75.1, wall=1112 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:51:11] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 19.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.19 GiB is allocated by PyTorch, and 1.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:51:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:51:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:51:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 25        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78985 MiB |  79045 MiB | 192495 GiB | 192418 GiB |
|       from large pool |  78809 MiB |  78869 MiB | 191310 GiB | 191233 GiB |
|       from small pool |    176 MiB |    177 MiB |   1184 GiB |   1184 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78985 MiB |  79045 MiB | 192495 GiB | 192418 GiB |
|       from large pool |  78809 MiB |  78869 MiB | 191310 GiB | 191233 GiB |
|       from small pool |    176 MiB |    177 MiB |   1184 GiB |   1184 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78955 MiB |  79015 MiB | 192089 GiB | 192011 GiB |
|       from large pool |  78780 MiB |  78839 MiB | 190905 GiB | 190828 GiB |
|       from small pool |    175 MiB |    176 MiB |   1183 GiB |   1183 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80486 MiB |  80504 MiB | 344160 MiB | 263674 MiB |
|       from large pool |  80304 MiB |  80304 MiB | 341040 MiB | 260736 MiB |
|       from small pool |    182 MiB |    440 MiB |   3120 MiB |   2938 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1440 MiB |   6985 MiB | 189266 GiB | 189264 GiB |
|       from large pool |   1434 MiB |   6979 MiB | 187910 GiB | 187909 GiB |
|       from small pool |      5 MiB |     25 MiB |   1355 GiB |   1355 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3436    |    3439    |   13210 K  |   13206 K  |
|       from large pool |     580    |     581    |    6069 K  |    6068 K  |
|       from small pool |    2856    |    2859    |    7140 K  |    7138 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3436    |    3439    |   13210 K  |   13206 K  |
|       from large pool |     580    |     581    |    6069 K  |    6068 K  |
|       from small pool |    2856    |    2859    |    7140 K  |    7138 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     205    |     330    |    3294    |    3089    |
|       from large pool |     114    |     114    |    1734    |    1620    |
|       from small pool |      91    |     220    |    1560    |    1469    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     165    |     165    |    7469 K  |    7469 K  |
|       from large pool |      82    |      83    |    4004 K  |    4004 K  |
|       from small pool |      83    |      83    |    3465 K  |    3465 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:51:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:51:11] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:51:14]    INFO >> epoch 005:    606 / 1539 loss=3.633, wps=2817.5, ups=3.99, wpb=706.8, bsz=706.8, num_updates=6750, lr=0.000354, gnorm=5.278, clip=0, train_wall=7, gb_free=73.2, wall=1125 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:51:21]    INFO >> epoch 005:    656 / 1539 loss=3.671, wps=5303.4, ups=6.7, wpb=791.8, bsz=791.8, num_updates=6800, lr=0.000354, gnorm=5.502, clip=0, train_wall=7, gb_free=73.1, wall=1132 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:51:30]    INFO >> epoch 005:    706 / 1539 loss=3.651, wps=4972.7, ups=6.96, wpb=714, bsz=714, num_updates=6850, lr=0.000354, gnorm=6.436, clip=0, train_wall=7, gb_free=74.5, wall=1139 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:51:37]    INFO >> epoch 005:    756 / 1539 loss=3.713, wps=5046.3, ups=6.88, wpb=733.7, bsz=733.7, num_updates=6900, lr=0.000354, gnorm=5.134, clip=0, train_wall=7, gb_free=74.1, wall=1146 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:51:45]    INFO >> epoch 005:    806 / 1539 loss=3.653, wps=4345.6, ups=6.57, wpb=661, bsz=661, num_updates=6950, lr=0.000354, gnorm=5.111, clip=0, train_wall=7, gb_free=67.5, wall=1154 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:51:52]    INFO >> epoch 005:    856 / 1539 loss=3.594, wps=4722.6, ups=7.39, wpb=638.6, bsz=638.6, num_updates=7000, lr=0.000354, gnorm=5.208, clip=0, train_wall=6, gb_free=74.9, wall=1161 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:51:59]    INFO >> epoch 005:    906 / 1539 loss=3.597, wps=4851.1, ups=6.81, wpb=712.6, bsz=712.6, num_updates=7050, lr=0.000354, gnorm=4.962, clip=0, train_wall=7, gb_free=73, wall=1168 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:52:07] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 241.25 MiB is free. Including non-PyTorch memory, this process has 78.88 GiB memory in use. Of the allocated memory 74.09 GiB is allocated by PyTorch, and 4.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:52:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:52:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:52:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72057 MiB |  75867 MiB | 201896 GiB | 201826 GiB |
|       from large pool |  72042 MiB |  75852 MiB | 200660 GiB | 200589 GiB |
|       from small pool |     15 MiB |     21 MiB |   1236 GiB |   1236 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72057 MiB |  75867 MiB | 201896 GiB | 201826 GiB |
|       from large pool |  72042 MiB |  75852 MiB | 200660 GiB | 200589 GiB |
|       from small pool |     15 MiB |     21 MiB |   1236 GiB |   1236 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  75850 MiB | 201469 GiB | 201398 GiB |
|       from large pool |  72027 MiB |  75835 MiB | 200234 GiB | 200164 GiB |
|       from small pool |     15 MiB |     21 MiB |   1234 GiB |   1234 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80264 MiB |  80426 MiB | 344160 MiB | 263896 MiB |
|       from large pool |  80244 MiB |  80244 MiB | 341040 MiB | 260796 MiB |
|       from small pool |     20 MiB |    182 MiB |   3120 MiB |   3100 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2750 MiB |   7203 MiB | 198828 GiB | 198825 GiB |
|       from large pool |   2745 MiB |   7193 MiB | 197413 GiB | 197411 GiB |
|       from small pool |      4 MiB |     17 MiB |   1414 GiB |   1414 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     511    |   13826 K  |   13826 K  |
|       from large pool |     308    |     317    |    6382 K  |    6382 K  |
|       from small pool |     194    |     240    |    7444 K  |    7443 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     511    |   13826 K  |   13826 K  |
|       from large pool |     308    |     317    |    6382 K  |    6382 K  |
|       from small pool |     194    |     240    |    7444 K  |    7443 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     123    |     204    |    3294    |    3171    |
|       from large pool |     113    |     113    |    1734    |    1621    |
|       from small pool |      10    |      91    |    1560    |    1550    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     111    |     111    |    7806 K  |    7806 K  |
|       from large pool |      91    |      91    |    4208 K  |    4208 K  |
|       from small pool |      20    |      42    |    3598 K  |    3598 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:52:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:52:07] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:52:08]    INFO >> epoch 005:    957 / 1539 loss=3.705, wps=4377, ups=6.55, wpb=668.1, bsz=668.1, num_updates=7100, lr=0.000354, gnorm=5.25, clip=0, train_wall=6, gb_free=71.4, wall=1176 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:52:16]    INFO >> epoch 005:   1007 / 1539 loss=3.343, wps=5732.3, ups=6.08, wpb=942.7, bsz=942.7, num_updates=7150, lr=0.000354, gnorm=5.867, clip=2, train_wall=8, gb_free=70.5, wall=1184 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:52:24]    INFO >> epoch 005:   1057 / 1539 loss=3.57, wps=5003, ups=6.51, wpb=768, bsz=768, num_updates=7200, lr=0.000354, gnorm=5.079, clip=0, train_wall=7, gb_free=76.9, wall=1192 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:52:31]    INFO >> epoch 005:   1107 / 1539 loss=3.652, wps=4749.6, ups=7.17, wpb=662.3, bsz=662.3, num_updates=7250, lr=0.000354, gnorm=4.869, clip=0, train_wall=6, gb_free=72.1, wall=1199 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:52:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 139.25 MiB is free. Including non-PyTorch memory, this process has 78.98 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:52:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:52:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:52:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76954 MiB |  77477 MiB | 206802 GiB | 206727 GiB |
|       from large pool |  76944 MiB |  77466 MiB | 205536 GiB | 205461 GiB |
|       from small pool |     10 MiB |     18 MiB |   1265 GiB |   1265 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76954 MiB |  77477 MiB | 206802 GiB | 206727 GiB |
|       from large pool |  76944 MiB |  77466 MiB | 205536 GiB | 205461 GiB |
|       from small pool |     10 MiB |     18 MiB |   1265 GiB |   1265 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76934 MiB |  77455 MiB | 206365 GiB | 206290 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 205101 GiB | 205026 GiB |
|       from small pool |     10 MiB |     18 MiB |   1263 GiB |   1263 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80366 MiB |  80366 MiB | 351058 MiB | 270692 MiB |
|       from large pool |  80344 MiB |  80344 MiB | 347736 MiB | 267392 MiB |
|       from small pool |     22 MiB |    222 MiB |   3322 MiB |   3300 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3411 MiB |   7797 MiB | 203895 GiB | 203892 GiB |
|       from large pool |   3399 MiB |   7786 MiB | 202446 GiB | 202443 GiB |
|       from small pool |     11 MiB |     23 MiB |   1448 GiB |   1448 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     509    |   14157 K  |   14157 K  |
|       from large pool |     315    |     322    |    6538 K  |    6538 K  |
|       from small pool |     187    |     240    |    7619 K  |    7618 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     509    |   14157 K  |   14157 K  |
|       from large pool |     315    |     322    |    6538 K  |    6538 K  |
|       from small pool |     187    |     240    |    7619 K  |    7618 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     111    |     229    |    3403    |    3292    |
|       from large pool |     100    |     118    |    1742    |    1642    |
|       from small pool |      11    |     111    |    1661    |    1650    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     100    |     100    |    7992 K  |    7992 K  |
|       from large pool |      79    |      79    |    4310 K  |    4310 K  |
|       from small pool |      21    |      46    |    3682 K  |    3682 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:52:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:52:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:52:40]    INFO >> epoch 005:   1158 / 1539 loss=3.517, wps=4756.2, ups=6.12, wpb=777.1, bsz=777.1, num_updates=7300, lr=0.000354, gnorm=5.382, clip=0, train_wall=7, gb_free=73.5, wall=1207 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:52:47]    INFO >> epoch 005:   1208 / 1539 loss=3.826, wps=4324.3, ups=7.25, wpb=596.4, bsz=596.4, num_updates=7350, lr=0.000354, gnorm=4.441, clip=0, train_wall=6, gb_free=71.4, wall=1214 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:52:54]    INFO >> epoch 005:   1258 / 1539 loss=3.508, wps=5068.9, ups=7.36, wpb=688.6, bsz=688.6, num_updates=7400, lr=0.000354, gnorm=5.27, clip=2, train_wall=6, gb_free=75.8, wall=1221 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:53:01]    INFO >> epoch 005:   1308 / 1539 loss=3.682, wps=4739.2, ups=7.27, wpb=651.5, bsz=651.5, num_updates=7450, lr=0.000354, gnorm=4.656, clip=0, train_wall=6, gb_free=73.6, wall=1227 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:53:08]    INFO >> epoch 005:   1358 / 1539 loss=3.572, wps=4969, ups=6.88, wpb=722.6, bsz=722.6, num_updates=7500, lr=0.000354, gnorm=5.101, clip=0, train_wall=7, gb_free=72.1, wall=1235 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:53:17]    INFO >> epoch 005:   1408 / 1539 loss=3.769, wps=4569.5, ups=7.07, wpb=646.4, bsz=646.4, num_updates=7550, lr=0.000354, gnorm=4.923, clip=0, train_wall=7, gb_free=71.9, wall=1242 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:53:24]    INFO >> epoch 005:   1458 / 1539 loss=3.497, wps=5355.1, ups=7.12, wpb=752.5, bsz=752.5, num_updates=7600, lr=0.000354, gnorm=6.311, clip=2, train_wall=7, gb_free=72.4, wall=1249 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:53:31]    INFO >> epoch 005:   1508 / 1539 loss=3.63, wps=5026, ups=6.7, wpb=750.5, bsz=750.5, num_updates=7650, lr=0.000354, gnorm=4.977, clip=0, train_wall=7, gb_free=75.9, wall=1256 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:53:36]    INFO >> epoch 005 | loss 3.63 | wps 4529.5 | ups 6.3 | wpb 718.6 | bsz 718.6 | num_updates 7681 | lr 0.000354 | gnorm 5.278 | clip 0.3 | train_wall 207 | gb_free 65.4 | wall 1261 (progress_bar.py:267, print())[0m
[33m[2025-11-21 06:53:36] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:53:50]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.76 | wps 11906.5 | wpb 5412.5 | bsz 5412.5 | num_updates 7681 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 06:53:50]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 06:53:50]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 5 @ 7681 updates, score 3.76) (writing took 0.011221 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 06:53:50] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:53:53]    INFO >> epoch 006:     19 / 1539 loss=3.668, wps=1868.9, ups=2.42, wpb=773.3, bsz=773.3, num_updates=7700, lr=0.000327, gnorm=5.972, clip=0, train_wall=7, gb_free=68.3, wall=1277 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:54:01]    INFO >> epoch 006:     69 / 1539 loss=3.618, wps=5430.3, ups=6.46, wpb=841.2, bsz=841.2, num_updates=7750, lr=0.000327, gnorm=4.978, clip=0, train_wall=7, gb_free=75.1, wall=1285 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:54:08]    INFO >> epoch 006:    119 / 1539 loss=3.557, wps=4939.7, ups=7.19, wpb=687.2, bsz=687.2, num_updates=7800, lr=0.000327, gnorm=4.724, clip=0, train_wall=6, gb_free=73.7, wall=1292 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:54:15]    INFO >> epoch 006:    169 / 1539 loss=3.642, wps=4817.3, ups=6.87, wpb=701.3, bsz=701.3, num_updates=7850, lr=0.000327, gnorm=5.053, clip=0, train_wall=7, gb_free=73, wall=1299 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:54:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 139.25 MiB is free. Including non-PyTorch memory, this process has 78.98 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 4.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:54:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 29        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75051 MiB |  75994 MiB | 226272 GiB | 226199 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 224883 GiB | 224809 GiB |
|       from small pool |     10 MiB |     12 MiB |   1389 GiB |   1389 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75051 MiB |  75994 MiB | 226272 GiB | 226199 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 224883 GiB | 224809 GiB |
|       from small pool |     10 MiB |     12 MiB |   1389 GiB |   1389 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75032 MiB |  75975 MiB | 225797 GiB | 225724 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 224409 GiB | 224336 GiB |
|       from small pool |     10 MiB |     12 MiB |   1387 GiB |   1387 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80366 MiB |  80440 MiB | 351132 MiB | 270766 MiB |
|       from large pool |  80344 MiB |  80344 MiB | 347736 MiB | 267392 MiB |
|       from small pool |     22 MiB |     96 MiB |   3396 MiB |   3374 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5314 MiB |   9037 MiB | 223036 GiB | 223030 GiB |
|       from large pool |   5303 MiB |   9025 MiB | 221450 GiB | 221444 GiB |
|       from small pool |     11 MiB |     21 MiB |   1585 GiB |   1585 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     501    |     509    |   15474 K  |   15473 K  |
|       from large pool |     314    |     322    |    7099 K  |    7098 K  |
|       from small pool |     187    |     240    |    8375 K  |    8375 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     501    |     509    |   15474 K  |   15473 K  |
|       from large pool |     314    |     322    |    7099 K  |    7098 K  |
|       from small pool |     187    |     240    |    8375 K  |    8375 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     111    |     148    |    3440    |    3329    |
|       from large pool |     100    |     100    |    1742    |    1642    |
|       from small pool |      11    |      48    |    1698    |    1687    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     105    |     107    |    8744 K  |    8743 K  |
|       from large pool |      86    |      88    |    4673 K  |    4673 K  |
|       from small pool |      19    |      42    |    4070 K  |    4070 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:54:24]    INFO >> epoch 006:    220 / 1539 loss=3.565, wps=4621, ups=6.59, wpb=701.4, bsz=701.4, num_updates=7900, lr=0.000327, gnorm=4.632, clip=0, train_wall=6, gb_free=75.3, wall=1307 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:54:25] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 75.07 GiB is allocated by PyTorch, and 3.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:54:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76872 MiB |  76932 MiB | 227324 GiB | 227249 GiB |
|       from large pool |  76717 MiB |  76776 MiB | 225928 GiB | 225854 GiB |
|       from small pool |    155 MiB |    156 MiB |   1395 GiB |   1395 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76872 MiB |  76932 MiB | 227324 GiB | 227249 GiB |
|       from large pool |  76717 MiB |  76776 MiB | 225928 GiB | 225854 GiB |
|       from small pool |    155 MiB |    156 MiB |   1395 GiB |   1395 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76851 MiB |  76911 MiB | 226847 GiB | 226772 GiB |
|       from large pool |  76697 MiB |  76756 MiB | 225453 GiB | 225378 GiB |
|       from small pool |    154 MiB |    155 MiB |   1393 GiB |   1393 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB | 351270 MiB | 270766 MiB |
|       from large pool |  80344 MiB |  80344 MiB | 347736 MiB | 267392 MiB |
|       from small pool |    160 MiB |    160 MiB |   3534 MiB |   3374 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3631 MiB |   6898 MiB | 224155 GiB | 224152 GiB |
|       from large pool |   3626 MiB |   6894 MiB | 222562 GiB | 222558 GiB |
|       from small pool |      4 MiB |     21 MiB |   1593 GiB |   1593 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3044    |    3045    |   15542 K  |   15539 K  |
|       from large pool |     545    |     546    |    7129 K  |    7128 K  |
|       from small pool |    2499    |    2500    |    8413 K  |    8410 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3044    |    3045    |   15542 K  |   15539 K  |
|       from large pool |     545    |     546    |    7129 K  |    7128 K  |
|       from small pool |    2499    |    2500    |    8413 K  |    8410 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     180    |     180    |    3509    |    3329    |
|       from large pool |     100    |     100    |    1742    |    1642    |
|       from small pool |      80    |      80    |    1767    |    1687    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     154    |     157    |    8782 K  |    8782 K  |
|       from large pool |      80    |      83    |    4692 K  |    4692 K  |
|       from small pool |      74    |      77    |    4089 K  |    4089 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:25] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:54:32]    INFO >> epoch 006:    271 / 1539 loss=3.64, wps=4173.2, ups=6.5, wpb=641.8, bsz=641.8, num_updates=7950, lr=0.000327, gnorm=4.725, clip=0, train_wall=6, gb_free=75, wall=1314 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:54:39]    INFO >> epoch 006:    321 / 1539 loss=3.543, wps=5435.4, ups=6.52, wpb=834, bsz=834, num_updates=8000, lr=0.000327, gnorm=5.408, clip=0, train_wall=7, gb_free=72.9, wall=1322 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:54:47]    INFO >> epoch 006:    371 / 1539 loss=3.676, wps=4426.5, ups=6.79, wpb=651.5, bsz=651.5, num_updates=8050, lr=0.000327, gnorm=4.397, clip=0, train_wall=7, gb_free=74.4, wall=1329 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:54:55]    INFO >> epoch 006:    421 / 1539 loss=3.595, wps=5049.2, ups=7.33, wpb=688.9, bsz=688.9, num_updates=8100, lr=0.000327, gnorm=5.425, clip=0, train_wall=6, gb_free=73.9, wall=1336 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:54:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.65 GiB is free. Including non-PyTorch memory, this process has 76.47 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 5.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:54:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72054 MiB |  72454 MiB | 233017 GiB | 232946 GiB |
|       from large pool |  72039 MiB |  72439 MiB | 231588 GiB | 231517 GiB |
|       from small pool |     15 MiB |     18 MiB |   1429 GiB |   1429 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72054 MiB |  72454 MiB | 233017 GiB | 232946 GiB |
|       from large pool |  72039 MiB |  72439 MiB | 231588 GiB | 231517 GiB |
|       from small pool |     15 MiB |     18 MiB |   1429 GiB |   1429 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  72442 MiB | 232526 GiB | 232456 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 231099 GiB | 231029 GiB |
|       from small pool |     15 MiB |     18 MiB |   1427 GiB |   1427 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77794 MiB |  77984 MiB | 386472 MiB | 308678 MiB |
|       from large pool |  77776 MiB |  77776 MiB | 382890 MiB | 305114 MiB |
|       from small pool |     18 MiB |    208 MiB |   3582 MiB |   3564 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5739 MiB |   7725 MiB | 229792 GiB | 229786 GiB |
|       from large pool |   5736 MiB |   7720 MiB | 228159 GiB | 228154 GiB |
|       from small pool |      2 MiB |     23 MiB |   1632 GiB |   1632 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     509    |   15935 K  |   15934 K  |
|       from large pool |     308    |     315    |    7321 K  |    7321 K  |
|       from small pool |     194    |     240    |    8613 K  |    8613 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     509    |   15935 K  |   15934 K  |
|       from large pool |     308    |     315    |    7321 K  |    7321 K  |
|       from small pool |     194    |     240    |    8613 K  |    8613 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     146    |     241    |    3584    |    3438    |
|       from large pool |     137    |     137    |    1793    |    1656    |
|       from small pool |       9    |     104    |    1791    |    1782    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     137    |     137    |    9004 K  |    9004 K  |
|       from large pool |     119    |     119    |    4820 K  |    4820 K  |
|       from small pool |      18    |      46    |    4184 K  |    4184 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:54:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:55:03]    INFO >> epoch 006:    472 / 1539 loss=3.775, wps=4958.4, ups=6.53, wpb=759.1, bsz=759.1, num_updates=8150, lr=0.000327, gnorm=5.099, clip=0, train_wall=7, gb_free=74, wall=1344 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:55:10]    INFO >> epoch 006:    522 / 1539 loss=3.581, wps=4867.4, ups=6.64, wpb=733.1, bsz=733.1, num_updates=8200, lr=0.000327, gnorm=4.801, clip=0, train_wall=7, gb_free=71.8, wall=1351 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:55:18]    INFO >> epoch 006:    572 / 1539 loss=3.673, wps=5371, ups=6.76, wpb=794, bsz=794, num_updates=8250, lr=0.000327, gnorm=4.612, clip=0, train_wall=7, gb_free=73.3, wall=1359 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:55:26]    INFO >> epoch 006:    622 / 1539 loss=3.561, wps=4750, ups=7.43, wpb=639.3, bsz=639.3, num_updates=8300, lr=0.000327, gnorm=4.814, clip=0, train_wall=6, gb_free=75.1, wall=1365 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:55:33]    INFO >> epoch 006:    672 / 1539 loss=3.668, wps=5029.2, ups=6.95, wpb=723.8, bsz=723.8, num_updates=8350, lr=0.000327, gnorm=4.617, clip=0, train_wall=7, gb_free=73, wall=1373 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:55:39]    INFO >> epoch 006:    722 / 1539 loss=3.552, wps=4844.3, ups=7.64, wpb=634.1, bsz=634.1, num_updates=8400, lr=0.000327, gnorm=4.3, clip=0, train_wall=6, gb_free=75.4, wall=1379 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:55:47]    INFO >> epoch 006:    772 / 1539 loss=3.401, wps=5307.9, ups=6.44, wpb=823.8, bsz=823.8, num_updates=8450, lr=0.000327, gnorm=4.857, clip=0, train_wall=7, gb_free=72.3, wall=1387 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:55:54]    INFO >> epoch 006:    822 / 1539 loss=3.681, wps=4455.6, ups=7.13, wpb=624.8, bsz=624.8, num_updates=8500, lr=0.000327, gnorm=4.408, clip=0, train_wall=7, gb_free=72.4, wall=1394 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:56:02]    INFO >> epoch 006:    872 / 1539 loss=3.531, wps=5155.4, ups=7.11, wpb=725.2, bsz=725.2, num_updates=8550, lr=0.000327, gnorm=4.561, clip=0, train_wall=7, gb_free=73.1, wall=1401 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:56:10]    INFO >> epoch 006:    922 / 1539 loss=3.711, wps=5061, ups=6.82, wpb=741.9, bsz=741.9, num_updates=8600, lr=0.000327, gnorm=5.292, clip=0, train_wall=7, gb_free=70.1, wall=1408 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:56:18]    INFO >> epoch 006:    972 / 1539 loss=3.221, wps=5126.3, ups=6.23, wpb=822.9, bsz=822.9, num_updates=8650, lr=0.000327, gnorm=5.61, clip=0, train_wall=7, gb_free=72.2, wall=1416 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:56:25]    INFO >> epoch 006:   1022 / 1539 loss=3.501, wps=4709.9, ups=6.64, wpb=709.4, bsz=709.4, num_updates=8700, lr=0.000327, gnorm=4.843, clip=0, train_wall=7, gb_free=74.9, wall=1424 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:56:35]    INFO >> epoch 006:   1072 / 1539 loss=3.744, wps=4636.3, ups=5.84, wpb=794.2, bsz=794.2, num_updates=8750, lr=0.000327, gnorm=5.198, clip=0, train_wall=8, gb_free=68, wall=1432 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:56:43]    INFO >> epoch 006:   1122 / 1539 loss=3.56, wps=4701.9, ups=6.25, wpb=752.3, bsz=752.3, num_updates=8800, lr=0.000327, gnorm=5.068, clip=0, train_wall=7, gb_free=74, wall=1440 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:56:50]    INFO >> epoch 006:   1172 / 1539 loss=3.676, wps=5065.4, ups=6.98, wpb=725.2, bsz=725.2, num_updates=8850, lr=0.000327, gnorm=5.563, clip=0, train_wall=7, gb_free=67, wall=1448 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:56:58]    INFO >> epoch 006:   1222 / 1539 loss=3.688, wps=4397.3, ups=6.43, wpb=683.4, bsz=683.4, num_updates=8900, lr=0.000327, gnorm=4.943, clip=0, train_wall=7, gb_free=74.2, wall=1455 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:57:06]    INFO >> epoch 006:   1272 / 1539 loss=3.646, wps=4625.4, ups=7.26, wpb=637.1, bsz=637.1, num_updates=8950, lr=0.000327, gnorm=4.389, clip=0, train_wall=6, gb_free=74.6, wall=1462 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:57:14]    INFO >> epoch 006:   1322 / 1539 loss=3.446, wps=5059.8, ups=6.72, wpb=752.8, bsz=752.8, num_updates=9000, lr=0.000327, gnorm=5.44, clip=0, train_wall=7, gb_free=74.1, wall=1470 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:57:21]    INFO >> epoch 006:   1372 / 1539 loss=3.622, wps=4190, ups=7.18, wpb=583.3, bsz=583.3, num_updates=9050, lr=0.000327, gnorm=4.347, clip=0, train_wall=6, gb_free=68.4, wall=1477 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:57:28]    INFO >> epoch 006:   1422 / 1539 loss=3.592, wps=5194.9, ups=6.87, wpb=756.3, bsz=756.3, num_updates=9100, lr=0.000327, gnorm=5.048, clip=0, train_wall=7, gb_free=68, wall=1484 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:57:36]    INFO >> epoch 006:   1472 / 1539 loss=3.291, wps=4653.1, ups=6.42, wpb=725.1, bsz=725.1, num_updates=9150, lr=0.000327, gnorm=5.968, clip=0, train_wall=7, gb_free=67.8, wall=1492 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:57:45]    INFO >> epoch 006:   1522 / 1539 loss=3.659, wps=4571.7, ups=6.83, wpb=669.4, bsz=669.4, num_updates=9200, lr=0.000327, gnorm=4.339, clip=0, train_wall=7, gb_free=68.4, wall=1499 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:57:47]    INFO >> epoch 006 | loss 3.587 | wps 4589 | ups 6.39 | wpb 718.6 | bsz 718.6 | num_updates 9217 | lr 0.000327 | gnorm 4.891 | clip 0 | train_wall 209 | gb_free 73.2 | wall 1502 (progress_bar.py:267, print())[0m
[33m[2025-11-21 06:57:47] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:58:00]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.765 | wps 11373 | wpb 5412.5 | bsz 5412.5 | num_updates 9217 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 06:58:01]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 06:58:01]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 6 @ 9217 updates, score 3.765) (writing took 0.013810 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 06:58:01] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 06:58:07]    INFO >> epoch 007:     33 / 1539 loss=3.622, wps=1633.7, ups=2.26, wpb=723.6, bsz=723.6, num_updates=9250, lr=0.000295, gnorm=4.728, clip=0, train_wall=8, gb_free=64.8, wall=1521 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:58:09] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 2.64 GiB is free. Including non-PyTorch memory, this process has 76.47 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 5.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:58:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:58:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:58:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 33        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72054 MiB |  72454 MiB | 267074 GiB | 267004 GiB |
|       from large pool |  72039 MiB |  72439 MiB | 265434 GiB | 265363 GiB |
|       from small pool |     15 MiB |     22 MiB |   1640 GiB |   1640 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72054 MiB |  72454 MiB | 267074 GiB | 267004 GiB |
|       from large pool |  72039 MiB |  72439 MiB | 265434 GiB | 265363 GiB |
|       from small pool |     15 MiB |     22 MiB |   1640 GiB |   1640 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  72442 MiB | 266512 GiB | 266441 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 264873 GiB | 264803 GiB |
|       from small pool |     15 MiB |     22 MiB |   1638 GiB |   1638 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77798 MiB |  78216 MiB | 386894 MiB | 309096 MiB |
|       from large pool |  77776 MiB |  77776 MiB | 382890 MiB | 305114 MiB |
|       from small pool |     22 MiB |    440 MiB |   4004 MiB |   3982 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5743 MiB |   7729 MiB | 259888 GiB | 259883 GiB |
|       from large pool |   5736 MiB |   7720 MiB | 258015 GiB | 258009 GiB |
|       from small pool |      6 MiB |     31 MiB |   1873 GiB |   1873 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     509    |   18259 K  |   18259 K  |
|       from large pool |     308    |     315    |    8365 K  |    8364 K  |
|       from small pool |     194    |     240    |    9894 K  |    9894 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     509    |   18259 K  |   18259 K  |
|       from large pool |     308    |     315    |    8365 K  |    8364 K  |
|       from small pool |     194    |     240    |    9894 K  |    9894 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     148    |     357    |    3795    |    3647    |
|       from large pool |     137    |     137    |    1793    |    1656    |
|       from small pool |      11    |     220    |    2002    |    1991    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     139    |     139    |   10348 K  |   10348 K  |
|       from large pool |     119    |     119    |    5524 K  |    5524 K  |
|       from small pool |      20    |      52    |    4823 K  |    4823 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:58:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:58:09] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:58:16]    INFO >> epoch 007:     84 / 1539 loss=3.563, wps=4257.8, ups=6.58, wpb=647.3, bsz=647.3, num_updates=9300, lr=0.000295, gnorm=4.727, clip=0, train_wall=6, gb_free=72.7, wall=1529 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:58:23]    INFO >> epoch 007:    134 / 1539 loss=3.603, wps=5012.9, ups=7.27, wpb=689.3, bsz=689.3, num_updates=9350, lr=0.000295, gnorm=5.027, clip=0, train_wall=6, gb_free=72.5, wall=1536 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:58:30]    INFO >> epoch 007:    184 / 1539 loss=3.531, wps=5677.5, ups=6.66, wpb=852.2, bsz=852.2, num_updates=9400, lr=0.000295, gnorm=4.467, clip=0, train_wall=7, gb_free=75.4, wall=1543 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:58:37]    INFO >> epoch 007:    234 / 1539 loss=3.53, wps=5430.3, ups=6.74, wpb=805.3, bsz=805.3, num_updates=9450, lr=0.000295, gnorm=5.322, clip=0, train_wall=7, gb_free=71.8, wall=1551 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:58:46]    INFO >> epoch 007:    284 / 1539 loss=3.587, wps=4835.3, ups=6.66, wpb=726.5, bsz=726.5, num_updates=9500, lr=0.000295, gnorm=4.514, clip=0, train_wall=7, gb_free=73.9, wall=1558 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:58:53] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 25.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:58:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:58:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:58:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 35        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79180 MiB |  79240 MiB | 274354 GiB | 274276 GiB |
|       from large pool |  79002 MiB |  79062 MiB | 272669 GiB | 272592 GiB |
|       from small pool |    178 MiB |    179 MiB |   1684 GiB |   1684 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79180 MiB |  79240 MiB | 274354 GiB | 274276 GiB |
|       from large pool |  79002 MiB |  79062 MiB | 272669 GiB | 272592 GiB |
|       from small pool |    178 MiB |    179 MiB |   1684 GiB |   1684 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79136 MiB |  79195 MiB | 273774 GiB | 273697 GiB |
|       from large pool |  78958 MiB |  79018 MiB | 272092 GiB | 272015 GiB |
|       from small pool |    177 MiB |    178 MiB |   1682 GiB |   1682 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80480 MiB |  80480 MiB | 389616 MiB | 309136 MiB |
|       from large pool |  80296 MiB |  80296 MiB | 385410 MiB | 305114 MiB |
|       from small pool |    184 MiB |    222 MiB |   4206 MiB |   4022 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1239 MiB |   9307 MiB | 266716 GiB | 266715 GiB |
|       from large pool |   1233 MiB |   9301 MiB | 264791 GiB | 264790 GiB |
|       from small pool |      5 MiB |     19 MiB |   1924 GiB |   1924 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3469    |    3472    |   18762 K  |   18759 K  |
|       from large pool |     583    |     584    |    8607 K  |    8607 K  |
|       from small pool |    2886    |    2889    |   10154 K  |   10152 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3469    |    3472    |   18762 K  |   18759 K  |
|       from large pool |     583    |     584    |    8607 K  |    8607 K  |
|       from small pool |    2886    |    2889    |   10154 K  |   10152 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     271    |     289    |    3938    |    3667    |
|       from large pool |     179    |     179    |    1835    |    1656    |
|       from small pool |      92    |     111    |    2103    |    2011    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     190    |     191    |   10633 K  |   10633 K  |
|       from large pool |     105    |     109    |    5686 K  |    5686 K  |
|       from small pool |      85    |      86    |    4946 K  |    4946 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:58:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:58:53] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:58:55]    INFO >> epoch 007:    335 / 1539 loss=3.65, wps=3888.8, ups=5.87, wpb=662, bsz=662, num_updates=9550, lr=0.000295, gnorm=4.082, clip=0, train_wall=7, gb_free=75.9, wall=1567 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:59:02]    INFO >> epoch 007:    385 / 1539 loss=3.582, wps=4314.9, ups=7.15, wpb=603.8, bsz=603.8, num_updates=9600, lr=0.000295, gnorm=4.848, clip=0, train_wall=6, gb_free=74, wall=1574 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:59:09]    INFO >> epoch 007:    435 / 1539 loss=3.543, wps=4278.5, ups=6.8, wpb=629.3, bsz=629.3, num_updates=9650, lr=0.000295, gnorm=4.421, clip=0, train_wall=7, gb_free=72.5, wall=1581 (progress_bar.py:258, log())[0m
[33m[2025-11-21 06:59:15] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 1.07 GiB is free. Including non-PyTorch memory, this process has 78.05 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 3.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 06:59:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:59:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:59:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 37        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75050 MiB |  75993 MiB | 278269 GiB | 278196 GiB |
|       from large pool |  75039 MiB |  75982 MiB | 276565 GiB | 276492 GiB |
|       from small pool |     10 MiB |     15 MiB |   1704 GiB |   1704 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75050 MiB |  75993 MiB | 278269 GiB | 278196 GiB |
|       from large pool |  75039 MiB |  75982 MiB | 276565 GiB | 276492 GiB |
|       from small pool |     10 MiB |     15 MiB |   1704 GiB |   1704 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75032 MiB |  75975 MiB | 277681 GiB | 277608 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 275979 GiB | 275906 GiB |
|       from small pool |     10 MiB |     15 MiB |   1701 GiB |   1701 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79414 MiB |  80420 MiB | 392026 MiB | 312612 MiB |
|       from large pool |  79394 MiB |  80236 MiB | 387820 MiB | 308426 MiB |
|       from small pool |     20 MiB |    184 MiB |   4206 MiB |   4186 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4363 MiB |   8679 MiB | 270101 GiB | 270097 GiB |
|       from large pool |   4354 MiB |   8670 MiB | 268153 GiB | 268149 GiB |
|       from small pool |      9 MiB |     21 MiB |   1947 GiB |   1947 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     501    |     509    |   19005 K  |   19005 K  |
|       from large pool |     314    |     322    |    8733 K  |    8733 K  |
|       from small pool |     187    |     240    |   10272 K  |   10272 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     501    |     509    |   19005 K  |   19005 K  |
|       from large pool |     314    |     322    |    8733 K  |    8733 K  |
|       from small pool |     187    |     240    |   10272 K  |   10272 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     152    |     270    |    3943    |    3791    |
|       from large pool |     142    |     178    |    1840    |    1698    |
|       from small pool |      10    |      92    |    2103    |    2093    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     142    |     142    |   10770 K  |   10770 K  |
|       from large pool |     123    |     123    |    5773 K  |    5773 K  |
|       from small pool |      19    |      42    |    4996 K  |    4996 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:59:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 06:59:15] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 06:59:17]    INFO >> epoch 007:    486 / 1539 loss=3.614, wps=4766.9, ups=6.09, wpb=782.3, bsz=782.3, num_updates=9700, lr=0.000295, gnorm=4.824, clip=0, train_wall=7, gb_free=72.3, wall=1589 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:59:26]    INFO >> epoch 007:    536 / 1539 loss=3.458, wps=4830, ups=7.17, wpb=673.4, bsz=673.4, num_updates=9750, lr=0.000295, gnorm=4.526, clip=0, train_wall=6, gb_free=75, wall=1596 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:59:33]    INFO >> epoch 007:    586 / 1539 loss=3.718, wps=4733.9, ups=7.12, wpb=665.3, bsz=665.3, num_updates=9800, lr=0.000295, gnorm=4.732, clip=0, train_wall=7, gb_free=73.7, wall=1603 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:59:40]    INFO >> epoch 007:    636 / 1539 loss=3.513, wps=4532.2, ups=6.76, wpb=670.8, bsz=670.8, num_updates=9850, lr=0.000295, gnorm=5.532, clip=2, train_wall=7, gb_free=75.1, wall=1611 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:59:48]    INFO >> epoch 007:    686 / 1539 loss=3.401, wps=5544.4, ups=6.56, wpb=845.3, bsz=845.3, num_updates=9900, lr=0.000295, gnorm=5.024, clip=0, train_wall=7, gb_free=72.3, wall=1618 (progress_bar.py:258, log())[0m
[32m[2025-11-21 06:59:56]    INFO >> epoch 007:    736 / 1539 loss=3.618, wps=5020.1, ups=6.91, wpb=726.6, bsz=726.6, num_updates=9950, lr=0.000295, gnorm=4.188, clip=0, train_wall=7, gb_free=75.1, wall=1625 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:00:04]    INFO >> epoch 007:    786 / 1539 loss=3.637, wps=4482.4, ups=6.93, wpb=646.6, bsz=646.6, num_updates=10000, lr=0.000295, gnorm=4.139, clip=0, train_wall=7, gb_free=72.6, wall=1633 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:00:11]    INFO >> epoch 007:    836 / 1539 loss=3.568, wps=4995.7, ups=6.92, wpb=722, bsz=722, num_updates=10050, lr=0.000295, gnorm=4.232, clip=0, train_wall=7, gb_free=73.1, wall=1640 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:00:18]    INFO >> epoch 007:    886 / 1539 loss=3.623, wps=4684.8, ups=6.84, wpb=684.6, bsz=684.6, num_updates=10100, lr=0.000295, gnorm=4.551, clip=0, train_wall=7, gb_free=64.9, wall=1647 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:00:25]    INFO >> epoch 007:    936 / 1539 loss=3.469, wps=4842.6, ups=7.39, wpb=655.4, bsz=655.4, num_updates=10150, lr=0.000295, gnorm=4.948, clip=0, train_wall=6, gb_free=74.6, wall=1654 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:00:34]    INFO >> epoch 007:    986 / 1539 loss=3.514, wps=5179.7, ups=6.72, wpb=770.6, bsz=770.6, num_updates=10200, lr=0.000295, gnorm=4.915, clip=0, train_wall=7, gb_free=73.7, wall=1661 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:00:41]    INFO >> epoch 007:   1036 / 1539 loss=3.451, wps=4666.8, ups=6.67, wpb=699.9, bsz=699.9, num_updates=10250, lr=0.000295, gnorm=5.163, clip=0, train_wall=7, gb_free=72.6, wall=1669 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:00:49]    INFO >> epoch 007:   1086 / 1539 loss=3.585, wps=4667.5, ups=6.57, wpb=710.2, bsz=710.2, num_updates=10300, lr=0.000295, gnorm=4.573, clip=0, train_wall=7, gb_free=75.3, wall=1676 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:00:57]    INFO >> epoch 007:   1136 / 1539 loss=3.484, wps=5025.8, ups=6.37, wpb=788.4, bsz=788.4, num_updates=10350, lr=0.000295, gnorm=4.103, clip=0, train_wall=7, gb_free=72.1, wall=1684 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:01:06]    INFO >> epoch 007:   1186 / 1539 loss=3.516, wps=5220.4, ups=6.43, wpb=812, bsz=812, num_updates=10400, lr=0.000295, gnorm=4.696, clip=0, train_wall=7, gb_free=74.8, wall=1692 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:01:13]    INFO >> epoch 007:   1236 / 1539 loss=3.472, wps=4772.9, ups=7.09, wpb=673.4, bsz=673.4, num_updates=10450, lr=0.000295, gnorm=4.663, clip=0, train_wall=7, gb_free=75.3, wall=1699 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:01:20]    INFO >> epoch 007:   1286 / 1539 loss=3.512, wps=4561.6, ups=6.88, wpb=663.1, bsz=663.1, num_updates=10500, lr=0.000295, gnorm=4.452, clip=0, train_wall=7, gb_free=75.7, wall=1706 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:01:27]    INFO >> epoch 007:   1336 / 1539 loss=3.611, wps=4759.3, ups=7.02, wpb=678.4, bsz=678.4, num_updates=10550, lr=0.000295, gnorm=4.345, clip=0, train_wall=7, gb_free=74.3, wall=1714 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:01:36]    INFO >> epoch 007:   1386 / 1539 loss=3.473, wps=4478.6, ups=6.8, wpb=658.5, bsz=658.5, num_updates=10600, lr=0.000295, gnorm=4.728, clip=0, train_wall=7, gb_free=75.2, wall=1721 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:01:45]    INFO >> epoch 007:   1436 / 1539 loss=3.428, wps=5437.8, ups=5.78, wpb=941.4, bsz=941.4, num_updates=10650, lr=0.000295, gnorm=4.964, clip=0, train_wall=8, gb_free=71.9, wall=1730 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:01:52]    INFO >> epoch 007:   1486 / 1539 loss=3.518, wps=5140.3, ups=7.06, wpb=727.7, bsz=727.7, num_updates=10700, lr=0.000295, gnorm=4.83, clip=0, train_wall=7, gb_free=72.7, wall=1737 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:01:59]    INFO >> epoch 007:   1536 / 1539 loss=3.263, wps=4799.5, ups=6.54, wpb=734.2, bsz=734.2, num_updates=10750, lr=0.000295, gnorm=4.655, clip=0, train_wall=7, gb_free=72.6, wall=1744 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:02:00]    INFO >> epoch 007 | loss 3.533 | wps 4540.1 | ups 6.32 | wpb 718.6 | bsz 718.6 | num_updates 10753 | lr 0.000295 | gnorm 4.683 | clip 0.1 | train_wall 211 | gb_free 71.4 | wall 1745 (progress_bar.py:267, print())[0m
[33m[2025-11-21 07:02:00] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:02:15]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.696 | wps 11086.3 | wpb 5412.5 | bsz 5412.5 | num_updates 10753 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 07:02:15]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 07:02:15]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 7 @ 10753 updates, score 3.696) (writing took 0.013364 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 07:02:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:02:22]    INFO >> epoch 008:     47 / 1539 loss=3.401, wps=1687.6, ups=2.31, wpb=730.5, bsz=730.5, num_updates=10800, lr=0.000262, gnorm=4.932, clip=0, train_wall=7, gb_free=73.5, wall=1766 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:02:30]    INFO >> epoch 008:     97 / 1539 loss=3.51, wps=4983.1, ups=6.55, wpb=760.7, bsz=760.7, num_updates=10850, lr=0.000262, gnorm=4.734, clip=0, train_wall=7, gb_free=74.7, wall=1774 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:02:37]    INFO >> epoch 008:    147 / 1539 loss=3.597, wps=4391.3, ups=7.23, wpb=607.6, bsz=607.6, num_updates=10900, lr=0.000262, gnorm=3.901, clip=0, train_wall=6, gb_free=75.5, wall=1780 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:02:46]    INFO >> epoch 008:    197 / 1539 loss=3.437, wps=5141.9, ups=6.11, wpb=841.4, bsz=841.4, num_updates=10950, lr=0.000262, gnorm=5.395, clip=0, train_wall=8, gb_free=59.3, wall=1789 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:02:54]    INFO >> epoch 008:    247 / 1539 loss=3.556, wps=5241.7, ups=6.95, wpb=753.8, bsz=753.8, num_updates=11000, lr=0.000262, gnorm=4.403, clip=0, train_wall=7, gb_free=72.2, wall=1796 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:03:01]    INFO >> epoch 008:    297 / 1539 loss=3.617, wps=4781.8, ups=6.74, wpb=709.8, bsz=709.8, num_updates=11050, lr=0.000262, gnorm=4.612, clip=0, train_wall=7, gb_free=70.8, wall=1803 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:03:08]    INFO >> epoch 008:    347 / 1539 loss=3.518, wps=4419.9, ups=6.77, wpb=653.3, bsz=653.3, num_updates=11100, lr=0.000262, gnorm=4.786, clip=0, train_wall=7, gb_free=74.2, wall=1811 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:03:18]    INFO >> epoch 008:    397 / 1539 loss=3.55, wps=4736.7, ups=6.37, wpb=743.5, bsz=743.5, num_updates=11150, lr=0.000262, gnorm=4.486, clip=0, train_wall=7, gb_free=72.6, wall=1819 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:03:25]    INFO >> epoch 008:    447 / 1539 loss=3.48, wps=5639.1, ups=6.57, wpb=857.8, bsz=857.8, num_updates=11200, lr=0.000262, gnorm=4.786, clip=0, train_wall=7, gb_free=72.4, wall=1826 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:03:32]    INFO >> epoch 008:    497 / 1539 loss=3.558, wps=5230.5, ups=7.1, wpb=736.7, bsz=736.7, num_updates=11250, lr=0.000262, gnorm=4.548, clip=0, train_wall=7, gb_free=74, wall=1833 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:03:40]    INFO >> epoch 008:    547 / 1539 loss=3.428, wps=4426.1, ups=6.73, wpb=657.9, bsz=657.9, num_updates=11300, lr=0.000262, gnorm=4.609, clip=0, train_wall=7, gb_free=75.1, wall=1841 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:03:47]    INFO >> epoch 008:    597 / 1539 loss=3.637, wps=4586.5, ups=6.67, wpb=688.1, bsz=688.1, num_updates=11350, lr=0.000262, gnorm=4.603, clip=0, train_wall=7, gb_free=73.5, wall=1848 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:03:55]    INFO >> epoch 008:    647 / 1539 loss=3.596, wps=4713.2, ups=7.35, wpb=641.2, bsz=641.2, num_updates=11400, lr=0.000262, gnorm=4.062, clip=0, train_wall=6, gb_free=71.5, wall=1855 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:04:02]    INFO >> epoch 008:    697 / 1539 loss=3.398, wps=4894.6, ups=7.09, wpb=690, bsz=690, num_updates=11450, lr=0.000262, gnorm=4.887, clip=0, train_wall=7, gb_free=73.7, wall=1862 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:04:09]    INFO >> epoch 008:    747 / 1539 loss=3.651, wps=4709.7, ups=7.16, wpb=657.8, bsz=657.8, num_updates=11500, lr=0.000262, gnorm=4.216, clip=0, train_wall=6, gb_free=74.4, wall=1869 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:04:17]    INFO >> epoch 008:    797 / 1539 loss=3.39, wps=5130.1, ups=6.57, wpb=780.3, bsz=780.3, num_updates=11550, lr=0.000262, gnorm=4.837, clip=0, train_wall=7, gb_free=60.1, wall=1877 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:04:25]    INFO >> epoch 008:    847 / 1539 loss=3.567, wps=4421.6, ups=7.39, wpb=598.3, bsz=598.3, num_updates=11600, lr=0.000262, gnorm=4.202, clip=0, train_wall=6, gb_free=70.3, wall=1883 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:04:33]    INFO >> epoch 008:    897 / 1539 loss=3.431, wps=5272.2, ups=6.46, wpb=815.6, bsz=815.6, num_updates=11650, lr=0.000262, gnorm=4.284, clip=0, train_wall=7, gb_free=70.9, wall=1891 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:04:40]    INFO >> epoch 008:    947 / 1539 loss=3.462, wps=4676.5, ups=7.16, wpb=653.5, bsz=653.5, num_updates=11700, lr=0.000262, gnorm=5.081, clip=0, train_wall=6, gb_free=72.5, wall=1898 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:04:46] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 31.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.14 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:04:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:04:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:04:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 39        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78928 MiB |  78988 MiB | 335153 GiB | 335076 GiB |
|       from large pool |  78753 MiB |  78813 MiB | 333097 GiB | 333020 GiB |
|       from small pool |    175 MiB |    177 MiB |   2055 GiB |   2055 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78928 MiB |  78988 MiB | 335153 GiB | 335076 GiB |
|       from large pool |  78753 MiB |  78813 MiB | 333097 GiB | 333020 GiB |
|       from small pool |    175 MiB |    177 MiB |   2055 GiB |   2055 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78895 MiB |  78955 MiB | 334438 GiB | 334361 GiB |
|       from large pool |  78720 MiB |  78780 MiB | 332385 GiB | 332308 GiB |
|       from small pool |    175 MiB |    176 MiB |   2052 GiB |   2052 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80474 MiB |  80494 MiB | 393350 MiB | 312876 MiB |
|       from large pool |  80294 MiB |  80294 MiB | 388720 MiB | 308426 MiB |
|       from small pool |    180 MiB |    440 MiB |   4630 MiB |   4450 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1485 MiB |   9187 MiB | 322867 GiB | 322865 GiB |
|       from large pool |   1480 MiB |   9181 MiB | 320518 GiB | 320516 GiB |
|       from small pool |      4 MiB |     25 MiB |   2348 GiB |   2348 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3425    |    3428    |   22966 K  |   22962 K  |
|       from large pool |     579    |     580    |   10575 K  |   10575 K  |
|       from small pool |    2846    |    2849    |   12390 K  |   12387 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3425    |    3428    |   22966 K  |   22962 K  |
|       from large pool |     579    |     580    |   10575 K  |   10575 K  |
|       from small pool |    2846    |    2849    |   12390 K  |   12387 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     247    |     373    |    4170    |    3923    |
|       from large pool |     157    |     157    |    1855    |    1698    |
|       from small pool |      90    |     220    |    2315    |    2225    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     188    |     189    |   13035 K  |   13034 K  |
|       from large pool |     106    |     108    |    7014 K  |    7014 K  |
|       from small pool |      82    |      83    |    6020 K  |    6020 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:04:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:04:46] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:04:47]    INFO >> epoch 008:    998 / 1539 loss=3.579, wps=3854.3, ups=6.55, wpb=588.2, bsz=588.2, num_updates=11750, lr=0.000262, gnorm=4.03, clip=0, train_wall=6, gb_free=74.8, wall=1906 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:04:57]    INFO >> epoch 008:   1048 / 1539 loss=3.379, wps=4932.8, ups=5.87, wpb=840.2, bsz=840.2, num_updates=11800, lr=0.000262, gnorm=4.427, clip=0, train_wall=8, gb_free=74.8, wall=1914 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:05:04]    INFO >> epoch 008:   1098 / 1539 loss=3.577, wps=4467.6, ups=6.93, wpb=645.1, bsz=645.1, num_updates=11850, lr=0.000262, gnorm=4.362, clip=0, train_wall=7, gb_free=70.9, wall=1921 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:05:11]    INFO >> epoch 008:   1148 / 1539 loss=3.614, wps=4360.5, ups=7.22, wpb=603.6, bsz=603.6, num_updates=11900, lr=0.000262, gnorm=4.192, clip=0, train_wall=6, gb_free=73.9, wall=1928 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:05:15] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 563.25 MiB is free. Including non-PyTorch memory, this process has 78.57 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 4.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:05:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:05:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:05:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 41        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75049 MiB |  75992 MiB | 339973 GiB | 339900 GiB |
|       from large pool |  75039 MiB |  75981 MiB | 337892 GiB | 337819 GiB |
|       from small pool |     10 MiB |     16 MiB |   2081 GiB |   2081 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75049 MiB |  75992 MiB | 339973 GiB | 339900 GiB |
|       from large pool |  75039 MiB |  75981 MiB | 337892 GiB | 337819 GiB |
|       from small pool |     10 MiB |     16 MiB |   2081 GiB |   2081 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75032 MiB |  75975 MiB | 339248 GiB | 339175 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 337169 GiB | 337096 GiB |
|       from small pool |     10 MiB |     16 MiB |   2078 GiB |   2078 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79942 MiB |  80414 MiB | 394314 MiB | 314372 MiB |
|       from large pool |  79922 MiB |  80234 MiB | 389684 MiB | 309762 MiB |
|       from small pool |     20 MiB |    180 MiB |   4630 MiB |   4610 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4892 MiB |  10361 MiB | 327190 GiB | 327185 GiB |
|       from large pool |   4882 MiB |  10352 MiB | 324811 GiB | 324807 GiB |
|       from small pool |      9 MiB |     25 MiB |   2378 GiB |   2378 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     501    |     509    |   23277 K  |   23276 K  |
|       from large pool |     314    |     322    |   10734 K  |   10734 K  |
|       from small pool |     187    |     232    |   12542 K  |   12542 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     501    |     509    |   23277 K  |   23276 K  |
|       from large pool |     314    |     322    |   10734 K  |   10734 K  |
|       from small pool |     187    |     232    |   12542 K  |   12542 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     152    |     246    |    4172    |    4020    |
|       from large pool |     142    |     156    |    1857    |    1715    |
|       from small pool |      10    |      90    |    2315    |    2305    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     145    |     145    |   13210 K  |   13210 K  |
|       from large pool |     127    |     127    |    7122 K  |    7122 K  |
|       from small pool |      18    |      48    |    6087 K  |    6087 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:05:15] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:05:15] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:05:19]    INFO >> epoch 008:   1199 / 1539 loss=3.578, wps=4454.4, ups=6.28, wpb=708.8, bsz=708.8, num_updates=11950, lr=0.000262, gnorm=3.937, clip=0, train_wall=7, gb_free=63, wall=1936 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:05:29]    INFO >> epoch 008:   1249 / 1539 loss=3.345, wps=5611.9, ups=5.71, wpb=983.4, bsz=983.4, num_updates=12000, lr=0.000262, gnorm=5.22, clip=0, train_wall=8, gb_free=70.5, wall=1945 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:05:37]    INFO >> epoch 008:   1299 / 1539 loss=3.585, wps=5007.9, ups=6.98, wpb=717.5, bsz=717.5, num_updates=12050, lr=0.000262, gnorm=4.114, clip=0, train_wall=7, gb_free=75.3, wall=1952 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:05:44]    INFO >> epoch 008:   1349 / 1539 loss=3.522, wps=4738.9, ups=6.79, wpb=697.5, bsz=697.5, num_updates=12100, lr=0.000262, gnorm=4.576, clip=0, train_wall=7, gb_free=73.7, wall=1960 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:05:46] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 682.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 565.25 MiB is free. Including non-PyTorch memory, this process has 78.56 GiB memory in use. Of the allocated memory 73.42 GiB is allocated by PyTorch, and 4.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:05:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:05:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:05:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 42        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72055 MiB |  75182 MiB | 345383 GiB | 345313 GiB |
|       from large pool |  72039 MiB |  75166 MiB | 343268 GiB | 343198 GiB |
|       from small pool |     15 MiB |     16 MiB |   2114 GiB |   2114 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72055 MiB |  75182 MiB | 345383 GiB | 345313 GiB |
|       from large pool |  72039 MiB |  75166 MiB | 343268 GiB | 343198 GiB |
|       from small pool |     15 MiB |     16 MiB |   2114 GiB |   2114 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  75169 MiB | 344646 GiB | 344576 GiB |
|       from large pool |  72027 MiB |  75153 MiB | 342534 GiB | 342464 GiB |
|       from small pool |     15 MiB |     16 MiB |   2111 GiB |   2111 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79940 MiB |  80362 MiB | 394734 MiB | 314794 MiB |
|       from large pool |  79922 MiB |  79922 MiB | 389684 MiB | 309762 MiB |
|       from small pool |     18 MiB |    440 MiB |   5050 MiB |   5032 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5156 MiB |   9383 MiB | 332449 GiB | 332444 GiB |
|       from large pool |   5154 MiB |   9380 MiB | 330031 GiB | 330026 GiB |
|       from small pool |      2 MiB |     27 MiB |   2417 GiB |   2417 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     510    |   23643 K  |   23642 K  |
|       from large pool |     308    |     316    |   10903 K  |   10903 K  |
|       from small pool |     194    |     232    |   12739 K  |   12739 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     510    |   23643 K  |   23642 K  |
|       from large pool |     308    |     316    |   10903 K  |   10903 K  |
|       from small pool |     194    |     232    |   12739 K  |   12739 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     151    |     362    |    4382    |    4231    |
|       from large pool |     142    |     142    |    1857    |    1715    |
|       from small pool |       9    |     220    |    2525    |    2516    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     149    |     149    |   13423 K  |   13423 K  |
|       from large pool |     130    |     130    |    7236 K  |    7236 K  |
|       from small pool |      19    |      43    |    6187 K  |    6187 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:05:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:05:46] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:05:52]    INFO >> epoch 008:   1400 / 1539 loss=3.496, wps=4855.6, ups=6.27, wpb=774.5, bsz=774.5, num_updates=12150, lr=0.000262, gnorm=4.134, clip=0, train_wall=7, gb_free=72.8, wall=1968 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:06:00]    INFO >> epoch 008:   1450 / 1539 loss=3.637, wps=4728.8, ups=6.31, wpb=749.5, bsz=749.5, num_updates=12200, lr=0.000262, gnorm=4.838, clip=0, train_wall=7, gb_free=34.1, wall=1975 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:06:09]    INFO >> epoch 008:   1500 / 1539 loss=3.512, wps=4502.9, ups=6.79, wpb=663.2, bsz=663.2, num_updates=12250, lr=0.000262, gnorm=4.666, clip=0, train_wall=7, gb_free=74.1, wall=1983 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:06:14]    INFO >> epoch 008 | loss 3.512 | wps 4531 | ups 6.31 | wpb 718.6 | bsz 718.6 | num_updates 12289 | lr 0.000262 | gnorm 4.524 | clip 0 | train_wall 211 | gb_free 75.4 | wall 1988 (progress_bar.py:267, print())[0m
[33m[2025-11-21 07:06:14] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:06:27]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.792 | wps 12079.3 | wpb 5412.5 | bsz 5412.5 | num_updates 12289 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 07:06:27]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 07:06:27]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 8 @ 12289 updates, score 3.792) (writing took 0.014593 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 07:06:27] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:06:28]    INFO >> epoch 009:     11 / 1539 loss=3.495, wps=1685.7, ups=2.51, wpb=671.4, bsz=671.4, num_updates=12300, lr=0.000227, gnorm=4.152, clip=0, train_wall=6, gb_free=74.3, wall=2003 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:06:37]    INFO >> epoch 009:     61 / 1539 loss=3.591, wps=4378.9, ups=6.92, wpb=632.8, bsz=632.8, num_updates=12350, lr=0.000227, gnorm=3.957, clip=0, train_wall=7, gb_free=74.6, wall=2010 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:06:45]    INFO >> epoch 009:    111 / 1539 loss=3.425, wps=4895.6, ups=6.7, wpb=730.3, bsz=730.3, num_updates=12400, lr=0.000227, gnorm=4.792, clip=0, train_wall=7, gb_free=70.8, wall=2017 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:06:52]    INFO >> epoch 009:    161 / 1539 loss=3.377, wps=5494, ups=6.53, wpb=840.8, bsz=840.8, num_updates=12450, lr=0.000227, gnorm=5.019, clip=0, train_wall=7, gb_free=74, wall=2025 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:06:59]    INFO >> epoch 009:    211 / 1539 loss=3.561, wps=4627.3, ups=7.21, wpb=642.1, bsz=642.1, num_updates=12500, lr=0.000227, gnorm=4.053, clip=0, train_wall=6, gb_free=72.9, wall=2032 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:07:06]    INFO >> epoch 009:    261 / 1539 loss=3.539, wps=5366.9, ups=7.25, wpb=739.9, bsz=739.9, num_updates=12550, lr=0.000227, gnorm=4.428, clip=0, train_wall=6, gb_free=74.3, wall=2039 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:07:14]    INFO >> epoch 009:    311 / 1539 loss=3.528, wps=4823.6, ups=7.21, wpb=668.9, bsz=668.9, num_updates=12600, lr=0.000227, gnorm=4.616, clip=0, train_wall=6, gb_free=75, wall=2046 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:07:23]    INFO >> epoch 009:    361 / 1539 loss=3.464, wps=4876.1, ups=5.79, wpb=842.7, bsz=842.7, num_updates=12650, lr=0.000227, gnorm=4.991, clip=0, train_wall=8, gb_free=71.8, wall=2055 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:07:25] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 883.25 MiB is free. Including non-PyTorch memory, this process has 78.25 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 3.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:07:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:07:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:07:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 43        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75052 MiB |  75995 MiB | 363822 GiB | 363749 GiB |
|       from large pool |  75041 MiB |  75984 MiB | 361590 GiB | 361516 GiB |
|       from small pool |     10 MiB |     14 MiB |   2232 GiB |   2232 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75052 MiB |  75995 MiB | 363822 GiB | 363749 GiB |
|       from large pool |  75041 MiB |  75984 MiB | 361590 GiB | 361516 GiB |
|       from small pool |     10 MiB |     14 MiB |   2232 GiB |   2232 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75032 MiB |  75975 MiB | 363049 GiB | 362976 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 360819 GiB | 360746 GiB |
|       from small pool |     10 MiB |     14 MiB |   2229 GiB |   2229 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79622 MiB |  79724 MiB | 397246 MiB | 317624 MiB |
|       from large pool |  79604 MiB |  79604 MiB | 392094 MiB | 312490 MiB |
|       from small pool |     18 MiB |    120 MiB |   5152 MiB |   5134 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4569 MiB |   8470 MiB | 348188 GiB | 348184 GiB |
|       from large pool |   4562 MiB |   8463 MiB | 345640 GiB | 345635 GiB |
|       from small pool |      7 MiB |     17 MiB |   2548 GiB |   2548 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     501    |     509    |   24879 K  |   24879 K  |
|       from large pool |     314    |     322    |   11421 K  |   11421 K  |
|       from small pool |     187    |     226    |   13458 K  |   13457 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     501    |     509    |   24879 K  |   24879 K  |
|       from large pool |     314    |     322    |   11421 K  |   11421 K  |
|       from small pool |     187    |     226    |   13458 K  |   13457 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     155    |     206    |    4438    |    4283    |
|       from large pool |     146    |     146    |    1862    |    1716    |
|       from small pool |       9    |      60    |    2576    |    2567    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     145    |     145    |   14145 K  |   14144 K  |
|       from large pool |     126    |     126    |    7588 K  |    7588 K  |
|       from small pool |      19    |      39    |    6556 K  |    6556 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:07:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:07:25] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:07:31]    INFO >> epoch 009:    412 / 1539 loss=3.538, wps=4376.9, ups=6.35, wpb=689.6, bsz=689.6, num_updates=12700, lr=0.000227, gnorm=4.2, clip=0, train_wall=7, gb_free=75.1, wall=2062 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:07:38]    INFO >> epoch 009:    462 / 1539 loss=3.43, wps=5048.9, ups=7.1, wpb=711.3, bsz=711.3, num_updates=12750, lr=0.000227, gnorm=4.323, clip=0, train_wall=7, gb_free=71.4, wall=2069 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:07:46]    INFO >> epoch 009:    512 / 1539 loss=3.467, wps=5176.2, ups=6.95, wpb=745.2, bsz=745.2, num_updates=12800, lr=0.000227, gnorm=4.532, clip=0, train_wall=7, gb_free=75.9, wall=2077 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:07:54]    INFO >> epoch 009:    562 / 1539 loss=3.488, wps=4915.5, ups=6.33, wpb=776.5, bsz=776.5, num_updates=12850, lr=0.000227, gnorm=4.926, clip=0, train_wall=7, gb_free=74.3, wall=2085 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:07:57] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 77.14 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:07:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:07:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:07:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 45        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78866 MiB |  78986 MiB | 369215 GiB | 369138 GiB |
|       from large pool |  78691 MiB |  78811 MiB | 366952 GiB | 366875 GiB |
|       from small pool |    175 MiB |    176 MiB |   2263 GiB |   2263 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78866 MiB |  78986 MiB | 369215 GiB | 369138 GiB |
|       from large pool |  78691 MiB |  78811 MiB | 366952 GiB | 366875 GiB |
|       from small pool |    175 MiB |    176 MiB |   2263 GiB |   2263 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78835 MiB |  78954 MiB | 368430 GiB | 368353 GiB |
|       from large pool |  78661 MiB |  78780 MiB | 366170 GiB | 366093 GiB |
|       from small pool |    174 MiB |    175 MiB |   2260 GiB |   2260 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB | 398130 MiB | 317626 MiB |
|       from large pool |  80324 MiB |  80324 MiB | 392814 MiB | 312490 MiB |
|       from small pool |    180 MiB |    182 MiB |   5316 MiB |   5136 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1517 MiB |   9395 MiB | 353355 GiB | 353354 GiB |
|       from large pool |   1512 MiB |   9391 MiB | 350770 GiB | 350769 GiB |
|       from small pool |      4 MiB |     23 MiB |   2584 GiB |   2584 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3414    |    3417    |   25239 K  |   25236 K  |
|       from large pool |     578    |     580    |   11598 K  |   11598 K  |
|       from small pool |    2836    |    2839    |   13641 K  |   13638 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3414    |    3417    |   25239 K  |   25236 K  |
|       from large pool |     578    |     580    |   11598 K  |   11598 K  |
|       from small pool |    2836    |    2839    |   13641 K  |   13638 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     248    |     248    |    4532    |    4284    |
|       from large pool |     158    |     158    |    1874    |    1716    |
|       from small pool |      90    |      91    |    2658    |    2568    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     193    |     194    |   14347 K  |   14347 K  |
|       from large pool |     109    |     111    |    7708 K  |    7708 K  |
|       from small pool |      84    |      85    |    6639 K  |    6639 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:07:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:07:57] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:08:02]    INFO >> epoch 009:    613 / 1539 loss=3.502, wps=4052.7, ups=6.31, wpb=642.6, bsz=642.6, num_updates=12900, lr=0.000227, gnorm=4.53, clip=0, train_wall=7, gb_free=72.2, wall=2092 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:08:10]    INFO >> epoch 009:    663 / 1539 loss=3.544, wps=4555.4, ups=6.8, wpb=669.5, bsz=669.5, num_updates=12950, lr=0.000227, gnorm=4.407, clip=0, train_wall=7, gb_free=61.1, wall=2100 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:08:18]    INFO >> epoch 009:    713 / 1539 loss=3.588, wps=4944.5, ups=7.19, wpb=687.7, bsz=687.7, num_updates=13000, lr=0.000227, gnorm=4.404, clip=0, train_wall=6, gb_free=76.7, wall=2107 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:08:26]    INFO >> epoch 009:    763 / 1539 loss=3.513, wps=4497.9, ups=6.55, wpb=687, bsz=687, num_updates=13050, lr=0.000227, gnorm=4.083, clip=0, train_wall=7, gb_free=72.7, wall=2114 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:08:33]    INFO >> epoch 009:    813 / 1539 loss=3.511, wps=5272.7, ups=6.98, wpb=755.6, bsz=755.6, num_updates=13100, lr=0.000227, gnorm=5.037, clip=0, train_wall=7, gb_free=74.5, wall=2122 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:08:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 283.25 MiB is free. Including non-PyTorch memory, this process has 78.84 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 7.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:08:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:08:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:08:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 46        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72057 MiB |  72457 MiB | 375528 GiB | 375457 GiB |
|       from large pool |  72042 MiB |  72441 MiB | 373230 GiB | 373159 GiB |
|       from small pool |     15 MiB |     16 MiB |   2297 GiB |   2297 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72057 MiB |  72457 MiB | 375528 GiB | 375457 GiB |
|       from large pool |  72042 MiB |  72441 MiB | 373230 GiB | 373159 GiB |
|       from small pool |     15 MiB |     16 MiB |   2297 GiB |   2297 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  72442 MiB | 374728 GiB | 374658 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 372434 GiB | 372363 GiB |
|       from small pool |     15 MiB |     16 MiB |   2294 GiB |   2294 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80222 MiB |  80384 MiB | 398130 MiB | 317908 MiB |
|       from large pool |  80204 MiB |  80204 MiB | 392814 MiB | 312610 MiB |
|       from small pool |     18 MiB |    180 MiB |   5316 MiB |   5298 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8164 MiB |   9199 MiB | 359178 GiB | 359170 GiB |
|       from large pool |   8161 MiB |   9196 MiB | 356554 GiB | 356546 GiB |
|       from small pool |      2 MiB |     21 MiB |   2624 GiB |   2624 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     509    |   25655 K  |   25654 K  |
|       from large pool |     308    |     315    |   11810 K  |   11810 K  |
|       from small pool |     194    |     232    |   13844 K  |   13844 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     509    |   25655 K  |   25654 K  |
|       from large pool |     308    |     315    |   11810 K  |   11810 K  |
|       from small pool |     194    |     232    |   13844 K  |   13844 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     165    |     246    |    4532    |    4367    |
|       from large pool |     156    |     156    |    1874    |    1718    |
|       from small pool |       9    |      90    |    2658    |    2649    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     159    |     159    |   14580 K  |   14580 K  |
|       from large pool |     141    |     141    |    7851 K  |    7851 K  |
|       from small pool |      18    |      41    |    6728 K  |    6728 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:08:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:08:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:08:40]    INFO >> epoch 009:    864 / 1539 loss=3.562, wps=3989.4, ups=6.71, wpb=594.8, bsz=594.8, num_updates=13150, lr=0.000227, gnorm=4.119, clip=0, train_wall=6, gb_free=73.4, wall=2129 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:08:47]    INFO >> epoch 009:    914 / 1539 loss=3.571, wps=4778.6, ups=7.22, wpb=662, bsz=662, num_updates=13200, lr=0.000227, gnorm=4.234, clip=0, train_wall=6, gb_free=71.1, wall=2136 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:08:57]    INFO >> epoch 009:    964 / 1539 loss=3.443, wps=4661, ups=5.57, wpb=837.1, bsz=837.1, num_updates=13250, lr=0.000227, gnorm=4.849, clip=0, train_wall=8, gb_free=74.8, wall=2145 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:09:05]    INFO >> epoch 009:   1014 / 1539 loss=3.501, wps=4738.8, ups=6.88, wpb=688.6, bsz=688.6, num_updates=13300, lr=0.000227, gnorm=4.31, clip=0, train_wall=7, gb_free=73.1, wall=2152 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:09:12]    INFO >> epoch 009:   1064 / 1539 loss=3.39, wps=5219.2, ups=6.72, wpb=777.1, bsz=777.1, num_updates=13350, lr=0.000227, gnorm=4.667, clip=0, train_wall=7, gb_free=72, wall=2160 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:09:19]    INFO >> epoch 009:   1114 / 1539 loss=3.62, wps=4779.3, ups=6.78, wpb=704.4, bsz=704.4, num_updates=13400, lr=0.000227, gnorm=4.436, clip=0, train_wall=7, gb_free=68.8, wall=2167 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:09:28]    INFO >> epoch 009:   1164 / 1539 loss=3.473, wps=5141.7, ups=7.18, wpb=716.5, bsz=716.5, num_updates=13450, lr=0.000227, gnorm=4.651, clip=0, train_wall=6, gb_free=69.7, wall=2174 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:09:36]    INFO >> epoch 009:   1214 / 1539 loss=3.272, wps=5400.1, ups=6.15, wpb=878.3, bsz=878.3, num_updates=13500, lr=0.000227, gnorm=5.2, clip=0, train_wall=8, gb_free=35.4, wall=2182 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:09:44]    INFO >> epoch 009:   1264 / 1539 loss=3.531, wps=4993.6, ups=6.59, wpb=758.3, bsz=758.3, num_updates=13550, lr=0.000227, gnorm=4.511, clip=0, train_wall=7, gb_free=70.2, wall=2190 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:09:51]    INFO >> epoch 009:   1314 / 1539 loss=3.597, wps=4596, ups=6.78, wpb=677.9, bsz=677.9, num_updates=13600, lr=0.000227, gnorm=4.614, clip=0, train_wall=7, gb_free=74.9, wall=2197 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:09:59]    INFO >> epoch 009:   1364 / 1539 loss=3.494, wps=4503.3, ups=6.89, wpb=653.4, bsz=653.4, num_updates=13650, lr=0.000227, gnorm=4.527, clip=0, train_wall=7, gb_free=72.8, wall=2204 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:10:07]    INFO >> epoch 009:   1414 / 1539 loss=3.392, wps=5410.3, ups=6.78, wpb=798.3, bsz=798.3, num_updates=13700, lr=0.000227, gnorm=4.355, clip=0, train_wall=7, gb_free=73.6, wall=2212 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:10:15]    INFO >> epoch 009:   1464 / 1539 loss=3.479, wps=4874.4, ups=5.83, wpb=835.4, bsz=835.4, num_updates=13750, lr=0.000227, gnorm=4.178, clip=0, train_wall=8, gb_free=74.6, wall=2220 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:10:22]    INFO >> epoch 009:   1514 / 1539 loss=3.479, wps=4415.3, ups=7.36, wpb=599.8, bsz=599.8, num_updates=13800, lr=0.000227, gnorm=4.552, clip=0, train_wall=6, gb_free=70.6, wall=2227 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:10:26]    INFO >> epoch 009 | loss 3.493 | wps 4550.7 | ups 6.33 | wpb 718.6 | bsz 718.6 | num_updates 13825 | lr 0.000227 | gnorm 4.511 | clip 0 | train_wall 211 | gb_free 74.9 | wall 2231 (progress_bar.py:267, print())[0m
[33m[2025-11-21 07:10:26] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:10:40]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.741 | wps 11888.2 | wpb 5412.5 | bsz 5412.5 | num_updates 13825 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 07:10:41]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 07:10:41]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 9 @ 13825 updates, score 3.741) (writing took 0.010898 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 07:10:41] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:10:46]    INFO >> epoch 010:     25 / 1539 loss=3.535, wps=1660.4, ups=2.26, wpb=733.7, bsz=733.7, num_updates=13850, lr=0.000193, gnorm=4.296, clip=0, train_wall=8, gb_free=69.4, wall=2249 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:10:53]    INFO >> epoch 010:     75 / 1539 loss=3.504, wps=4427.2, ups=7.19, wpb=615.8, bsz=615.8, num_updates=13900, lr=0.000193, gnorm=4.703, clip=0, train_wall=6, gb_free=75, wall=2256 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:11:00]    INFO >> epoch 010:    125 / 1539 loss=3.443, wps=4898.7, ups=6.82, wpb=718.7, bsz=718.7, num_updates=13950, lr=0.000193, gnorm=4.036, clip=0, train_wall=7, gb_free=72.9, wall=2263 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:11:08]    INFO >> epoch 010:    175 / 1539 loss=3.45, wps=5130.3, ups=6.96, wpb=736.8, bsz=736.8, num_updates=14000, lr=0.000193, gnorm=5.043, clip=0, train_wall=7, gb_free=73, wall=2271 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:11:17]    INFO >> epoch 010:    225 / 1539 loss=3.463, wps=4500, ups=6.23, wpb=722.5, bsz=722.5, num_updates=14050, lr=0.000193, gnorm=4.289, clip=0, train_wall=7, gb_free=68.4, wall=2279 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:11:25]    INFO >> epoch 010:    275 / 1539 loss=3.482, wps=4907.7, ups=6.06, wpb=810, bsz=810, num_updates=14100, lr=0.000193, gnorm=4.099, clip=0, train_wall=8, gb_free=72.9, wall=2287 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:11:32] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 277.25 MiB is free. Including non-PyTorch memory, this process has 78.85 GiB memory in use. Of the allocated memory 74.09 GiB is allocated by PyTorch, and 4.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:11:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:11:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:11:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 50        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72059 MiB |  75868 MiB | 407061 GiB | 406991 GiB |
|       from large pool |  72044 MiB |  75853 MiB | 404562 GiB | 404492 GiB |
|       from small pool |     15 MiB |     16 MiB |   2499 GiB |   2499 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72059 MiB |  75868 MiB | 407061 GiB | 406991 GiB |
|       from large pool |  72044 MiB |  75853 MiB | 404562 GiB | 404492 GiB |
|       from small pool |     15 MiB |     16 MiB |   2499 GiB |   2499 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  75850 MiB | 406194 GiB | 406123 GiB |
|       from large pool |  72027 MiB |  75835 MiB | 403698 GiB | 403628 GiB |
|       from small pool |     15 MiB |     16 MiB |   2495 GiB |   2495 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80228 MiB |  80228 MiB | 450050 MiB | 369822 MiB |
|       from large pool |  80208 MiB |  80208 MiB | 444108 MiB | 363900 MiB |
|       from small pool |     20 MiB |    222 MiB |   5942 MiB |   5922 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4758 MiB |   8350 MiB | 387150 GiB | 387145 GiB |
|       from large pool |   4753 MiB |   8342 MiB | 384297 GiB | 384292 GiB |
|       from small pool |      4 MiB |     19 MiB |   2852 GiB |   2852 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     511    |   27837 K  |   27836 K  |
|       from large pool |     308    |     317    |   12768 K  |   12768 K  |
|       from small pool |     194    |     226    |   15068 K  |   15068 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     511    |   27837 K  |   27836 K  |
|       from large pool |     308    |     317    |   12768 K  |   12768 K  |
|       from small pool |     194    |     226    |   15068 K  |   15068 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     147    |     245    |    5069    |    4922    |
|       from large pool |     137    |     137    |    2098    |    1961    |
|       from small pool |      10    |     111    |    2971    |    2961    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     138    |     138    |   15850 K  |   15850 K  |
|       from large pool |     117    |     117    |    8496 K  |    8496 K  |
|       from small pool |      21    |      41    |    7353 K  |    7353 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:11:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:11:32] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:11:32]    INFO >> epoch 010:    326 / 1539 loss=3.521, wps=4600.6, ups=6.51, wpb=706.8, bsz=706.8, num_updates=14150, lr=0.000193, gnorm=4.538, clip=0, train_wall=6, gb_free=73.5, wall=2295 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:11:41]    INFO >> epoch 010:    376 / 1539 loss=3.452, wps=4913.7, ups=6.73, wpb=729.7, bsz=729.7, num_updates=14200, lr=0.000193, gnorm=4.31, clip=0, train_wall=7, gb_free=72.2, wall=2302 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:11:49]    INFO >> epoch 010:    426 / 1539 loss=3.524, wps=4318.2, ups=6.83, wpb=632.4, bsz=632.4, num_updates=14250, lr=0.000193, gnorm=4.263, clip=0, train_wall=7, gb_free=71.9, wall=2309 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:11:56]    INFO >> epoch 010:    476 / 1539 loss=3.417, wps=4486.8, ups=6.69, wpb=670.3, bsz=670.3, num_updates=14300, lr=0.000193, gnorm=4.006, clip=0, train_wall=7, gb_free=60.6, wall=2317 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:12:03]    INFO >> epoch 010:    526 / 1539 loss=3.411, wps=4484.2, ups=7.11, wpb=631, bsz=631, num_updates=14350, lr=0.000193, gnorm=4.538, clip=0, train_wall=7, gb_free=72.9, wall=2324 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:12:10]    INFO >> epoch 010:    576 / 1539 loss=3.551, wps=4565.5, ups=6.89, wpb=662.7, bsz=662.7, num_updates=14400, lr=0.000193, gnorm=3.92, clip=0, train_wall=7, gb_free=71.4, wall=2331 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:12:19]    INFO >> epoch 010:    626 / 1539 loss=3.503, wps=4934.2, ups=7.01, wpb=704.3, bsz=704.3, num_updates=14450, lr=0.000193, gnorm=4.579, clip=0, train_wall=7, gb_free=71.4, wall=2338 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:12:26]    INFO >> epoch 010:    676 / 1539 loss=3.439, wps=5288.9, ups=6.87, wpb=770.2, bsz=770.2, num_updates=14500, lr=0.000193, gnorm=4.888, clip=0, train_wall=7, gb_free=75.2, wall=2346 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:12:34]    INFO >> epoch 010:    726 / 1539 loss=3.212, wps=5333, ups=6.62, wpb=806.1, bsz=806.1, num_updates=14550, lr=0.000193, gnorm=3.968, clip=0, train_wall=7, gb_free=73.9, wall=2353 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:12:41]    INFO >> epoch 010:    776 / 1539 loss=3.525, wps=4859.3, ups=6.5, wpb=747.4, bsz=747.4, num_updates=14600, lr=0.000193, gnorm=4.494, clip=0, train_wall=7, gb_free=72, wall=2361 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:12:50]    INFO >> epoch 010:    826 / 1539 loss=3.459, wps=4514.6, ups=6.76, wpb=668.1, bsz=668.1, num_updates=14650, lr=0.000193, gnorm=4.562, clip=0, train_wall=7, gb_free=71.9, wall=2368 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:12:58]    INFO >> epoch 010:    876 / 1539 loss=3.479, wps=4581.3, ups=6.54, wpb=700.3, bsz=700.3, num_updates=14700, lr=0.000193, gnorm=4.282, clip=2, train_wall=7, gb_free=71.8, wall=2376 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:13:06]    INFO >> epoch 010:    926 / 1539 loss=3.405, wps=5480.1, ups=6.35, wpb=862.5, bsz=862.5, num_updates=14750, lr=0.000193, gnorm=5.216, clip=0, train_wall=7, gb_free=73.9, wall=2384 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:13:12]    INFO >> epoch 010:    976 / 1539 loss=3.52, wps=4706.1, ups=7.3, wpb=645.1, bsz=645.1, num_updates=14800, lr=0.000193, gnorm=3.968, clip=0, train_wall=6, gb_free=72.2, wall=2391 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:13:21]    INFO >> epoch 010:   1026 / 1539 loss=3.528, wps=4801.3, ups=6.77, wpb=709, bsz=709, num_updates=14850, lr=0.000193, gnorm=4.439, clip=0, train_wall=7, gb_free=74.7, wall=2398 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:13:29]    INFO >> epoch 010:   1076 / 1539 loss=3.504, wps=5292.7, ups=6.73, wpb=786.3, bsz=786.3, num_updates=14900, lr=0.000193, gnorm=4.697, clip=0, train_wall=7, gb_free=71.9, wall=2405 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:13:36]    INFO >> epoch 010:   1126 / 1539 loss=3.46, wps=4323.3, ups=6.93, wpb=623.8, bsz=623.8, num_updates=14950, lr=0.000193, gnorm=4.058, clip=0, train_wall=7, gb_free=74.4, wall=2413 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:13:44]    INFO >> epoch 010:   1176 / 1539 loss=3.406, wps=4979.9, ups=6.37, wpb=781.8, bsz=781.8, num_updates=15000, lr=0.000193, gnorm=4.503, clip=0, train_wall=7, gb_free=76, wall=2420 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:13:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 45.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.33 GiB is allocated by PyTorch, and 1.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:13:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:13:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:13:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 52        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79130 MiB |  79190 MiB | 429215 GiB | 429137 GiB |
|       from large pool |  78952 MiB |  79012 MiB | 426584 GiB | 426506 GiB |
|       from small pool |    177 MiB |    178 MiB |   2631 GiB |   2630 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79130 MiB |  79190 MiB | 429215 GiB | 429137 GiB |
|       from large pool |  78952 MiB |  79012 MiB | 426584 GiB | 426506 GiB |
|       from small pool |    177 MiB |    178 MiB |   2631 GiB |   2630 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79076 MiB |  79135 MiB | 428298 GiB | 428221 GiB |
|       from large pool |  78899 MiB |  78958 MiB | 425671 GiB | 425594 GiB |
|       from small pool |    176 MiB |    178 MiB |   2627 GiB |   2627 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80460 MiB |  80478 MiB | 453954 MiB | 373494 MiB |
|       from large pool |  80278 MiB |  80278 MiB | 447588 MiB | 367310 MiB |
|       from small pool |    182 MiB |    440 MiB |   6366 MiB |   6184 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1269 MiB |  10941 MiB | 409546 GiB | 409545 GiB |
|       from large pool |   1265 MiB |  10933 MiB | 406540 GiB | 406539 GiB |
|       from small pool |      4 MiB |     21 MiB |   3005 GiB |   3005 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3458    |    3461    |   29373 K  |   29369 K  |
|       from large pool |     582    |     583    |   13518 K  |   13518 K  |
|       from small pool |    2876    |    2879    |   15854 K  |   15851 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3458    |    3461    |   29373 K  |   29369 K  |
|       from large pool |     582    |     583    |   13518 K  |   13518 K  |
|       from small pool |    2876    |    2879    |   15854 K  |   15851 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     284    |     409    |    5339    |    5055    |
|       from large pool |     193    |     193    |    2156    |    1963    |
|       from small pool |      91    |     220    |    3183    |    3092    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     183    |     184    |   16705 K  |   16704 K  |
|       from large pool |     102    |     108    |    8988 K  |    8988 K  |
|       from small pool |      81    |      82    |    7716 K  |    7716 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:13:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:13:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:13:52]    INFO >> epoch 010:   1227 / 1539 loss=3.474, wps=4720.7, ups=5.95, wpb=793.8, bsz=793.8, num_updates=15050, lr=0.000193, gnorm=5.319, clip=0, train_wall=7, gb_free=74.9, wall=2429 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:14:01]    INFO >> epoch 010:   1277 / 1539 loss=3.312, wps=5160.5, ups=6.57, wpb=785.2, bsz=785.2, num_updates=15100, lr=0.000193, gnorm=4.784, clip=0, train_wall=7, gb_free=72.5, wall=2436 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:14:08]    INFO >> epoch 010:   1327 / 1539 loss=3.549, wps=4198.1, ups=6.7, wpb=626.3, bsz=626.3, num_updates=15150, lr=0.000193, gnorm=4.205, clip=0, train_wall=7, gb_free=72.8, wall=2444 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:14:16]    INFO >> epoch 010:   1377 / 1539 loss=3.475, wps=4645.3, ups=6.32, wpb=734.9, bsz=734.9, num_updates=15200, lr=0.000193, gnorm=4.968, clip=0, train_wall=7, gb_free=70.4, wall=2452 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:14:24]    INFO >> epoch 010:   1427 / 1539 loss=3.483, wps=4782.9, ups=6.53, wpb=732.6, bsz=732.6, num_updates=15250, lr=0.000193, gnorm=3.805, clip=0, train_wall=7, gb_free=76.7, wall=2459 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:14:32] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 419.25 MiB is free. Including non-PyTorch memory, this process has 78.71 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.55 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:14:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:14:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:14:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 54        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76955 MiB |  77478 MiB | 436949 GiB | 436873 GiB |
|       from large pool |  76944 MiB |  77467 MiB | 434273 GiB | 434198 GiB |
|       from small pool |     10 MiB |     21 MiB |   2675 GiB |   2675 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76955 MiB |  77478 MiB | 436949 GiB | 436873 GiB |
|       from large pool |  76944 MiB |  77467 MiB | 434273 GiB | 434198 GiB |
|       from small pool |     10 MiB |     21 MiB |   2675 GiB |   2675 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76934 MiB |  77455 MiB | 436015 GiB | 435940 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 433344 GiB | 433269 GiB |
|       from small pool |     10 MiB |     21 MiB |   2671 GiB |   2671 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80086 MiB |  80424 MiB | 459228 MiB | 379142 MiB |
|       from large pool |  80068 MiB |  80218 MiB | 452838 MiB | 372770 MiB |
|       from small pool |     18 MiB |    206 MiB |   6390 MiB |   6372 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3130 MiB |   8864 MiB | 416901 GiB | 416898 GiB |
|       from large pool |   3123 MiB |   8856 MiB | 413844 GiB | 413841 GiB |
|       from small pool |      7 MiB |     31 MiB |   3057 GiB |   3057 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     509    |   29895 K  |   29895 K  |
|       from large pool |     315    |     322    |   13776 K  |   13776 K  |
|       from small pool |     187    |     240    |   16118 K  |   16118 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     509    |   29895 K  |   29895 K  |
|       from large pool |     315    |     322    |   13776 K  |   13776 K  |
|       from small pool |     187    |     240    |   16118 K  |   16118 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     116    |     295    |    5356    |    5240    |
|       from large pool |     107    |     192    |    2161    |    2054    |
|       from small pool |       9    |     103    |    3195    |    3186    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     111    |     111    |   16998 K  |   16997 K  |
|       from large pool |      93    |      93    |    9160 K  |    9160 K  |
|       from small pool |      18    |      49    |    7837 K  |    7837 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:14:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:14:32] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:14:33]    INFO >> epoch 010:   1478 / 1539 loss=3.595, wps=4027.4, ups=6.58, wpb=611.8, bsz=611.8, num_updates=15300, lr=0.000193, gnorm=4.348, clip=0, train_wall=6, gb_free=72.9, wall=2467 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:14:40]    INFO >> epoch 010:   1528 / 1539 loss=3.484, wps=5011.1, ups=6.74, wpb=743.6, bsz=743.6, num_updates=15350, lr=0.000193, gnorm=3.896, clip=0, train_wall=7, gb_free=72.6, wall=2475 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:14:42]    INFO >> epoch 010 | loss 3.465 | wps 4498.9 | ups 6.26 | wpb 718.6 | bsz 718.6 | num_updates 15361 | lr 0.000193 | gnorm 4.419 | clip 0.1 | train_wall 213 | gb_free 71.9 | wall 2476 (progress_bar.py:267, print())[0m
[33m[2025-11-21 07:14:42] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:14:55]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.74 | wps 11836.5 | wpb 5412.5 | bsz 5412.5 | num_updates 15361 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 07:14:55]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 07:14:55]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 10 @ 15361 updates, score 3.74) (writing took 0.015333 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 07:14:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:15:02]    INFO >> epoch 011:     39 / 1539 loss=3.518, wps=1682, ups=2.45, wpb=686.5, bsz=686.5, num_updates=15400, lr=0.000161, gnorm=4.491, clip=0, train_wall=7, gb_free=75, wall=2495 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:15:10]    INFO >> epoch 011:     89 / 1539 loss=3.456, wps=5218.5, ups=6.8, wpb=767.8, bsz=767.8, num_updates=15450, lr=0.000161, gnorm=4.753, clip=0, train_wall=7, gb_free=66.7, wall=2502 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:15:16]    INFO >> epoch 011:    139 / 1539 loss=3.541, wps=4751.5, ups=7.24, wpb=656.7, bsz=656.7, num_updates=15500, lr=0.000161, gnorm=4.5, clip=0, train_wall=6, gb_free=72.2, wall=2509 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:15:24]    INFO >> epoch 011:    189 / 1539 loss=3.269, wps=5057.9, ups=6.74, wpb=750.9, bsz=750.9, num_updates=15550, lr=0.000161, gnorm=4.345, clip=0, train_wall=7, gb_free=74.9, wall=2517 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:15:31]    INFO >> epoch 011:    239 / 1539 loss=3.462, wps=4986.1, ups=7.04, wpb=708.2, bsz=708.2, num_updates=15600, lr=0.000161, gnorm=4.682, clip=0, train_wall=7, gb_free=72.9, wall=2524 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:15:39]    INFO >> epoch 011:    289 / 1539 loss=3.419, wps=4766.6, ups=7.15, wpb=667, bsz=667, num_updates=15650, lr=0.000161, gnorm=4.586, clip=0, train_wall=7, gb_free=74.9, wall=2531 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:15:47]    INFO >> epoch 011:    339 / 1539 loss=3.312, wps=4830.4, ups=6.57, wpb=735.6, bsz=735.6, num_updates=15700, lr=0.000161, gnorm=4.753, clip=0, train_wall=7, gb_free=65.3, wall=2538 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:15:54]    INFO >> epoch 011:    389 / 1539 loss=3.5, wps=4844.2, ups=7.35, wpb=659.4, bsz=659.4, num_updates=15750, lr=0.000161, gnorm=4.255, clip=0, train_wall=6, gb_free=75.1, wall=2545 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:16:01]    INFO >> epoch 011:    439 / 1539 loss=3.492, wps=4535.7, ups=7.22, wpb=628.2, bsz=628.2, num_updates=15800, lr=0.000161, gnorm=4.748, clip=0, train_wall=6, gb_free=73.4, wall=2552 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:16:10]    INFO >> epoch 011:    489 / 1539 loss=3.535, wps=4152.8, ups=6.45, wpb=643.9, bsz=643.9, num_updates=15850, lr=0.000161, gnorm=4.552, clip=0, train_wall=7, gb_free=76.3, wall=2560 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:16:17]    INFO >> epoch 011:    539 / 1539 loss=3.604, wps=4546.7, ups=6.93, wpb=655.8, bsz=655.8, num_updates=15900, lr=0.000161, gnorm=4.091, clip=0, train_wall=7, gb_free=71.7, wall=2567 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:16:25]    INFO >> epoch 011:    589 / 1539 loss=3.235, wps=4937.6, ups=5.98, wpb=826.1, bsz=826.1, num_updates=15950, lr=0.000161, gnorm=4.736, clip=0, train_wall=8, gb_free=72.7, wall=2575 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:16:33]    INFO >> epoch 011:    639 / 1539 loss=3.571, wps=5170.2, ups=6.54, wpb=790.9, bsz=790.9, num_updates=16000, lr=0.000161, gnorm=3.941, clip=0, train_wall=7, gb_free=73.4, wall=2583 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:16:40]    INFO >> epoch 011:    689 / 1539 loss=3.528, wps=4797.1, ups=6.73, wpb=713.1, bsz=713.1, num_updates=16050, lr=0.000161, gnorm=4.573, clip=0, train_wall=7, gb_free=75.7, wall=2590 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:16:50]    INFO >> epoch 011:    739 / 1539 loss=3.47, wps=4294.4, ups=6.2, wpb=692.4, bsz=692.4, num_updates=16100, lr=0.000161, gnorm=4.178, clip=0, train_wall=7, gb_free=72.1, wall=2599 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:16:57] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 2 has a total capacity of 79.14 GiB of which 281.25 MiB is free. Including non-PyTorch memory, this process has 78.84 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:16:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:16:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:16:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 57        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76953 MiB |  77476 MiB | 462854 GiB | 462779 GiB |
|       from large pool |  76943 MiB |  77465 MiB | 460015 GiB | 459940 GiB |
|       from small pool |     10 MiB |     18 MiB |   2839 GiB |   2839 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76953 MiB |  77476 MiB | 462854 GiB | 462779 GiB |
|       from large pool |  76943 MiB |  77465 MiB | 460015 GiB | 459940 GiB |
|       from small pool |     10 MiB |     18 MiB |   2839 GiB |   2839 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76934 MiB |  77455 MiB | 461869 GiB | 461794 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 459034 GiB | 458959 GiB |
|       from small pool |     10 MiB |     18 MiB |   2835 GiB |   2835 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80224 MiB |  80224 MiB | 462516 MiB | 382292 MiB |
|       from large pool |  80206 MiB |  80206 MiB | 455704 MiB | 375498 MiB |
|       from small pool |     18 MiB |    440 MiB |   6812 MiB |   6794 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3270 MiB |   8717 MiB | 443520 GiB | 443517 GiB |
|       from large pool |   3262 MiB |   8709 MiB | 440277 GiB | 440273 GiB |
|       from small pool |      7 MiB |     23 MiB |   3243 GiB |   3243 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     509    |   31683 K  |   31683 K  |
|       from large pool |     315    |     322    |   14561 K  |   14561 K  |
|       from small pool |     187    |     240    |   17122 K  |   17122 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     509    |   31683 K  |   31683 K  |
|       from large pool |     315    |     322    |   14561 K  |   14561 K  |
|       from small pool |     187    |     240    |   17122 K  |   17122 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     118    |     328    |    5570    |    5452    |
|       from large pool |     109    |     109    |    2164    |    2055    |
|       from small pool |       9    |     220    |    3406    |    3397    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     119    |     119    |   18011 K  |   18011 K  |
|       from large pool |      99    |      99    |    9665 K  |    9665 K  |
|       from small pool |      20    |      48    |    8346 K  |    8346 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:16:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:16:57] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:16:58]    INFO >> epoch 011:    790 / 1539 loss=3.437, wps=4043.7, ups=6.4, wpb=631.5, bsz=631.5, num_updates=16150, lr=0.000161, gnorm=4.037, clip=0, train_wall=7, gb_free=4.3, wall=2606 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:17:05]    INFO >> epoch 011:    840 / 1539 loss=3.393, wps=4320.9, ups=7.09, wpb=609.7, bsz=609.7, num_updates=16200, lr=0.000161, gnorm=4.256, clip=0, train_wall=7, gb_free=72.5, wall=2613 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:17:12]    INFO >> epoch 011:    890 / 1539 loss=3.517, wps=5609.7, ups=6.96, wpb=806.2, bsz=806.2, num_updates=16250, lr=0.000161, gnorm=4.336, clip=0, train_wall=7, gb_free=65.9, wall=2621 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:17:21]    INFO >> epoch 011:    940 / 1539 loss=3.433, wps=5220.1, ups=6.72, wpb=776.8, bsz=776.8, num_updates=16300, lr=0.000161, gnorm=4.721, clip=2, train_wall=7, gb_free=72.6, wall=2628 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:17:28]    INFO >> epoch 011:    990 / 1539 loss=3.361, wps=4872.3, ups=6.92, wpb=704.4, bsz=704.4, num_updates=16350, lr=0.000161, gnorm=5.147, clip=0, train_wall=7, gb_free=71.2, wall=2635 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:17:35]    INFO >> epoch 011:   1040 / 1539 loss=3.408, wps=4626.7, ups=6.79, wpb=681.9, bsz=681.9, num_updates=16400, lr=0.000161, gnorm=4.09, clip=0, train_wall=7, gb_free=65.2, wall=2643 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:17:42] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 57.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 77.24 GiB is allocated by PyTorch, and 1.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:17:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:17:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:17:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 58        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79036 MiB |  79096 MiB | 470446 GiB | 470369 GiB |
|       from large pool |  78859 MiB |  78919 MiB | 467561 GiB | 467484 GiB |
|       from small pool |    177 MiB |    178 MiB |   2884 GiB |   2884 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79036 MiB |  79096 MiB | 470446 GiB | 470369 GiB |
|       from large pool |  78859 MiB |  78919 MiB | 467561 GiB | 467484 GiB |
|       from small pool |    177 MiB |    178 MiB |   2884 GiB |   2884 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79016 MiB |  79075 MiB | 469445 GiB | 469368 GiB |
|       from large pool |  78839 MiB |  78899 MiB | 466565 GiB | 466488 GiB |
|       from small pool |    176 MiB |    177 MiB |   2880 GiB |   2880 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80448 MiB |  80450 MiB | 462742 MiB | 382294 MiB |
|       from large pool |  80266 MiB |  80266 MiB | 455764 MiB | 375498 MiB |
|       from small pool |    182 MiB |    184 MiB |   6978 MiB |   6796 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1351 MiB |   7746 MiB | 451936 GiB | 451934 GiB |
|       from large pool |   1346 MiB |   7741 MiB | 448640 GiB | 448639 GiB |
|       from small pool |      4 MiB |     18 MiB |   3295 GiB |   3295 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    3447    |    3450    |   32201 K  |   32197 K  |
|       from large pool |     581    |     582    |   14814 K  |   14813 K  |
|       from small pool |    2866    |    2869    |   17387 K  |   17384 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    3447    |    3450    |   32201 K  |   32197 K  |
|       from large pool |     581    |     582    |   14814 K  |   14813 K  |
|       from small pool |    2866    |    2869    |   17387 K  |   17384 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     201    |     202    |    5654    |    5453    |
|       from large pool |     110    |     110    |    2165    |    2055    |
|       from small pool |      91    |      92    |    3489    |    3398    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     176    |     178    |   18293 K  |   18293 K  |
|       from large pool |      92    |      94    |    9826 K  |    9826 K  |
|       from small pool |      84    |      86    |    8467 K  |    8467 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:17:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:17:42] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:17:44]    INFO >> epoch 011:   1091 / 1539 loss=3.411, wps=4669.9, ups=5.8, wpb=804.9, bsz=804.9, num_updates=16450, lr=0.000161, gnorm=4.651, clip=0, train_wall=7, gb_free=76.1, wall=2651 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:17:53]    INFO >> epoch 011:   1141 / 1539 loss=3.338, wps=5759.3, ups=6.53, wpb=881.7, bsz=881.7, num_updates=16500, lr=0.000161, gnorm=4.806, clip=0, train_wall=7, gb_free=52.7, wall=2659 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:18:00]    INFO >> epoch 011:   1191 / 1539 loss=3.361, wps=4342.7, ups=6.84, wpb=635, bsz=635, num_updates=16550, lr=0.000161, gnorm=4.623, clip=0, train_wall=7, gb_free=76.9, wall=2666 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:18:08]    INFO >> epoch 011:   1241 / 1539 loss=3.368, wps=4671.4, ups=6.33, wpb=738.4, bsz=738.4, num_updates=16600, lr=0.000161, gnorm=4.423, clip=0, train_wall=7, gb_free=74.7, wall=2674 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:18:15]    INFO >> epoch 011:   1291 / 1539 loss=3.547, wps=5270.4, ups=6.85, wpb=769.1, bsz=769.1, num_updates=16650, lr=0.000161, gnorm=3.747, clip=0, train_wall=7, gb_free=74, wall=2681 (progress_bar.py:258, log())[0m
[33m[2025-11-21 07:18:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 2 has a total capacity of 79.14 GiB of which 281.25 MiB is free. Including non-PyTorch memory, this process has 78.84 GiB memory in use. Of the allocated memory 74.09 GiB is allocated by PyTorch, and 4.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 07:18:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:18:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:18:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 59        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72060 MiB |  75869 MiB | 476750 GiB | 476680 GiB |
|       from large pool |  72044 MiB |  75853 MiB | 473829 GiB | 473758 GiB |
|       from small pool |     15 MiB |     20 MiB |   2921 GiB |   2921 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72060 MiB |  75869 MiB | 476750 GiB | 476680 GiB |
|       from large pool |  72044 MiB |  75853 MiB | 473829 GiB | 473758 GiB |
|       from small pool |     15 MiB |     20 MiB |   2921 GiB |   2921 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72042 MiB |  75850 MiB | 475737 GiB | 475666 GiB |
|       from large pool |  72027 MiB |  75835 MiB | 472819 GiB | 472749 GiB |
|       from small pool |     15 MiB |     20 MiB |   2917 GiB |   2917 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80224 MiB |  80412 MiB | 462766 MiB | 382542 MiB |
|       from large pool |  80206 MiB |  80206 MiB | 455764 MiB | 375558 MiB |
|       from small pool |     18 MiB |    206 MiB |   7002 MiB |   6984 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5435 MiB |   8212 MiB | 458796 GiB | 458791 GiB |
|       from large pool |   5433 MiB |   8208 MiB | 455458 GiB | 455453 GiB |
|       from small pool |      2 MiB |     25 MiB |   3338 GiB |   3338 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     502    |     511    |   32623 K  |   32622 K  |
|       from large pool |     308    |     317    |   15017 K  |   15017 K  |
|       from small pool |     194    |     240    |   17605 K  |   17605 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     502    |     511    |   32623 K  |   32622 K  |
|       from large pool |     308    |     317    |   15017 K  |   15017 K  |
|       from small pool |     194    |     240    |   17605 K  |   17605 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     118    |     212    |    5666    |    5548    |
|       from large pool |     109    |     109    |    2165    |    2056    |
|       from small pool |       9    |     103    |    3501    |    3492    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     109    |     111    |   18527 K  |   18527 K  |
|       from large pool |      90    |      93    |    9956 K  |    9955 K  |
|       from small pool |      19    |      45    |    8571 K  |    8571 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:18:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 07:18:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 07:18:25]    INFO >> epoch 011:   1342 / 1539 loss=3.498, wps=4142.4, ups=6.09, wpb=680.6, bsz=680.6, num_updates=16700, lr=0.000161, gnorm=4.177, clip=0, train_wall=7, gb_free=73.5, wall=2690 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:18:32]    INFO >> epoch 011:   1392 / 1539 loss=3.555, wps=5168.7, ups=6.69, wpb=772.6, bsz=772.6, num_updates=16750, lr=0.000161, gnorm=4.14, clip=0, train_wall=7, gb_free=72.4, wall=2697 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:18:40]    INFO >> epoch 011:   1442 / 1539 loss=3.511, wps=4846.7, ups=6.84, wpb=708.6, bsz=708.6, num_updates=16800, lr=0.000161, gnorm=4.02, clip=0, train_wall=7, gb_free=62.2, wall=2704 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:18:47]    INFO >> epoch 011:   1492 / 1539 loss=3.319, wps=5154.3, ups=6.45, wpb=799.4, bsz=799.4, num_updates=16850, lr=0.000161, gnorm=4.19, clip=0, train_wall=7, gb_free=72.3, wall=2712 (progress_bar.py:258, log())[0m
[32m[2025-11-21 07:18:54]    INFO >> epoch 011 | loss 3.44 | wps 4544.1 | ups 6.32 | wpb 718.6 | bsz 718.6 | num_updates 16897 | lr 0.000161 | gnorm 4.422 | clip 0.1 | train_wall 211 | gb_free 74.7 | wall 2719 (progress_bar.py:267, print())[0m
[33m[2025-11-21 07:18:54] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 07:19:09]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.613 | wps 11688 | wpb 5412.5 | bsz 5412.5 | num_updates 16897 | best_loss 4.97 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 07:19:09]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 07:19:09]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_last.pt (epoch 11 @ 16897 updates, score 3.613) (writing took 0.011081 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-21 07:19:09]    INFO >> æ—©åœ: éªŒè¯æ€§èƒ½å·²10è½®æœªæå‡ (train_enhanced.py:616, single_main())[0m
[32m[2025-11-21 07:19:09]    INFO >> è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ 2660.4 ç§’ (train_enhanced.py:626, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 07:19:10]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 07:19:10]    INFO >> æ‰€æœ‰æ—¥å¿—å·²ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs (train_enhanced.py:631, single_main())[0m
[32m[2025-11-21 07:19:10]    INFO >> 
================================================================================ (train_enhanced.py:634, single_main())[0m
[32m[2025-11-21 07:19:10]    INFO >> å¼€å§‹æµ‹è¯•... (train_enhanced.py:635, single_main())[0m
[32m[2025-11-21 07:19:10]    INFO >> ================================================================================ (train_enhanced.py:636, single_main())[0m
[32m[2025-11-21 07:19:10]    INFO >> åŠ è½½æœ€ä½³checkpoint: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/checkpoint_best.pt (train_enhanced.py:50, run_test_after_training())[0m
[32m[2025-11-21 07:19:10]    INFO >> æµ‹è¯•é›†: test (train_enhanced.py:51, run_test_after_training())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/checkpoint_utils.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(
[32m[2025-11-21 07:20:04]    INFO >> 
================================================================================ (train_enhanced.py:168, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> æµ‹è¯•ç»“æœ: (train_enhanced.py:169, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> -------------------------------------------------------------------------------- (train_enhanced.py:170, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> å¹³å‡Loss:      3.9465 (train_enhanced.py:171, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> Acc@1:         18.62% (train_enhanced.py:172, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> Acc@5:         59.31% (train_enhanced.py:173, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> Acc@1 (å«any): 18.62% (train_enhanced.py:174, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> Acc@5 (å«any): 59.31% (train_enhanced.py:175, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> ================================================================================ (train_enhanced.py:176, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> æµ‹è¯•ç»“æœå·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/checkpoints/res.txt (train_enhanced.py:187, run_test_after_training())[0m
[32m[2025-11-21 07:20:04]    INFO >> è®­ç»ƒæ—¥å¿—å·²æ›´æ–°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs (train_enhanced.py:222, run_test_after_training())[0m
[TrainingLogger] æ—¥å¿—ç›®å½•: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs
[TrainingLogger] åŸå§‹è¾“å‡ºå°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/training_output.log
[TrainingLogger] Epoch 1 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 2 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 3 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 4 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 5 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 6 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 7 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 8 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 9 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 10 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
[TrainingLogger] Epoch 11 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/layers_1/logs/metrics.json
