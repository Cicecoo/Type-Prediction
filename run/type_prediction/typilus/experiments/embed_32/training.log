[32m[2025-11-21 00:12:48]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/config.yml (train_enhanced.py:666, cli_main())[0m
[32m[2025-11-21 00:12:48]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:694, cli_main())[0m
[32m[2025-11-21 00:12:48]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs (train_enhanced.py:561, single_main())[0m
[32m[2025-11-21 00:12:48]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 00:12:48]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-21 00:12:48]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-21 00:12:59]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 32, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=32, out_features=32, bias=False)
      (2): Dropout(p=0.1, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=32, out_features=32, bias=False)
          (OCCURRENCE_OF): Linear(in_features=32, out_features=32, bias=False)
          (NEXT): Linear(in_features=32, out_features=32, bias=False)
          (SUBTOKEN_OF): Linear(in_features=32, out_features=32, bias=False)
          (COMPUTED_FROM): Linear(in_features=32, out_features=32, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=32, out_features=32, bias=False)
          (NEXT_USE): Linear(in_features=32, out_features=32, bias=False)
          (RETURNS_TO): Linear(in_features=32, out_features=32, bias=False)
          (_CHILD): Linear(in_features=32, out_features=32, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=32, out_features=32, bias=False)
          (_NEXT): Linear(in_features=32, out_features=32, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=32, out_features=32, bias=False)
          (_COMPUTED_FROM): Linear(in_features=32, out_features=32, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=32, out_features=32, bias=False)
          (_NEXT_USE): Linear(in_features=32, out_features=32, bias=False)
          (_RETURNS_TO): Linear(in_features=32, out_features=32, bias=False)
        )
        (rnn_cell): GRUCell(32, 32)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=32, out_features=32, bias=False)
          (OCCURRENCE_OF): Linear(in_features=32, out_features=32, bias=False)
          (NEXT): Linear(in_features=32, out_features=32, bias=False)
          (SUBTOKEN_OF): Linear(in_features=32, out_features=32, bias=False)
          (COMPUTED_FROM): Linear(in_features=32, out_features=32, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=32, out_features=32, bias=False)
          (NEXT_USE): Linear(in_features=32, out_features=32, bias=False)
          (RETURNS_TO): Linear(in_features=32, out_features=32, bias=False)
          (_CHILD): Linear(in_features=32, out_features=32, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=32, out_features=32, bias=False)
          (_NEXT): Linear(in_features=32, out_features=32, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=32, out_features=32, bias=False)
          (_COMPUTED_FROM): Linear(in_features=32, out_features=32, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=32, out_features=32, bias=False)
          (_NEXT_USE): Linear(in_features=32, out_features=32, bias=False)
          (_RETURNS_TO): Linear(in_features=32, out_features=32, bias=False)
        )
        (rnn_cell): GRUCell(64, 32)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=32, out_features=32, bias=False)
      (1): Dropout(p=0.1, inplace=False)
      (2): Linear(in_features=32, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:568, single_main())[0m
[32m[2025-11-21 00:12:59]    INFO >> æ¨¡åž‹: typilus, æŸå¤±å‡½æ•°: TypilusCriterion (train_enhanced.py:569, single_main())[0m
[32m[2025-11-21 00:12:59]    INFO >> æ¨¡åž‹å‚æ•°: 373795 (å¯è®­ç»ƒ: 373795) (train_enhanced.py:570, single_main())[0m
[32m[2025-11-21 00:12:59]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 00:12:59]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80579 MB ; used memory = 1340 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 00:12:59]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-21 00:12:59]    INFO >> ä½¿ç”¨ 1 ä¸ªGPUè®­ç»ƒ (train_enhanced.py:576, single_main())[0m
[32m[2025-11-21 00:12:59]    INFO >> no existing checkpoint found /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-21 00:12:59]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-21 00:14:20]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-21 00:14:20] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 00:14:27]    INFO >> epoch 001:     50 / 1539 loss=5.561, wps=5506.2, ups=7.62, wpb=720, bsz=720, num_updates=50, lr=0.0004, gnorm=6.392, clip=0, train_wall=6, gb_free=76.9, wall=83 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:14:33]    INFO >> epoch 001:    100 / 1539 loss=5.666, wps=6594.7, ups=8.26, wpb=798.5, bsz=798.5, num_updates=100, lr=0.0004, gnorm=6.722, clip=0, train_wall=5, gb_free=75.6, wall=89 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:14:39]    INFO >> epoch 001:    150 / 1539 loss=5.666, wps=7148.1, ups=8.73, wpb=818.6, bsz=818.6, num_updates=150, lr=0.0004, gnorm=7.876, clip=0, train_wall=5, gb_free=76.4, wall=95 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:14:44]    INFO >> epoch 001:    200 / 1539 loss=5.656, wps=6097.5, ups=9.59, wpb=635.8, bsz=635.8, num_updates=200, lr=0.0004, gnorm=9.186, clip=0, train_wall=4, gb_free=76.4, wall=100 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:14:51]    INFO >> epoch 001:    250 / 1539 loss=5.804, wps=5556.8, ups=8.71, wpb=638, bsz=638, num_updates=250, lr=0.0004, gnorm=9.324, clip=0, train_wall=5, gb_free=76.9, wall=106 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:14:57]    INFO >> epoch 001:    300 / 1539 loss=5.725, wps=6157.5, ups=7.81, wpb=788.8, bsz=788.8, num_updates=300, lr=0.0004, gnorm=9.754, clip=0, train_wall=6, gb_free=74.3, wall=113 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:15:03]    INFO >> epoch 001:    350 / 1539 loss=5.876, wps=5886.4, ups=8.85, wpb=665.5, bsz=665.5, num_updates=350, lr=0.0004, gnorm=10.477, clip=2, train_wall=5, gb_free=75, wall=118 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:15:09]    INFO >> epoch 001:    400 / 1539 loss=5.73, wps=7083.4, ups=8.24, wpb=859.7, bsz=859.7, num_updates=400, lr=0.0004, gnorm=11.428, clip=0, train_wall=5, gb_free=75, wall=124 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:15:13] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 51.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.10 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:15:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:15:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:15:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78893 MiB |  78953 MiB |   6726 GiB |   6649 GiB |
|       from large pool |  78445 MiB |  78505 MiB |   6646 GiB |   6569 GiB |
|       from small pool |    447 MiB |    448 MiB |     79 GiB |     79 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78893 MiB |  78953 MiB |   6726 GiB |   6649 GiB |
|       from large pool |  78445 MiB |  78505 MiB |   6646 GiB |   6569 GiB |
|       from small pool |    447 MiB |    448 MiB |     79 GiB |     79 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78537 MiB |  78597 MiB |   6678 GiB |   6601 GiB |
|       from large pool |  78091 MiB |  78151 MiB |   6598 GiB |   6522 GiB |
|       from small pool |    445 MiB |    446 MiB |     79 GiB |     79 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80456 MiB |  80456 MiB | 133742 MiB |  53286 MiB |
|       from large pool |  79998 MiB |  79998 MiB | 132854 MiB |  52856 MiB |
|       from small pool |    458 MiB |    458 MiB |    888 MiB |    430 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1502 MiB |   4359 MiB |   3213 GiB |   3211 GiB |
|       from large pool |   1492 MiB |   4356 MiB |   3117 GiB |   3116 GiB |
|       from small pool |     10 MiB |     26 MiB |     95 GiB |     95 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8394    |    8397    |     917 K  |     908 K  |
|       from large pool |    1039    |    1040    |     414 K  |     413 K  |
|       from small pool |    7355    |    7358    |     502 K  |     495 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8394    |    8397    |     917 K  |     908 K  |
|       from large pool |    1039    |    1040    |     414 K  |     413 K  |
|       from small pool |    7355    |    7358    |     502 K  |     495 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |    1067    |    1158    |    2190    |    1123    |
|       from large pool |     838    |     941    |    1746    |     908    |
|       from small pool |     229    |     229    |     444    |     215    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     332    |     332    |  559856    |  559524    |
|       from large pool |     140    |     140    |  316163    |  316023    |
|       from small pool |     192    |     192    |  243693    |  243501    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:15:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:15:13] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:15:22]    INFO >> epoch 001:    451 / 1539 loss=5.821, wps=2562, ups=4.17, wpb=614.5, bsz=614.5, num_updates=450, lr=0.0004, gnorm=10.841, clip=2, train_wall=5, gb_free=77.2, wall=136 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:15:30]    INFO >> epoch 001:    501 / 1539 loss=5.777, wps=4962.9, ups=6.51, wpb=762.8, bsz=762.8, num_updates=500, lr=0.0004, gnorm=10.374, clip=2, train_wall=7, gb_free=76.1, wall=144 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:15:36]    INFO >> epoch 001:    551 / 1539 loss=5.87, wps=5961.4, ups=9.13, wpb=653.1, bsz=653.1, num_updates=550, lr=0.0004, gnorm=10.254, clip=0, train_wall=5, gb_free=72.7, wall=149 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:15:41]    INFO >> epoch 001:    601 / 1539 loss=5.874, wps=5909.2, ups=8.83, wpb=668.9, bsz=668.9, num_updates=600, lr=0.0004, gnorm=10.29, clip=2, train_wall=5, gb_free=77.5, wall=155 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:15:47]    INFO >> epoch 001:    651 / 1539 loss=5.849, wps=6189.6, ups=8.71, wpb=710.3, bsz=710.3, num_updates=650, lr=0.0004, gnorm=9.807, clip=0, train_wall=5, gb_free=75.3, wall=161 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:15:55]    INFO >> epoch 001:    701 / 1539 loss=5.721, wps=5302.9, ups=7.87, wpb=673.4, bsz=673.4, num_updates=700, lr=0.0004, gnorm=9.862, clip=0, train_wall=5, gb_free=75.4, wall=167 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:01]    INFO >> epoch 001:    751 / 1539 loss=5.608, wps=6433.5, ups=8.46, wpb=760.6, bsz=760.6, num_updates=750, lr=0.0004, gnorm=10.257, clip=2, train_wall=5, gb_free=64.3, wall=173 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:06]    INFO >> epoch 001:    801 / 1539 loss=5.803, wps=6272.3, ups=8.74, wpb=717.9, bsz=717.9, num_updates=800, lr=0.0004, gnorm=9.836, clip=2, train_wall=5, gb_free=76.1, wall=179 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:12]    INFO >> epoch 001:    851 / 1539 loss=5.635, wps=5614.8, ups=8.74, wpb=642.6, bsz=642.6, num_updates=850, lr=0.0004, gnorm=10.342, clip=0, train_wall=5, gb_free=77.3, wall=185 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:18]    INFO >> epoch 001:    901 / 1539 loss=5.641, wps=5926.2, ups=8.97, wpb=660.3, bsz=660.3, num_updates=900, lr=0.0004, gnorm=10.222, clip=0, train_wall=5, gb_free=77.5, wall=190 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:23]    INFO >> epoch 001:    951 / 1539 loss=5.637, wps=6266.9, ups=8.77, wpb=714.2, bsz=714.2, num_updates=950, lr=0.0004, gnorm=10.706, clip=2, train_wall=5, gb_free=75.3, wall=196 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:31]    INFO >> epoch 001:   1001 / 1539 loss=5.585, wps=5555.2, ups=7.64, wpb=727.3, bsz=727.3, num_updates=1000, lr=0.0004, gnorm=10.374, clip=2, train_wall=6, gb_free=73.6, wall=202 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:37]    INFO >> epoch 001:   1051 / 1539 loss=5.541, wps=6676.7, ups=8.26, wpb=808.7, bsz=808.7, num_updates=1050, lr=0.0004, gnorm=9.971, clip=0, train_wall=5, gb_free=76.7, wall=208 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:16:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.15 GiB is allocated by PyTorch, and 1.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:16:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:16:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:16:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78945 MiB |  79005 MiB |  17406 GiB |  17329 GiB |
|       from large pool |  78585 MiB |  78645 MiB |  17216 GiB |  17139 GiB |
|       from small pool |    359 MiB |    360 MiB |    189 GiB |    189 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78945 MiB |  79005 MiB |  17406 GiB |  17329 GiB |
|       from large pool |  78585 MiB |  78645 MiB |  17216 GiB |  17139 GiB |
|       from small pool |    359 MiB |    360 MiB |    189 GiB |    189 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78581 MiB |  78640 MiB |  17303 GiB |  17226 GiB |
|       from large pool |  78224 MiB |  78283 MiB |  17114 GiB |  17037 GiB |
|       from small pool |    357 MiB |    358 MiB |    189 GiB |    189 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80464 MiB |  80482 MiB | 211170 MiB | 130706 MiB |
|       from large pool |  80066 MiB |  80084 MiB | 209898 MiB | 129832 MiB |
|       from small pool |    398 MiB |    398 MiB |   1272 MiB |    874 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1458 MiB |   6178 MiB |  13499 GiB |  13497 GiB |
|       from large pool |   1420 MiB |   6174 MiB |  13272 GiB |  13271 GiB |
|       from small pool |     38 MiB |     39 MiB |    226 GiB |    226 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6832    |    6835    |    2249 K  |    2242 K  |
|       from large pool |     903    |     904    |    1059 K  |    1059 K  |
|       from small pool |    5929    |    5932    |    1189 K  |    1183 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6832    |    6835    |    2249 K  |    2242 K  |
|       from large pool |     903    |     904    |    1059 K  |    1059 K  |
|       from small pool |    5929    |    5932    |    1189 K  |    1183 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     774    |     775    |    2914    |    2140    |
|       from large pool |     575    |     576    |    2278    |    1703    |
|       from small pool |     199    |     199    |     636    |     437    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     471    |     472    |    1320 K  |    1320 K  |
|       from large pool |     110    |     111    |     768 K  |     768 K  |
|       from small pool |     361    |     362    |     552 K  |     551 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:16:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:16:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:16:44]    INFO >> epoch 001:   1102 / 1539 loss=5.486, wps=6224, ups=7, wpb=888.7, bsz=888.7, num_updates=1100, lr=0.0004, gnorm=10.71, clip=4, train_wall=6, gb_free=76.1, wall=216 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:50]    INFO >> epoch 001:   1152 / 1539 loss=5.553, wps=6094.8, ups=8.26, wpb=737.9, bsz=737.9, num_updates=1150, lr=0.0004, gnorm=9.845, clip=0, train_wall=5, gb_free=76.9, wall=222 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:16:56]    INFO >> epoch 001:   1202 / 1539 loss=5.492, wps=5762.2, ups=8.56, wpb=672.9, bsz=672.9, num_updates=1200, lr=0.0004, gnorm=10.257, clip=0, train_wall=5, gb_free=76.1, wall=227 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:17:04]    INFO >> epoch 001:   1252 / 1539 loss=5.501, wps=6154.8, ups=8.43, wpb=730.1, bsz=730.1, num_updates=1250, lr=0.0004, gnorm=10.966, clip=0, train_wall=5, gb_free=75.4, wall=233 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:17:10]    INFO >> epoch 001:   1302 / 1539 loss=5.421, wps=6092.4, ups=8.16, wpb=746.9, bsz=746.9, num_updates=1300, lr=0.0004, gnorm=9.267, clip=0, train_wall=5, gb_free=73.8, wall=240 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:17:16]    INFO >> epoch 001:   1352 / 1539 loss=5.537, wps=5307.1, ups=8.52, wpb=622.9, bsz=622.9, num_updates=1350, lr=0.0004, gnorm=9.089, clip=0, train_wall=5, gb_free=77.9, wall=245 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:17:22]    INFO >> epoch 001:   1402 / 1539 loss=5.473, wps=6189.7, ups=8.35, wpb=741.7, bsz=741.7, num_updates=1400, lr=0.0004, gnorm=9.363, clip=0, train_wall=5, gb_free=75.5, wall=251 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:17:27]    INFO >> epoch 001:   1452 / 1539 loss=5.486, wps=6032.3, ups=8.56, wpb=704.9, bsz=704.9, num_updates=1450, lr=0.0004, gnorm=9.287, clip=0, train_wall=5, gb_free=74.9, wall=257 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:17:33]    INFO >> epoch 001:   1502 / 1539 loss=5.335, wps=6050.5, ups=8.62, wpb=702.3, bsz=702.3, num_updates=1500, lr=0.0004, gnorm=9.971, clip=0, train_wall=5, gb_free=76.6, wall=263 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:17:39]    INFO >> epoch 001 | loss 5.633 | wps 5821.4 | ups 8.08 | wpb 720.8 | bsz 720.8 | num_updates 1537 | lr 0.0004 | gnorm 9.752 | clip 0.7 | train_wall 158 | gb_free 78.2 | wall 267 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:17:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:17:48]    INFO >> epoch 001 | valid on 'valid' subset | loss 5.506 | wps 16347.4 | wpb 5412.5 | bsz 5412.5 | num_updates 1537 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:370: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:371: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-21 00:17:49]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:17:49]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_best.pt (epoch 1 @ 1537 updates, score 5.506) (writing took 0.013126 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:17:49] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-21 00:17:50]    INFO >> epoch 002:     13 / 1539 loss=5.361, wps=2298.6, ups=3.22, wpb=714.8, bsz=714.8, num_updates=1550, lr=0.0004, gnorm=8.534, clip=0, train_wall=5, gb_free=76.9, wall=279 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:17:56]    INFO >> epoch 002:     63 / 1539 loss=5.342, wps=6090.2, ups=9.05, wpb=672.6, bsz=672.6, num_updates=1600, lr=0.0004, gnorm=8.732, clip=0, train_wall=5, gb_free=76.5, wall=284 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:01]    INFO >> epoch 002:    113 / 1539 loss=5.28, wps=6323.8, ups=8.82, wpb=717.1, bsz=717.1, num_updates=1650, lr=0.0004, gnorm=8.934, clip=0, train_wall=5, gb_free=76.1, wall=290 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:07]    INFO >> epoch 002:    163 / 1539 loss=5.147, wps=6245.6, ups=8.65, wpb=722.3, bsz=722.3, num_updates=1700, lr=0.0004, gnorm=8.915, clip=0, train_wall=5, gb_free=75.8, wall=296 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:14]    INFO >> epoch 002:    213 / 1539 loss=5.285, wps=5969.9, ups=8.37, wpb=713.4, bsz=713.4, num_updates=1750, lr=0.0004, gnorm=10.408, clip=0, train_wall=5, gb_free=76.6, wall=302 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:21]    INFO >> epoch 002:    263 / 1539 loss=5.031, wps=6882.8, ups=7.99, wpb=861.4, bsz=861.4, num_updates=1800, lr=0.0004, gnorm=9.109, clip=2, train_wall=6, gb_free=77, wall=308 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:27]    INFO >> epoch 002:    313 / 1539 loss=5.238, wps=5540.8, ups=8.45, wpb=655.5, bsz=655.5, num_updates=1850, lr=0.0004, gnorm=8.997, clip=0, train_wall=5, gb_free=76.2, wall=314 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:32]    INFO >> epoch 002:    363 / 1539 loss=5.148, wps=5878, ups=8.86, wpb=663.3, bsz=663.3, num_updates=1900, lr=0.0004, gnorm=9.089, clip=0, train_wall=5, gb_free=72.4, wall=319 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:38]    INFO >> epoch 002:    413 / 1539 loss=5.165, wps=5970.9, ups=8.4, wpb=710.4, bsz=710.4, num_updates=1950, lr=0.0004, gnorm=8.357, clip=0, train_wall=5, gb_free=70.8, wall=325 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:45]    INFO >> epoch 002:    463 / 1539 loss=5.093, wps=6061.8, ups=7.69, wpb=788.1, bsz=788.1, num_updates=2000, lr=0.0004, gnorm=10.154, clip=2, train_wall=6, gb_free=76.2, wall=332 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:50]    INFO >> epoch 002:    513 / 1539 loss=5.144, wps=6349.2, ups=8.88, wpb=714.8, bsz=714.8, num_updates=2050, lr=0.0004, gnorm=9.822, clip=0, train_wall=5, gb_free=74.2, wall=337 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:18:56]    INFO >> epoch 002:    563 / 1539 loss=5.147, wps=5665.9, ups=9, wpb=629.4, bsz=629.4, num_updates=2100, lr=0.0004, gnorm=9.192, clip=0, train_wall=5, gb_free=76.3, wall=343 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:19:02]    INFO >> epoch 002:    613 / 1539 loss=5.08, wps=6786.5, ups=7.88, wpb=860.8, bsz=860.8, num_updates=2150, lr=0.0004, gnorm=9.688, clip=2, train_wall=6, gb_free=77.6, wall=349 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:19:08]    INFO >> epoch 002:    663 / 1539 loss=4.996, wps=6129.3, ups=8.09, wpb=758.1, bsz=758.1, num_updates=2200, lr=0.0004, gnorm=9.871, clip=0, train_wall=6, gb_free=76, wall=356 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:19:17]    INFO >> epoch 002:    713 / 1539 loss=5.082, wps=5779.6, ups=8.34, wpb=693, bsz=693, num_updates=2250, lr=0.0004, gnorm=9.263, clip=2, train_wall=5, gb_free=77.5, wall=362 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:19:23]    INFO >> epoch 002:    763 / 1539 loss=4.994, wps=5782.3, ups=8.38, wpb=689.7, bsz=689.7, num_updates=2300, lr=0.0004, gnorm=8.574, clip=2, train_wall=5, gb_free=73.8, wall=367 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:19:29]    INFO >> epoch 002:    813 / 1539 loss=5.068, wps=5505.4, ups=8.6, wpb=640.4, bsz=640.4, num_updates=2350, lr=0.0004, gnorm=7.969, clip=0, train_wall=5, gb_free=72.7, wall=373 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:19:35]    INFO >> epoch 002:    863 / 1539 loss=4.917, wps=5894.3, ups=7.89, wpb=747.2, bsz=747.2, num_updates=2400, lr=0.0004, gnorm=9.114, clip=4, train_wall=6, gb_free=75.4, wall=380 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:19:41]    INFO >> epoch 002:    913 / 1539 loss=5.022, wps=5821.3, ups=8.65, wpb=673.2, bsz=673.2, num_updates=2450, lr=0.0004, gnorm=7.555, clip=0, train_wall=5, gb_free=75.4, wall=385 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:19:47]    INFO >> epoch 002:    963 / 1539 loss=4.963, wps=5520, ups=8.71, wpb=633.4, bsz=633.4, num_updates=2500, lr=0.0004, gnorm=8.41, clip=2, train_wall=5, gb_free=77.1, wall=391 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:19:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 45.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.87 GiB is allocated by PyTorch, and 1.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:19:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:19:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:19:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 5         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78656 MiB |  78715 MiB |  41925 GiB |  41848 GiB |
|       from large pool |  78209 MiB |  78269 MiB |  41461 GiB |  41385 GiB |
|       from small pool |    446 MiB |    447 MiB |    463 GiB |    462 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78656 MiB |  78715 MiB |  41925 GiB |  41848 GiB |
|       from large pool |  78209 MiB |  78269 MiB |  41461 GiB |  41385 GiB |
|       from small pool |    446 MiB |    447 MiB |    463 GiB |    462 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78417 MiB |  78476 MiB |  41700 GiB |  41623 GiB |
|       from large pool |  77972 MiB |  78032 MiB |  41237 GiB |  41161 GiB |
|       from small pool |    444 MiB |    445 MiB |    462 GiB |    462 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80464 MiB | 211230 MiB | 130768 MiB |
|       from large pool |  80006 MiB |  80006 MiB | 209898 MiB | 129892 MiB |
|       from small pool |    456 MiB |    458 MiB |   1332 MiB |    876 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1805 MiB |   4507 MiB |  32047 GiB |  32045 GiB |
|       from large pool |   1796 MiB |   4505 MiB |  31504 GiB |  31502 GiB |
|       from small pool |      9 MiB |     26 MiB |    542 GiB |    542 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8374    |    8377    |    5379 K  |    5371 K  |
|       from large pool |    1037    |    1038    |    2478 K  |    2477 K  |
|       from small pool |    7337    |    7340    |    2901 K  |    2894 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8374    |    8377    |    5379 K  |    5371 K  |
|       from large pool |    1037    |    1038    |    2478 K  |    2477 K  |
|       from small pool |    7337    |    7340    |    2901 K  |    2894 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     802    |     803    |    2944    |    2142    |
|       from large pool |     574    |     574    |    2278    |    1704    |
|       from small pool |     228    |     229    |     666    |     438    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     324    |     326    |    3163 K  |    3163 K  |
|       from large pool |     134    |     134    |    1793 K  |    1793 K  |
|       from small pool |     190    |     192    |    1369 K  |    1369 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:19:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:19:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:19:55]    INFO >> epoch 002:   1014 / 1539 loss=4.848, wps=6195.2, ups=7.09, wpb=874.4, bsz=874.4, num_updates=2550, lr=0.0004, gnorm=8.141, clip=0, train_wall=6, gb_free=75.9, wall=398 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:01]    INFO >> epoch 002:   1064 / 1539 loss=4.84, wps=6259.6, ups=8.14, wpb=769.2, bsz=769.2, num_updates=2600, lr=0.0004, gnorm=8.119, clip=0, train_wall=6, gb_free=71, wall=404 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:08]    INFO >> epoch 002:   1114 / 1539 loss=4.604, wps=5936.1, ups=7.73, wpb=767.6, bsz=767.6, num_updates=2650, lr=0.0004, gnorm=8.716, clip=0, train_wall=6, gb_free=77.1, wall=411 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:14]    INFO >> epoch 002:   1164 / 1539 loss=4.715, wps=6587.6, ups=8.28, wpb=795.4, bsz=795.4, num_updates=2700, lr=0.0004, gnorm=10.402, clip=4, train_wall=6, gb_free=76.1, wall=417 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:20]    INFO >> epoch 002:   1214 / 1539 loss=4.899, wps=5736.3, ups=8.19, wpb=700.1, bsz=700.1, num_updates=2750, lr=0.0004, gnorm=7.383, clip=0, train_wall=6, gb_free=77, wall=423 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:20:24] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 5.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.22 GiB is allocated by PyTorch, and 3.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:20:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:20:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:20:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76966 MiB |  77026 MiB |  45794 GiB |  45719 GiB |
|       from large pool |  76626 MiB |  76686 MiB |  45288 GiB |  45213 GiB |
|       from small pool |    340 MiB |    341 MiB |    506 GiB |    506 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76966 MiB |  77026 MiB |  45794 GiB |  45719 GiB |
|       from large pool |  76626 MiB |  76686 MiB |  45288 GiB |  45213 GiB |
|       from small pool |    340 MiB |    341 MiB |    506 GiB |    506 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76666 MiB |  76725 MiB |  45550 GiB |  45475 GiB |
|       from large pool |  76328 MiB |  76387 MiB |  45044 GiB |  44970 GiB |
|       from small pool |    338 MiB |    339 MiB |    505 GiB |    505 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB | 211352 MiB | 130850 MiB |
|       from large pool |  80126 MiB |  80126 MiB | 210018 MiB | 129892 MiB |
|       from small pool |    376 MiB |    456 MiB |   1334 MiB |    958 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3475 MiB |   7773 MiB |  35130 GiB |  35127 GiB |
|       from large pool |   3439 MiB |   7742 MiB |  34535 GiB |  34532 GiB |
|       from small pool |     35 MiB |     37 MiB |    594 GiB |    594 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6482    |    6485    |    5882 K  |    5875 K  |
|       from large pool |     871    |     872    |    2707 K  |    2707 K  |
|       from small pool |    5611    |    5614    |    3174 K  |    3168 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6482    |    6485    |    5882 K  |    5875 K  |
|       from large pool |     871    |     872    |    2707 K  |    2707 K  |
|       from small pool |    5611    |    5614    |    3174 K  |    3168 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     764    |     802    |    2947    |    2183    |
|       from large pool |     576    |     576    |    2280    |    1704    |
|       from small pool |     188    |     228    |     667    |     479    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     529    |     530    |    3461 K  |    3460 K  |
|       from large pool |     185    |     185    |    1959 K  |    1959 K  |
|       from small pool |     344    |     345    |    1501 K  |    1501 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:20:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:20:24] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:20:28]    INFO >> epoch 002:   1265 / 1539 loss=4.729, wps=5719.3, ups=7.46, wpb=767.1, bsz=767.1, num_updates=2800, lr=0.0004, gnorm=7.471, clip=0, train_wall=6, gb_free=75.1, wall=430 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:34]    INFO >> epoch 002:   1315 / 1539 loss=4.628, wps=5810.8, ups=8.74, wpb=664.7, bsz=664.7, num_updates=2850, lr=0.0004, gnorm=7.828, clip=0, train_wall=5, gb_free=75.9, wall=435 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:40]    INFO >> epoch 002:   1365 / 1539 loss=4.636, wps=5870.1, ups=8.3, wpb=707, bsz=707, num_updates=2900, lr=0.0004, gnorm=6.967, clip=0, train_wall=5, gb_free=74.3, wall=441 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:46]    INFO >> epoch 002:   1415 / 1539 loss=4.687, wps=5726.8, ups=8.5, wpb=673.7, bsz=673.7, num_updates=2950, lr=0.0004, gnorm=7.784, clip=0, train_wall=5, gb_free=75.5, wall=447 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:52]    INFO >> epoch 002:   1465 / 1539 loss=4.654, wps=5262, ups=7.58, wpb=693.9, bsz=693.9, num_updates=3000, lr=0.0004, gnorm=7.691, clip=0, train_wall=6, gb_free=74.6, wall=454 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:20:58]    INFO >> epoch 002:   1515 / 1539 loss=4.636, wps=5386.1, ups=7.93, wpb=679.1, bsz=679.1, num_updates=3050, lr=0.0004, gnorm=7.667, clip=2, train_wall=6, gb_free=77.3, wall=460 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:21:02]    INFO >> epoch 002 | loss 4.959 | wps 5650 | ups 7.84 | wpb 720.8 | bsz 720.8 | num_updates 3074 | lr 0.0004 | gnorm 8.671 | clip 0.7 | train_wall 169 | gb_free 76 | wall 463 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:21:02] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:21:10]    INFO >> epoch 002 | valid on 'valid' subset | loss 4.75 | wps 18543.1 | wpb 5412.5 | bsz 5412.5 | num_updates 3074 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:21:10]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:21:10]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 2 @ 3074 updates, score 4.75) (writing took 0.011590 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:21:10] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:21:13]    INFO >> epoch 003:     26 / 1539 loss=4.561, wps=2301.7, ups=3.4, wpb=677.9, bsz=677.9, num_updates=3100, lr=0.000392, gnorm=8.329, clip=0, train_wall=5, gb_free=77.3, wall=475 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:21:19]    INFO >> epoch 003:     76 / 1539 loss=4.647, wps=6543, ups=8.43, wpb=776.1, bsz=776.1, num_updates=3150, lr=0.000392, gnorm=7.845, clip=0, train_wall=5, gb_free=72.9, wall=481 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:21:26]    INFO >> epoch 003:    126 / 1539 loss=4.555, wps=6498, ups=7.51, wpb=865.1, bsz=865.1, num_updates=3200, lr=0.000392, gnorm=8.939, clip=0, train_wall=6, gb_free=75.3, wall=488 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:21:34]    INFO >> epoch 003:    176 / 1539 loss=4.702, wps=5729.9, ups=8.6, wpb=666.3, bsz=666.3, num_updates=3250, lr=0.000392, gnorm=7.524, clip=0, train_wall=5, gb_free=74.5, wall=493 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:21:39] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 5.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.22 GiB is allocated by PyTorch, and 3.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:21:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:21:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:21:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 8         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76966 MiB |  77026 MiB |  56388 GiB |  56313 GiB |
|       from large pool |  76626 MiB |  76686 MiB |  55760 GiB |  55685 GiB |
|       from small pool |    340 MiB |    341 MiB |    628 GiB |    628 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76966 MiB |  77026 MiB |  56388 GiB |  56313 GiB |
|       from large pool |  76626 MiB |  76686 MiB |  55760 GiB |  55685 GiB |
|       from small pool |    340 MiB |    341 MiB |    628 GiB |    628 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76666 MiB |  76725 MiB |  56099 GiB |  56024 GiB |
|       from large pool |  76328 MiB |  76387 MiB |  55471 GiB |  55396 GiB |
|       from small pool |    338 MiB |    339 MiB |    627 GiB |    627 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB | 211414 MiB | 130912 MiB |
|       from large pool |  80126 MiB |  80126 MiB | 210078 MiB | 129952 MiB |
|       from small pool |    376 MiB |    378 MiB |   1336 MiB |    960 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3475 MiB |   7773 MiB |  42880 GiB |  42877 GiB |
|       from large pool |   3439 MiB |   7742 MiB |  42148 GiB |  42145 GiB |
|       from small pool |     35 MiB |     37 MiB |    732 GiB |    732 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6482    |    6485    |    7182 K  |    7176 K  |
|       from large pool |     871    |     872    |    3236 K  |    3235 K  |
|       from small pool |    5611    |    5614    |    3945 K  |    3940 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6482    |    6485    |    7182 K  |    7176 K  |
|       from large pool |     871    |     872    |    3236 K  |    3235 K  |
|       from small pool |    5611    |    5614    |    3945 K  |    3940 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     764    |     765    |    2949    |    2185    |
|       from large pool |     576    |     576    |    2281    |    1705    |
|       from small pool |     188    |     189    |     668    |     480    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     530    |     531    |    4244 K  |    4244 K  |
|       from large pool |     185    |     185    |    2343 K  |    2343 K  |
|       from small pool |     345    |     346    |    1901 K  |    1901 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:21:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:21:39] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:21:41]    INFO >> epoch 003:    227 / 1539 loss=4.672, wps=5662.3, ups=7.17, wpb=789.3, bsz=789.3, num_updates=3300, lr=0.000392, gnorm=6.724, clip=0, train_wall=6, gb_free=76.4, wall=500 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:21:47]    INFO >> epoch 003:    277 / 1539 loss=4.408, wps=6011.2, ups=8.17, wpb=735.6, bsz=735.6, num_updates=3350, lr=0.000392, gnorm=7.908, clip=0, train_wall=6, gb_free=76.1, wall=506 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:21:54]    INFO >> epoch 003:    327 / 1539 loss=4.449, wps=6518.8, ups=7.88, wpb=827.1, bsz=827.1, num_updates=3400, lr=0.000392, gnorm=7.157, clip=0, train_wall=6, gb_free=75.6, wall=513 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:00]    INFO >> epoch 003:    377 / 1539 loss=4.491, wps=5450.7, ups=8.2, wpb=664.4, bsz=664.4, num_updates=3450, lr=0.000392, gnorm=7.262, clip=0, train_wall=6, gb_free=74.1, wall=519 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:07]    INFO >> epoch 003:    427 / 1539 loss=4.61, wps=5470.9, ups=8.47, wpb=646.2, bsz=646.2, num_updates=3500, lr=0.000392, gnorm=6.669, clip=0, train_wall=5, gb_free=75.9, wall=525 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:13]    INFO >> epoch 003:    477 / 1539 loss=4.402, wps=6352.1, ups=8.19, wpb=775.9, bsz=775.9, num_updates=3550, lr=0.000392, gnorm=8.016, clip=0, train_wall=6, gb_free=73.2, wall=531 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:19]    INFO >> epoch 003:    527 / 1539 loss=4.334, wps=5721.6, ups=8.05, wpb=710.7, bsz=710.7, num_updates=3600, lr=0.000392, gnorm=7.833, clip=2, train_wall=6, gb_free=76.6, wall=537 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:26]    INFO >> epoch 003:    577 / 1539 loss=4.362, wps=6151.6, ups=7.88, wpb=780.7, bsz=780.7, num_updates=3650, lr=0.000392, gnorm=7.353, clip=0, train_wall=6, gb_free=74.4, wall=543 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:31]    INFO >> epoch 003:    627 / 1539 loss=4.455, wps=5875, ups=8.89, wpb=660.9, bsz=660.9, num_updates=3700, lr=0.000392, gnorm=7.52, clip=0, train_wall=5, gb_free=74.9, wall=549 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:39]    INFO >> epoch 003:    677 / 1539 loss=4.452, wps=6356.9, ups=8.21, wpb=774.2, bsz=774.2, num_updates=3750, lr=0.000392, gnorm=6.575, clip=0, train_wall=6, gb_free=71.9, wall=555 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:45]    INFO >> epoch 003:    727 / 1539 loss=4.224, wps=6059.9, ups=7.7, wpb=787.5, bsz=787.5, num_updates=3800, lr=0.000392, gnorm=6.947, clip=0, train_wall=6, gb_free=76.4, wall=562 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:22:52]    INFO >> epoch 003:    777 / 1539 loss=4.113, wps=6236.3, ups=7.78, wpb=801.7, bsz=801.7, num_updates=3850, lr=0.000392, gnorm=7.334, clip=2, train_wall=6, gb_free=46.7, wall=568 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:22:53] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 29.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.94 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:22:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:22:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:22:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78727 MiB |  78787 MiB |  65588 GiB |  65511 GiB |
|       from large pool |  78280 MiB |  78340 MiB |  64859 GiB |  64782 GiB |
|       from small pool |    447 MiB |    448 MiB |    728 GiB |    728 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78727 MiB |  78787 MiB |  65588 GiB |  65511 GiB |
|       from large pool |  78280 MiB |  78340 MiB |  64859 GiB |  64782 GiB |
|       from small pool |    447 MiB |    448 MiB |    728 GiB |    728 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78477 MiB |  78537 MiB |  65250 GiB |  65173 GiB |
|       from large pool |  78032 MiB |  78091 MiB |  64522 GiB |  64446 GiB |
|       from small pool |    445 MiB |    446 MiB |    727 GiB |    727 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80478 MiB |  80506 MiB | 212816 MiB | 132338 MiB |
|       from large pool |  80022 MiB |  80066 MiB | 211398 MiB | 131376 MiB |
|       from small pool |    456 MiB |    458 MiB |   1418 MiB |    962 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1690 MiB |   4509 MiB |  50098 GiB |  50096 GiB |
|       from large pool |   1681 MiB |   4505 MiB |  49246 GiB |  49245 GiB |
|       from small pool |      8 MiB |     18 MiB |    851 GiB |    851 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8385    |    8388    |    8370 K  |    8362 K  |
|       from large pool |    1038    |    1039    |    3797 K  |    3796 K  |
|       from small pool |    7347    |    7350    |    4573 K  |    4565 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8385    |    8388    |    8370 K  |    8362 K  |
|       from large pool |    1038    |    1039    |    3797 K  |    3796 K  |
|       from small pool |    7347    |    7350    |    4573 K  |    4565 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     824    |     825    |    3012    |    2188    |
|       from large pool |     596    |     596    |    2303    |    1707    |
|       from small pool |     228    |     229    |     709    |     481    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     323    |     326    |    4941 K  |    4940 K  |
|       from large pool |     133    |     133    |    2747 K  |    2746 K  |
|       from small pool |     190    |     193    |    2194 K  |    2194 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:22:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:22:53] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:22:58]    INFO >> epoch 003:    828 / 1539 loss=4.276, wps=5573.7, ups=7.51, wpb=742.6, bsz=742.6, num_updates=3900, lr=0.000392, gnorm=8.318, clip=0, train_wall=5, gb_free=76.4, wall=575 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:04]    INFO >> epoch 003:    878 / 1539 loss=4.34, wps=6122.9, ups=8.32, wpb=735.9, bsz=735.9, num_updates=3950, lr=0.000392, gnorm=6.808, clip=2, train_wall=6, gb_free=76.1, wall=581 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:10]    INFO >> epoch 003:    928 / 1539 loss=4.359, wps=5608.4, ups=8.23, wpb=681.4, bsz=681.4, num_updates=4000, lr=0.000392, gnorm=6.127, clip=0, train_wall=6, gb_free=75.4, wall=587 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:17]    INFO >> epoch 003:    978 / 1539 loss=4.342, wps=5174.3, ups=7.34, wpb=705, bsz=705, num_updates=4050, lr=0.000392, gnorm=6.47, clip=0, train_wall=6, gb_free=76.1, wall=594 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:23]    INFO >> epoch 003:   1028 / 1539 loss=4.159, wps=5305.4, ups=8.2, wpb=646.9, bsz=646.9, num_updates=4100, lr=0.000392, gnorm=6.641, clip=0, train_wall=6, gb_free=77, wall=600 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:29]    INFO >> epoch 003:   1078 / 1539 loss=4.222, wps=5677.8, ups=8.41, wpb=675.4, bsz=675.4, num_updates=4150, lr=0.000392, gnorm=6.136, clip=0, train_wall=5, gb_free=75.3, wall=606 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:35]    INFO >> epoch 003:   1128 / 1539 loss=4.16, wps=5974, ups=8.6, wpb=695, bsz=695, num_updates=4200, lr=0.000392, gnorm=6.248, clip=0, train_wall=5, gb_free=74.6, wall=612 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:42]    INFO >> epoch 003:   1178 / 1539 loss=3.858, wps=6207.1, ups=7.43, wpb=835.2, bsz=835.2, num_updates=4250, lr=0.000392, gnorm=6.032, clip=0, train_wall=6, gb_free=75.8, wall=618 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:51]    INFO >> epoch 003:   1228 / 1539 loss=4.093, wps=5945.9, ups=8.03, wpb=740.1, bsz=740.1, num_updates=4300, lr=0.000392, gnorm=7.032, clip=0, train_wall=6, gb_free=75.4, wall=624 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:23:57]    INFO >> epoch 003:   1278 / 1539 loss=4.155, wps=5273.2, ups=8.45, wpb=624.4, bsz=624.4, num_updates=4350, lr=0.000392, gnorm=5.748, clip=0, train_wall=5, gb_free=77.3, wall=630 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:24:03]    INFO >> epoch 003:   1328 / 1539 loss=4.165, wps=5815.3, ups=8.55, wpb=679.9, bsz=679.9, num_updates=4400, lr=0.000392, gnorm=5.755, clip=0, train_wall=5, gb_free=76.1, wall=636 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:24:08]    INFO >> epoch 003:   1378 / 1539 loss=4.085, wps=5858.4, ups=8.46, wpb=692.2, bsz=692.2, num_updates=4450, lr=0.000392, gnorm=6.486, clip=0, train_wall=5, gb_free=75.5, wall=642 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:24:14]    INFO >> epoch 003:   1428 / 1539 loss=4.103, wps=5874.2, ups=8.64, wpb=680.1, bsz=680.1, num_updates=4500, lr=0.000392, gnorm=6.285, clip=0, train_wall=5, gb_free=73.5, wall=648 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:24:22]    INFO >> epoch 003:   1478 / 1539 loss=4.195, wps=5157.4, ups=8.32, wpb=619.7, bsz=619.7, num_updates=4550, lr=0.000392, gnorm=5.669, clip=0, train_wall=5, gb_free=77.1, wall=654 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:24:28]    INFO >> epoch 003:   1528 / 1539 loss=4.152, wps=5657.7, ups=8.29, wpb=682.4, bsz=682.4, num_updates=4600, lr=0.000392, gnorm=6.969, clip=0, train_wall=6, gb_free=76, wall=660 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:24:29]    INFO >> epoch 003 | loss 4.323 | wps 5596.6 | ups 7.76 | wpb 720.8 | bsz 720.8 | num_updates 4611 | lr 0.000392 | gnorm 6.949 | clip 0.2 | train_wall 172 | gb_free 77 | wall 661 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:24:29] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:24:37]    INFO >> epoch 003 | valid on 'valid' subset | loss 4.132 | wps 18267.6 | wpb 5412.5 | bsz 5412.5 | num_updates 4611 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:24:38]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:24:38]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 3 @ 4611 updates, score 4.132) (writing took 0.015244 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:24:38] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:24:42]    INFO >> epoch 004:     39 / 1539 loss=4.093, wps=2646.4, ups=3.41, wpb=776.8, bsz=776.8, num_updates=4650, lr=0.000376, gnorm=5.976, clip=2, train_wall=5, gb_free=76.1, wall=675 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:24:48]    INFO >> epoch 004:     89 / 1539 loss=4.115, wps=5649.5, ups=8.38, wpb=674.1, bsz=674.1, num_updates=4700, lr=0.000376, gnorm=5.666, clip=0, train_wall=5, gb_free=75.5, wall=681 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:24:57]    INFO >> epoch 004:    139 / 1539 loss=3.642, wps=7460.3, ups=7.03, wpb=1061, bsz=1061, num_updates=4750, lr=0.000376, gnorm=7.379, clip=2, train_wall=7, gb_free=64.1, wall=688 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:03]    INFO >> epoch 004:    189 / 1539 loss=3.969, wps=6554.1, ups=8.61, wpb=761.2, bsz=761.2, num_updates=4800, lr=0.000376, gnorm=6.217, clip=0, train_wall=5, gb_free=77.2, wall=694 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:09]    INFO >> epoch 004:    239 / 1539 loss=4.107, wps=5970.2, ups=8.08, wpb=739.3, bsz=739.3, num_updates=4850, lr=0.000376, gnorm=6.848, clip=0, train_wall=6, gb_free=73.4, wall=700 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:15]    INFO >> epoch 004:    289 / 1539 loss=4.145, wps=5392, ups=8.41, wpb=641.5, bsz=641.5, num_updates=4900, lr=0.000376, gnorm=5.483, clip=0, train_wall=5, gb_free=76.4, wall=706 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:21]    INFO >> epoch 004:    339 / 1539 loss=4.036, wps=5862.3, ups=8.51, wpb=689.2, bsz=689.2, num_updates=4950, lr=0.000376, gnorm=5.951, clip=0, train_wall=5, gb_free=74.6, wall=712 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:28]    INFO >> epoch 004:    389 / 1539 loss=3.851, wps=6188.4, ups=8.07, wpb=766.4, bsz=766.4, num_updates=5000, lr=0.000376, gnorm=6.252, clip=0, train_wall=6, gb_free=76.1, wall=718 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:34]    INFO >> epoch 004:    439 / 1539 loss=4.121, wps=4992, ups=8.1, wpb=616.4, bsz=616.4, num_updates=5050, lr=0.000376, gnorm=5.583, clip=0, train_wall=6, gb_free=77.6, wall=724 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:40]    INFO >> epoch 004:    489 / 1539 loss=4.116, wps=5909, ups=8.16, wpb=724.4, bsz=724.4, num_updates=5100, lr=0.000376, gnorm=5.369, clip=0, train_wall=6, gb_free=74.1, wall=730 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:46]    INFO >> epoch 004:    539 / 1539 loss=4.019, wps=5711.5, ups=8.24, wpb=693.3, bsz=693.3, num_updates=5150, lr=0.000376, gnorm=6.082, clip=0, train_wall=6, gb_free=77.1, wall=736 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:25:52]    INFO >> epoch 004:    589 / 1539 loss=4.019, wps=5183.1, ups=8.35, wpb=621, bsz=621, num_updates=5200, lr=0.000376, gnorm=5.232, clip=0, train_wall=5, gb_free=76, wall=742 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:25:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 29.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.94 GiB is allocated by PyTorch, and 1.65 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:25:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:25:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:25:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 11        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78727 MiB |  78787 MiB |  88799 GiB |  88722 GiB |
|       from large pool |  78280 MiB |  78340 MiB |  87813 GiB |  87736 GiB |
|       from small pool |    447 MiB |    448 MiB |    985 GiB |    985 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78727 MiB |  78787 MiB |  88799 GiB |  88722 GiB |
|       from large pool |  78280 MiB |  78340 MiB |  87813 GiB |  87736 GiB |
|       from small pool |    447 MiB |    448 MiB |    985 GiB |    985 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78477 MiB |  78537 MiB |  88346 GiB |  88269 GiB |
|       from large pool |  78032 MiB |  78091 MiB |  87362 GiB |  87285 GiB |
|       from small pool |    445 MiB |    446 MiB |    984 GiB |    984 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80478 MiB |  80480 MiB | 212878 MiB | 132400 MiB |
|       from large pool |  80022 MiB |  80022 MiB | 211458 MiB | 131436 MiB |
|       from small pool |    456 MiB |    458 MiB |   1420 MiB |    964 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1690 MiB |   4509 MiB |  67725 GiB |  67723 GiB |
|       from large pool |   1681 MiB |   4505 MiB |  66577 GiB |  66575 GiB |
|       from small pool |      8 MiB |     20 MiB |   1148 GiB |   1148 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8385    |    8388    |   11315 K  |   11307 K  |
|       from large pool |    1038    |    1039    |    5128 K  |    5127 K  |
|       from small pool |    7347    |    7350    |    6186 K  |    6179 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8385    |    8388    |   11315 K  |   11307 K  |
|       from large pool |    1038    |    1039    |    5128 K  |    5127 K  |
|       from small pool |    7347    |    7350    |    6186 K  |    6179 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     824    |     825    |    3014    |    2190    |
|       from large pool |     596    |     596    |    2304    |    1708    |
|       from small pool |     228    |     229    |     710    |     482    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     323    |     324    |    6670 K  |    6669 K  |
|       from large pool |     133    |     133    |    3709 K  |    3709 K  |
|       from small pool |     190    |     191    |    2960 K  |    2960 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:25:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:25:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:25:59]    INFO >> epoch 004:    640 / 1539 loss=3.92, wps=5087.4, ups=7.37, wpb=689.9, bsz=689.9, num_updates=5250, lr=0.000376, gnorm=5.908, clip=0, train_wall=6, gb_free=75.9, wall=749 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:05]    INFO >> epoch 004:    690 / 1539 loss=3.921, wps=5192.5, ups=8.26, wpb=628.6, bsz=628.6, num_updates=5300, lr=0.000376, gnorm=5.703, clip=0, train_wall=6, gb_free=75.7, wall=755 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:13]    INFO >> epoch 004:    740 / 1539 loss=4.074, wps=4996.5, ups=6.55, wpb=762.3, bsz=762.3, num_updates=5350, lr=0.000376, gnorm=5.785, clip=0, train_wall=7, gb_free=76.1, wall=763 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:19]    INFO >> epoch 004:    790 / 1539 loss=4.005, wps=5667.3, ups=8.3, wpb=683.1, bsz=683.1, num_updates=5400, lr=0.000376, gnorm=5.036, clip=0, train_wall=6, gb_free=76.2, wall=769 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:25]    INFO >> epoch 004:    840 / 1539 loss=3.947, wps=5830, ups=8.05, wpb=724.6, bsz=724.6, num_updates=5450, lr=0.000376, gnorm=5.475, clip=0, train_wall=6, gb_free=75.7, wall=775 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:34]    INFO >> epoch 004:    890 / 1539 loss=3.914, wps=6503, ups=7.95, wpb=817.5, bsz=817.5, num_updates=5500, lr=0.000376, gnorm=6.161, clip=0, train_wall=6, gb_free=75.4, wall=781 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:40]    INFO >> epoch 004:    940 / 1539 loss=3.923, wps=5681.6, ups=8.74, wpb=650.1, bsz=650.1, num_updates=5550, lr=0.000376, gnorm=4.789, clip=0, train_wall=5, gb_free=77.9, wall=787 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:46]    INFO >> epoch 004:    990 / 1539 loss=3.858, wps=6081.5, ups=8, wpb=759.9, bsz=759.9, num_updates=5600, lr=0.000376, gnorm=5.862, clip=0, train_wall=6, gb_free=76.8, wall=793 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:52]    INFO >> epoch 004:   1040 / 1539 loss=4.082, wps=5290.9, ups=8.74, wpb=605.5, bsz=605.5, num_updates=5650, lr=0.000376, gnorm=5.048, clip=0, train_wall=5, gb_free=76.2, wall=799 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:26:58]    INFO >> epoch 004:   1090 / 1539 loss=4.012, wps=6147, ups=8.04, wpb=764.8, bsz=764.8, num_updates=5700, lr=0.000376, gnorm=5.765, clip=0, train_wall=6, gb_free=76.4, wall=805 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:27:04]    INFO >> epoch 004:   1140 / 1539 loss=3.958, wps=5351.8, ups=8.56, wpb=625.5, bsz=625.5, num_updates=5750, lr=0.000376, gnorm=4.897, clip=0, train_wall=5, gb_free=78, wall=811 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:27:12]    INFO >> epoch 004:   1190 / 1539 loss=3.982, wps=6034.8, ups=7.83, wpb=771, bsz=771, num_updates=5800, lr=0.000376, gnorm=5.713, clip=0, train_wall=6, gb_free=75.2, wall=817 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:27:17]    INFO >> epoch 004:   1240 / 1539 loss=3.852, wps=5756, ups=8.85, wpb=650.4, bsz=650.4, num_updates=5850, lr=0.000376, gnorm=5.039, clip=0, train_wall=5, gb_free=76.5, wall=823 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:27:23]    INFO >> epoch 004:   1290 / 1539 loss=3.926, wps=5270.9, ups=8.21, wpb=641.7, bsz=641.7, num_updates=5900, lr=0.000376, gnorm=4.992, clip=0, train_wall=6, gb_free=77.6, wall=829 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:27:29]    INFO >> epoch 004:   1340 / 1539 loss=3.892, wps=6250.9, ups=8.37, wpb=746.5, bsz=746.5, num_updates=5950, lr=0.000376, gnorm=5.204, clip=0, train_wall=5, gb_free=76.2, wall=835 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:27:36]    INFO >> epoch 004:   1390 / 1539 loss=3.78, wps=6439.8, ups=7.9, wpb=815.3, bsz=815.3, num_updates=6000, lr=0.000376, gnorm=5.765, clip=0, train_wall=6, gb_free=76.4, wall=841 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:27:37] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 57.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 76.61 GiB is allocated by PyTorch, and 1.95 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:27:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:27:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:27:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 13        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78390 MiB |  78450 MiB | 101000 GiB | 100923 GiB |
|       from large pool |  78035 MiB |  78095 MiB |  99886 GiB |  99810 GiB |
|       from small pool |    354 MiB |    355 MiB |   1113 GiB |   1113 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78390 MiB |  78450 MiB | 101000 GiB | 100923 GiB |
|       from large pool |  78035 MiB |  78095 MiB |  99886 GiB |  99810 GiB |
|       from small pool |    354 MiB |    355 MiB |   1113 GiB |   1113 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78043 MiB |  78102 MiB | 100482 GiB | 100406 GiB |
|       from large pool |  77690 MiB |  77750 MiB |  99370 GiB |  99294 GiB |
|       from small pool |    352 MiB |    353 MiB |   1111 GiB |   1111 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80450 MiB |  80468 MiB | 253090 MiB | 172640 MiB |
|       from large pool |  80058 MiB |  80076 MiB | 251300 MiB | 171242 MiB |
|       from small pool |    392 MiB |    392 MiB |   1790 MiB |   1398 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1999 MiB |   6220 MiB |  77422 GiB |  77420 GiB |
|       from large pool |   1962 MiB |   6216 MiB |  76123 GiB |  76121 GiB |
|       from small pool |     37 MiB |     38 MiB |   1298 GiB |   1298 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6735    |    6738    |   12870 K  |   12863 K  |
|       from large pool |     894    |     895    |    5886 K  |    5885 K  |
|       from small pool |    5841    |    5844    |    6983 K  |    6977 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6735    |    6738    |   12870 K  |   12863 K  |
|       from large pool |     894    |     895    |    5886 K  |    5885 K  |
|       from small pool |    5841    |    5844    |    6983 K  |    6977 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     811    |     812    |    3710    |    2899    |
|       from large pool |     615    |     616    |    2815    |    2200    |
|       from small pool |     196    |     196    |     895    |     699    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     517    |     517    |    7574 K  |    7574 K  |
|       from large pool |     162    |     162    |    4260 K  |    4260 K  |
|       from small pool |     355    |     355    |    3314 K  |    3313 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:27:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:27:37] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:27:44]    INFO >> epoch 004:   1441 / 1539 loss=3.917, wps=5476.5, ups=7.32, wpb=748.2, bsz=748.2, num_updates=6050, lr=0.000376, gnorm=5.335, clip=0, train_wall=6, gb_free=75.9, wall=848 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:27:50]    INFO >> epoch 004:   1491 / 1539 loss=3.964, wps=5682.9, ups=8.18, wpb=694.8, bsz=694.8, num_updates=6100, lr=0.000376, gnorm=5.134, clip=0, train_wall=6, gb_free=75.9, wall=854 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:27:56]    INFO >> epoch 004 | loss 3.948 | wps 5555.1 | ups 7.71 | wpb 720.8 | bsz 720.8 | num_updates 6148 | lr 0.000376 | gnorm 5.635 | clip 0.1 | train_wall 173 | gb_free 74.9 | wall 861 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:27:56] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:28:05]    INFO >> epoch 004 | valid on 'valid' subset | loss 3.953 | wps 17068.4 | wpb 5412.5 | bsz 5412.5 | num_updates 6148 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:28:06]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:28:06]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 4 @ 6148 updates, score 3.953) (writing took 0.009196 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:28:06] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:28:06]    INFO >> epoch 005:      2 / 1539 loss=3.536, wps=2507.7, ups=3.12, wpb=803.1, bsz=803.1, num_updates=6150, lr=0.000354, gnorm=5.309, clip=0, train_wall=6, gb_free=75.6, wall=870 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:28:13]    INFO >> epoch 005:     52 / 1539 loss=3.971, wps=5410.4, ups=7.68, wpb=704.7, bsz=704.7, num_updates=6200, lr=0.000354, gnorm=4.936, clip=0, train_wall=6, gb_free=75.2, wall=877 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:28:20]    INFO >> epoch 005:    102 / 1539 loss=3.937, wps=5837.1, ups=8.48, wpb=688.6, bsz=688.6, num_updates=6250, lr=0.000354, gnorm=5.285, clip=0, train_wall=5, gb_free=74.5, wall=883 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:28:26]    INFO >> epoch 005:    152 / 1539 loss=3.936, wps=5527.6, ups=8.29, wpb=667.1, bsz=667.1, num_updates=6300, lr=0.000354, gnorm=4.901, clip=0, train_wall=6, gb_free=78.3, wall=889 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:28:32]    INFO >> epoch 005:    202 / 1539 loss=3.866, wps=5727.3, ups=8.45, wpb=677.9, bsz=677.9, num_updates=6350, lr=0.000354, gnorm=5.979, clip=0, train_wall=5, gb_free=76.1, wall=895 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:28:38]    INFO >> epoch 005:    252 / 1539 loss=3.77, wps=5921.1, ups=7.48, wpb=791.4, bsz=791.4, num_updates=6400, lr=0.000354, gnorm=5.53, clip=0, train_wall=6, gb_free=77.8, wall=901 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:28:45]    INFO >> epoch 005:    302 / 1539 loss=3.891, wps=6163.1, ups=7.77, wpb=793.5, bsz=793.5, num_updates=6450, lr=0.000354, gnorm=5.547, clip=0, train_wall=6, gb_free=76.5, wall=908 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:28:53]    INFO >> epoch 005:    352 / 1539 loss=3.543, wps=6086.3, ups=7.92, wpb=768.4, bsz=768.4, num_updates=6500, lr=0.000354, gnorm=5.055, clip=0, train_wall=6, gb_free=77.5, wall=914 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:28:59]    INFO >> epoch 005:    402 / 1539 loss=3.499, wps=6398.2, ups=7.38, wpb=867.3, bsz=867.3, num_updates=6550, lr=0.000354, gnorm=5.627, clip=2, train_wall=6, gb_free=76.7, wall=921 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:29:05]    INFO >> epoch 005:    452 / 1539 loss=3.903, wps=5038, ups=8.6, wpb=585.7, bsz=585.7, num_updates=6600, lr=0.000354, gnorm=4.996, clip=0, train_wall=5, gb_free=75.8, wall=927 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:29:11]    INFO >> epoch 005:    502 / 1539 loss=3.865, wps=5901.8, ups=8.18, wpb=721.4, bsz=721.4, num_updates=6650, lr=0.000354, gnorm=4.773, clip=0, train_wall=6, gb_free=78.5, wall=933 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:29:23]    INFO >> epoch 005:    552 / 1539 loss=3.862, wps=3117.7, ups=4.65, wpb=670, bsz=670, num_updates=6700, lr=0.000354, gnorm=4.941, clip=0, train_wall=10, gb_free=74.8, wall=944 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:29:28] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 51.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.12 GiB is allocated by PyTorch, and 1.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:29:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:29:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:29:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 14        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78908 MiB |  78967 MiB | 114959 GiB | 114882 GiB |
|       from large pool |  78459 MiB |  78518 MiB | 113676 GiB | 113600 GiB |
|       from small pool |    448 MiB |    450 MiB |   1282 GiB |   1282 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78908 MiB |  78967 MiB | 114959 GiB | 114882 GiB |
|       from large pool |  78459 MiB |  78518 MiB | 113676 GiB | 113600 GiB |
|       from small pool |    448 MiB |    450 MiB |   1282 GiB |   1282 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78657 MiB |  78717 MiB | 114375 GiB | 114298 GiB |
|       from large pool |  78210 MiB |  78270 MiB | 113094 GiB | 113017 GiB |
|       from small pool |    446 MiB |    448 MiB |   1280 GiB |   1280 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80456 MiB |  80458 MiB | 253158 MiB | 172702 MiB |
|       from large pool |  79998 MiB |  79998 MiB | 251300 MiB | 171302 MiB |
|       from small pool |    458 MiB |    460 MiB |   1858 MiB |   1400 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1547 MiB |   4073 MiB |  87226 GiB |  87225 GiB |
|       from large pool |   1538 MiB |   4069 MiB |  85733 GiB |  85732 GiB |
|       from small pool |      9 MiB |     24 MiB |   1492 GiB |   1492 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8418    |    8421    |   14686 K  |   14677 K  |
|       from large pool |    1041    |    1042    |    6637 K  |    6636 K  |
|       from small pool |    7377    |    7380    |    8048 K  |    8041 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8418    |    8421    |   14686 K  |   14677 K  |
|       from large pool |    1041    |    1042    |    6637 K  |    6636 K  |
|       from small pool |    7377    |    7380    |    8048 K  |    8041 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     843    |     844    |    3744    |    2901    |
|       from large pool |     614    |     614    |    2815    |    2201    |
|       from small pool |     229    |     230    |     929    |     700    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     350    |     351    |    8669 K  |    8668 K  |
|       from large pool |     159    |     159    |    4804 K  |    4803 K  |
|       from small pool |     191    |     192    |    3865 K  |    3864 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:29:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:29:28] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:29:30]    INFO >> epoch 005:    603 / 1539 loss=3.87, wps=5196.9, ups=7.25, wpb=716.7, bsz=716.7, num_updates=6750, lr=0.000354, gnorm=5.036, clip=0, train_wall=6, gb_free=76.5, wall=950 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:29:37]    INFO >> epoch 005:    653 / 1539 loss=3.859, wps=6124.3, ups=7.93, wpb=772.1, bsz=772.1, num_updates=6800, lr=0.000354, gnorm=5.171, clip=0, train_wall=6, gb_free=77, wall=957 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:29:43]    INFO >> epoch 005:    703 / 1539 loss=3.795, wps=5734.4, ups=8.02, wpb=715.3, bsz=715.3, num_updates=6850, lr=0.000354, gnorm=5.858, clip=0, train_wall=6, gb_free=73.7, wall=963 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:29:49]    INFO >> epoch 005:    753 / 1539 loss=3.972, wps=6030.1, ups=8.33, wpb=723.6, bsz=723.6, num_updates=6900, lr=0.000354, gnorm=5.444, clip=0, train_wall=5, gb_free=76, wall=969 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:29:57]    INFO >> epoch 005:    803 / 1539 loss=3.887, wps=5323.8, ups=7.75, wpb=686.6, bsz=686.6, num_updates=6950, lr=0.000354, gnorm=5.353, clip=0, train_wall=6, gb_free=74.9, wall=975 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:30:03]    INFO >> epoch 005:    853 / 1539 loss=3.916, wps=5171.7, ups=8.14, wpb=635.5, bsz=635.5, num_updates=7000, lr=0.000354, gnorm=5.273, clip=0, train_wall=6, gb_free=74.9, wall=982 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:30:09]    INFO >> epoch 005:    903 / 1539 loss=3.755, wps=5430.8, ups=7.71, wpb=704.8, bsz=704.8, num_updates=7050, lr=0.000354, gnorm=5.278, clip=0, train_wall=6, gb_free=72.8, wall=988 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:30:16]    INFO >> epoch 005:    953 / 1539 loss=3.995, wps=5446.2, ups=7.42, wpb=734.4, bsz=734.4, num_updates=7100, lr=0.000354, gnorm=5.05, clip=0, train_wall=6, gb_free=75.2, wall=995 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:30:23]    INFO >> epoch 005:   1003 / 1539 loss=3.554, wps=6834.2, ups=7.25, wpb=943.3, bsz=943.3, num_updates=7150, lr=0.000354, gnorm=5.505, clip=0, train_wall=6, gb_free=76.3, wall=1002 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:30:31]    INFO >> epoch 005:   1053 / 1539 loss=3.822, wps=5750.7, ups=7.44, wpb=772.8, bsz=772.8, num_updates=7200, lr=0.000354, gnorm=5.078, clip=0, train_wall=6, gb_free=74.9, wall=1008 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:30:37]    INFO >> epoch 005:   1103 / 1539 loss=3.784, wps=5358.6, ups=8.02, wpb=667.9, bsz=667.9, num_updates=7250, lr=0.000354, gnorm=5.35, clip=0, train_wall=6, gb_free=74.1, wall=1015 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:30:40] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.53 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:30:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:30:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:30:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 16        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77283 MiB |  77343 MiB | 123520 GiB | 123445 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 122148 GiB | 122073 GiB |
|       from small pool |    343 MiB |    344 MiB |   1372 GiB |   1371 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77283 MiB |  77343 MiB | 123520 GiB | 123445 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 122148 GiB | 122073 GiB |
|       from small pool |    343 MiB |    344 MiB |   1372 GiB |   1371 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76965 MiB |  77024 MiB | 122893 GiB | 122817 GiB |
|       from large pool |  76624 MiB |  76683 MiB | 121522 GiB | 121447 GiB |
|       from small pool |    341 MiB |    342 MiB |   1370 GiB |   1369 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80498 MiB | 253280 MiB | 172782 MiB |
|       from large pool |  80118 MiB |  80118 MiB | 251420 MiB | 171302 MiB |
|       from small pool |    380 MiB |    458 MiB |   1860 MiB |   1480 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3154 MiB |   6329 MiB |  93921 GiB |  93918 GiB |
|       from large pool |   3118 MiB |   6296 MiB |  92322 GiB |  92319 GiB |
|       from small pool |     36 MiB |     37 MiB |   1599 GiB |   1599 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6537    |    6540    |   15765 K  |   15759 K  |
|       from large pool |     876    |     877    |    7154 K  |    7153 K  |
|       from small pool |    5661    |    5664    |    8611 K  |    8605 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6537    |    6540    |   15765 K  |   15759 K  |
|       from large pool |     876    |     877    |    7154 K  |    7153 K  |
|       from small pool |    5661    |    5664    |    8611 K  |    8605 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     806    |     843    |    3747    |    2941    |
|       from large pool |     616    |     616    |    2817    |    2201    |
|       from small pool |     190    |     229    |     930    |     740    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     547    |     548    |    9300 K  |    9300 K  |
|       from large pool |     203    |     203    |    5176 K  |    5176 K  |
|       from small pool |     344    |     345    |    4124 K  |    4124 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:30:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:30:40] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:30:44]    INFO >> epoch 005:   1154 / 1539 loss=3.885, wps=5492.8, ups=7.2, wpb=762.8, bsz=762.8, num_updates=7300, lr=0.000354, gnorm=4.963, clip=0, train_wall=6, gb_free=75, wall=1022 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:30:50]    INFO >> epoch 005:   1204 / 1539 loss=3.877, wps=5023.4, ups=8.25, wpb=608.9, bsz=608.9, num_updates=7350, lr=0.000354, gnorm=5.252, clip=2, train_wall=6, gb_free=76.3, wall=1028 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:30:56]    INFO >> epoch 005:   1254 / 1539 loss=3.791, wps=5839.4, ups=8.44, wpb=691.6, bsz=691.6, num_updates=7400, lr=0.000354, gnorm=4.874, clip=0, train_wall=5, gb_free=77.3, wall=1034 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:31:03]    INFO >> epoch 005:   1304 / 1539 loss=3.844, wps=5391.4, ups=8.26, wpb=652.5, bsz=652.5, num_updates=7450, lr=0.000354, gnorm=5.081, clip=0, train_wall=6, gb_free=73, wall=1040 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:31:10]    INFO >> epoch 005:   1354 / 1539 loss=3.799, wps=5767.7, ups=8.14, wpb=708.9, bsz=708.9, num_updates=7500, lr=0.000354, gnorm=5.03, clip=0, train_wall=6, gb_free=75.8, wall=1046 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:31:16]    INFO >> epoch 005:   1404 / 1539 loss=3.895, wps=5438, ups=8.32, wpb=653.7, bsz=653.7, num_updates=7550, lr=0.000354, gnorm=4.768, clip=0, train_wall=6, gb_free=75, wall=1052 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:31:22]    INFO >> epoch 005:   1454 / 1539 loss=3.885, wps=5451.5, ups=8.04, wpb=677.9, bsz=677.9, num_updates=7600, lr=0.000354, gnorm=5.143, clip=0, train_wall=6, gb_free=76.6, wall=1058 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:31:28]    INFO >> epoch 005:   1504 / 1539 loss=3.696, wps=6102.9, ups=7.57, wpb=805.9, bsz=805.9, num_updates=7650, lr=0.000354, gnorm=5.257, clip=0, train_wall=6, gb_free=76.9, wall=1065 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:31:33]    INFO >> epoch 005 | loss 3.822 | wps 5316.5 | ups 7.38 | wpb 720.8 | bsz 720.8 | num_updates 7685 | lr 0.000354 | gnorm 5.211 | clip 0.1 | train_wall 181 | gb_free 71 | wall 1069 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:31:33] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:31:44]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.89 | wps 16025.8 | wpb 5412.5 | bsz 5412.5 | num_updates 7685 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:31:44]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:31:44]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 5 @ 7685 updates, score 3.89) (writing took 0.011951 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:31:44] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:31:46]    INFO >> epoch 006:     15 / 1539 loss=3.778, wps=2385.5, ups=3.08, wpb=773.7, bsz=773.7, num_updates=7700, lr=0.000327, gnorm=5.052, clip=0, train_wall=6, gb_free=74.8, wall=1081 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:31:52]    INFO >> epoch 006:     65 / 1539 loss=3.725, wps=6309.7, ups=7.82, wpb=806.6, bsz=806.6, num_updates=7750, lr=0.000327, gnorm=5.134, clip=0, train_wall=6, gb_free=75, wall=1087 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:31:59]    INFO >> epoch 006:    115 / 1539 loss=3.792, wps=5886.5, ups=8, wpb=736.2, bsz=736.2, num_updates=7800, lr=0.000327, gnorm=5.321, clip=0, train_wall=6, gb_free=78, wall=1093 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:32:05]    INFO >> epoch 006:    165 / 1539 loss=3.83, wps=5505.1, ups=7.9, wpb=696.7, bsz=696.7, num_updates=7850, lr=0.000327, gnorm=4.512, clip=0, train_wall=6, gb_free=76.6, wall=1100 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:32:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.53 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:32:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:32:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:32:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 17        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77283 MiB |  77343 MiB | 135202 GiB | 135127 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 133696 GiB | 133621 GiB |
|       from small pool |    343 MiB |    344 MiB |   1506 GiB |   1506 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77283 MiB |  77343 MiB | 135202 GiB | 135127 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 133696 GiB | 133621 GiB |
|       from small pool |    343 MiB |    344 MiB |   1506 GiB |   1506 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76965 MiB |  77024 MiB | 134521 GiB | 134446 GiB |
|       from large pool |  76624 MiB |  76683 MiB | 133017 GiB | 132942 GiB |
|       from small pool |    341 MiB |    342 MiB |   1504 GiB |   1503 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 253342 MiB | 172844 MiB |
|       from large pool |  80118 MiB |  80118 MiB | 251480 MiB | 171362 MiB |
|       from small pool |    380 MiB |    382 MiB |   1862 MiB |   1482 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3154 MiB |   6330 MiB | 102074 GiB | 102071 GiB |
|       from large pool |   3118 MiB |   6296 MiB | 100323 GiB | 100320 GiB |
|       from small pool |     36 MiB |     38 MiB |   1750 GiB |   1750 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6537    |    6540    |   17220 K  |   17213 K  |
|       from large pool |     876    |     877    |    7766 K  |    7765 K  |
|       from small pool |    5661    |    5664    |    9453 K  |    9448 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6537    |    6540    |   17220 K  |   17213 K  |
|       from large pool |     876    |     877    |    7766 K  |    7765 K  |
|       from small pool |    5661    |    5664    |    9453 K  |    9448 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     806    |     807    |    3749    |    2943    |
|       from large pool |     616    |     616    |    2818    |    2202    |
|       from small pool |     190    |     191    |     931    |     741    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     553    |     553    |   10166 K  |   10165 K  |
|       from large pool |     203    |     203    |    5621 K  |    5621 K  |
|       from small pool |     350    |     350    |    4544 K  |    4544 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:32:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:32:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:32:13]    INFO >> epoch 006:    216 / 1539 loss=3.762, wps=5185.9, ups=7.35, wpb=705.9, bsz=705.9, num_updates=7900, lr=0.000327, gnorm=4.871, clip=0, train_wall=6, gb_free=73.7, wall=1107 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:32:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:32:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:32:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:32:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77886 MiB |  77946 MiB | 135885 GiB | 135809 GiB |
|       from large pool |  77448 MiB |  77507 MiB | 134369 GiB | 134293 GiB |
|       from small pool |    438 MiB |    439 MiB |   1516 GiB |   1515 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77886 MiB |  77946 MiB | 135885 GiB | 135809 GiB |
|       from large pool |  77448 MiB |  77507 MiB | 134369 GiB | 134293 GiB |
|       from small pool |    438 MiB |    439 MiB |   1516 GiB |   1515 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77635 MiB |  77695 MiB | 135200 GiB | 135124 GiB |
|       from large pool |  77199 MiB |  77258 MiB | 133686 GiB | 133611 GiB |
|       from small pool |    436 MiB |    437 MiB |   1513 GiB |   1513 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80506 MiB |  80506 MiB | 253410 MiB | 172904 MiB |
|       from large pool |  80058 MiB |  80058 MiB | 251480 MiB | 171422 MiB |
|       from small pool |    448 MiB |    448 MiB |   1930 MiB |   1482 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2619 MiB |   4071 MiB | 102597 GiB | 102594 GiB |
|       from large pool |   2609 MiB |   4069 MiB | 100834 GiB | 100832 GiB |
|       from small pool |      9 MiB |     26 MiB |   1762 GiB |   1762 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8227    |    8228    |   17316 K  |   17308 K  |
|       from large pool |    1024    |    1025    |    7801 K  |    7800 K  |
|       from small pool |    7203    |    7204    |    9515 K  |    9508 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8227    |    8228    |   17316 K  |   17308 K  |
|       from large pool |    1024    |    1025    |    7801 K  |    7800 K  |
|       from small pool |    7203    |    7204    |    9515 K  |    9508 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     839    |     839    |    3783    |    2944    |
|       from large pool |     615    |     615    |    2818    |    2203    |
|       from small pool |     224    |     224    |     965    |     741    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     341    |     344    |   10226 K  |   10226 K  |
|       from large pool |     159    |     159    |    5646 K  |    5645 K  |
|       from small pool |     182    |     185    |    4580 K  |    4580 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:32:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:32:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:32:20]    INFO >> epoch 006:    267 / 1539 loss=3.887, wps=4829.6, ups=7.31, wpb=660.9, bsz=660.9, num_updates=7950, lr=0.000327, gnorm=4.477, clip=0, train_wall=6, gb_free=76.9, wall=1113 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:32:26]    INFO >> epoch 006:    317 / 1539 loss=3.705, wps=5842.6, ups=8.43, wpb=693.3, bsz=693.3, num_updates=8000, lr=0.000327, gnorm=5.464, clip=0, train_wall=5, gb_free=75.3, wall=1119 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:32:32]    INFO >> epoch 006:    367 / 1539 loss=3.537, wps=6036, ups=7.74, wpb=779.8, bsz=779.8, num_updates=8050, lr=0.000327, gnorm=5.288, clip=2, train_wall=6, gb_free=76.1, wall=1126 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:32:38]    INFO >> epoch 006:    417 / 1539 loss=3.722, wps=5814.1, ups=8.42, wpb=690.6, bsz=690.6, num_updates=8100, lr=0.000327, gnorm=4.871, clip=0, train_wall=5, gb_free=76.1, wall=1132 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:32:47]    INFO >> epoch 006:    467 / 1539 loss=3.787, wps=5834.5, ups=6.99, wpb=835, bsz=835, num_updates=8150, lr=0.000327, gnorm=5.28, clip=0, train_wall=6, gb_free=78.1, wall=1139 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:32:53]    INFO >> epoch 006:    517 / 1539 loss=3.743, wps=5726.7, ups=7.98, wpb=717.3, bsz=717.3, num_updates=8200, lr=0.000327, gnorm=4.619, clip=0, train_wall=6, gb_free=74.1, wall=1145 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:32:59]    INFO >> epoch 006:    567 / 1539 loss=3.837, wps=6207.5, ups=7.86, wpb=790.1, bsz=790.1, num_updates=8250, lr=0.000327, gnorm=4.846, clip=0, train_wall=6, gb_free=75.1, wall=1152 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:05]    INFO >> epoch 006:    617 / 1539 loss=3.787, wps=5352.1, ups=8.31, wpb=644.4, bsz=644.4, num_updates=8300, lr=0.000327, gnorm=4.173, clip=0, train_wall=5, gb_free=76.4, wall=1158 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:12]    INFO >> epoch 006:    667 / 1539 loss=3.796, wps=5847.9, ups=7.9, wpb=740.3, bsz=740.3, num_updates=8350, lr=0.000327, gnorm=4.916, clip=0, train_wall=6, gb_free=75.6, wall=1164 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:19]    INFO >> epoch 006:    717 / 1539 loss=3.803, wps=5482.4, ups=8.74, wpb=627.3, bsz=627.3, num_updates=8400, lr=0.000327, gnorm=4.312, clip=0, train_wall=5, gb_free=77, wall=1170 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:26]    INFO >> epoch 006:    767 / 1539 loss=3.461, wps=6044.5, ups=7.26, wpb=832.6, bsz=832.6, num_updates=8450, lr=0.000327, gnorm=4.723, clip=0, train_wall=6, gb_free=74.5, wall=1177 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:32]    INFO >> epoch 006:    817 / 1539 loss=3.853, wps=5036.4, ups=8.38, wpb=601.3, bsz=601.3, num_updates=8500, lr=0.000327, gnorm=4.331, clip=0, train_wall=5, gb_free=76, wall=1182 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:38]    INFO >> epoch 006:    867 / 1539 loss=3.68, wps=6012.5, ups=8.21, wpb=732.2, bsz=732.2, num_updates=8550, lr=0.000327, gnorm=4.979, clip=0, train_wall=6, gb_free=76.8, wall=1189 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:44]    INFO >> epoch 006:    917 / 1539 loss=3.792, wps=5597.1, ups=7.77, wpb=720.8, bsz=720.8, num_updates=8600, lr=0.000327, gnorm=4.875, clip=0, train_wall=6, gb_free=70.3, wall=1195 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:53]    INFO >> epoch 006:    967 / 1539 loss=3.792, wps=6070.7, ups=7.2, wpb=843.1, bsz=843.1, num_updates=8650, lr=0.000327, gnorm=4.821, clip=0, train_wall=6, gb_free=74.9, wall=1202 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:33:59]    INFO >> epoch 006:   1017 / 1539 loss=3.762, wps=5877.7, ups=8.16, wpb=719.9, bsz=719.9, num_updates=8700, lr=0.000327, gnorm=4.378, clip=0, train_wall=6, gb_free=70.4, wall=1208 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:06]    INFO >> epoch 006:   1067 / 1539 loss=3.911, wps=5224.5, ups=6.84, wpb=763.3, bsz=763.3, num_updates=8750, lr=0.000327, gnorm=5.045, clip=0, train_wall=7, gb_free=73.6, wall=1215 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:13]    INFO >> epoch 006:   1117 / 1539 loss=3.723, wps=5488.3, ups=7.6, wpb=721.9, bsz=721.9, num_updates=8800, lr=0.000327, gnorm=4.844, clip=0, train_wall=6, gb_free=75.8, wall=1222 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:19]    INFO >> epoch 006:   1167 / 1539 loss=3.787, wps=5939.9, ups=7.74, wpb=767.6, bsz=767.6, num_updates=8850, lr=0.000327, gnorm=4.837, clip=0, train_wall=6, gb_free=76, wall=1228 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:27]    INFO >> epoch 006:   1217 / 1539 loss=3.801, wps=5370, ups=7.83, wpb=685.6, bsz=685.6, num_updates=8900, lr=0.000327, gnorm=5.23, clip=0, train_wall=6, gb_free=73.6, wall=1235 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:33]    INFO >> epoch 006:   1267 / 1539 loss=3.784, wps=5044, ups=7.83, wpb=643.8, bsz=643.8, num_updates=8950, lr=0.000327, gnorm=4.718, clip=0, train_wall=6, gb_free=76.6, wall=1241 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:39]    INFO >> epoch 006:   1317 / 1539 loss=3.707, wps=5872.2, ups=7.92, wpb=741.6, bsz=741.6, num_updates=9000, lr=0.000327, gnorm=4.632, clip=0, train_wall=6, gb_free=76, wall=1248 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:45]    INFO >> epoch 006:   1367 / 1539 loss=3.807, wps=5172.4, ups=8.53, wpb=606.1, bsz=606.1, num_updates=9050, lr=0.000327, gnorm=4.402, clip=0, train_wall=5, gb_free=76, wall=1253 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:51]    INFO >> epoch 006:   1417 / 1539 loss=3.763, wps=5956, ups=8.16, wpb=729.8, bsz=729.8, num_updates=9100, lr=0.000327, gnorm=4.856, clip=0, train_wall=6, gb_free=76.2, wall=1260 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:34:59]    INFO >> epoch 006:   1467 / 1539 loss=3.604, wps=5771.6, ups=8.02, wpb=719.8, bsz=719.8, num_updates=9150, lr=0.000327, gnorm=4.465, clip=2, train_wall=6, gb_free=74.6, wall=1266 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:35:05]    INFO >> epoch 006:   1517 / 1539 loss=3.767, wps=5701.3, ups=8.24, wpb=692, bsz=692, num_updates=9200, lr=0.000327, gnorm=4.58, clip=0, train_wall=6, gb_free=76.2, wall=1272 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:35:08]    INFO >> epoch 006 | loss 3.755 | wps 5394 | ups 7.48 | wpb 720.8 | bsz 720.8 | num_updates 9222 | lr 0.000327 | gnorm 4.802 | clip 0.1 | train_wall 177 | gb_free 76 | wall 1274 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:35:08] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:35:17]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.879 | wps 16975 | wpb 5412.5 | bsz 5412.5 | num_updates 9222 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:35:17]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:35:17]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 6 @ 9222 updates, score 3.879) (writing took 0.009101 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:35:17] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:35:21]    INFO >> epoch 007:     28 / 1539 loss=3.901, wps=2129.2, ups=3.12, wpb=682, bsz=682, num_updates=9250, lr=0.000295, gnorm=5.279, clip=0, train_wall=6, gb_free=75.8, wall=1288 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:35:28]    INFO >> epoch 007:     78 / 1539 loss=3.774, wps=5559.2, ups=7.47, wpb=744.2, bsz=744.2, num_updates=9300, lr=0.000295, gnorm=4.511, clip=0, train_wall=6, gb_free=77.5, wall=1295 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:35:35]    INFO >> epoch 007:    128 / 1539 loss=3.766, wps=6000.9, ups=8.4, wpb=714.6, bsz=714.6, num_updates=9350, lr=0.000295, gnorm=4.777, clip=0, train_wall=5, gb_free=76.2, wall=1300 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:35:42]    INFO >> epoch 007:    178 / 1539 loss=3.497, wps=6202.9, ups=7.63, wpb=813.1, bsz=813.1, num_updates=9400, lr=0.000295, gnorm=4.583, clip=0, train_wall=6, gb_free=75.8, wall=1307 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:35:48]    INFO >> epoch 007:    228 / 1539 loss=3.754, wps=6513.7, ups=7.81, wpb=833.7, bsz=833.7, num_updates=9450, lr=0.000295, gnorm=5.121, clip=0, train_wall=6, gb_free=76.5, wall=1313 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:35:54]    INFO >> epoch 007:    278 / 1539 loss=3.714, wps=5782.7, ups=7.84, wpb=738, bsz=738, num_updates=9500, lr=0.000295, gnorm=5.057, clip=0, train_wall=6, gb_free=73.9, wall=1320 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:36:01] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:36:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:36:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:36:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 19        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77886 MiB |  77946 MiB | 164008 GiB | 163932 GiB |
|       from large pool |  77448 MiB |  77507 MiB | 162182 GiB | 162106 GiB |
|       from small pool |    438 MiB |    439 MiB |   1826 GiB |   1825 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77886 MiB |  77946 MiB | 164008 GiB | 163932 GiB |
|       from large pool |  77448 MiB |  77507 MiB | 162182 GiB | 162106 GiB |
|       from small pool |    438 MiB |    439 MiB |   1826 GiB |   1825 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77635 MiB |  77695 MiB | 163185 GiB | 163110 GiB |
|       from large pool |  77199 MiB |  77258 MiB | 161362 GiB | 161286 GiB |
|       from small pool |    436 MiB |    437 MiB |   1823 GiB |   1823 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80506 MiB |  80506 MiB | 253410 MiB | 172904 MiB |
|       from large pool |  80058 MiB |  80058 MiB | 251480 MiB | 171422 MiB |
|       from small pool |    448 MiB |    448 MiB |   1930 MiB |   1482 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2619 MiB |   4073 MiB | 123549 GiB | 123546 GiB |
|       from large pool |   2609 MiB |   4069 MiB | 121425 GiB | 121422 GiB |
|       from small pool |      9 MiB |     20 MiB |   2124 GiB |   2124 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8222    |    8223    |   20876 K  |   20868 K  |
|       from large pool |    1024    |    1025    |    9416 K  |    9415 K  |
|       from small pool |    7198    |    7200    |   11459 K  |   11452 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8222    |    8223    |   20876 K  |   20868 K  |
|       from large pool |    1024    |    1025    |    9416 K  |    9415 K  |
|       from small pool |    7198    |    7200    |   11459 K  |   11452 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     839    |     839    |    3783    |    2944    |
|       from large pool |     615    |     615    |    2818    |    2203    |
|       from small pool |     224    |     224    |     965    |     741    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     347    |     348    |   12327 K  |   12326 K  |
|       from large pool |     159    |     159    |    6814 K  |    6813 K  |
|       from small pool |     188    |     189    |    5513 K  |    5512 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:36:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:36:01] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:36:01]    INFO >> epoch 007:    329 / 1539 loss=3.749, wps=4635.1, ups=7.33, wpb=632, bsz=632, num_updates=9550, lr=0.000295, gnorm=4.209, clip=0, train_wall=6, gb_free=76.9, wall=1327 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:36:09]    INFO >> epoch 007:    379 / 1539 loss=3.878, wps=4794.3, ups=7.71, wpb=622.1, bsz=622.1, num_updates=9600, lr=0.000295, gnorm=4.535, clip=0, train_wall=6, gb_free=78.5, wall=1333 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:36:15]    INFO >> epoch 007:    429 / 1539 loss=3.735, wps=4960.6, ups=7.9, wpb=628.2, bsz=628.2, num_updates=9650, lr=0.000295, gnorm=4.392, clip=0, train_wall=6, gb_free=76.6, wall=1339 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:36:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.53 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:36:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:36:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:36:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77283 MiB |  77343 MiB | 166352 GiB | 166276 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 164501 GiB | 164426 GiB |
|       from small pool |    343 MiB |    344 MiB |   1850 GiB |   1850 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77283 MiB |  77343 MiB | 166352 GiB | 166276 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 164501 GiB | 164426 GiB |
|       from small pool |    343 MiB |    344 MiB |   1850 GiB |   1850 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76965 MiB |  77024 MiB | 165517 GiB | 165442 GiB |
|       from large pool |  76624 MiB |  76683 MiB | 163669 GiB | 163594 GiB |
|       from small pool |    341 MiB |    342 MiB |   1848 GiB |   1847 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80506 MiB | 253472 MiB | 172974 MiB |
|       from large pool |  80118 MiB |  80118 MiB | 251540 MiB | 171422 MiB |
|       from small pool |    380 MiB |    448 MiB |   1932 MiB |   1552 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3154 MiB |   6330 MiB | 125378 GiB | 125375 GiB |
|       from large pool |   3118 MiB |   6296 MiB | 123224 GiB | 123221 GiB |
|       from small pool |     36 MiB |     38 MiB |   2153 GiB |   2153 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6537    |    6540    |   21171 K  |   21165 K  |
|       from large pool |     876    |     877    |    9556 K  |    9555 K  |
|       from small pool |    5661    |    5664    |   11615 K  |   11609 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6537    |    6540    |   21171 K  |   21165 K  |
|       from large pool |     876    |     877    |    9556 K  |    9555 K  |
|       from small pool |    5661    |    5664    |   11615 K  |   11609 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     806    |     839    |    3785    |    2979    |
|       from large pool |     616    |     616    |    2819    |    2203    |
|       from small pool |     190    |     224    |     966    |     776    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     552    |     552    |   12500 K  |   12500 K  |
|       from large pool |     203    |     203    |    6915 K  |    6914 K  |
|       from small pool |     349    |     349    |    5585 K  |    5585 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:36:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:36:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:36:22]    INFO >> epoch 007:    480 / 1539 loss=3.674, wps=5371.8, ups=7.1, wpb=757, bsz=757, num_updates=9700, lr=0.000295, gnorm=4.524, clip=0, train_wall=6, gb_free=71.9, wall=1346 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:36:29]    INFO >> epoch 007:    530 / 1539 loss=3.671, wps=5685.7, ups=7.93, wpb=717.2, bsz=717.2, num_updates=9750, lr=0.000295, gnorm=4.961, clip=0, train_wall=6, gb_free=75.5, wall=1353 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:36:35]    INFO >> epoch 007:    580 / 1539 loss=3.767, wps=5108.7, ups=7.97, wpb=640.7, bsz=640.7, num_updates=9800, lr=0.000295, gnorm=4.477, clip=0, train_wall=6, gb_free=76.1, wall=1359 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:36:43]    INFO >> epoch 007:    630 / 1539 loss=3.755, wps=5378.2, ups=7.8, wpb=689.7, bsz=689.7, num_updates=9850, lr=0.000295, gnorm=5.128, clip=0, train_wall=6, gb_free=77.2, wall=1365 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:36:49]    INFO >> epoch 007:    680 / 1539 loss=3.527, wps=6393.8, ups=7.58, wpb=844, bsz=844, num_updates=9900, lr=0.000295, gnorm=5.172, clip=0, train_wall=6, gb_free=74.9, wall=1372 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:36:56]    INFO >> epoch 007:    730 / 1539 loss=3.782, wps=5338.1, ups=7.42, wpb=719.2, bsz=719.2, num_updates=9950, lr=0.000295, gnorm=4.654, clip=0, train_wall=6, gb_free=76.4, wall=1379 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:03]    INFO >> epoch 007:    780 / 1539 loss=3.871, wps=4836.2, ups=7.72, wpb=626.6, bsz=626.6, num_updates=10000, lr=0.000295, gnorm=4.383, clip=0, train_wall=6, gb_free=76.1, wall=1385 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:09]    INFO >> epoch 007:    830 / 1539 loss=3.768, wps=5987.6, ups=8.04, wpb=745.1, bsz=745.1, num_updates=10050, lr=0.000295, gnorm=4.33, clip=0, train_wall=6, gb_free=75.1, wall=1392 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:16]    INFO >> epoch 007:    880 / 1539 loss=3.748, wps=5411.3, ups=8, wpb=676.3, bsz=676.3, num_updates=10100, lr=0.000295, gnorm=4.793, clip=0, train_wall=6, gb_free=75.3, wall=1398 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:22]    INFO >> epoch 007:    930 / 1539 loss=3.801, wps=5495.9, ups=8.47, wpb=649.1, bsz=649.1, num_updates=10150, lr=0.000295, gnorm=4.257, clip=0, train_wall=5, gb_free=77.4, wall=1404 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:29]    INFO >> epoch 007:    980 / 1539 loss=3.531, wps=6055.5, ups=7.76, wpb=780, bsz=780, num_updates=10200, lr=0.000295, gnorm=5.232, clip=0, train_wall=6, gb_free=76.5, wall=1410 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:35]    INFO >> epoch 007:   1030 / 1539 loss=3.75, wps=5342.6, ups=8.14, wpb=656.4, bsz=656.4, num_updates=10250, lr=0.000295, gnorm=4.522, clip=0, train_wall=6, gb_free=76.5, wall=1416 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:42]    INFO >> epoch 007:   1080 / 1539 loss=3.719, wps=5616.6, ups=7.45, wpb=753.7, bsz=753.7, num_updates=10300, lr=0.000295, gnorm=4.653, clip=0, train_wall=6, gb_free=74.1, wall=1423 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:49]    INFO >> epoch 007:   1130 / 1539 loss=3.611, wps=6214.7, ups=7.89, wpb=788.1, bsz=788.1, num_updates=10350, lr=0.000295, gnorm=4.82, clip=0, train_wall=6, gb_free=77, wall=1429 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:37:56]    INFO >> epoch 007:   1180 / 1539 loss=3.642, wps=6133.9, ups=7.54, wpb=814, bsz=814, num_updates=10400, lr=0.000295, gnorm=5.017, clip=0, train_wall=6, gb_free=76.7, wall=1436 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:38:02]    INFO >> epoch 007:   1230 / 1539 loss=3.763, wps=5390.6, ups=8.07, wpb=667.6, bsz=667.6, num_updates=10450, lr=0.000295, gnorm=4.331, clip=0, train_wall=6, gb_free=75.7, wall=1442 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:38:08]    INFO >> epoch 007:   1280 / 1539 loss=3.641, wps=5513.2, ups=8.51, wpb=648, bsz=648, num_updates=10500, lr=0.000295, gnorm=4.441, clip=0, train_wall=5, gb_free=75, wall=1448 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:38:14]    INFO >> epoch 007:   1330 / 1539 loss=3.772, wps=5560.1, ups=8.36, wpb=665.2, bsz=665.2, num_updates=10550, lr=0.000295, gnorm=4.664, clip=0, train_wall=6, gb_free=76.5, wall=1454 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:38:22]    INFO >> epoch 007:   1380 / 1539 loss=3.645, wps=5483.5, ups=7.99, wpb=686.3, bsz=686.3, num_updates=10600, lr=0.000295, gnorm=4.836, clip=0, train_wall=6, gb_free=73.6, wall=1460 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:38:29]    INFO >> epoch 007:   1430 / 1539 loss=3.408, wps=6558.5, ups=7.03, wpb=932.5, bsz=932.5, num_updates=10650, lr=0.000295, gnorm=4.842, clip=0, train_wall=7, gb_free=73.9, wall=1467 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:38:35]    INFO >> epoch 007:   1480 / 1539 loss=3.763, wps=6006.8, ups=8.22, wpb=731, bsz=731, num_updates=10700, lr=0.000295, gnorm=4.526, clip=0, train_wall=6, gb_free=75.4, wall=1473 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:38:41]    INFO >> epoch 007:   1530 / 1539 loss=3.739, wps=5748.8, ups=7.62, wpb=754.5, bsz=754.5, num_updates=10750, lr=0.000295, gnorm=3.898, clip=0, train_wall=6, gb_free=77, wall=1480 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:38:42]    INFO >> epoch 007 | loss 3.703 | wps 5361.9 | ups 7.44 | wpb 720.8 | bsz 720.8 | num_updates 10759 | lr 0.000295 | gnorm 4.669 | clip 0 | train_wall 179 | gb_free 75 | wall 1481 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:38:42] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:38:51]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.859 | wps 16593.6 | wpb 5412.5 | bsz 5412.5 | num_updates 10759 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:38:52]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:38:52]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 7 @ 10759 updates, score 3.859) (writing took 0.012800 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:38:52] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:38:59]    INFO >> epoch 008:     41 / 1539 loss=3.547, wps=2287.9, ups=3.16, wpb=723.7, bsz=723.7, num_updates=10800, lr=0.000262, gnorm=4.625, clip=0, train_wall=6, gb_free=76.3, wall=1496 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:05]    INFO >> epoch 008:     91 / 1539 loss=3.764, wps=5802.2, ups=7.68, wpb=755.2, bsz=755.2, num_updates=10850, lr=0.000262, gnorm=4.436, clip=0, train_wall=6, gb_free=75.4, wall=1502 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:11]    INFO >> epoch 008:    141 / 1539 loss=3.743, wps=5025.2, ups=8.23, wpb=610.4, bsz=610.4, num_updates=10900, lr=0.000262, gnorm=4.258, clip=0, train_wall=6, gb_free=75.8, wall=1508 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:18]    INFO >> epoch 008:    191 / 1539 loss=3.685, wps=6143.1, ups=7.84, wpb=783.3, bsz=783.3, num_updates=10950, lr=0.000262, gnorm=4.411, clip=0, train_wall=6, gb_free=76.1, wall=1515 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:24]    INFO >> epoch 008:    241 / 1539 loss=3.46, wps=6199, ups=7.68, wpb=807.2, bsz=807.2, num_updates=11000, lr=0.000262, gnorm=5.327, clip=0, train_wall=6, gb_free=76.3, wall=1521 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:32]    INFO >> epoch 008:    291 / 1539 loss=3.813, wps=5306.2, ups=7.88, wpb=673.5, bsz=673.5, num_updates=11050, lr=0.000262, gnorm=4.772, clip=0, train_wall=6, gb_free=75.7, wall=1528 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:38]    INFO >> epoch 008:    341 / 1539 loss=3.797, wps=5190.6, ups=7.61, wpb=682.2, bsz=682.2, num_updates=11100, lr=0.000262, gnorm=4.742, clip=0, train_wall=6, gb_free=76.2, wall=1534 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:45]    INFO >> epoch 008:    391 / 1539 loss=3.753, wps=5817.2, ups=7.62, wpb=763.4, bsz=763.4, num_updates=11150, lr=0.000262, gnorm=4.446, clip=0, train_wall=6, gb_free=74, wall=1541 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:51]    INFO >> epoch 008:    441 / 1539 loss=3.637, wps=5575.6, ups=7.79, wpb=715.7, bsz=715.7, num_updates=11200, lr=0.000262, gnorm=4.728, clip=0, train_wall=6, gb_free=75.9, wall=1547 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:39:58]    INFO >> epoch 008:    491 / 1539 loss=3.686, wps=6090, ups=7.09, wpb=859.2, bsz=859.2, num_updates=11250, lr=0.000262, gnorm=4.664, clip=0, train_wall=6, gb_free=77.5, wall=1554 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:06]    INFO >> epoch 008:    541 / 1539 loss=3.735, wps=5401, ups=8.01, wpb=674.3, bsz=674.3, num_updates=11300, lr=0.000262, gnorm=4.049, clip=0, train_wall=6, gb_free=76.5, wall=1561 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:12]    INFO >> epoch 008:    591 / 1539 loss=3.736, wps=5399.6, ups=8.04, wpb=671.5, bsz=671.5, num_updates=11350, lr=0.000262, gnorm=4.443, clip=0, train_wall=6, gb_free=75.7, wall=1567 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:18]    INFO >> epoch 008:    641 / 1539 loss=3.724, wps=5346.7, ups=8.15, wpb=655.9, bsz=655.9, num_updates=11400, lr=0.000262, gnorm=4.328, clip=0, train_wall=6, gb_free=77, wall=1573 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:24]    INFO >> epoch 008:    691 / 1539 loss=3.661, wps=5585.8, ups=8.04, wpb=694.7, bsz=694.7, num_updates=11450, lr=0.000262, gnorm=4.254, clip=0, train_wall=6, gb_free=72.7, wall=1579 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:31]    INFO >> epoch 008:    741 / 1539 loss=3.805, wps=5142.8, ups=7.92, wpb=649.1, bsz=649.1, num_updates=11500, lr=0.000262, gnorm=3.695, clip=0, train_wall=6, gb_free=73.9, wall=1585 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:39]    INFO >> epoch 008:    791 / 1539 loss=3.661, wps=5448.2, ups=7.71, wpb=706.4, bsz=706.4, num_updates=11550, lr=0.000262, gnorm=4.359, clip=0, train_wall=6, gb_free=76.5, wall=1592 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:45]    INFO >> epoch 008:    841 / 1539 loss=3.709, wps=5565.1, ups=8.19, wpb=679.8, bsz=679.8, num_updates=11600, lr=0.000262, gnorm=4.258, clip=0, train_wall=6, gb_free=76.9, wall=1598 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:51]    INFO >> epoch 008:    891 / 1539 loss=3.776, wps=5160, ups=7.83, wpb=658.7, bsz=658.7, num_updates=11650, lr=0.000262, gnorm=3.971, clip=0, train_wall=6, gb_free=75.4, wall=1604 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:40:58]    INFO >> epoch 008:    941 / 1539 loss=3.392, wps=5868.4, ups=7.53, wpb=779.3, bsz=779.3, num_updates=11700, lr=0.000262, gnorm=4.848, clip=0, train_wall=6, gb_free=74.9, wall=1611 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:41:04] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 2.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:41:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:41:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:41:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 22        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77826 MiB |  77886 MiB | 200066 GiB | 199990 GiB |
|       from large pool |  77388 MiB |  77448 MiB | 197841 GiB | 197766 GiB |
|       from small pool |    438 MiB |    439 MiB |   2225 GiB |   2224 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77826 MiB |  77886 MiB | 200066 GiB | 199990 GiB |
|       from large pool |  77388 MiB |  77448 MiB | 197841 GiB | 197766 GiB |
|       from small pool |    438 MiB |    439 MiB |   2225 GiB |   2224 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77575 MiB |  77635 MiB | 199059 GiB | 198984 GiB |
|       from large pool |  77139 MiB |  77199 MiB | 196838 GiB | 196762 GiB |
|       from small pool |    436 MiB |    437 MiB |   2221 GiB |   2221 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80506 MiB |  80506 MiB | 253540 MiB | 173034 MiB |
|       from large pool |  80058 MiB |  80058 MiB | 251540 MiB | 171482 MiB |
|       from small pool |    448 MiB |    448 MiB |   2000 MiB |   1552 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2679 MiB |   4073 MiB | 150766 GiB | 150764 GiB |
|       from large pool |   2669 MiB |   4069 MiB | 148176 GiB | 148174 GiB |
|       from small pool |      9 MiB |     22 MiB |   2590 GiB |   2590 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8218    |    8219    |   25530 K  |   25522 K  |
|       from large pool |    1023    |    1024    |   11567 K  |   11566 K  |
|       from small pool |    7195    |    7196    |   13963 K  |   13956 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8218    |    8219    |   25530 K  |   25522 K  |
|       from large pool |    1023    |    1024    |   11567 K  |   11566 K  |
|       from small pool |    7195    |    7196    |   13963 K  |   13956 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     839    |     839    |    3819    |    2980    |
|       from large pool |     615    |     615    |    2819    |    2204    |
|       from small pool |     224    |     224    |    1000    |     776    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     345    |     348    |   15068 K  |   15068 K  |
|       from large pool |     159    |     159    |    8369 K  |    8369 K  |
|       from small pool |     186    |     189    |    6698 K  |    6698 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:41:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:41:04] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:41:05]    INFO >> epoch 008:    992 / 1539 loss=3.781, wps=4628.9, ups=7.41, wpb=625, bsz=625, num_updates=11750, lr=0.000262, gnorm=4.344, clip=0, train_wall=6, gb_free=77.9, wall=1618 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:41:13]    INFO >> epoch 008:   1042 / 1539 loss=3.552, wps=5796.1, ups=7.18, wpb=807.1, bsz=807.1, num_updates=11800, lr=0.000262, gnorm=5.274, clip=0, train_wall=6, gb_free=76.2, wall=1625 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:41:19]    INFO >> epoch 008:   1092 / 1539 loss=3.699, wps=5202.9, ups=7.78, wpb=669.2, bsz=669.2, num_updates=11850, lr=0.000262, gnorm=4.252, clip=0, train_wall=6, gb_free=75.1, wall=1631 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:41:25]    INFO >> epoch 008:   1142 / 1539 loss=3.8, wps=4873.2, ups=8.08, wpb=603.1, bsz=603.1, num_updates=11900, lr=0.000262, gnorm=4.118, clip=0, train_wall=6, gb_free=75.9, wall=1637 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:41:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.53 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:41:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:41:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:41:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 24        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77283 MiB |  77343 MiB | 202941 GiB | 202865 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 200685 GiB | 200610 GiB |
|       from small pool |    343 MiB |    344 MiB |   2256 GiB |   2255 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77283 MiB |  77343 MiB | 202941 GiB | 202865 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 200685 GiB | 200610 GiB |
|       from small pool |    343 MiB |    344 MiB |   2256 GiB |   2255 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76965 MiB |  77024 MiB | 201919 GiB | 201844 GiB |
|       from large pool |  76624 MiB |  76683 MiB | 199666 GiB | 199591 GiB |
|       from small pool |    341 MiB |    342 MiB |   2252 GiB |   2252 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80506 MiB | 253602 MiB | 173104 MiB |
|       from large pool |  80118 MiB |  80118 MiB | 251600 MiB | 171482 MiB |
|       from small pool |    380 MiB |    448 MiB |   2002 MiB |   1622 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3154 MiB |   6329 MiB | 152977 GiB | 152974 GiB |
|       from large pool |   3118 MiB |   6296 MiB | 150349 GiB | 150346 GiB |
|       from small pool |     36 MiB |     37 MiB |   2627 GiB |   2627 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6537    |    6540    |   25901 K  |   25895 K  |
|       from large pool |     876    |     877    |   11743 K  |   11742 K  |
|       from small pool |    5661    |    5664    |   14158 K  |   14152 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6537    |    6540    |   25901 K  |   25895 K  |
|       from large pool |     876    |     877    |   11743 K  |   11742 K  |
|       from small pool |    5661    |    5664    |   14158 K  |   14152 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     806    |     839    |    3821    |    3015    |
|       from large pool |     616    |     616    |    2820    |    2204    |
|       from small pool |     190    |     224    |    1001    |     811    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     548    |     549    |   15285 K  |   15285 K  |
|       from large pool |     203    |     203    |    8496 K  |    8496 K  |
|       from small pool |     345    |     346    |    6789 K  |    6788 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:41:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:41:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:41:33]    INFO >> epoch 008:   1193 / 1539 loss=3.815, wps=4703.7, ups=6.65, wpb=707.8, bsz=707.8, num_updates=11950, lr=0.000262, gnorm=4.264, clip=0, train_wall=6, gb_free=67.8, wall=1645 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:41:40]    INFO >> epoch 008:   1243 / 1539 loss=3.483, wps=6273.6, ups=6.84, wpb=916.7, bsz=916.7, num_updates=12000, lr=0.000262, gnorm=4.374, clip=0, train_wall=7, gb_free=74.5, wall=1652 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:41:48]    INFO >> epoch 008:   1293 / 1539 loss=3.614, wps=5904, ups=7.69, wpb=767.6, bsz=767.6, num_updates=12050, lr=0.000262, gnorm=4.765, clip=0, train_wall=6, gb_free=74.4, wall=1659 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:41:55]    INFO >> epoch 008:   1343 / 1539 loss=3.671, wps=5528.7, ups=7.68, wpb=719.6, bsz=719.6, num_updates=12100, lr=0.000262, gnorm=4.769, clip=0, train_wall=6, gb_free=76.3, wall=1665 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:42:02]    INFO >> epoch 008:   1393 / 1539 loss=3.878, wps=5607.6, ups=7.05, wpb=795.2, bsz=795.2, num_updates=12150, lr=0.000262, gnorm=3.526, clip=0, train_wall=6, gb_free=75.6, wall=1672 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:42:08]    INFO >> epoch 008:   1443 / 1539 loss=3.734, wps=6127.1, ups=7.93, wpb=772.6, bsz=772.6, num_updates=12200, lr=0.000262, gnorm=4.572, clip=0, train_wall=6, gb_free=74.6, wall=1679 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:42:15]    INFO >> epoch 008:   1493 / 1539 loss=3.686, wps=5166.4, ups=7.54, wpb=684.9, bsz=684.9, num_updates=12250, lr=0.000262, gnorm=4.758, clip=0, train_wall=6, gb_free=72.5, wall=1685 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:42:22]    INFO >> epoch 008 | loss 3.686 | wps 5273.3 | ups 7.32 | wpb 720.8 | bsz 720.8 | num_updates 12296 | lr 0.000262 | gnorm 4.448 | clip 0 | train_wall 182 | gb_free 77.2 | wall 1691 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:42:22] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:42:31]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.859 | wps 17166.7 | wpb 5412.5 | bsz 5412.5 | num_updates 12296 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:42:31]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:42:31]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 8 @ 12296 updates, score 3.859) (writing took 0.011932 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:42:31] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:42:32]    INFO >> epoch 009:      4 / 1539 loss=3.664, wps=2193.9, ups=3.18, wpb=690, bsz=690, num_updates=12300, lr=0.000227, gnorm=4.247, clip=0, train_wall=6, gb_free=78.3, wall=1701 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:42:38]    INFO >> epoch 009:     54 / 1539 loss=3.775, wps=4829.5, ups=8.22, wpb=587.9, bsz=587.9, num_updates=12350, lr=0.000227, gnorm=4.122, clip=0, train_wall=6, gb_free=76.5, wall=1707 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:42:44]    INFO >> epoch 009:    104 / 1539 loss=3.693, wps=5927.3, ups=7.8, wpb=760.2, bsz=760.2, num_updates=12400, lr=0.000227, gnorm=4.072, clip=0, train_wall=6, gb_free=74.1, wall=1713 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:42:52]    INFO >> epoch 009:    154 / 1539 loss=3.69, wps=5812.6, ups=8.03, wpb=723.8, bsz=723.8, num_updates=12450, lr=0.000227, gnorm=4.067, clip=0, train_wall=6, gb_free=75.8, wall=1720 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:42:58]    INFO >> epoch 009:    204 / 1539 loss=3.614, wps=5915, ups=7.9, wpb=748.9, bsz=748.9, num_updates=12500, lr=0.000227, gnorm=4.203, clip=0, train_wall=6, gb_free=76.7, wall=1726 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:43:04]    INFO >> epoch 009:    254 / 1539 loss=3.675, wps=6099.8, ups=8.29, wpb=736, bsz=736, num_updates=12550, lr=0.000227, gnorm=4.18, clip=0, train_wall=6, gb_free=75.2, wall=1732 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:43:11]    INFO >> epoch 009:    304 / 1539 loss=3.641, wps=5279.5, ups=7.65, wpb=689.9, bsz=689.9, num_updates=12600, lr=0.000227, gnorm=4.948, clip=0, train_wall=6, gb_free=75, wall=1739 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:43:18]    INFO >> epoch 009:    354 / 1539 loss=3.621, wps=5971.4, ups=7.27, wpb=820.9, bsz=820.9, num_updates=12650, lr=0.000227, gnorm=4.927, clip=0, train_wall=6, gb_free=75.3, wall=1745 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:43:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.53 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:43:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:43:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:43:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77283 MiB |  77343 MiB | 217361 GiB | 217285 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 214943 GiB | 214867 GiB |
|       from small pool |    343 MiB |    344 MiB |   2418 GiB |   2417 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77283 MiB |  77343 MiB | 217361 GiB | 217285 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 214943 GiB | 214867 GiB |
|       from small pool |    343 MiB |    344 MiB |   2418 GiB |   2417 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76965 MiB |  77024 MiB | 216273 GiB | 216198 GiB |
|       from large pool |  76624 MiB |  76683 MiB | 213858 GiB | 213784 GiB |
|       from small pool |    341 MiB |    342 MiB |   2414 GiB |   2414 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 253718 MiB | 173220 MiB |
|       from large pool |  80118 MiB |  80118 MiB | 251660 MiB | 171542 MiB |
|       from small pool |    380 MiB |    434 MiB |   2058 MiB |   1678 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3154 MiB |   6330 MiB | 163150 GiB | 163147 GiB |
|       from large pool |   3118 MiB |   6296 MiB | 160338 GiB | 160335 GiB |
|       from small pool |     36 MiB |     38 MiB |   2811 GiB |   2811 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6537    |    6540    |   27675 K  |   27669 K  |
|       from large pool |     876    |     877    |   12495 K  |   12495 K  |
|       from small pool |    5661    |    5664    |   15179 K  |   15173 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6537    |    6540    |   27675 K  |   27669 K  |
|       from large pool |     876    |     877    |   12495 K  |   12495 K  |
|       from small pool |    5661    |    5664    |   15179 K  |   15173 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     806    |     832    |    3850    |    3044    |
|       from large pool |     616    |     616    |    2821    |    2205    |
|       from small pool |     190    |     217    |    1029    |     839    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     553    |     553    |   16338 K  |   16338 K  |
|       from large pool |     203    |     203    |    9041 K  |    9040 K  |
|       from small pool |     350    |     350    |    7297 K  |    7297 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:43:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:43:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:43:26]    INFO >> epoch 009:    405 / 1539 loss=3.645, wps=4814.2, ups=7.04, wpb=684, bsz=684, num_updates=12700, lr=0.000227, gnorm=4.469, clip=0, train_wall=6, gb_free=73.6, wall=1753 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:43:32]    INFO >> epoch 009:    455 / 1539 loss=3.711, wps=5800.4, ups=8.19, wpb=708.3, bsz=708.3, num_updates=12750, lr=0.000227, gnorm=4.251, clip=0, train_wall=6, gb_free=77.7, wall=1759 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:43:38]    INFO >> epoch 009:    505 / 1539 loss=3.688, wps=6062.6, ups=7.93, wpb=764.3, bsz=764.3, num_updates=12800, lr=0.000227, gnorm=4.212, clip=0, train_wall=6, gb_free=76.6, wall=1765 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:43:45]    INFO >> epoch 009:    555 / 1539 loss=3.549, wps=5784.2, ups=7.63, wpb=757.6, bsz=757.6, num_updates=12850, lr=0.000227, gnorm=4.429, clip=0, train_wall=6, gb_free=76.2, wall=1772 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:43:48] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 2.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:43:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:43:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:43:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 27        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77826 MiB |  77886 MiB | 220570 GiB | 220494 GiB |
|       from large pool |  77388 MiB |  77448 MiB | 218117 GiB | 218041 GiB |
|       from small pool |    438 MiB |    439 MiB |   2453 GiB |   2453 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77826 MiB |  77886 MiB | 220570 GiB | 220494 GiB |
|       from large pool |  77388 MiB |  77448 MiB | 218117 GiB | 218041 GiB |
|       from small pool |    438 MiB |    439 MiB |   2453 GiB |   2453 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77575 MiB |  77635 MiB | 219466 GiB | 219390 GiB |
|       from large pool |  77139 MiB |  77199 MiB | 217016 GiB | 216940 GiB |
|       from small pool |    436 MiB |    437 MiB |   2449 GiB |   2449 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80506 MiB |  80506 MiB | 253786 MiB | 173280 MiB |
|       from large pool |  80058 MiB |  80058 MiB | 251660 MiB | 171602 MiB |
|       from small pool |    448 MiB |    448 MiB |   2126 MiB |   1678 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2679 MiB |   4073 MiB | 165649 GiB | 165647 GiB |
|       from large pool |   2669 MiB |   4069 MiB | 162795 GiB | 162793 GiB |
|       from small pool |      9 MiB |     22 MiB |   2854 GiB |   2854 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8218    |    8219    |   28092 K  |   28084 K  |
|       from large pool |    1023    |    1024    |   12690 K  |   12689 K  |
|       from small pool |    7195    |    7196    |   15401 K  |   15394 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8218    |    8219    |   28092 K  |   28084 K  |
|       from large pool |    1023    |    1024    |   12690 K  |   12689 K  |
|       from small pool |    7195    |    7196    |   15401 K  |   15394 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     839    |     839    |    3884    |    3045    |
|       from large pool |     615    |     615    |    2821    |    2206    |
|       from small pool |     224    |     224    |    1063    |     839    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     344    |     347    |   16584 K  |   16584 K  |
|       from large pool |     159    |     159    |    9182 K  |    9182 K  |
|       from small pool |     185    |     188    |    7401 K  |    7401 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:43:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:43:48] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:43:52]    INFO >> epoch 009:    606 / 1539 loss=3.734, wps=4845.1, ups=7.23, wpb=670.4, bsz=670.4, num_updates=12900, lr=0.000227, gnorm=4.117, clip=0, train_wall=6, gb_free=78.5, wall=1778 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:00]    INFO >> epoch 009:    656 / 1539 loss=3.749, wps=5202.5, ups=7.74, wpb=672.1, bsz=672.1, num_updates=12950, lr=0.000227, gnorm=3.891, clip=0, train_wall=6, gb_free=77, wall=1785 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:06]    INFO >> epoch 009:    706 / 1539 loss=3.722, wps=5377.9, ups=7.84, wpb=685.7, bsz=685.7, num_updates=13000, lr=0.000227, gnorm=4.013, clip=0, train_wall=6, gb_free=76.1, wall=1791 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:13]    INFO >> epoch 009:    756 / 1539 loss=3.717, wps=4914.8, ups=7.67, wpb=641.1, bsz=641.1, num_updates=13050, lr=0.000227, gnorm=3.696, clip=0, train_wall=6, gb_free=74.7, wall=1798 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:19]    INFO >> epoch 009:    806 / 1539 loss=3.534, wps=5944.9, ups=7.72, wpb=769.8, bsz=769.8, num_updates=13100, lr=0.000227, gnorm=5.157, clip=0, train_wall=6, gb_free=77.2, wall=1804 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:26]    INFO >> epoch 009:    856 / 1539 loss=3.908, wps=4924.4, ups=7.23, wpb=681.2, bsz=681.2, num_updates=13150, lr=0.000227, gnorm=3.79, clip=0, train_wall=6, gb_free=76.4, wall=1811 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:33]    INFO >> epoch 009:    906 / 1539 loss=3.74, wps=5365.4, ups=8.11, wpb=661.4, bsz=661.4, num_updates=13200, lr=0.000227, gnorm=4.386, clip=0, train_wall=6, gb_free=76, wall=1817 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:41]    INFO >> epoch 009:    956 / 1539 loss=3.625, wps=5791.2, ups=6.75, wpb=858.4, bsz=858.4, num_updates=13250, lr=0.000227, gnorm=4.311, clip=0, train_wall=7, gb_free=77, wall=1825 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:47]    INFO >> epoch 009:   1006 / 1539 loss=3.63, wps=5283.6, ups=7.98, wpb=662.5, bsz=662.5, num_updates=13300, lr=0.000227, gnorm=4.296, clip=0, train_wall=6, gb_free=76, wall=1831 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:44:54]    INFO >> epoch 009:   1056 / 1539 loss=3.517, wps=5993.7, ups=7.76, wpb=772.1, bsz=772.1, num_updates=13350, lr=0.000227, gnorm=4.502, clip=0, train_wall=6, gb_free=74.4, wall=1837 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:00]    INFO >> epoch 009:   1106 / 1539 loss=3.693, wps=5507.8, ups=7.89, wpb=698.5, bsz=698.5, num_updates=13400, lr=0.000227, gnorm=4.527, clip=0, train_wall=6, gb_free=70.1, wall=1844 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:07]    INFO >> epoch 009:   1156 / 1539 loss=3.723, wps=5787.2, ups=8.07, wpb=716.9, bsz=716.9, num_updates=13450, lr=0.000227, gnorm=4.61, clip=0, train_wall=6, gb_free=77.5, wall=1850 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:14]    INFO >> epoch 009:   1206 / 1539 loss=3.641, wps=5624.1, ups=7.89, wpb=713, bsz=713, num_updates=13500, lr=0.000227, gnorm=4.246, clip=0, train_wall=6, gb_free=76.2, wall=1856 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:21]    INFO >> epoch 009:   1256 / 1539 loss=3.579, wps=6598.7, ups=7.21, wpb=915.7, bsz=915.7, num_updates=13550, lr=0.000227, gnorm=3.797, clip=0, train_wall=6, gb_free=76.6, wall=1863 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:27]    INFO >> epoch 009:   1306 / 1539 loss=3.797, wps=5518, ups=7.9, wpb=698.4, bsz=698.4, num_updates=13600, lr=0.000227, gnorm=3.87, clip=0, train_wall=6, gb_free=77.8, wall=1870 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:33]    INFO >> epoch 009:   1356 / 1539 loss=3.648, wps=5381, ups=7.97, wpb=674.8, bsz=674.8, num_updates=13650, lr=0.000227, gnorm=4.211, clip=0, train_wall=6, gb_free=74.1, wall=1876 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:41]    INFO >> epoch 009:   1406 / 1539 loss=3.468, wps=6172, ups=7.96, wpb=775.2, bsz=775.2, num_updates=13700, lr=0.000227, gnorm=4.758, clip=0, train_wall=6, gb_free=75.4, wall=1882 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:48]    INFO >> epoch 009:   1456 / 1539 loss=3.465, wps=5865.1, ups=7.02, wpb=836, bsz=836, num_updates=13750, lr=0.000227, gnorm=4.203, clip=0, train_wall=7, gb_free=77.9, wall=1889 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:54]    INFO >> epoch 009:   1506 / 1539 loss=3.711, wps=5010.3, ups=8.17, wpb=613.6, bsz=613.6, num_updates=13800, lr=0.000227, gnorm=4.067, clip=0, train_wall=6, gb_free=75.6, wall=1895 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:45:59]    INFO >> epoch 009 | loss 3.66 | wps 5308.4 | ups 7.36 | wpb 720.8 | bsz 720.8 | num_updates 13833 | lr 0.000227 | gnorm 4.279 | clip 0 | train_wall 181 | gb_free 77 | wall 1900 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:45:59] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:46:08]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.826 | wps 16208.2 | wpb 5412.5 | bsz 5412.5 | num_updates 13833 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:46:08]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:46:08]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 9 @ 13833 updates, score 3.826) (writing took 0.011596 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:46:08] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:46:10]    INFO >> epoch 010:     17 / 1539 loss=3.726, wps=2001.3, ups=3.1, wpb=646.5, bsz=646.5, num_updates=13850, lr=0.000193, gnorm=4.158, clip=0, train_wall=6, gb_free=75, wall=1912 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:46:19]    INFO >> epoch 010:     67 / 1539 loss=3.669, wps=5141.6, ups=7.42, wpb=692.6, bsz=692.6, num_updates=13900, lr=0.000193, gnorm=4.512, clip=0, train_wall=6, gb_free=76.9, wall=1918 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:46:25]    INFO >> epoch 010:    117 / 1539 loss=3.596, wps=5747.8, ups=8.04, wpb=715.2, bsz=715.2, num_updates=13950, lr=0.000193, gnorm=4.579, clip=0, train_wall=6, gb_free=75.4, wall=1925 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:46:31]    INFO >> epoch 010:    167 / 1539 loss=3.676, wps=5963.4, ups=8.23, wpb=724.9, bsz=724.9, num_updates=14000, lr=0.000193, gnorm=4.094, clip=0, train_wall=6, gb_free=70.4, wall=1931 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:46:38]    INFO >> epoch 010:    217 / 1539 loss=3.587, wps=5347.4, ups=7.15, wpb=747.9, bsz=747.9, num_updates=14050, lr=0.000193, gnorm=4.524, clip=0, train_wall=6, gb_free=76.4, wall=1938 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:46:46]    INFO >> epoch 010:    267 / 1539 loss=3.638, wps=5831.2, ups=7.25, wpb=804.6, bsz=804.6, num_updates=14100, lr=0.000193, gnorm=3.898, clip=0, train_wall=6, gb_free=77, wall=1945 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:46:52]    INFO >> epoch 010:    317 / 1539 loss=3.682, wps=5578.1, ups=8.07, wpb=691.3, bsz=691.3, num_updates=14150, lr=0.000193, gnorm=4.186, clip=0, train_wall=6, gb_free=77.9, wall=1951 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:46:59]    INFO >> epoch 010:    367 / 1539 loss=3.69, wps=5756.5, ups=6.96, wpb=827.5, bsz=827.5, num_updates=14200, lr=0.000193, gnorm=4.229, clip=0, train_wall=6, gb_free=76.5, wall=1958 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:05]    INFO >> epoch 010:    417 / 1539 loss=3.789, wps=5070.2, ups=8.25, wpb=614.7, bsz=614.7, num_updates=14250, lr=0.000193, gnorm=3.986, clip=0, train_wall=6, gb_free=76.2, wall=1964 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:12]    INFO >> epoch 010:    467 / 1539 loss=3.635, wps=5366.6, ups=7.96, wpb=674.4, bsz=674.4, num_updates=14300, lr=0.000193, gnorm=3.947, clip=0, train_wall=6, gb_free=76.4, wall=1970 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:18]    INFO >> epoch 010:    517 / 1539 loss=3.665, wps=4767.3, ups=8.09, wpb=589, bsz=589, num_updates=14350, lr=0.000193, gnorm=4.043, clip=0, train_wall=6, gb_free=76.4, wall=1976 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:26]    INFO >> epoch 010:    567 / 1539 loss=3.666, wps=5542.2, ups=7.8, wpb=710.4, bsz=710.4, num_updates=14400, lr=0.000193, gnorm=4.024, clip=0, train_wall=6, gb_free=75.3, wall=1983 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:32]    INFO >> epoch 010:    617 / 1539 loss=3.717, wps=5431.7, ups=7.79, wpb=697, bsz=697, num_updates=14450, lr=0.000193, gnorm=3.762, clip=0, train_wall=6, gb_free=76.3, wall=1989 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:39]    INFO >> epoch 010:    667 / 1539 loss=3.623, wps=5244, ups=7.74, wpb=677.7, bsz=677.7, num_updates=14500, lr=0.000193, gnorm=4.24, clip=0, train_wall=6, gb_free=75.9, wall=1996 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:45]    INFO >> epoch 010:    717 / 1539 loss=3.412, wps=6556.5, ups=7.26, wpb=902.6, bsz=902.6, num_updates=14550, lr=0.000193, gnorm=4.185, clip=0, train_wall=6, gb_free=74.1, wall=2003 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:53]    INFO >> epoch 010:    767 / 1539 loss=3.638, wps=5861.5, ups=7.93, wpb=739, bsz=739, num_updates=14600, lr=0.000193, gnorm=4.225, clip=0, train_wall=6, gb_free=77.5, wall=2009 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:47:59]    INFO >> epoch 010:    817 / 1539 loss=3.613, wps=5579.7, ups=8.09, wpb=690.1, bsz=690.1, num_updates=14650, lr=0.000193, gnorm=4.502, clip=0, train_wall=6, gb_free=76, wall=2015 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:48:05]    INFO >> epoch 010:    867 / 1539 loss=3.678, wps=5118.7, ups=8.24, wpb=621.3, bsz=621.3, num_updates=14700, lr=0.000193, gnorm=4.059, clip=0, train_wall=6, gb_free=78.2, wall=2021 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:48:12]    INFO >> epoch 010:    917 / 1539 loss=3.542, wps=7104.3, ups=7.54, wpb=942.8, bsz=942.8, num_updates=14750, lr=0.000193, gnorm=4.589, clip=0, train_wall=6, gb_free=76.1, wall=2028 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:48:18]    INFO >> epoch 010:    967 / 1539 loss=3.761, wps=5350.9, ups=8.54, wpb=626.6, bsz=626.6, num_updates=14800, lr=0.000193, gnorm=3.801, clip=0, train_wall=5, gb_free=76.5, wall=2034 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:48:24]    INFO >> epoch 010:   1017 / 1539 loss=3.685, wps=5341.5, ups=8.17, wpb=653.6, bsz=653.6, num_updates=14850, lr=0.000193, gnorm=4.21, clip=0, train_wall=6, gb_free=76.9, wall=2040 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:48:30]    INFO >> epoch 010:   1067 / 1539 loss=3.685, wps=6361.3, ups=8.17, wpb=778.4, bsz=778.4, num_updates=14900, lr=0.000193, gnorm=4.163, clip=0, train_wall=6, gb_free=77.7, wall=2046 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:48:36]    INFO >> epoch 010:   1117 / 1539 loss=3.624, wps=5464, ups=7.92, wpb=689.7, bsz=689.7, num_updates=14950, lr=0.000193, gnorm=4.116, clip=0, train_wall=6, gb_free=74.7, wall=2052 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:48:43]    INFO >> epoch 010:   1167 / 1539 loss=3.636, wps=5194.5, ups=8.02, wpb=647.8, bsz=647.8, num_updates=15000, lr=0.000193, gnorm=4.471, clip=0, train_wall=6, gb_free=75.1, wall=2058 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:48:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.06 GiB is allocated by PyTorch, and 2.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:48:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:48:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:48:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77886 MiB |  77946 MiB | 256350 GiB | 256274 GiB |
|       from large pool |  77448 MiB |  77507 MiB | 253502 GiB | 253426 GiB |
|       from small pool |    438 MiB |    439 MiB |   2847 GiB |   2847 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77886 MiB |  77946 MiB | 256350 GiB | 256274 GiB |
|       from large pool |  77448 MiB |  77507 MiB | 253502 GiB | 253426 GiB |
|       from small pool |    438 MiB |    439 MiB |   2847 GiB |   2847 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77635 MiB |  77695 MiB | 255067 GiB | 254991 GiB |
|       from large pool |  77199 MiB |  77258 MiB | 252223 GiB | 252148 GiB |
|       from small pool |    436 MiB |    437 MiB |   2843 GiB |   2843 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80506 MiB |  80506 MiB | 253786 MiB | 173280 MiB |
|       from large pool |  80058 MiB |  80058 MiB | 251660 MiB | 171602 MiB |
|       from small pool |    448 MiB |    448 MiB |   2126 MiB |   1678 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2619 MiB |   4073 MiB | 192524 GiB | 192521 GiB |
|       from large pool |   2609 MiB |   4069 MiB | 189209 GiB | 189207 GiB |
|       from small pool |      9 MiB |     20 MiB |   3314 GiB |   3314 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8222    |    8223    |   32657 K  |   32648 K  |
|       from large pool |    1024    |    1025    |   14787 K  |   14786 K  |
|       from small pool |    7198    |    7200    |   17869 K  |   17862 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8222    |    8223    |   32657 K  |   32648 K  |
|       from large pool |    1024    |    1025    |   14787 K  |   14786 K  |
|       from small pool |    7198    |    7200    |   17869 K  |   17862 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     839    |     839    |    3884    |    3045    |
|       from large pool |     615    |     615    |    2821    |    2206    |
|       from small pool |     224    |     224    |    1063    |     839    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     347    |     348    |   19266 K  |   19265 K  |
|       from large pool |     159    |     159    |   10698 K  |   10697 K  |
|       from small pool |     188    |     189    |    8568 K  |    8567 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:48:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:48:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:48:50]    INFO >> epoch 010:   1218 / 1539 loss=3.599, wps=5949.8, ups=6.5, wpb=915.3, bsz=915.3, num_updates=15050, lr=0.000193, gnorm=4.42, clip=0, train_wall=6, gb_free=75.2, wall=2066 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:48:57]    INFO >> epoch 010:   1268 / 1539 loss=3.461, wps=5794.1, ups=7.46, wpb=776.5, bsz=776.5, num_updates=15100, lr=0.000193, gnorm=4.471, clip=0, train_wall=6, gb_free=76.3, wall=2073 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:49:06]    INFO >> epoch 010:   1318 / 1539 loss=3.733, wps=4902.8, ups=7.81, wpb=627.8, bsz=627.8, num_updates=15150, lr=0.000193, gnorm=4.136, clip=0, train_wall=6, gb_free=73.7, wall=2079 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:49:13]    INFO >> epoch 010:   1368 / 1539 loss=3.568, wps=5848.6, ups=7.65, wpb=764.9, bsz=764.9, num_updates=15200, lr=0.000193, gnorm=4.495, clip=0, train_wall=6, gb_free=77, wall=2086 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:49:19]    INFO >> epoch 010:   1418 / 1539 loss=3.592, wps=5555.9, ups=7.73, wpb=718.3, bsz=718.3, num_updates=15250, lr=0.000193, gnorm=4.243, clip=0, train_wall=6, gb_free=75.9, wall=2092 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:49:25]    INFO >> epoch 010:   1468 / 1539 loss=3.734, wps=5015.3, ups=8.2, wpb=611.6, bsz=611.6, num_updates=15300, lr=0.000193, gnorm=3.788, clip=0, train_wall=6, gb_free=77.4, wall=2098 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:49:27] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.53 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:49:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:49:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:49:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77283 MiB |  77343 MiB | 260961 GiB | 260886 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 258063 GiB | 257988 GiB |
|       from small pool |    343 MiB |    344 MiB |   2898 GiB |   2898 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77283 MiB |  77343 MiB | 260961 GiB | 260886 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 258063 GiB | 257988 GiB |
|       from small pool |    343 MiB |    344 MiB |   2898 GiB |   2898 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76965 MiB |  77024 MiB | 259654 GiB | 259579 GiB |
|       from large pool |  76624 MiB |  76683 MiB | 256760 GiB | 256685 GiB |
|       from small pool |    341 MiB |    342 MiB |   2894 GiB |   2893 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80506 MiB | 253848 MiB | 173350 MiB |
|       from large pool |  80118 MiB |  80118 MiB | 251720 MiB | 171602 MiB |
|       from small pool |    380 MiB |    448 MiB |   2128 MiB |   1748 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3154 MiB |   6329 MiB | 196141 GiB | 196138 GiB |
|       from large pool |   3118 MiB |   6296 MiB | 192767 GiB | 192764 GiB |
|       from small pool |     36 MiB |     37 MiB |   3374 GiB |   3374 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6537    |    6540    |   33259 K  |   33252 K  |
|       from large pool |     876    |     877    |   15071 K  |   15070 K  |
|       from small pool |    5661    |    5664    |   18187 K  |   18182 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6537    |    6540    |   33259 K  |   33252 K  |
|       from large pool |     876    |     877    |   15071 K  |   15070 K  |
|       from small pool |    5661    |    5664    |   18187 K  |   18182 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     806    |     839    |    3886    |    3080    |
|       from large pool |     616    |     616    |    2822    |    2206    |
|       from small pool |     190    |     224    |    1064    |     874    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     544    |     544    |   19618 K  |   19618 K  |
|       from large pool |     203    |     203    |   10902 K  |   10902 K  |
|       from small pool |     341    |     341    |    8716 K  |    8715 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:49:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:49:27] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:49:32]    INFO >> epoch 010:   1519 / 1539 loss=3.653, wps=5113.4, ups=7.06, wpb=724.3, bsz=724.3, num_updates=15350, lr=0.000193, gnorm=4.139, clip=0, train_wall=6, gb_free=76, wall=2105 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:49:36]    INFO >> epoch 010 | loss 3.634 | wps 5323.2 | ups 7.39 | wpb 720.8 | bsz 720.8 | num_updates 15370 | lr 0.000193 | gnorm 4.201 | clip 0 | train_wall 181 | gb_free 75.3 | wall 2108 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:49:36] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:49:46]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.708 | wps 16081.7 | wpb 5412.5 | bsz 5412.5 | num_updates 15370 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:49:46]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:49:46]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 10 @ 15370 updates, score 3.708) (writing took 0.009244 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-21 00:49:46] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:49:50]    INFO >> epoch 011:     30 / 1539 loss=3.65, wps=2230.2, ups=3.1, wpb=720, bsz=720, num_updates=15400, lr=0.000161, gnorm=4.145, clip=0, train_wall=6, gb_free=78.3, wall=2122 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:49:56]    INFO >> epoch 011:     80 / 1539 loss=3.72, wps=5281.1, ups=8.03, wpb=657.4, bsz=657.4, num_updates=15450, lr=0.000161, gnorm=4.163, clip=0, train_wall=6, gb_free=74.5, wall=2128 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:02]    INFO >> epoch 011:    130 / 1539 loss=3.676, wps=6096.4, ups=8.16, wpb=746.7, bsz=746.7, num_updates=15500, lr=0.000161, gnorm=4.3, clip=0, train_wall=6, gb_free=75.7, wall=2134 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:10]    INFO >> epoch 011:    180 / 1539 loss=3.637, wps=5672.7, ups=8.19, wpb=692.3, bsz=692.3, num_updates=15550, lr=0.000161, gnorm=3.638, clip=0, train_wall=6, gb_free=75.9, wall=2140 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:16]    INFO >> epoch 011:    230 / 1539 loss=3.56, wps=5979.8, ups=7.76, wpb=771, bsz=771, num_updates=15600, lr=0.000161, gnorm=4.336, clip=0, train_wall=6, gb_free=77, wall=2146 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:23]    INFO >> epoch 011:    280 / 1539 loss=3.666, wps=5304.8, ups=7.68, wpb=691.1, bsz=691.1, num_updates=15650, lr=0.000161, gnorm=4.06, clip=0, train_wall=6, gb_free=73.9, wall=2153 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:29]    INFO >> epoch 011:    330 / 1539 loss=3.551, wps=5627.7, ups=8.1, wpb=694.8, bsz=694.8, num_updates=15700, lr=0.000161, gnorm=4.246, clip=0, train_wall=6, gb_free=76.1, wall=2159 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:35]    INFO >> epoch 011:    380 / 1539 loss=3.603, wps=5394.2, ups=7.94, wpb=679.6, bsz=679.6, num_updates=15750, lr=0.000161, gnorm=4.064, clip=0, train_wall=6, gb_free=76.9, wall=2165 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:43]    INFO >> epoch 011:    430 / 1539 loss=3.698, wps=4986.3, ups=7.78, wpb=640.5, bsz=640.5, num_updates=15800, lr=0.000161, gnorm=4.012, clip=0, train_wall=6, gb_free=75.7, wall=2172 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:50]    INFO >> epoch 011:    480 / 1539 loss=3.72, wps=4643.4, ups=7.32, wpb=634, bsz=634, num_updates=15850, lr=0.000161, gnorm=4.047, clip=0, train_wall=6, gb_free=70.4, wall=2179 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:50:56]    INFO >> epoch 011:    530 / 1539 loss=3.857, wps=5005.6, ups=7.89, wpb=634.2, bsz=634.2, num_updates=15900, lr=0.000161, gnorm=3.69, clip=0, train_wall=6, gb_free=73.6, wall=2185 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:51:03]    INFO >> epoch 011:    580 / 1539 loss=3.395, wps=5894.1, ups=7.11, wpb=829.2, bsz=829.2, num_updates=15950, lr=0.000161, gnorm=4.414, clip=0, train_wall=6, gb_free=76.9, wall=2192 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:51:10]    INFO >> epoch 011:    630 / 1539 loss=3.528, wps=5967.5, ups=7.47, wpb=799, bsz=799, num_updates=16000, lr=0.000161, gnorm=4.251, clip=0, train_wall=6, gb_free=77.9, wall=2199 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:51:17]    INFO >> epoch 011:    680 / 1539 loss=3.803, wps=5741.6, ups=7.8, wpb=736.4, bsz=736.4, num_updates=16050, lr=0.000161, gnorm=3.812, clip=0, train_wall=6, gb_free=75.7, wall=2205 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:51:24]    INFO >> epoch 011:    730 / 1539 loss=3.591, wps=5022.6, ups=7.72, wpb=650.5, bsz=650.5, num_updates=16100, lr=0.000161, gnorm=3.791, clip=0, train_wall=6, gb_free=75.6, wall=2212 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:51:30]    INFO >> epoch 011:    780 / 1539 loss=3.679, wps=5055.7, ups=7.61, wpb=664.4, bsz=664.4, num_updates=16150, lr=0.000161, gnorm=3.963, clip=0, train_wall=6, gb_free=77.4, wall=2218 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:51:32] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 9.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.53 GiB is allocated by PyTorch, and 3.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:51:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:51:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:51:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77283 MiB |  77343 MiB | 276457 GiB | 276382 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 273381 GiB | 273306 GiB |
|       from small pool |    343 MiB |    344 MiB |   3075 GiB |   3075 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77283 MiB |  77343 MiB | 276457 GiB | 276382 GiB |
|       from large pool |  76939 MiB |  76999 MiB | 273381 GiB | 273306 GiB |
|       from small pool |    343 MiB |    344 MiB |   3075 GiB |   3075 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76965 MiB |  77024 MiB | 275075 GiB | 275000 GiB |
|       from large pool |  76624 MiB |  76683 MiB | 272004 GiB | 271929 GiB |
|       from small pool |    341 MiB |    342 MiB |   3071 GiB |   3070 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80498 MiB | 253964 MiB | 173466 MiB |
|       from large pool |  80118 MiB |  80118 MiB | 251780 MiB | 171662 MiB |
|       from small pool |    380 MiB |    434 MiB |   2184 MiB |   1804 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3154 MiB |   6329 MiB | 207163 GiB | 207159 GiB |
|       from large pool |   3118 MiB |   6296 MiB | 203586 GiB | 203583 GiB |
|       from small pool |     36 MiB |     37 MiB |   3576 GiB |   3576 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6537    |    6540    |   35231 K  |   35224 K  |
|       from large pool |     876    |     877    |   15927 K  |   15926 K  |
|       from small pool |    5661    |    5664    |   19304 K  |   19298 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6537    |    6540    |   35231 K  |   35224 K  |
|       from large pool |     876    |     877    |   15927 K  |   15926 K  |
|       from small pool |    5661    |    5664    |   19304 K  |   19298 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     806    |     832    |    3915    |    3109    |
|       from large pool |     616    |     616    |    2823    |    2207    |
|       from small pool |     190    |     217    |    1092    |     902    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     547    |     549    |   20802 K  |   20802 K  |
|       from large pool |     203    |     203    |   11523 K  |   11522 K  |
|       from small pool |     344    |     346    |    9279 K  |    9279 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:51:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:51:32] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:51:38]    INFO >> epoch 011:    831 / 1539 loss=3.642, wps=4380.5, ups=7.11, wpb=615.7, bsz=615.7, num_updates=16200, lr=0.000161, gnorm=3.84, clip=0, train_wall=6, gb_free=76.7, wall=2225 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:51:44]    INFO >> epoch 011:    881 / 1539 loss=3.65, wps=6020.9, ups=8.06, wpb=747.5, bsz=747.5, num_updates=16250, lr=0.000161, gnorm=4.203, clip=0, train_wall=6, gb_free=74.7, wall=2231 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:51:52]    INFO >> epoch 011:    931 / 1539 loss=3.58, wps=6178.5, ups=7.71, wpb=801.4, bsz=801.4, num_updates=16300, lr=0.000161, gnorm=4.006, clip=0, train_wall=6, gb_free=75.8, wall=2238 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:51:58]    INFO >> epoch 011:    981 / 1539 loss=3.608, wps=5477, ups=8.01, wpb=683.5, bsz=683.5, num_updates=16350, lr=0.000161, gnorm=3.938, clip=0, train_wall=6, gb_free=75, wall=2244 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:52:04]    INFO >> epoch 011:   1031 / 1539 loss=3.657, wps=5357.3, ups=7.61, wpb=703.9, bsz=703.9, num_updates=16400, lr=0.000161, gnorm=3.889, clip=0, train_wall=6, gb_free=75.1, wall=2251 (progress_bar.py:258, log())[0m
[33m[2025-11-21 00:52:11] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 2 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.00 GiB is allocated by PyTorch, and 2.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-21 00:52:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:52:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:52:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 33        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77826 MiB |  77886 MiB | 280967 GiB | 280891 GiB |
|       from large pool |  77388 MiB |  77448 MiB | 277841 GiB | 277765 GiB |
|       from small pool |    438 MiB |    439 MiB |   3125 GiB |   3125 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77826 MiB |  77886 MiB | 280967 GiB | 280891 GiB |
|       from large pool |  77388 MiB |  77448 MiB | 277841 GiB | 277765 GiB |
|       from small pool |    438 MiB |    439 MiB |   3125 GiB |   3125 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77575 MiB |  77635 MiB | 279562 GiB | 279486 GiB |
|       from large pool |  77139 MiB |  77199 MiB | 276441 GiB | 276365 GiB |
|       from small pool |    436 MiB |    437 MiB |   3121 GiB |   3120 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80506 MiB |  80506 MiB | 254032 MiB | 173526 MiB |
|       from large pool |  80058 MiB |  80058 MiB | 251780 MiB | 171722 MiB |
|       from small pool |    448 MiB |    448 MiB |   2252 MiB |   1804 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2679 MiB |   4073 MiB | 210652 GiB | 210649 GiB |
|       from large pool |   2669 MiB |   4069 MiB | 207015 GiB | 207013 GiB |
|       from small pool |      9 MiB |     20 MiB |   3636 GiB |   3636 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    8218    |    8219    |   35821 K  |   35813 K  |
|       from large pool |    1023    |    1024    |   16205 K  |   16204 K  |
|       from small pool |    7195    |    7196    |   19616 K  |   19609 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    8218    |    8219    |   35821 K  |   35813 K  |
|       from large pool |    1023    |    1024    |   16205 K  |   16204 K  |
|       from small pool |    7195    |    7196    |   19616 K  |   19609 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     839    |     839    |    3949    |    3110    |
|       from large pool |     615    |     615    |    2823    |    2208    |
|       from small pool |     224    |     224    |    1126    |     902    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     344    |     347    |   21150 K  |   21150 K  |
|       from large pool |     159    |     159    |   11724 K  |   11724 K  |
|       from small pool |     185    |     188    |    9425 K  |    9425 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:52:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-21 00:52:11] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-21 00:52:12]    INFO >> epoch 011:   1082 / 1539 loss=3.524, wps=5715.1, ups=7.01, wpb=814.9, bsz=814.9, num_updates=16450, lr=0.000161, gnorm=3.974, clip=0, train_wall=6, gb_free=76.2, wall=2258 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:52:18]    INFO >> epoch 011:   1132 / 1539 loss=3.238, wps=6099.5, ups=7.52, wpb=811.2, bsz=811.2, num_updates=16500, lr=0.000161, gnorm=4.337, clip=2, train_wall=6, gb_free=72.8, wall=2265 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:52:26]    INFO >> epoch 011:   1182 / 1539 loss=3.575, wps=5555.8, ups=7.7, wpb=721.4, bsz=721.4, num_updates=16550, lr=0.000161, gnorm=4.38, clip=0, train_wall=6, gb_free=76.2, wall=2271 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:52:33]    INFO >> epoch 011:   1232 / 1539 loss=3.582, wps=5233.6, ups=7.24, wpb=722.5, bsz=722.5, num_updates=16600, lr=0.000161, gnorm=3.974, clip=0, train_wall=6, gb_free=74.9, wall=2278 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:52:39]    INFO >> epoch 011:   1282 / 1539 loss=3.706, wps=6024.6, ups=7.66, wpb=786.2, bsz=786.2, num_updates=16650, lr=0.000161, gnorm=4.324, clip=0, train_wall=6, gb_free=76.1, wall=2284 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:52:47]    INFO >> epoch 011:   1332 / 1539 loss=3.717, wps=5215, ups=6.98, wpb=747, bsz=747, num_updates=16700, lr=0.000161, gnorm=3.885, clip=0, train_wall=6, gb_free=76.7, wall=2292 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:52:53]    INFO >> epoch 011:   1382 / 1539 loss=3.676, wps=5845.3, ups=7.76, wpb=753.1, bsz=753.1, num_updates=16750, lr=0.000161, gnorm=4.702, clip=0, train_wall=6, gb_free=76.7, wall=2298 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:53:01]    INFO >> epoch 011:   1432 / 1539 loss=3.727, wps=5490.5, ups=7.51, wpb=731.2, bsz=731.2, num_updates=16800, lr=0.000161, gnorm=4.019, clip=0, train_wall=6, gb_free=75.6, wall=2305 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:53:08]    INFO >> epoch 011:   1482 / 1539 loss=3.296, wps=5633.9, ups=7.22, wpb=780.7, bsz=780.7, num_updates=16850, lr=0.000161, gnorm=4.71, clip=2, train_wall=6, gb_free=76.4, wall=2312 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:53:15]    INFO >> epoch 011:   1532 / 1539 loss=3.616, wps=5705.9, ups=7.61, wpb=750.1, bsz=750.1, num_updates=16900, lr=0.000161, gnorm=3.913, clip=0, train_wall=6, gb_free=76.9, wall=2318 (progress_bar.py:258, log())[0m
[32m[2025-11-21 00:53:15]    INFO >> epoch 011 | loss 3.613 | wps 5249.4 | ups 7.28 | wpb 720.8 | bsz 720.8 | num_updates 16907 | lr 0.000161 | gnorm 4.09 | clip 0.1 | train_wall 182 | gb_free 76.8 | wall 2319 (progress_bar.py:267, print())[0m
[33m[2025-11-21 00:53:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-21 00:53:24]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.818 | wps 16817.8 | wpb 5412.5 | bsz 5412.5 | num_updates 16907 | best_loss 5.506 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:53:25]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:53:25]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_last.pt (epoch 11 @ 16907 updates, score 3.818) (writing took 0.012256 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-21 00:53:25]    INFO >> æ—©åœ: éªŒè¯æ€§èƒ½å·²10è½®æœªæå‡ (train_enhanced.py:616, single_main())[0m
[32m[2025-11-21 00:53:25]    INFO >> è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ 2252.0 ç§’ (train_enhanced.py:626, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-21 00:53:25]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/plots/training.png (train_enhanced.py:374, plot())[0m
[32m[2025-11-21 00:53:25]    INFO >> æ‰€æœ‰æ—¥å¿—å·²ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs (train_enhanced.py:631, single_main())[0m
[32m[2025-11-21 00:53:25]    INFO >> 
================================================================================ (train_enhanced.py:634, single_main())[0m
[32m[2025-11-21 00:53:25]    INFO >> å¼€å§‹æµ‹è¯•... (train_enhanced.py:635, single_main())[0m
[32m[2025-11-21 00:53:25]    INFO >> ================================================================================ (train_enhanced.py:636, single_main())[0m
[32m[2025-11-21 00:53:25]    INFO >> åŠ è½½æœ€ä½³checkpoint: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/checkpoint_best.pt (train_enhanced.py:50, run_test_after_training())[0m
[32m[2025-11-21 00:53:25]    INFO >> æµ‹è¯•é›†: test (train_enhanced.py:51, run_test_after_training())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/checkpoint_utils.py:212: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  state = torch.load(
[32m[2025-11-21 00:54:19]    INFO >> 
================================================================================ (train_enhanced.py:168, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> æµ‹è¯•ç»“æžœ: (train_enhanced.py:169, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> -------------------------------------------------------------------------------- (train_enhanced.py:170, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> å¹³å‡Loss:      3.9709 (train_enhanced.py:171, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> Acc@1:         18.18% (train_enhanced.py:172, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> Acc@5:         51.89% (train_enhanced.py:173, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> Acc@1 (å«any): 18.18% (train_enhanced.py:174, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> Acc@5 (å«any): 51.89% (train_enhanced.py:175, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> ================================================================================ (train_enhanced.py:176, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> æµ‹è¯•ç»“æžœå·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/checkpoints/res.txt (train_enhanced.py:187, run_test_after_training())[0m
[32m[2025-11-21 00:54:19]    INFO >> è®­ç»ƒæ—¥å¿—å·²æ›´æ–°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs (train_enhanced.py:222, run_test_after_training())[0m
[TrainingLogger] æ—¥å¿—ç›®å½•: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs
[TrainingLogger] åŽŸå§‹è¾“å‡ºå°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/training_output.log
[TrainingLogger] Epoch 1 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 2 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 3 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 4 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 5 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 6 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 7 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 8 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 9 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 10 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
[TrainingLogger] Epoch 11 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/embed_32/logs/metrics.json
