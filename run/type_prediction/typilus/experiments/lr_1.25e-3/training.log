[32m[2025-11-19 12:51:30]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/config.yml (train_enhanced.py:461, cli_main())[0m
[32m[2025-11-19 12:51:30]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:489, cli_main())[0m
[32m[2025-11-19 12:51:31]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/logs (train_enhanced.py:368, single_main())[0m
[32m[2025-11-19 12:51:31]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-19 12:51:31]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-19 12:51:31]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-19 12:51:38]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 64, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=64, out_features=64, bias=False)
      (2): Dropout(p=0.1, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(64, 64)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(128, 64)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=False)
      (1): Dropout(p=0.1, inplace=False)
      (2): Linear(in_features=64, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:375, single_main())[0m
[32m[2025-11-19 12:51:38]    INFO >> æ¨¡åž‹: typilus, æŸå¤±å‡½æ•°: TypilusCriterion (train_enhanced.py:376, single_main())[0m
[32m[2025-11-19 12:51:38]    INFO >> æ¨¡åž‹å‚æ•°: 847843 (å¯è®­ç»ƒ: 847843) (train_enhanced.py:377, single_main())[0m
[32m[2025-11-19 12:51:38]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-19 12:51:38]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80579 MB ; used memory = 1340 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-19 12:51:38]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-19 12:51:38]    INFO >> ä½¿ç”¨ 1 ä¸ªGPUè®­ç»ƒ (train_enhanced.py:383, single_main())[0m
[32m[2025-11-19 12:51:38]    INFO >> no existing checkpoint found /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-19 12:51:38]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-19 12:52:38]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-19 12:52:38] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-19 12:52:45]    INFO >> epoch 001:     50 / 1539 loss=5.829, wps=5269.8, ups=7.29, wpb=720, bsz=720, num_updates=50, lr=0.00125, gnorm=7.9, clip=0, train_wall=7, gb_free=74.2, wall=64 (progress_bar.py:258, log())[0m
[33m[2025-11-19 12:52:46] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 73.64 GiB is allocated by PyTorch, and 4.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 12:52:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:52:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:52:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:52:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75347 MiB |  75407 MiB |   1756 GiB |   1682 GiB |
|       from large pool |  74991 MiB |  75051 MiB |   1743 GiB |   1670 GiB |
|       from small pool |    356 MiB |    357 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75347 MiB |  75407 MiB |   1756 GiB |   1682 GiB |
|       from large pool |  74991 MiB |  75051 MiB |   1743 GiB |   1670 GiB |
|       from small pool |    356 MiB |    357 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75195 MiB |  75254 MiB |   1750 GiB |   1676 GiB |
|       from large pool |  74841 MiB |  74900 MiB |   1737 GiB |   1664 GiB |
|       from small pool |    354 MiB |    355 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80498 MiB |  91994 MiB |  11532 MiB |
|       from large pool |  80070 MiB |  80136 MiB |  91590 MiB |  11520 MiB |
|       from small pool |    392 MiB |    394 MiB |    404 MiB |     12 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5054 MiB |   5960 MiB |    875 GiB |    870 GiB |
|       from large pool |   5018 MiB |   5948 MiB |    859 GiB |    855 GiB |
|       from small pool |     35 MiB |     37 MiB |     15 GiB |     14 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6579    |    6582    |  136626    |  130047    |
|       from large pool |     868    |     869    |   57719    |   56851    |
|       from small pool |    5711    |    5714    |   78907    |   73196    |
|---------------------------------------------------------------------------|
| Active allocs         |    6579    |    6582    |  136626    |  130047    |
|       from large pool |     868    |     869    |   57719    |   56851    |
|       from small pool |    5711    |    5714    |   78907    |   73196    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     758    |     793    |    1006    |     248    |
|       from large pool |     562    |     612    |     804    |     242    |
|       from small pool |     196    |     197    |     202    |       6    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     654    |     656    |   86039    |   85385    |
|       from large pool |     306    |     306    |   44256    |   43950    |
|       from small pool |     348    |     350    |   41783    |   41435    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:52:46] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 12:52:52]    INFO >> epoch 001:    101 / 1539 loss=6.145, wps=4696.5, ups=7.69, wpb=610.7, bsz=610.7, num_updates=100, lr=0.00125, gnorm=7.555, clip=0, train_wall=5, gb_free=75.6, wall=71 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:52:59]    INFO >> epoch 001:    151 / 1539 loss=5.958, wps=5718.4, ups=7, wpb=816.7, bsz=816.7, num_updates=150, lr=0.00125, gnorm=8.021, clip=0, train_wall=7, gb_free=74.2, wall=78 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:53:05]    INFO >> epoch 001:    201 / 1539 loss=5.844, wps=5236.8, ups=8.16, wpb=641.7, bsz=641.7, num_updates=200, lr=0.00125, gnorm=7.145, clip=0, train_wall=6, gb_free=74.8, wall=84 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:53:13]    INFO >> epoch 001:    251 / 1539 loss=5.745, wps=5002.9, ups=7.84, wpb=637.9, bsz=637.9, num_updates=250, lr=0.00125, gnorm=6.727, clip=0, train_wall=6, gb_free=71.5, wall=90 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:53:20]    INFO >> epoch 001:    301 / 1539 loss=5.551, wps=5443.2, ups=6.92, wpb=786.5, bsz=786.5, num_updates=300, lr=0.00125, gnorm=7.164, clip=0, train_wall=7, gb_free=73.8, wall=97 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:53:26]    INFO >> epoch 001:    351 / 1539 loss=5.545, wps=5211.4, ups=7.76, wpb=671.6, bsz=671.6, num_updates=350, lr=0.00125, gnorm=7.386, clip=0, train_wall=6, gb_free=72.4, wall=104 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:53:33]    INFO >> epoch 001:    401 / 1539 loss=5.147, wps=5965.7, ups=7, wpb=851.7, bsz=851.7, num_updates=400, lr=0.00125, gnorm=7.732, clip=4, train_wall=7, gb_free=73.2, wall=111 (progress_bar.py:258, log())[0m
[33m[2025-11-19 12:53:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.81 GiB is allocated by PyTorch, and 823.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 12:53:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:53:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:53:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:53:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79614 MiB |  79674 MiB |  12419 GiB |  12342 GiB |
|       from large pool |  79520 MiB |  79580 MiB |  12346 GiB |  12268 GiB |
|       from small pool |     94 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79614 MiB |  79674 MiB |  12419 GiB |  12342 GiB |
|       from large pool |  79520 MiB |  79580 MiB |  12346 GiB |  12268 GiB |
|       from small pool |     94 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79544 MiB |  79604 MiB |  12379 GiB |  12302 GiB |
|       from large pool |  79450 MiB |  79510 MiB |  12306 GiB |  12228 GiB |
|       from small pool |     93 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 166722 MiB |  86224 MiB |
|       from large pool |  80400 MiB |  80400 MiB | 166242 MiB |  85842 MiB |
|       from small pool |     98 MiB |    392 MiB |    480 MiB |    382 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    823 MiB |   3119 MiB |   6143 GiB |   6143 GiB |
|       from large pool |    819 MiB |   3112 MiB |   6057 GiB |   6057 GiB |
|       from small pool |      3 MiB |     23 MiB |     85 GiB |     85 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2036    |    2039    |     869 K  |     867 K  |
|       from large pool |     477    |     478    |     421 K  |     421 K  |
|       from small pool |    1559    |    1562    |     447 K  |     445 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2036    |    2039    |     869 K  |     867 K  |
|       from large pool |     477    |     478    |     421 K  |     421 K  |
|       from small pool |    1559    |    1562    |     447 K  |     445 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     251    |     757    |    1201    |     950    |
|       from large pool |     202    |     561    |     961    |     759    |
|       from small pool |      49    |     196    |     240    |     191    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     132    |     133    |  540020    |  539888    |
|       from large pool |      79    |      81    |  324754    |  324675    |
|       from small pool |      53    |      55    |  215266    |  215213    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:53:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 12:53:48]    INFO >> epoch 001:    452 / 1539 loss=5.028, wps=2470.3, ups=3.91, wpb=631.6, bsz=631.6, num_updates=450, lr=0.00125, gnorm=7.023, clip=0, train_wall=6, gb_free=71.8, wall=124 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:53:57]    INFO >> epoch 001:    502 / 1539 loss=4.781, wps=3993.7, ups=5.37, wpb=743.1, bsz=743.1, num_updates=500, lr=0.00125, gnorm=7.7, clip=0, train_wall=9, gb_free=72.6, wall=133 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:54:04]    INFO >> epoch 001:    552 / 1539 loss=4.479, wps=4921.2, ups=7.49, wpb=657, bsz=657, num_updates=550, lr=0.00125, gnorm=6.186, clip=0, train_wall=6, gb_free=65.5, wall=140 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:54:10]    INFO >> epoch 001:    602 / 1539 loss=4.384, wps=5005.7, ups=7.49, wpb=668.6, bsz=668.6, num_updates=600, lr=0.00125, gnorm=7.66, clip=2, train_wall=6, gb_free=73.2, wall=146 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:54:19]    INFO >> epoch 001:    652 / 1539 loss=4.29, wps=5025.8, ups=7.06, wpb=712.2, bsz=712.2, num_updates=650, lr=0.00125, gnorm=6.132, clip=0, train_wall=6, gb_free=73.5, wall=154 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:54:26]    INFO >> epoch 001:    702 / 1539 loss=4.043, wps=4425.4, ups=6.58, wpb=672.5, bsz=672.5, num_updates=700, lr=0.00125, gnorm=6.913, clip=0, train_wall=7, gb_free=74.1, wall=161 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:54:33]    INFO >> epoch 001:    752 / 1539 loss=4.039, wps=5407.8, ups=7.14, wpb=757.4, bsz=757.4, num_updates=750, lr=0.00125, gnorm=6.695, clip=0, train_wall=6, gb_free=73.7, wall=168 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:54:40]    INFO >> epoch 001:    802 / 1539 loss=4.285, wps=5510.7, ups=7.6, wpb=725.2, bsz=725.2, num_updates=800, lr=0.00125, gnorm=6.937, clip=2, train_wall=6, gb_free=73.4, wall=175 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:54:47]    INFO >> epoch 001:    852 / 1539 loss=4.185, wps=4669.3, ups=7.28, wpb=641.1, bsz=641.1, num_updates=850, lr=0.00125, gnorm=5.842, clip=0, train_wall=6, gb_free=71.8, wall=182 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:54:53]    INFO >> epoch 001:    902 / 1539 loss=4.069, wps=5014.6, ups=7.6, wpb=660.1, bsz=660.1, num_updates=900, lr=0.00125, gnorm=5.896, clip=0, train_wall=6, gb_free=72.1, wall=188 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:55:00]    INFO >> epoch 001:    952 / 1539 loss=4.085, wps=5297.4, ups=7.43, wpb=713.2, bsz=713.2, num_updates=950, lr=0.00125, gnorm=6.129, clip=0, train_wall=6, gb_free=71.8, wall=195 (progress_bar.py:258, log())[0m
[33m[2025-11-19 12:55:03] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 3 has a total capacity of 79.14 GiB of which 901.25 MiB is free. Including non-PyTorch memory, this process has 78.24 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 12:55:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:55:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:55:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:55:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76322 MiB |  78597 MiB |  28415 GiB |  28340 GiB |
|       from large pool |  76303 MiB |  78579 MiB |  28261 GiB |  28187 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76322 MiB |  78597 MiB |  28415 GiB |  28340 GiB |
|       from large pool |  76303 MiB |  78579 MiB |  28261 GiB |  28187 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB |  28342 GiB |  28268 GiB |
|       from large pool |  76285 MiB |  78562 MiB |  28189 GiB |  28114 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79604 MiB |  79604 MiB | 305536 MiB | 225932 MiB |
|       from large pool |  79580 MiB |  79580 MiB | 304982 MiB | 225402 MiB |
|       from small pool |     24 MiB |     98 MiB |    554 MiB |    530 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3281 MiB |   4789 MiB |  24274 GiB |  24271 GiB |
|       from large pool |   3276 MiB |   4783 MiB |  24097 GiB |  24094 GiB |
|       from small pool |      5 MiB |     27 MiB |    177 GiB |    177 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     638    |     644    |    1881 K  |    1880 K  |
|       from large pool |     340    |     346    |     949 K  |     948 K  |
|       from small pool |     298    |     354    |     931 K  |     931 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     638    |     644    |    1881 K  |    1880 K  |
|       from large pool |     340    |     346    |     949 K  |     948 K  |
|       from small pool |     298    |     354    |     931 K  |     931 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     123    |    1327    |    1231    |
|       from large pool |      84    |      84    |    1050    |     966    |
|       from small pool |      12    |      49    |     277    |     265    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     115    |     117    |    1087 K  |    1087 K  |
|       from large pool |      93    |      95    |     661 K  |     661 K  |
|       from small pool |      22    |      56    |     426 K  |     426 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:55:03] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 12:55:09]    INFO >> epoch 001:   1003 / 1539 loss=3.984, wps=3713.6, ups=5.72, wpb=649.4, bsz=649.4, num_updates=1000, lr=0.00125, gnorm=5.639, clip=0, train_wall=6, gb_free=72.1, wall=204 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:55:16]    INFO >> epoch 001:   1053 / 1539 loss=3.936, wps=5792.4, ups=7.04, wpb=822.6, bsz=822.6, num_updates=1050, lr=0.00125, gnorm=6.588, clip=0, train_wall=7, gb_free=68, wall=211 (progress_bar.py:258, log())[0m
[33m[2025-11-19 12:55:26] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 3 has a total capacity of 79.14 GiB of which 197.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 12:55:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:55:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:55:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:55:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 9         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76955 MiB |  77477 MiB |  32758 GiB |  32683 GiB |
|       from large pool |  76942 MiB |  77464 MiB |  32579 GiB |  32504 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76955 MiB |  77477 MiB |  32758 GiB |  32683 GiB |
|       from large pool |  76942 MiB |  77464 MiB |  32579 GiB |  32504 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB |  32677 GiB |  32602 GiB |
|       from large pool |  76923 MiB |  77445 MiB |  32498 GiB |  32423 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80308 MiB |  80308 MiB | 310972 MiB | 230664 MiB |
|       from large pool |  80284 MiB |  80284 MiB | 310232 MiB | 229948 MiB |
|       from small pool |     24 MiB |    210 MiB |    740 MiB |    716 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3352 MiB |   9143 MiB |  29231 GiB |  29228 GiB |
|       from large pool |   3341 MiB |   9131 MiB |  29025 GiB |  29022 GiB |
|       from small pool |     11 MiB |     23 MiB |    206 GiB |    206 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     604    |     611    |    2166 K  |    2165 K  |
|       from large pool |     315    |     322    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1080 K  |    1079 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     604    |     611    |    2166 K  |    2165 K  |
|       from large pool |     315    |     322    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1080 K  |    1079 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     189    |    1425    |    1337    |
|       from large pool |      76    |      84    |    1055    |     979    |
|       from small pool |      12    |     105    |     370    |     358    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      84    |      84    |    1245 K  |    1245 K  |
|       from large pool |      60    |      60    |     746 K  |     746 K  |
|       from small pool |      24    |      53    |     498 K  |     498 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:55:26] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 12:55:27]    INFO >> epoch 001:   1104 / 1539 loss=4.088, wps=5643.3, ups=6.07, wpb=929.6, bsz=929.6, num_updates=1100, lr=0.00125, gnorm=7.484, clip=0, train_wall=7, gb_free=72.8, wall=219 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:55:33]    INFO >> epoch 001:   1154 / 1539 loss=4.011, wps=5219.4, ups=7.54, wpb=692.6, bsz=692.6, num_updates=1150, lr=0.00125, gnorm=6.244, clip=0, train_wall=6, gb_free=73.1, wall=226 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:55:40]    INFO >> epoch 001:   1204 / 1539 loss=3.901, wps=5205.9, ups=7.67, wpb=678.4, bsz=678.4, num_updates=1200, lr=0.00125, gnorm=5.843, clip=0, train_wall=6, gb_free=71.2, wall=232 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:55:47]    INFO >> epoch 001:   1254 / 1539 loss=3.996, wps=5448.3, ups=7.45, wpb=730.9, bsz=730.9, num_updates=1250, lr=0.00125, gnorm=6.904, clip=0, train_wall=6, gb_free=71.3, wall=239 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:55:54]    INFO >> epoch 001:   1304 / 1539 loss=3.887, wps=5256.6, ups=7.15, wpb=735.4, bsz=735.4, num_updates=1300, lr=0.00125, gnorm=5.829, clip=0, train_wall=6, gb_free=74.3, wall=246 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:56:02]    INFO >> epoch 001:   1354 / 1539 loss=4.096, wps=4855.2, ups=7.39, wpb=657.2, bsz=657.2, num_updates=1350, lr=0.00125, gnorm=6.029, clip=0, train_wall=6, gb_free=73.4, wall=253 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:56:09]    INFO >> epoch 001:   1404 / 1539 loss=3.868, wps=5104.1, ups=7.16, wpb=712.5, bsz=712.5, num_updates=1400, lr=0.00125, gnorm=6.692, clip=2, train_wall=6, gb_free=73.3, wall=260 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:56:15]    INFO >> epoch 001:   1454 / 1539 loss=3.976, wps=5252.6, ups=7.48, wpb=702.2, bsz=702.2, num_updates=1450, lr=0.00125, gnorm=5.935, clip=0, train_wall=6, gb_free=71.6, wall=266 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:56:22]    INFO >> epoch 001:   1504 / 1539 loss=3.877, wps=5076.9, ups=7.3, wpb=695.9, bsz=695.9, num_updates=1500, lr=0.00125, gnorm=6.203, clip=0, train_wall=6, gb_free=70.8, wall=273 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:56:27]    INFO >> epoch 001 | loss 4.543 | wps 4958.5 | ups 6.96 | wpb 712.7 | bsz 712.7 | num_updates 1535 | lr 0.00125 | gnorm 6.77 | clip 0.4 | train_wall 193 | gb_free 76.6 | wall 278 (progress_bar.py:267, print())[0m
[33m[2025-11-19 12:56:27] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 12:56:41]    INFO >> epoch 001 | valid on 'valid' subset | loss 3.886 | wps 12365.5 | wpb 5412.5 | bsz 5412.5 | num_updates 1535 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:177: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:177: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:177: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:177: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:177: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:177: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:177: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:177: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:177: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:177: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:177: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:178: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:178: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:178: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:178: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:178: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:178: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:178: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:178: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:178: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:178: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:178: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-19 12:56:41]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/logs/plots/training.png (train_enhanced.py:181, plot())[0m
[32m[2025-11-19 12:56:41]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/checkpoints/checkpoint_best.pt (epoch 1 @ 1535 updates, score 3.886) (writing took 0.015568 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 12:56:41] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-19 12:56:43]    INFO >> epoch 002:     15 / 1539 loss=3.837, wps=1886.4, ups=2.58, wpb=731.3, bsz=731.3, num_updates=1550, lr=0.00125, gnorm=7.142, clip=2, train_wall=6, gb_free=74.4, wall=292 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:56:49]    INFO >> epoch 002:     65 / 1539 loss=3.875, wps=5255.2, ups=7.98, wpb=658.8, bsz=658.8, num_updates=1600, lr=0.00125, gnorm=5.485, clip=0, train_wall=6, gb_free=73.4, wall=299 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:56:56]    INFO >> epoch 002:    115 / 1539 loss=3.841, wps=5255.6, ups=7.35, wpb=714.6, bsz=714.6, num_updates=1650, lr=0.00125, gnorm=5.393, clip=0, train_wall=6, gb_free=65.7, wall=306 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:57:04]    INFO >> epoch 002:    165 / 1539 loss=3.66, wps=5460.5, ups=7.39, wpb=738.8, bsz=738.8, num_updates=1700, lr=0.00125, gnorm=5.544, clip=0, train_wall=6, gb_free=73.6, wall=312 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:57:11]    INFO >> epoch 002:    215 / 1539 loss=3.837, wps=5315.1, ups=7.55, wpb=704.2, bsz=704.2, num_updates=1750, lr=0.00125, gnorm=5.544, clip=2, train_wall=6, gb_free=71.2, wall=319 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:57:18]    INFO >> epoch 002:    265 / 1539 loss=3.755, wps=5827.7, ups=6.67, wpb=873.5, bsz=873.5, num_updates=1800, lr=0.00125, gnorm=5.182, clip=0, train_wall=7, gb_free=74.7, wall=326 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:57:25]    INFO >> epoch 002:    315 / 1539 loss=3.825, wps=4850.5, ups=7.52, wpb=645.1, bsz=645.1, num_updates=1850, lr=0.00125, gnorm=4.82, clip=0, train_wall=6, gb_free=71.9, wall=333 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:57:32]    INFO >> epoch 002:    365 / 1539 loss=3.838, wps=4854.3, ups=7.4, wpb=655.8, bsz=655.8, num_updates=1900, lr=0.00125, gnorm=5.664, clip=0, train_wall=6, gb_free=74, wall=340 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:57:40]    INFO >> epoch 002:    415 / 1539 loss=3.678, wps=5096.4, ups=7.15, wpb=713.2, bsz=713.2, num_updates=1950, lr=0.00125, gnorm=5.264, clip=0, train_wall=7, gb_free=76.2, wall=347 (progress_bar.py:258, log())[0m
[33m[2025-11-19 12:57:41] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 3 has a total capacity of 79.14 GiB of which 197.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 3.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 12:57:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:57:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:57:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:57:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76327 MiB |  78599 MiB |  61960 GiB |  61886 GiB |
|       from large pool |  76308 MiB |  78581 MiB |  61612 GiB |  61537 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76327 MiB |  78599 MiB |  61960 GiB |  61886 GiB |
|       from large pool |  76308 MiB |  78581 MiB |  61612 GiB |  61537 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB |  61824 GiB |  61750 GiB |
|       from large pool |  76285 MiB |  78562 MiB |  61476 GiB |  61402 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80308 MiB |  80494 MiB | 311158 MiB | 230850 MiB |
|       from large pool |  80284 MiB |  80284 MiB | 310232 MiB | 229948 MiB |
|       from small pool |     24 MiB |    210 MiB |    926 MiB |    902 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3980 MiB |   5490 MiB |  61023 GiB |  61019 GiB |
|       from large pool |   3975 MiB |   5484 MiB |  60627 GiB |  60623 GiB |
|       from small pool |      5 MiB |     27 MiB |    395 GiB |    395 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |    4090 K  |    4090 K  |
|       from large pool |     340    |     346    |    1969 K  |    1969 K  |
|       from small pool |     300    |     355    |    2121 K  |    2120 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |    4090 K  |    4090 K  |
|       from large pool |     340    |     346    |    1969 K  |    1969 K  |
|       from small pool |     300    |     355    |    2121 K  |    2120 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     181    |    1518    |    1430    |
|       from large pool |      76    |      76    |    1055    |     979    |
|       from small pool |      12    |     105    |     463    |     451    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      93    |      99    |    2309 K  |    2309 K  |
|       from large pool |      70    |      76    |    1305 K  |    1305 K  |
|       from small pool |      23    |      51    |    1004 K  |    1004 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:57:41] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 12:57:47]    INFO >> epoch 002:    466 / 1539 loss=3.782, wps=4890.6, ups=6.7, wpb=729.4, bsz=729.4, num_updates=2000, lr=0.00125, gnorm=5.209, clip=0, train_wall=6, gb_free=71.6, wall=354 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:57:54]    INFO >> epoch 002:    516 / 1539 loss=3.812, wps=5103.3, ups=7.2, wpb=708.4, bsz=708.4, num_updates=2050, lr=0.00125, gnorm=6.139, clip=0, train_wall=7, gb_free=71.2, wall=361 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:58:01]    INFO >> epoch 002:    566 / 1539 loss=3.868, wps=4665.6, ups=7.5, wpb=622.3, bsz=622.3, num_updates=2100, lr=0.00125, gnorm=5.274, clip=0, train_wall=6, gb_free=74, wall=368 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:58:08]    INFO >> epoch 002:    616 / 1539 loss=3.68, wps=6016.4, ups=6.91, wpb=871.1, bsz=871.1, num_updates=2150, lr=0.00125, gnorm=5.39, clip=0, train_wall=7, gb_free=68.6, wall=375 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:58:17]    INFO >> epoch 002:    666 / 1539 loss=3.546, wps=5530.5, ups=7.16, wpb=772.9, bsz=772.9, num_updates=2200, lr=0.00125, gnorm=5.716, clip=2, train_wall=7, gb_free=68.4, wall=382 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:58:23]    INFO >> epoch 002:    716 / 1539 loss=3.866, wps=5214.1, ups=7.36, wpb=708.6, bsz=708.6, num_updates=2250, lr=0.00125, gnorm=4.961, clip=0, train_wall=6, gb_free=73.1, wall=389 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:58:30]    INFO >> epoch 002:    766 / 1539 loss=3.708, wps=4916.4, ups=7.39, wpb=665.2, bsz=665.2, num_updates=2300, lr=0.00125, gnorm=4.588, clip=0, train_wall=6, gb_free=75.7, wall=396 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:58:37]    INFO >> epoch 002:    816 / 1539 loss=3.863, wps=4833.4, ups=7.37, wpb=656, bsz=656, num_updates=2350, lr=0.00125, gnorm=5.216, clip=0, train_wall=6, gb_free=72.2, wall=402 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:58:45]    INFO >> epoch 002:    866 / 1539 loss=3.828, wps=5070.3, ups=7.04, wpb=719.8, bsz=719.8, num_updates=2400, lr=0.00125, gnorm=5.208, clip=0, train_wall=7, gb_free=75.7, wall=410 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:58:52]    INFO >> epoch 002:    916 / 1539 loss=3.715, wps=4787.2, ups=7.13, wpb=671.5, bsz=671.5, num_updates=2450, lr=0.00125, gnorm=4.66, clip=0, train_wall=7, gb_free=73.1, wall=417 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:58:59]    INFO >> epoch 002:    966 / 1539 loss=3.681, wps=4713.8, ups=7.39, wpb=637.9, bsz=637.9, num_updates=2500, lr=0.00125, gnorm=4.722, clip=0, train_wall=6, gb_free=69.2, wall=423 (progress_bar.py:258, log())[0m
[33m[2025-11-19 12:59:00] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 37.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.54 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 12:59:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:59:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:59:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:59:00] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79337 MiB |  79397 MiB |  78153 GiB |  78075 GiB |
|       from large pool |  78941 MiB |  79001 MiB |  77714 GiB |  77637 GiB |
|       from small pool |    395 MiB |    396 MiB |    438 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79337 MiB |  79397 MiB |  78153 GiB |  78075 GiB |
|       from large pool |  78941 MiB |  79001 MiB |  77714 GiB |  77637 GiB |
|       from small pool |    395 MiB |    396 MiB |    438 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79098 MiB |  79157 MiB |  77983 GiB |  77905 GiB |
|       from large pool |  78705 MiB |  78764 MiB |  77545 GiB |  77468 GiB |
|       from small pool |    393 MiB |    394 MiB |    437 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80468 MiB |  80470 MiB | 335872 MiB | 255404 MiB |
|       from large pool |  80032 MiB |  80032 MiB | 334532 MiB | 254500 MiB |
|       from small pool |    436 MiB |    438 MiB |   1340 MiB |    904 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1070 MiB |   4610 MiB |  79115 GiB |  79114 GiB |
|       from large pool |   1030 MiB |   4603 MiB |  78616 GiB |  78615 GiB |
|       from small pool |     40 MiB |     41 MiB |    498 GiB |    498 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7296    |    7299    |    5179 K  |    5172 K  |
|       from large pool |     933    |     934    |    2516 K  |    2515 K  |
|       from small pool |    6363    |    6366    |    2663 K  |    2656 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7296    |    7299    |    5179 K  |    5172 K  |
|       from large pool |     933    |     934    |    2516 K  |    2515 K  |
|       from small pool |    6363    |    6366    |    2663 K  |    2656 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     690    |     691    |    2130    |    1440    |
|       from large pool |     472    |     472    |    1460    |     988    |
|       from small pool |     218    |     219    |     670    |     452    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     454    |     454    |    2903 K  |    2902 K  |
|       from large pool |      63    |      63    |    1651 K  |    1651 K  |
|       from small pool |     391    |     391    |    1251 K  |    1251 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:59:00] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-19 12:59:05] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.95 GiB is allocated by PyTorch, and 1.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 12:59:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:59:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:59:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:59:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 15        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78678 MiB |  78798 MiB |  79117 GiB |  79040 GiB |
|       from large pool |  78593 MiB |  78713 MiB |  78673 GiB |  78596 GiB |
|       from small pool |     85 MiB |     86 MiB |    443 GiB |    443 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78678 MiB |  78798 MiB |  79117 GiB |  79040 GiB |
|       from large pool |  78593 MiB |  78713 MiB |  78673 GiB |  78596 GiB |
|       from small pool |     85 MiB |     86 MiB |    443 GiB |    443 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78642 MiB |  78762 MiB |  78944 GiB |  78868 GiB |
|       from large pool |  78558 MiB |  78677 MiB |  78501 GiB |  78425 GiB |
|       from small pool |     84 MiB |     85 MiB |    443 GiB |    442 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB | 356340 MiB | 275836 MiB |
|       from large pool |  80416 MiB |  80416 MiB | 354934 MiB | 274518 MiB |
|       from small pool |     88 MiB |    436 MiB |   1406 MiB |   1318 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1705 MiB |   8357 MiB |  79995 GiB |  79994 GiB |
|       from large pool |   1702 MiB |   8349 MiB |  79489 GiB |  79488 GiB |
|       from small pool |      2 MiB |     29 MiB |    505 GiB |    505 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1873    |    1876    |    5242 K  |    5240 K  |
|       from large pool |     462    |     464    |    2546 K  |    2545 K  |
|       from small pool |    1411    |    1414    |    2696 K  |    2694 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1873    |    1876    |    5242 K  |    5240 K  |
|       from large pool |     462    |     464    |    2546 K  |    2545 K  |
|       from small pool |    1411    |    1414    |    2696 K  |    2694 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     221    |     689    |    2198    |    1977    |
|       from large pool |     177    |     471    |    1495    |    1318    |
|       from small pool |      44    |     218    |     703    |     659    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     167    |     168    |    2940 K  |    2940 K  |
|       from large pool |     120    |     125    |    1671 K  |    1671 K  |
|       from small pool |      47    |      59    |    1269 K  |    1269 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:59:05] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 12:59:07]    INFO >> epoch 002:   1018 / 1539 loss=3.745, wps=4294.3, ups=6.24, wpb=687.9, bsz=687.9, num_updates=2550, lr=0.00125, gnorm=5.165, clip=0, train_wall=6, gb_free=72.9, wall=431 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:59:14]    INFO >> epoch 002:   1068 / 1539 loss=3.613, wps=5360.6, ups=7.02, wpb=763.7, bsz=763.7, num_updates=2600, lr=0.00125, gnorm=4.577, clip=0, train_wall=7, gb_free=73.5, wall=438 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:59:23]    INFO >> epoch 002:   1118 / 1539 loss=3.344, wps=5490.3, ups=6.93, wpb=791.9, bsz=791.9, num_updates=2650, lr=0.00125, gnorm=5.251, clip=0, train_wall=7, gb_free=69.2, wall=446 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:59:30]    INFO >> epoch 002:   1168 / 1539 loss=3.778, wps=5520.5, ups=7.17, wpb=770.1, bsz=770.1, num_updates=2700, lr=0.00125, gnorm=4.767, clip=0, train_wall=7, gb_free=72.3, wall=453 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:59:37]    INFO >> epoch 002:   1218 / 1539 loss=3.684, wps=5043.3, ups=7.1, wpb=710, bsz=710, num_updates=2750, lr=0.00125, gnorm=4.523, clip=0, train_wall=7, gb_free=70.9, wall=460 (progress_bar.py:258, log())[0m
[33m[2025-11-19 12:59:40] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 121.25 MiB is free. Including non-PyTorch memory, this process has 79.00 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 5.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 12:59:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:59:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:59:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:59:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72821 MiB |  75425 MiB |  86354 GiB |  86283 GiB |
|       from large pool |  72808 MiB |  75412 MiB |  85870 GiB |  85799 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72821 MiB |  75425 MiB |  86354 GiB |  86283 GiB |
|       from large pool |  72808 MiB |  75412 MiB |  85870 GiB |  85799 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB |  86167 GiB |  86096 GiB |
|       from large pool |  72788 MiB |  75391 MiB |  85684 GiB |  85613 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80384 MiB |  80384 MiB | 387082 MiB | 306698 MiB |
|       from large pool |  80360 MiB |  80360 MiB | 385538 MiB | 305178 MiB |
|       from small pool |     24 MiB |    226 MiB |   1544 MiB |   1520 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5660 MiB |  11732 MiB |  87437 GiB |  87432 GiB |
|       from large pool |   5649 MiB |  11720 MiB |  86885 GiB |  86880 GiB |
|       from small pool |     11 MiB |     25 MiB |    551 GiB |    551 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |    5719 K  |    5718 K  |
|       from large pool |     312    |     320    |    2781 K  |    2781 K  |
|       from small pool |     291    |     356    |    2937 K  |    2937 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |    5719 K  |    5718 K  |
|       from large pool |     312    |     320    |    2781 K  |    2781 K  |
|       from small pool |     291    |     356    |    2937 K  |    2937 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     120    |     229    |    2295    |    2175    |
|       from large pool |     108    |     116    |    1523    |    1415    |
|       from small pool |      12    |     113    |     772    |     760    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     118    |     118    |    3207 K  |    3206 K  |
|       from large pool |      91    |      91    |    1823 K  |    1823 K  |
|       from small pool |      27    |      54    |    1383 K  |    1383 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 12:59:40] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 12:59:44]    INFO >> epoch 002:   1269 / 1539 loss=3.541, wps=5081, ups=6.64, wpb=765.3, bsz=765.3, num_updates=2800, lr=0.00125, gnorm=5.456, clip=0, train_wall=7, gb_free=70.5, wall=467 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:59:52]    INFO >> epoch 002:   1319 / 1539 loss=3.731, wps=4947.2, ups=7.48, wpb=661.5, bsz=661.5, num_updates=2850, lr=0.00125, gnorm=4.284, clip=0, train_wall=6, gb_free=75, wall=474 (progress_bar.py:258, log())[0m
[32m[2025-11-19 12:59:59]    INFO >> epoch 002:   1369 / 1539 loss=3.694, wps=5326.9, ups=7.31, wpb=728.5, bsz=728.5, num_updates=2900, lr=0.00125, gnorm=4.218, clip=0, train_wall=6, gb_free=70.5, wall=481 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:00:06]    INFO >> epoch 002:   1419 / 1539 loss=3.642, wps=4843.3, ups=7.41, wpb=653.6, bsz=653.6, num_updates=2950, lr=0.00125, gnorm=4.2, clip=0, train_wall=6, gb_free=64.8, wall=488 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:00:15]    INFO >> epoch 002:   1469 / 1539 loss=3.694, wps=3949.2, ups=5.62, wpb=702.5, bsz=702.5, num_updates=3000, lr=0.00125, gnorm=4.153, clip=0, train_wall=8, gb_free=70.3, wall=496 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:00:22]    INFO >> epoch 002:   1519 / 1539 loss=3.632, wps=4781.4, ups=7.1, wpb=673.3, bsz=673.3, num_updates=3050, lr=0.00125, gnorm=4.615, clip=2, train_wall=7, gb_free=74.2, wall=503 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:00:26]    INFO >> epoch 002 | loss 3.724 | wps 4785.6 | ups 6.72 | wpb 712.7 | bsz 712.7 | num_updates 3070 | lr 0.00125 | gnorm 5.038 | clip 0.2 | train_wall 201 | gb_free 72.4 | wall 506 (progress_bar.py:267, print())[0m
[33m[2025-11-19 13:00:26] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 13:00:38]    INFO >> epoch 002 | valid on 'valid' subset | loss 3.762 | wps 13300.8 | wpb 5412.5 | bsz 5412.5 | num_updates 3070 | best_loss 3.886 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 13:00:38]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/logs/plots/training.png (train_enhanced.py:181, plot())[0m
[32m[2025-11-19 13:00:38]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/checkpoints/checkpoint_last.pt (epoch 2 @ 3070 updates, score 3.762) (writing took 0.012872 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 13:00:38] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 13:00:42]    INFO >> epoch 003:     30 / 1539 loss=3.731, wps=1829.3, ups=2.69, wpb=681.1, bsz=681.1, num_updates=3100, lr=0.001225, gnorm=4.622, clip=0, train_wall=6, gb_free=70.7, wall=522 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:00:49]    INFO >> epoch 003:     80 / 1539 loss=3.682, wps=5615.2, ups=7.27, wpb=772.8, bsz=772.8, num_updates=3150, lr=0.001225, gnorm=4.394, clip=0, train_wall=6, gb_free=73.4, wall=529 (progress_bar.py:258, log())[0m
[33m[2025-11-19 13:00:52] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 3 has a total capacity of 79.14 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 78.04 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 2.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 13:00:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:00:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:00:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:00:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76326 MiB |  78600 MiB | 102451 GiB | 102377 GiB |
|       from large pool |  76307 MiB |  78583 MiB | 101871 GiB | 101797 GiB |
|       from small pool |     18 MiB |     19 MiB |    580 GiB |    580 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76326 MiB |  78600 MiB | 102451 GiB | 102377 GiB |
|       from large pool |  76307 MiB |  78583 MiB | 101871 GiB | 101797 GiB |
|       from small pool |     18 MiB |     19 MiB |    580 GiB |    580 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 102235 GiB | 102160 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 101655 GiB | 101581 GiB |
|       from small pool |     18 MiB |     19 MiB |    579 GiB |    579 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79400 MiB |  79400 MiB | 483464 MiB | 404064 MiB |
|       from large pool |  79376 MiB |  79376 MiB | 481770 MiB | 402394 MiB |
|       from small pool |     24 MiB |     74 MiB |   1694 MiB |   1670 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3073 MiB |   4962 MiB | 103113 GiB | 103110 GiB |
|       from large pool |   3068 MiB |   4948 MiB | 102456 GiB | 102453 GiB |
|       from small pool |      5 MiB |     27 MiB |    657 GiB |    657 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |    6746 K  |    6745 K  |
|       from large pool |     340    |     346    |    3209 K  |    3208 K  |
|       from small pool |     300    |     356    |    3536 K  |    3536 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |    6746 K  |    6745 K  |
|       from large pool |     340    |     346    |    3209 K  |    3208 K  |
|       from small pool |     300    |     356    |    3536 K  |    3536 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     106    |     118    |    2420    |    2314    |
|       from large pool |      94    |      94    |    1573    |    1479    |
|       from small pool |      12    |      37    |     847    |     835    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     110    |     112    |    3792 K  |    3792 K  |
|       from large pool |      85    |      87    |    2095 K  |    2095 K  |
|       from small pool |      25    |      57    |    1696 K  |    1696 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:00:52] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 13:00:57]    INFO >> epoch 003:    131 / 1539 loss=3.644, wps=5044.3, ups=6.04, wpb=835, bsz=835, num_updates=3200, lr=0.001225, gnorm=5.22, clip=0, train_wall=6, gb_free=71.9, wall=537 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:01:05]    INFO >> epoch 003:    181 / 1539 loss=3.781, wps=4941.5, ups=7.52, wpb=656.8, bsz=656.8, num_updates=3250, lr=0.001225, gnorm=3.914, clip=0, train_wall=6, gb_free=73, wall=544 (progress_bar.py:258, log())[0m
[33m[2025-11-19 13:01:10] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 3 has a total capacity of 79.14 GiB of which 153.25 MiB is free. Including non-PyTorch memory, this process has 78.97 GiB memory in use. Of the allocated memory 75.67 GiB is allocated by PyTorch, and 2.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 13:01:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:01:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:01:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:01:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 23        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76958 MiB |  77481 MiB | 105988 GiB | 105913 GiB |
|       from large pool |  76945 MiB |  77468 MiB | 105386 GiB | 105311 GiB |
|       from small pool |     12 MiB |     21 MiB |    601 GiB |    601 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76958 MiB |  77481 MiB | 105988 GiB | 105913 GiB |
|       from large pool |  76945 MiB |  77468 MiB | 105386 GiB | 105311 GiB |
|       from small pool |     12 MiB |     21 MiB |    601 GiB |    601 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 105764 GiB | 105689 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 105164 GiB | 105089 GiB |
|       from small pool |     12 MiB |     21 MiB |    600 GiB |    600 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80352 MiB |  80352 MiB | 489864 MiB | 409512 MiB |
|       from large pool |  80326 MiB |  80326 MiB | 487984 MiB | 407658 MiB |
|       from small pool |     26 MiB |    210 MiB |   1880 MiB |   1854 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3393 MiB |   8163 MiB | 107067 GiB | 107064 GiB |
|       from large pool |   3380 MiB |   8148 MiB | 106386 GiB | 106383 GiB |
|       from small pool |     13 MiB |     27 MiB |    681 GiB |    681 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |    6985 K  |    6984 K  |
|       from large pool |     315    |     322    |    3323 K  |    3323 K  |
|       from small pool |     291    |     356    |    3662 K  |    3661 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |    6985 K  |    6984 K  |
|       from large pool |     315    |     322    |    3323 K  |    3323 K  |
|       from small pool |     291    |     356    |    3662 K  |    3661 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     100    |     199    |    2520    |    2420    |
|       from large pool |      87    |      94    |    1580    |    1493    |
|       from small pool |      13    |     105    |     940    |     927    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      95    |      95    |    3925 K  |    3925 K  |
|       from large pool |      69    |      69    |    2166 K  |    2166 K  |
|       from small pool |      26    |      58    |    1758 K  |    1758 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:01:10] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 13:01:13]    INFO >> epoch 003:    232 / 1539 loss=3.511, wps=4884.6, ups=6.37, wpb=767.2, bsz=767.2, num_updates=3300, lr=0.001225, gnorm=3.853, clip=0, train_wall=7, gb_free=74, wall=552 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:01:20]    INFO >> epoch 003:    282 / 1539 loss=3.553, wps=5456.3, ups=7.26, wpb=751.9, bsz=751.9, num_updates=3350, lr=0.001225, gnorm=4.575, clip=0, train_wall=6, gb_free=70.6, wall=559 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:01:27]    INFO >> epoch 003:    332 / 1539 loss=3.815, wps=5308.9, ups=6.72, wpb=790.4, bsz=790.4, num_updates=3400, lr=0.001225, gnorm=3.72, clip=0, train_wall=7, gb_free=73.8, wall=566 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:01:36]    INFO >> epoch 003:    382 / 1539 loss=3.615, wps=4708.8, ups=6.84, wpb=688.4, bsz=688.4, num_updates=3450, lr=0.001225, gnorm=4.366, clip=0, train_wall=7, gb_free=72.5, wall=573 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:01:43]    INFO >> epoch 003:    432 / 1539 loss=3.628, wps=4809.5, ups=7.31, wpb=657.8, bsz=657.8, num_updates=3500, lr=0.001225, gnorm=3.864, clip=0, train_wall=6, gb_free=66.2, wall=580 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:01:50]    INFO >> epoch 003:    482 / 1539 loss=3.5, wps=5327.9, ups=7.07, wpb=754, bsz=754, num_updates=3550, lr=0.001225, gnorm=4.212, clip=0, train_wall=7, gb_free=73.1, wall=587 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:01:57]    INFO >> epoch 003:    532 / 1539 loss=3.62, wps=5638.8, ups=7.32, wpb=770.5, bsz=770.5, num_updates=3600, lr=0.001225, gnorm=4.88, clip=2, train_wall=6, gb_free=73.8, wall=594 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:02:04]    INFO >> epoch 003:    582 / 1539 loss=3.5, wps=4995.8, ups=6.94, wpb=720.1, bsz=720.1, num_updates=3650, lr=0.001225, gnorm=4.307, clip=0, train_wall=7, gb_free=71.6, wall=601 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:02:12]    INFO >> epoch 003:    632 / 1539 loss=3.654, wps=5200.1, ups=7.71, wpb=674.5, bsz=674.5, num_updates=3700, lr=0.001225, gnorm=4.363, clip=0, train_wall=6, gb_free=66.5, wall=608 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:02:19]    INFO >> epoch 003:    682 / 1539 loss=3.685, wps=5383.7, ups=7.12, wpb=756.5, bsz=756.5, num_updates=3750, lr=0.001225, gnorm=3.908, clip=0, train_wall=7, gb_free=75.1, wall=615 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:02:27]    INFO >> epoch 003:    732 / 1539 loss=3.383, wps=5201.6, ups=6.42, wpb=810.3, bsz=810.3, num_updates=3800, lr=0.001225, gnorm=4.162, clip=0, train_wall=7, gb_free=73.9, wall=623 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:02:34]    INFO >> epoch 003:    782 / 1539 loss=3.158, wps=5554.6, ups=6.99, wpb=795, bsz=795, num_updates=3850, lr=0.001225, gnorm=4.272, clip=0, train_wall=7, gb_free=73.7, wall=630 (progress_bar.py:258, log())[0m
[33m[2025-11-19 13:02:35] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 29.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.96 GiB is allocated by PyTorch, and 1.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 13:02:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:02:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:02:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:02:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78744 MiB |  78804 MiB | 123508 GiB | 123431 GiB |
|       from large pool |  78658 MiB |  78718 MiB | 122810 GiB | 122733 GiB |
|       from small pool |     85 MiB |     86 MiB |    698 GiB |    697 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78744 MiB |  78804 MiB | 123508 GiB | 123431 GiB |
|       from large pool |  78658 MiB |  78718 MiB | 122810 GiB | 122733 GiB |
|       from small pool |     85 MiB |     86 MiB |    698 GiB |    697 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78703 MiB |  78762 MiB | 123247 GiB | 123170 GiB |
|       from large pool |  78617 MiB |  78677 MiB | 122550 GiB | 122473 GiB |
|       from small pool |     85 MiB |     86 MiB |    697 GiB |    696 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80476 MiB |  80476 MiB | 517426 MiB | 436950 MiB |
|       from large pool |  80386 MiB |  80386 MiB | 515324 MiB | 434938 MiB |
|       from small pool |     90 MiB |    246 MiB |   2102 MiB |   2012 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1671 MiB |   7867 MiB | 124794 GiB | 124792 GiB |
|       from large pool |   1667 MiB |   7857 MiB | 124001 GiB | 124000 GiB |
|       from small pool |      4 MiB |     27 MiB |    792 GiB |    792 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1884    |    1887    |    8145 K  |    8143 K  |
|       from large pool |     463    |     464    |    3900 K  |    3900 K  |
|       from small pool |    1421    |    1424    |    4244 K  |    4243 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1884    |    1887    |    8145 K  |    8143 K  |
|       from large pool |     463    |     464    |    3900 K  |    3900 K  |
|       from small pool |    1421    |    1424    |    4244 K  |    4243 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     225    |     300    |    2734    |    2509    |
|       from large pool |     180    |     180    |    1683    |    1503    |
|       from small pool |      45    |     123    |    1051    |    1006    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     169    |     169    |    4572 K  |    4572 K  |
|       from large pool |     121    |     124    |    2540 K  |    2540 K  |
|       from small pool |      48    |      51    |    2031 K  |    2031 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:02:35] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 13:02:42]    INFO >> epoch 003:    833 / 1539 loss=3.601, wps=5055.8, ups=6.87, wpb=736, bsz=736, num_updates=3900, lr=0.001225, gnorm=5.311, clip=2, train_wall=6, gb_free=73, wall=637 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:02:49]    INFO >> epoch 003:    883 / 1539 loss=3.683, wps=5050.1, ups=7.04, wpb=717, bsz=717, num_updates=3950, lr=0.001225, gnorm=4.556, clip=0, train_wall=7, gb_free=72.5, wall=644 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:02:57]    INFO >> epoch 003:    933 / 1539 loss=3.484, wps=5162.2, ups=7.08, wpb=729.5, bsz=729.5, num_updates=4000, lr=0.001225, gnorm=4.832, clip=0, train_wall=7, gb_free=67.3, wall=651 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:03:05]    INFO >> epoch 003:    983 / 1539 loss=3.695, wps=4271.5, ups=6.24, wpb=684.3, bsz=684.3, num_updates=4050, lr=0.001225, gnorm=4.821, clip=0, train_wall=7, gb_free=71.5, wall=659 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:03:11]    INFO >> epoch 003:   1033 / 1539 loss=3.65, wps=4653.7, ups=7.31, wpb=636.3, bsz=636.3, num_updates=4100, lr=0.001225, gnorm=3.888, clip=0, train_wall=6, gb_free=68.3, wall=666 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:03:19]    INFO >> epoch 003:   1083 / 1539 loss=3.513, wps=5203.4, ups=7.72, wpb=673.9, bsz=673.9, num_updates=4150, lr=0.001225, gnorm=4.675, clip=0, train_wall=6, gb_free=72.9, wall=673 (progress_bar.py:258, log())[0m
[33m[2025-11-19 13:03:26] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 39.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.52 GiB is allocated by PyTorch, and 2.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 13:03:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:03:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:03:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:03:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78301 MiB |  78361 MiB | 133746 GiB | 133670 GiB |
|       from large pool |  77916 MiB |  77976 MiB | 132993 GiB | 132917 GiB |
|       from small pool |    385 MiB |    386 MiB |    753 GiB |    752 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78301 MiB |  78361 MiB | 133746 GiB | 133670 GiB |
|       from large pool |  77916 MiB |  77976 MiB | 132993 GiB | 132917 GiB |
|       from small pool |    385 MiB |    386 MiB |    753 GiB |    752 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78077 MiB |  78137 MiB | 133463 GiB | 133387 GiB |
|       from large pool |  77694 MiB |  77753 MiB | 132711 GiB | 132635 GiB |
|       from small pool |    383 MiB |    384 MiB |    752 GiB |    751 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80466 MiB |  80504 MiB | 539302 MiB | 458836 MiB |
|       from large pool |  80042 MiB |  80326 MiB | 536864 MiB | 456822 MiB |
|       from small pool |    424 MiB |    426 MiB |   2438 MiB |   2014 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2104 MiB |   6644 MiB | 134072 GiB | 134070 GiB |
|       from large pool |   2065 MiB |   6637 MiB | 133215 GiB | 133213 GiB |
|       from small pool |     38 MiB |     40 MiB |    856 GiB |    856 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7109    |    7112    |    8817 K  |    8810 K  |
|       from large pool |     916    |     917    |    4239 K  |    4238 K  |
|       from small pool |    6193    |    6196    |    4578 K  |    4572 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7109    |    7112    |    8817 K  |    8810 K  |
|       from large pool |     916    |     917    |    4239 K  |    4238 K  |
|       from small pool |    6193    |    6196    |    4578 K  |    4572 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     742    |     743    |    3261    |    2519    |
|       from large pool |     530    |     530    |    2042    |    1512    |
|       from small pool |     212    |     213    |    1219    |    1007    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     510    |     511    |    4950 K  |    4949 K  |
|       from large pool |     132    |     132    |    2765 K  |    2765 K  |
|       from small pool |     378    |     379    |    2185 K  |    2184 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:03:26] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 13:03:27]    INFO >> epoch 003:   1134 / 1539 loss=3.578, wps=4637, ups=6.77, wpb=684.8, bsz=684.8, num_updates=4200, lr=0.001225, gnorm=4.325, clip=0, train_wall=6, gb_free=73, wall=680 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:03:34]    INFO >> epoch 003:   1184 / 1539 loss=3.572, wps=4817.1, ups=7.15, wpb=673.6, bsz=673.6, num_updates=4250, lr=0.001225, gnorm=4.574, clip=0, train_wall=7, gb_free=73.5, wall=687 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:03:41]    INFO >> epoch 003:   1234 / 1539 loss=3.607, wps=5047.8, ups=7.03, wpb=718.1, bsz=718.1, num_updates=4300, lr=0.001225, gnorm=4.436, clip=0, train_wall=7, gb_free=70.5, wall=694 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:03:48]    INFO >> epoch 003:   1284 / 1539 loss=3.356, wps=4800, ups=7.75, wpb=619.7, bsz=619.7, num_updates=4350, lr=0.001225, gnorm=4.421, clip=2, train_wall=6, gb_free=73.5, wall=701 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:03:55]    INFO >> epoch 003:   1334 / 1539 loss=3.539, wps=5382.4, ups=7.6, wpb=708, bsz=708, num_updates=4400, lr=0.001225, gnorm=4.504, clip=0, train_wall=6, gb_free=72.1, wall=707 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:04:02]    INFO >> epoch 003:   1384 / 1539 loss=3.447, wps=5210.4, ups=7.52, wpb=692.7, bsz=692.7, num_updates=4450, lr=0.001225, gnorm=4.287, clip=0, train_wall=6, gb_free=74.2, wall=714 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:04:08]    INFO >> epoch 003:   1434 / 1539 loss=3.593, wps=5030.9, ups=7.72, wpb=651.7, bsz=651.7, num_updates=4500, lr=0.001225, gnorm=4.343, clip=0, train_wall=6, gb_free=72.1, wall=720 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:04:15]    INFO >> epoch 003:   1484 / 1539 loss=3.524, wps=4708.1, ups=7.15, wpb=658.9, bsz=658.9, num_updates=4550, lr=0.001225, gnorm=4.057, clip=0, train_wall=7, gb_free=72.4, wall=727 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:04:23]    INFO >> epoch 003:   1534 / 1539 loss=3.606, wps=4952.6, ups=7.47, wpb=662.9, bsz=662.9, num_updates=4600, lr=0.001225, gnorm=4.499, clip=0, train_wall=6, gb_free=72, wall=734 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:04:24]    INFO >> epoch 003 | loss 3.573 | wps 4795.8 | ups 6.73 | wpb 712.7 | bsz 712.7 | num_updates 4605 | lr 0.001225 | gnorm 4.386 | clip 0.2 | train_wall 200 | gb_free 74.4 | wall 735 (progress_bar.py:267, print())[0m
[33m[2025-11-19 13:04:24] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 13:04:35]    INFO >> epoch 003 | valid on 'valid' subset | loss 3.734 | wps 13291.3 | wpb 5412.5 | bsz 5412.5 | num_updates 4605 | best_loss 3.886 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 13:04:36]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/logs/plots/training.png (train_enhanced.py:181, plot())[0m
[32m[2025-11-19 13:04:36]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/checkpoints/checkpoint_last.pt (epoch 3 @ 4605 updates, score 3.734) (writing took 0.013112 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 13:04:36] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 13:04:42]    INFO >> epoch 004:     45 / 1539 loss=3.407, wps=1997, ups=2.66, wpb=751.7, bsz=751.7, num_updates=4650, lr=0.001176, gnorm=4.944, clip=0, train_wall=7, gb_free=72.7, wall=753 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:04:49]    INFO >> epoch 004:     95 / 1539 loss=3.478, wps=5381.1, ups=7.12, wpb=756, bsz=756, num_updates=4700, lr=0.001176, gnorm=4.121, clip=0, train_wall=7, gb_free=67.8, wall=760 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:04:59]    INFO >> epoch 004:    145 / 1539 loss=3.32, wps=5819.3, ups=5.55, wpb=1048.6, bsz=1048.6, num_updates=4750, lr=0.001176, gnorm=5.35, clip=0, train_wall=9, gb_free=75.1, wall=769 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:05:06]    INFO >> epoch 004:    195 / 1539 loss=3.511, wps=5335.2, ups=7.61, wpb=701.3, bsz=701.3, num_updates=4800, lr=0.001176, gnorm=4.206, clip=0, train_wall=6, gb_free=71.9, wall=775 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:05:13]    INFO >> epoch 004:    245 / 1539 loss=3.638, wps=5327.9, ups=7.17, wpb=743.6, bsz=743.6, num_updates=4850, lr=0.001176, gnorm=4.918, clip=0, train_wall=7, gb_free=73.5, wall=782 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:05:20]    INFO >> epoch 004:    295 / 1539 loss=3.506, wps=4817.9, ups=7.44, wpb=647.4, bsz=647.4, num_updates=4900, lr=0.001176, gnorm=4.03, clip=0, train_wall=6, gb_free=74.6, wall=789 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:05:26]    INFO >> epoch 004:    345 / 1539 loss=3.548, wps=5203.7, ups=7.57, wpb=687.4, bsz=687.4, num_updates=4950, lr=0.001176, gnorm=3.628, clip=0, train_wall=6, gb_free=69.9, wall=796 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:05:35]    INFO >> epoch 004:    395 / 1539 loss=3.437, wps=5418.6, ups=7.12, wpb=761.1, bsz=761.1, num_updates=5000, lr=0.001176, gnorm=3.912, clip=0, train_wall=7, gb_free=67.8, wall=803 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:05:42]    INFO >> epoch 004:    445 / 1539 loss=3.613, wps=4132.3, ups=6.55, wpb=630.5, bsz=630.5, num_updates=5050, lr=0.001176, gnorm=4.52, clip=0, train_wall=7, gb_free=71.4, wall=810 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:05:49]    INFO >> epoch 004:    495 / 1539 loss=3.554, wps=5322.4, ups=7.39, wpb=720.1, bsz=720.1, num_updates=5100, lr=0.001176, gnorm=3.491, clip=0, train_wall=6, gb_free=74, wall=817 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:05:56]    INFO >> epoch 004:    545 / 1539 loss=3.597, wps=5026.3, ups=7.16, wpb=701.6, bsz=701.6, num_updates=5150, lr=0.001176, gnorm=3.94, clip=0, train_wall=7, gb_free=73.5, wall=824 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:06:04]    INFO >> epoch 004:    595 / 1539 loss=3.487, wps=4719.5, ups=7.49, wpb=629.8, bsz=629.8, num_updates=5200, lr=0.001176, gnorm=3.647, clip=0, train_wall=6, gb_free=71, wall=831 (progress_bar.py:258, log())[0m
[33m[2025-11-19 13:06:05] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 13.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 1.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 13:06:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:06:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:06:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:06:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79173 MiB |  79233 MiB | 167559 GiB | 167482 GiB |
|       from large pool |  79083 MiB |  79143 MiB | 166611 GiB | 166534 GiB |
|       from small pool |     89 MiB |     91 MiB |    948 GiB |    948 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79173 MiB |  79233 MiB | 167559 GiB | 167482 GiB |
|       from large pool |  79083 MiB |  79143 MiB | 166611 GiB | 166534 GiB |
|       from small pool |     89 MiB |     91 MiB |    948 GiB |    948 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79123 MiB |  79183 MiB | 167207 GiB | 167130 GiB |
|       from large pool |  79034 MiB |  79093 MiB | 166260 GiB | 166183 GiB |
|       from small pool |     89 MiB |     90 MiB |    946 GiB |    946 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80492 MiB |  80492 MiB | 564646 MiB | 484154 MiB |
|       from large pool |  80398 MiB |  80398 MiB | 562140 MiB | 481742 MiB |
|       from small pool |     94 MiB |     94 MiB |   2506 MiB |   2412 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1258 MiB |   7162 MiB | 164246 GiB | 164244 GiB |
|       from large pool |   1254 MiB |   7152 MiB | 163171 GiB | 163170 GiB |
|       from small pool |      4 MiB |     27 MiB |   1074 GiB |   1074 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1961    |    1964    |   11044 K  |   11042 K  |
|       from large pool |     470    |     471    |    5271 K  |    5271 K  |
|       from small pool |    1491    |    1494    |    5772 K  |    5770 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1961    |    1964    |   11044 K  |   11042 K  |
|       from large pool |     470    |     471    |    5271 K  |    5271 K  |
|       from small pool |    1491    |    1494    |    5772 K  |    5770 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     223    |     223    |    3348    |    3125    |
|       from large pool |     176    |     176    |    2095    |    1919    |
|       from small pool |      47    |      47    |    1253    |    1206    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     140    |     140    |    6216 K  |    6216 K  |
|       from large pool |      92    |      96    |    3444 K  |    3444 K  |
|       from small pool |      48    |      58    |    2772 K  |    2772 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:06:05] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 13:06:11]    INFO >> epoch 004:    646 / 1539 loss=3.525, wps=4750, ups=7.08, wpb=671.1, bsz=671.1, num_updates=5250, lr=0.001176, gnorm=3.599, clip=0, train_wall=6, gb_free=68.7, wall=838 (progress_bar.py:258, log())[0m
[33m[2025-11-19 13:06:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 3 has a total capacity of 79.14 GiB of which 983.25 MiB is free. Including non-PyTorch memory, this process has 78.16 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 13:06:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:06:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:06:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:06:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76323 MiB |  78596 MiB | 170178 GiB | 170104 GiB |
|       from large pool |  76304 MiB |  78579 MiB | 169217 GiB | 169142 GiB |
|       from small pool |     18 MiB |     24 MiB |    961 GiB |    961 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76323 MiB |  78596 MiB | 170178 GiB | 170104 GiB |
|       from large pool |  76304 MiB |  78579 MiB | 169217 GiB | 169142 GiB |
|       from small pool |     18 MiB |     24 MiB |    961 GiB |    961 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 169821 GiB | 169746 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 168861 GiB | 168786 GiB |
|       from small pool |     18 MiB |     24 MiB |    960 GiB |    959 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79522 MiB |  80432 MiB | 637044 MiB | 557522 MiB |
|       from large pool |  79496 MiB |  80338 MiB | 634538 MiB | 555042 MiB |
|       from small pool |     26 MiB |     94 MiB |   2506 MiB |   2480 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3198 MiB |   4143 MiB | 166742 GiB | 166738 GiB |
|       from large pool |   3191 MiB |   4135 MiB | 165652 GiB | 165649 GiB |
|       from small pool |      7 MiB |     23 MiB |   1089 GiB |   1089 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   11213 K  |   11212 K  |
|       from large pool |     340    |     346    |    5361 K  |    5361 K  |
|       from small pool |     300    |     356    |    5852 K  |    5851 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   11213 K  |   11212 K  |
|       from large pool |     340    |     346    |    5361 K  |    5361 K  |
|       from small pool |     300    |     356    |    5852 K  |    5851 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     107    |     222    |    3403    |    3296    |
|       from large pool |      94    |     175    |    2150    |    2056    |
|       from small pool |      13    |      47    |    1253    |    1240    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     126    |     128    |    6309 K  |    6309 K  |
|       from large pool |      98    |     100    |    3503 K  |    3503 K  |
|       from small pool |      28    |      51    |    2805 K  |    2805 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:06:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 13:06:20]    INFO >> epoch 004:    697 / 1539 loss=3.508, wps=3578.7, ups=5.62, wpb=636.6, bsz=636.6, num_updates=5300, lr=0.001176, gnorm=3.846, clip=0, train_wall=6, gb_free=67.8, wall=847 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:06:28]    INFO >> epoch 004:    747 / 1539 loss=3.545, wps=4292.4, ups=6.38, wpb=673.1, bsz=673.1, num_updates=5350, lr=0.001176, gnorm=3.237, clip=0, train_wall=7, gb_free=72.9, wall=855 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:06:36]    INFO >> epoch 004:    797 / 1539 loss=3.439, wps=5168.7, ups=7.28, wpb=710.1, bsz=710.1, num_updates=5400, lr=0.001176, gnorm=3.571, clip=0, train_wall=6, gb_free=72.3, wall=861 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:06:43]    INFO >> epoch 004:    847 / 1539 loss=3.464, wps=5420.6, ups=7.25, wpb=747.5, bsz=747.5, num_updates=5450, lr=0.001176, gnorm=4.049, clip=0, train_wall=6, gb_free=63.6, wall=868 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:06:50]    INFO >> epoch 004:    897 / 1539 loss=3.534, wps=5293.6, ups=6.8, wpb=778.3, bsz=778.3, num_updates=5500, lr=0.001176, gnorm=3.643, clip=0, train_wall=7, gb_free=70.2, wall=876 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:06:57]    INFO >> epoch 004:    947 / 1539 loss=3.468, wps=4834.1, ups=7.48, wpb=646.7, bsz=646.7, num_updates=5550, lr=0.001176, gnorm=3.453, clip=0, train_wall=6, gb_free=67.5, wall=882 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:07:04]    INFO >> epoch 004:    997 / 1539 loss=3.517, wps=5178.3, ups=6.77, wpb=764.9, bsz=764.9, num_updates=5600, lr=0.001176, gnorm=4.271, clip=0, train_wall=7, gb_free=75.5, wall=890 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:07:12]    INFO >> epoch 004:   1047 / 1539 loss=3.643, wps=4754.1, ups=7.56, wpb=628.6, bsz=628.6, num_updates=5650, lr=0.001176, gnorm=3.569, clip=0, train_wall=6, gb_free=72.5, wall=896 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:07:19]    INFO >> epoch 004:   1097 / 1539 loss=3.498, wps=5285.7, ups=7.24, wpb=729.6, bsz=729.6, num_updates=5700, lr=0.001176, gnorm=4.142, clip=0, train_wall=6, gb_free=71.1, wall=903 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:07:26]    INFO >> epoch 004:   1147 / 1539 loss=3.463, wps=4865.7, ups=7.19, wpb=676.4, bsz=676.4, num_updates=5750, lr=0.001176, gnorm=3.676, clip=0, train_wall=7, gb_free=56.5, wall=910 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:07:33]    INFO >> epoch 004:   1197 / 1539 loss=3.537, wps=5273, ups=6.99, wpb=754.7, bsz=754.7, num_updates=5800, lr=0.001176, gnorm=4.793, clip=2, train_wall=7, gb_free=75.6, wall=917 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:07:40]    INFO >> epoch 004:   1247 / 1539 loss=3.471, wps=4898.1, ups=7.8, wpb=627.8, bsz=627.8, num_updates=5850, lr=0.001176, gnorm=3.701, clip=0, train_wall=6, gb_free=75.6, wall=924 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:07:48]    INFO >> epoch 004:   1297 / 1539 loss=3.528, wps=4566.1, ups=7.08, wpb=644.6, bsz=644.6, num_updates=5900, lr=0.001176, gnorm=3.972, clip=0, train_wall=7, gb_free=74.6, wall=931 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:07:55]    INFO >> epoch 004:   1347 / 1539 loss=3.49, wps=5752.1, ups=7.18, wpb=801.1, bsz=801.1, num_updates=5950, lr=0.001176, gnorm=3.725, clip=0, train_wall=7, gb_free=66.8, wall=938 (progress_bar.py:258, log())[0m
[33m[2025-11-19 13:08:02] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 3 has a total capacity of 79.14 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 2.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 13:08:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:08:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:08:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:08:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 34        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75053 MiB |  75996 MiB | 190873 GiB | 190800 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 189802 GiB | 189729 GiB |
|       from small pool |     12 MiB |     13 MiB |   1070 GiB |   1070 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75053 MiB |  75996 MiB | 190873 GiB | 190800 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 189802 GiB | 189729 GiB |
|       from small pool |     12 MiB |     13 MiB |   1070 GiB |   1070 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 190472 GiB | 190399 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 189403 GiB | 189330 GiB |
|       from small pool |     12 MiB |     13 MiB |   1069 GiB |   1069 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78634 MiB |  79742 MiB | 645400 MiB | 566766 MiB |
|       from large pool |  78608 MiB |  79496 MiB | 642674 MiB | 564066 MiB |
|       from small pool |     26 MiB |    246 MiB |   2726 MiB |   2700 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3580 MiB |   7454 MiB | 189481 GiB | 189477 GiB |
|       from large pool |   3567 MiB |   7440 MiB | 188266 GiB | 188263 GiB |
|       from small pool |     13 MiB |     21 MiB |   1214 GiB |   1214 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   12564 K  |   12563 K  |
|       from large pool |     314    |     322    |    6052 K  |    6052 K  |
|       from small pool |     291    |     336    |    6511 K  |    6510 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   12564 K  |   12563 K  |
|       from large pool |     314    |     322    |    6052 K  |    6052 K  |
|       from small pool |     291    |     336    |    6511 K  |    6510 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     217    |    3521    |    3430    |
|       from large pool |      78    |      94    |    2158    |    2080    |
|       from small pool |      13    |     123    |    1363    |    1350    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      90    |    7038 K  |    7038 K  |
|       from large pool |      60    |      61    |    3937 K  |    3937 K  |
|       from small pool |      29    |      51    |    3100 K  |    3100 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:08:02] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 13:08:03]    INFO >> epoch 004:   1398 / 1539 loss=3.506, wps=5058.4, ups=6.39, wpb=791.5, bsz=791.5, num_updates=6000, lr=0.001176, gnorm=4.438, clip=0, train_wall=7, gb_free=71.6, wall=946 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:08:10]    INFO >> epoch 004:   1448 / 1539 loss=3.532, wps=5315.3, ups=7.47, wpb=711.7, bsz=711.7, num_updates=6050, lr=0.001176, gnorm=4.069, clip=0, train_wall=6, gb_free=73.5, wall=952 (progress_bar.py:258, log())[0m
[32m[2025-11-19 13:08:16]    INFO >> epoch 004:   1498 / 1539 loss=3.576, wps=5022.4, ups=7.36, wpb=682, bsz=682, num_updates=6100, lr=0.001176, gnorm=4.379, clip=0, train_wall=6, gb_free=74.2, wall=959 (progress_bar.py:258, log())[0m
[33m[2025-11-19 13:08:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 3 has a total capacity of 79.14 GiB of which 25.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.10 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 13:08:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:08:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:08:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:08:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 35        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78888 MiB |  78948 MiB | 194338 GiB | 194261 GiB |
|       from large pool |  78495 MiB |  78555 MiB | 193245 GiB | 193169 GiB |
|       from small pool |    393 MiB |    394 MiB |   1092 GiB |   1092 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78888 MiB |  78948 MiB | 194338 GiB | 194261 GiB |
|       from large pool |  78495 MiB |  78555 MiB | 193245 GiB | 193169 GiB |
|       from small pool |    393 MiB |    394 MiB |   1092 GiB |   1092 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78858 MiB |  78917 MiB | 193930 GiB | 193853 GiB |
|       from large pool |  78467 MiB |  78526 MiB | 192839 GiB | 192762 GiB |
|       from small pool |    390 MiB |    392 MiB |   1090 GiB |   1090 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80480 MiB |  80482 MiB | 647248 MiB | 566768 MiB |
|       from large pool |  80048 MiB |  80048 MiB | 644114 MiB | 564066 MiB |
|       from small pool |    432 MiB |    434 MiB |   3134 MiB |   2702 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1531 MiB |   4145 MiB | 193506 GiB | 193504 GiB |
|       from large pool |   1492 MiB |   4108 MiB | 192266 GiB | 192265 GiB |
|       from small pool |     38 MiB |     41 MiB |   1239 GiB |   1239 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7252    |    7255    |   12813 K  |   12806 K  |
|       from large pool |     929    |     930    |    6169 K  |    6168 K  |
|       from small pool |    6323    |    6326    |    6643 K  |    6637 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7252    |    7255    |   12813 K  |   12806 K  |
|       from large pool |     929    |     930    |    6169 K  |    6168 K  |
|       from small pool |    6323    |    6326    |    6643 K  |    6637 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     318    |     319    |    3749    |    3431    |
|       from large pool |     102    |     102    |    2182    |    2080    |
|       from small pool |     216    |     217    |    1567    |    1351    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     462    |     464    |    7175 K  |    7174 K  |
|       from large pool |      77    |      77    |    4009 K  |    4009 K  |
|       from small pool |     385    |     387    |    3165 K  |    3165 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 13:08:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 13:08:22]    INFO >> epoch 004 | loss 3.509 | wps 4752.1 | ups 6.67 | wpb 712.7 | bsz 712.7 | num_updates 6140 | lr 0.001176 | gnorm 4.031 | clip 0.1 | train_wall 202 | gb_free 70.1 | wall 965 (progress_bar.py:267, print())[0m
[33m[2025-11-19 13:08:22] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 13:08:33]    INFO >> epoch 004 | valid on 'valid' subset | loss 3.738 | wps 13290.5 | wpb 5412.5 | bsz 5412.5 | num_updates 6140 | best_loss 3.886 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 13:08:34]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/logs/plots/training.png (train_enhanced.py:181, plot())[0m
[32m[2025-11-19 13:08:34]    INFO >> saved checkpoint /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/checkpoints/checkpoint_last.pt (epoch 4 @ 6140 updates, score 3.738) (writing took 0.013032 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-19 13:08:34]    INFO >> æ—©åœ: éªŒè¯æ€§èƒ½å·²3è½®æœªæå‡ (train_enhanced.py:423, single_main())[0m
[32m[2025-11-19 13:08:34]    INFO >> è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ 919.7 ç§’ (train_enhanced.py:433, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 13:08:34]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/logs/plots/training.png (train_enhanced.py:181, plot())[0m
[32m[2025-11-19 13:08:34]    INFO >> æ‰€æœ‰æ—¥å¿—å·²ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/logs (train_enhanced.py:438, single_main())[0m
[TrainingLogger] æ—¥å¿—ç›®å½•: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/logs
[TrainingLogger] åŽŸå§‹è¾“å‡ºå°†ä¿å­˜åˆ°: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/logs/training_output.log
[TrainingLogger] Epoch 1 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/logs/metrics.json
[TrainingLogger] Epoch 2 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/logs/metrics.json
[TrainingLogger] Epoch 3 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/logs/metrics.json
[TrainingLogger] Epoch 4 æŒ‡æ ‡å·²ä¿å­˜: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiments/lr_1.25e-3/logs/metrics.json
