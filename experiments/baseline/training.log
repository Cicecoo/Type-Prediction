[32m[2025-11-18 23:08:28]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/config.yml (train_enhanced.py:382, cli_main())[0m
[32m[2025-11-18 23:08:28]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:410, cli_main())[0m
[32m[2025-11-18 23:08:28]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs (train_enhanced.py:296, single_main())[0m
[32m[2025-11-18 23:08:28]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-18 23:08:28]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-18 23:08:28]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-18 23:08:36]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 64, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=64, out_features=64, bias=False)
      (2): Dropout(p=0.1, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(64, 64)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(128, 64)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=False)
      (1): Dropout(p=0.1, inplace=False)
      (2): Linear(in_features=64, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:303, single_main())[0m
[32m[2025-11-18 23:08:36]    INFO >> æ¨¡åž‹: typilus, æŸå¤±å‡½æ•°: TypilusCriterion (train_enhanced.py:304, single_main())[0m
[32m[2025-11-18 23:08:36]    INFO >> æ¨¡åž‹å‚æ•°: 847843 (å¯è®­ç»ƒ: 847843) (train_enhanced.py:305, single_main())[0m
[32m[2025-11-18 23:08:37]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-18 23:08:37]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 77695 MB ; used memory = 4224 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-18 23:08:37]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-18 23:08:37]    INFO >> ä½¿ç”¨ 1 ä¸ªGPUè®­ç»ƒ (train_enhanced.py:311, single_main())[0m
[32m[2025-11-18 23:08:37]    INFO >> no existing checkpoint found /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-18 23:08:37]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-18 23:09:41]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-18 23:09:41] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-18 23:09:57]    INFO >> epoch 001:     50 / 1539 loss=5.601, wps=2381.5, ups=3.29, wpb=720, bsz=720, num_updates=50, lr=0.0004, gnorm=6.502, clip=0, train_wall=15, gb_free=74.2, wall=78 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:10:01] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 11.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.29 GiB memory in use. Of the allocated memory 70.86 GiB is allocated by PyTorch, and 4.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:10:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72499 MiB |  72559 MiB |   1750 GiB |   1679 GiB |
|       from large pool |  72171 MiB |  72231 MiB |   1738 GiB |   1667 GiB |
|       from small pool |    327 MiB |    328 MiB |     12 GiB |     11 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72499 MiB |  72559 MiB |   1750 GiB |   1679 GiB |
|       from large pool |  72171 MiB |  72231 MiB |   1738 GiB |   1667 GiB |
|       from small pool |    327 MiB |    328 MiB |     12 GiB |     11 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72373 MiB |  72432 MiB |   1744 GiB |   1673 GiB |
|       from large pool |  72047 MiB |  72106 MiB |   1732 GiB |   1661 GiB |
|       from small pool |    326 MiB |    327 MiB |     12 GiB |     11 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77610 MiB |  77612 MiB |  89142 MiB |  11532 MiB |
|       from large pool |  77250 MiB |  77256 MiB |  88770 MiB |  11520 MiB |
|       from small pool |    360 MiB |    362 MiB |    372 MiB |     12 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5050 MiB |   5960 MiB |    874 GiB |    869 GiB |
|       from large pool |   5018 MiB |   5948 MiB |    859 GiB |    854 GiB |
|       from small pool |     32 MiB |     34 MiB |     14 GiB |     14 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6062    |    6065    |  134370    |  128308    |
|       from large pool |     821    |     822    |   57531    |   56710    |
|       from small pool |    5241    |    5244    |   76839    |   71598    |
|---------------------------------------------------------------------------|
| Active allocs         |    6062    |    6065    |  134370    |  128308    |
|       from large pool |     821    |     822    |   57531    |   56710    |
|       from small pool |    5241    |    5244    |   76839    |   71598    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     695    |     729    |     943    |     248    |
|       from large pool |     515    |     564    |     757    |     242    |
|       from small pool |     180    |     181    |     186    |       6    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     625    |     627    |   84540    |   83915    |
|       from large pool |     306    |     306    |   44162    |   43856    |
|       from small pool |     319    |     321    |   40378    |   40059    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:10:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:10:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:10:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:10:01] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:10:13]    INFO >> epoch 001:    101 / 1539 loss=5.711, wps=2077.9, ups=3.4, wpb=610.7, bsz=610.7, num_updates=100, lr=0.0004, gnorm=6.97, clip=0, train_wall=13, gb_free=75.6, wall=92 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:10:29]    INFO >> epoch 001:    151 / 1539 loss=5.93, wps=2551.8, ups=3.12, wpb=816.7, bsz=816.7, num_updates=150, lr=0.0004, gnorm=7.273, clip=0, train_wall=15, gb_free=74.2, wall=108 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:10:44]    INFO >> epoch 001:    201 / 1539 loss=5.873, wps=2365.1, ups=3.69, wpb=641.7, bsz=641.7, num_updates=200, lr=0.0004, gnorm=7.46, clip=0, train_wall=13, gb_free=74.8, wall=122 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:10:58]    INFO >> epoch 001:    251 / 1539 loss=5.999, wps=2295.2, ups=3.6, wpb=637.9, bsz=637.9, num_updates=250, lr=0.0004, gnorm=7.459, clip=0, train_wall=13, gb_free=71.5, wall=136 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:11:15]    INFO >> epoch 001:    301 / 1539 loss=5.747, wps=2507.1, ups=3.19, wpb=786.5, bsz=786.5, num_updates=300, lr=0.0004, gnorm=7.02, clip=0, train_wall=15, gb_free=73.8, wall=152 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:11:29]    INFO >> epoch 001:    351 / 1539 loss=5.925, wps=2470, ups=3.68, wpb=671.6, bsz=671.6, num_updates=350, lr=0.0004, gnorm=7.239, clip=0, train_wall=13, gb_free=72.4, wall=165 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:11:45]    INFO >> epoch 001:    401 / 1539 loss=5.594, wps=2750.6, ups=3.23, wpb=851.7, bsz=851.7, num_updates=400, lr=0.0004, gnorm=7.958, clip=2, train_wall=15, gb_free=73.2, wall=181 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:11:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 33.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.27 GiB memory in use. Of the allocated memory 74.97 GiB is allocated by PyTorch, and 822.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:11:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76705 MiB |  76765 MiB |  12407 GiB |  12332 GiB |
|       from large pool |  76640 MiB |  76700 MiB |  12334 GiB |  12260 GiB |
|       from small pool |     65 MiB |     66 MiB |     72 GiB |     72 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76705 MiB |  76765 MiB |  12407 GiB |  12332 GiB |
|       from large pool |  76640 MiB |  76700 MiB |  12334 GiB |  12260 GiB |
|       from small pool |     65 MiB |     66 MiB |     72 GiB |     72 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76659 MiB |  76718 MiB |  12368 GiB |  12293 GiB |
|       from large pool |  76594 MiB |  76653 MiB |  12295 GiB |  12220 GiB |
|       from small pool |     64 MiB |     66 MiB |     72 GiB |     72 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77588 MiB |  77590 MiB | 160960 MiB |  83372 MiB |
|       from large pool |  77520 MiB |  77520 MiB | 160542 MiB |  83022 MiB |
|       from small pool |     68 MiB |    360 MiB |    418 MiB |    350 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    822 MiB |   3119 MiB |   6159 GiB |   6158 GiB |
|       from large pool |    819 MiB |   3112 MiB |   6074 GiB |   6073 GiB |
|       from small pool |      2 MiB |     23 MiB |     85 GiB |     85 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1508    |    1511    |     864 K  |     862 K  |
|       from large pool |     429    |     430    |     421 K  |     420 K  |
|       from small pool |    1079    |    1082    |     443 K  |     442 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1508    |    1511    |     864 K  |     862 K  |
|       from large pool |     429    |     430    |     421 K  |     420 K  |
|       from small pool |    1079    |    1082    |     443 K  |     442 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     188    |     694    |    1075    |     887    |
|       from large pool |     154    |     514    |     866    |     712    |
|       from small pool |      34    |     180    |     209    |     175    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     120    |     121    |  536769    |  536649    |
|       from large pool |      79    |      81    |  324465    |  324386    |
|       from small pool |      41    |      55    |  212304    |  212263    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:11:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:11:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:11:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:11:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:12:06]    INFO >> epoch 001:    452 / 1539 loss=5.837, wps=1518, ups=2.4, wpb=631.6, bsz=631.6, num_updates=450, lr=0.0004, gnorm=6.824, clip=0, train_wall=13, gb_free=71.8, wall=201 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:12:25]    INFO >> epoch 001:    502 / 1539 loss=5.715, wps=2070.8, ups=2.79, wpb=743.1, bsz=743.1, num_updates=500, lr=0.0004, gnorm=7.261, clip=0, train_wall=17, gb_free=72.6, wall=219 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:12:40]    INFO >> epoch 001:    552 / 1539 loss=5.705, wps=2297.9, ups=3.5, wpb=657, bsz=657, num_updates=550, lr=0.0004, gnorm=7.486, clip=2, train_wall=14, gb_free=65.5, wall=234 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:12:56]    INFO >> epoch 001:    602 / 1539 loss=5.748, wps=2318, ups=3.47, wpb=668.6, bsz=668.6, num_updates=600, lr=0.0004, gnorm=7.298, clip=0, train_wall=14, gb_free=73.2, wall=248 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:13:11]    INFO >> epoch 001:    652 / 1539 loss=5.603, wps=2325.7, ups=3.27, wpb=712.2, bsz=712.2, num_updates=650, lr=0.0004, gnorm=7.014, clip=0, train_wall=15, gb_free=73.5, wall=263 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:13:28]    INFO >> epoch 001:    702 / 1539 loss=5.63, wps=2094.9, ups=3.12, wpb=672.5, bsz=672.5, num_updates=700, lr=0.0004, gnorm=5.923, clip=0, train_wall=15, gb_free=74.1, wall=280 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:13:43]    INFO >> epoch 001:    752 / 1539 loss=5.494, wps=2507.1, ups=3.31, wpb=757.4, bsz=757.4, num_updates=750, lr=0.0004, gnorm=7.159, clip=2, train_wall=14, gb_free=73.7, wall=295 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:13:59]    INFO >> epoch 001:    802 / 1539 loss=5.523, wps=2584.8, ups=3.56, wpb=725.2, bsz=725.2, num_updates=800, lr=0.0004, gnorm=7.026, clip=0, train_wall=13, gb_free=73.4, wall=309 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:14:14]    INFO >> epoch 001:    852 / 1539 loss=5.552, wps=2166.9, ups=3.38, wpb=641.1, bsz=641.1, num_updates=850, lr=0.0004, gnorm=6.835, clip=0, train_wall=14, gb_free=71.8, wall=323 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:14:27]    INFO >> epoch 001:    902 / 1539 loss=5.533, wps=2391.9, ups=3.62, wpb=660.1, bsz=660.1, num_updates=900, lr=0.0004, gnorm=5.928, clip=0, train_wall=13, gb_free=72.1, wall=337 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:14:43]    INFO >> epoch 001:    952 / 1539 loss=5.455, wps=2414.8, ups=3.39, wpb=713.2, bsz=713.2, num_updates=950, lr=0.0004, gnorm=6.051, clip=0, train_wall=14, gb_free=71.8, wall=352 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:14:49] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 697.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 75.62 GiB memory in use. Of the allocated memory 74.09 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:14:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72061 MiB |  75870 MiB |  28387 GiB |  28317 GiB |
|       from large pool |  72043 MiB |  75853 MiB |  28234 GiB |  28163 GiB |
|       from small pool |     17 MiB |     18 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72061 MiB |  75870 MiB |  28387 GiB |  28317 GiB |
|       from large pool |  72043 MiB |  75853 MiB |  28234 GiB |  28163 GiB |
|       from small pool |     17 MiB |     18 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  75852 MiB |  28314 GiB |  28244 GiB |
|       from large pool |  72027 MiB |  75835 MiB |  28161 GiB |  28091 GiB |
|       from small pool |     17 MiB |     18 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  76924 MiB |  76924 MiB | 297094 MiB | 220170 MiB |
|       from large pool |  76900 MiB |  76900 MiB | 296602 MiB | 219702 MiB |
|       from small pool |     24 MiB |     98 MiB |    492 MiB |    468 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1452 MiB |   3665 MiB |  24376 GiB |  24374 GiB |
|       from large pool |   1446 MiB |   3649 MiB |  24199 GiB |  24197 GiB |
|       from small pool |      6 MiB |     27 MiB |    176 GiB |    176 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     604    |     613    |    1876 K  |    1875 K  |
|       from large pool |     308    |     317    |     948 K  |     948 K  |
|       from small pool |     296    |     354    |     927 K  |     927 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     604    |     613    |    1876 K  |    1875 K  |
|       from large pool |     308    |     317    |     948 K  |     948 K  |
|       from small pool |     296    |     354    |     927 K  |     927 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     122    |    1201    |    1105    |
|       from large pool |      84    |      84    |     955    |     871    |
|       from small pool |      12    |      49    |     246    |     234    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     110    |     111    |    1083 K  |    1083 K  |
|       from large pool |      88    |      89    |     660 K  |     660 K  |
|       from small pool |      22    |      56    |     422 K  |     422 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:14:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:14:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:14:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:14:49] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:15:01]    INFO >> epoch 001:   1003 / 1539 loss=5.35, wps=1894.1, ups=2.92, wpb=649.4, bsz=649.4, num_updates=1000, lr=0.0004, gnorm=7.002, clip=0, train_wall=14, gb_free=72.1, wall=369 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:15:17]    INFO >> epoch 001:   1053 / 1539 loss=5.426, wps=2656.4, ups=3.23, wpb=822.6, bsz=822.6, num_updates=1050, lr=0.0004, gnorm=7.285, clip=0, train_wall=15, gb_free=68, wall=385 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:15:33] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 167.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.14 GiB memory in use. Of the allocated memory 72.89 GiB is allocated by PyTorch, and 2.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:15:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 9         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72223 MiB |  74872 MiB |  32726 GiB |  32655 GiB |
|       from large pool |  72210 MiB |  74859 MiB |  32548 GiB |  32477 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72223 MiB |  74872 MiB |  32726 GiB |  32655 GiB |
|       from large pool |  72210 MiB |  74859 MiB |  32548 GiB |  32477 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72206 MiB |  74855 MiB |  32645 GiB |  32574 GiB |
|       from large pool |  72194 MiB |  74842 MiB |  32467 GiB |  32396 GiB |
|       from small pool |     12 MiB |     15 MiB |    177 GiB |    177 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77454 MiB |  77504 MiB | 305390 MiB | 227936 MiB |
|       from large pool |  77430 MiB |  77430 MiB | 304712 MiB | 227282 MiB |
|       from small pool |     24 MiB |    210 MiB |    678 MiB |    654 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3328 MiB |   7876 MiB |  29293 GiB |  29290 GiB |
|       from large pool |   3317 MiB |   7864 MiB |  29088 GiB |  29085 GiB |
|       from small pool |     11 MiB |     23 MiB |    205 GiB |    205 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     597    |     606    |    2161 K  |    2160 K  |
|       from large pool |     308    |     316    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1076 K  |    1075 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     597    |     606    |    2161 K  |    2160 K  |
|       from large pool |     308    |     316    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1076 K  |    1075 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      87    |     189    |    1299    |    1212    |
|       from large pool |      75    |      84    |     960    |     885    |
|       from small pool |      12    |     105    |     339    |     327    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      85    |      85    |    1241 K  |    1241 K  |
|       from large pool |      61    |      61    |     745 K  |     745 K  |
|       from small pool |      24    |      53    |     495 K  |     495 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:15:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:15:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:15:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:15:33] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:15:35]    INFO >> epoch 001:   1104 / 1539 loss=5.507, wps=2576.3, ups=2.77, wpb=929.6, bsz=929.6, num_updates=1100, lr=0.0004, gnorm=6.525, clip=0, train_wall=16, gb_free=72.8, wall=403 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:15:51]    INFO >> epoch 001:   1154 / 1539 loss=5.305, wps=2448.1, ups=3.53, wpb=692.6, bsz=692.6, num_updates=1150, lr=0.0004, gnorm=6.978, clip=0, train_wall=14, gb_free=73.1, wall=417 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:16:05]    INFO >> epoch 001:   1204 / 1539 loss=5.269, wps=2372.5, ups=3.5, wpb=678.4, bsz=678.4, num_updates=1200, lr=0.0004, gnorm=7.014, clip=0, train_wall=14, gb_free=71.2, wall=431 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:16:21]    INFO >> epoch 001:   1254 / 1539 loss=5.169, wps=2577.2, ups=3.53, wpb=730.9, bsz=730.9, num_updates=1250, lr=0.0004, gnorm=6.734, clip=0, train_wall=14, gb_free=71.3, wall=445 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:16:36]    INFO >> epoch 001:   1304 / 1539 loss=5.009, wps=2420.9, ups=3.29, wpb=735.4, bsz=735.4, num_updates=1300, lr=0.0004, gnorm=5.702, clip=0, train_wall=15, gb_free=74.3, wall=461 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:16:52]    INFO >> epoch 001:   1354 / 1539 loss=4.96, wps=2265.7, ups=3.45, wpb=657.2, bsz=657.2, num_updates=1350, lr=0.0004, gnorm=6.761, clip=2, train_wall=14, gb_free=73.4, wall=475 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:17:07]    INFO >> epoch 001:   1404 / 1539 loss=4.794, wps=2366.3, ups=3.32, wpb=712.5, bsz=712.5, num_updates=1400, lr=0.0004, gnorm=6.15, clip=0, train_wall=14, gb_free=73.3, wall=490 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:17:23]    INFO >> epoch 001:   1454 / 1539 loss=4.72, wps=2462.3, ups=3.51, wpb=702.2, bsz=702.2, num_updates=1450, lr=0.0004, gnorm=6.786, clip=2, train_wall=14, gb_free=71.6, wall=504 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:17:37]    INFO >> epoch 001:   1504 / 1539 loss=4.527, wps=2433, ups=3.5, wpb=695.9, bsz=695.9, num_updates=1500, lr=0.0004, gnorm=6.468, clip=0, train_wall=14, gb_free=70.8, wall=519 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:17:47]    INFO >> epoch 001 | loss 5.45 | wps 2345.7 | ups 3.29 | wpb 712.7 | bsz 712.7 | num_updates 1535 | lr 0.0004 | gnorm 6.856 | clip 0.3 | train_wall 436 | gb_free 76.6 | wall 529 (progress_bar.py:267, print())[0m
[33m[2025-11-18 23:17:47] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-18 23:18:14]    INFO >> epoch 001 | valid on 'valid' subset | loss 4.404 | wps 5796.6 | wpb 5412.5 | bsz 5412.5 | num_updates 1535 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-18 23:18:15]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-18 23:18:15]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_best.pt (epoch 1 @ 1535 updates, score 4.404) (writing took 0.016764 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-18 23:18:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-18 23:18:19]    INFO >> epoch 002:     15 / 1539 loss=4.547, wps=895.9, ups=1.22, wpb=731.3, bsz=731.3, num_updates=1550, lr=0.0004, gnorm=6.276, clip=0, train_wall=14, gb_free=74.4, wall=559 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:18:33]    INFO >> epoch 002:     65 / 1539 loss=4.369, wps=2542.8, ups=3.86, wpb=658.8, bsz=658.8, num_updates=1600, lr=0.0004, gnorm=6.504, clip=0, train_wall=13, gb_free=73.4, wall=572 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:18:48]    INFO >> epoch 002:    115 / 1539 loss=4.455, wps=2411.4, ups=3.37, wpb=714.6, bsz=714.6, num_updates=1650, lr=0.0004, gnorm=6.544, clip=0, train_wall=14, gb_free=65.7, wall=587 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:19:04]    INFO >> epoch 002:    165 / 1539 loss=4.258, wps=2534, ups=3.43, wpb=738.8, bsz=738.8, num_updates=1700, lr=0.0004, gnorm=5.905, clip=0, train_wall=14, gb_free=73.6, wall=602 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:19:18]    INFO >> epoch 002:    215 / 1539 loss=4.498, wps=2555.8, ups=3.63, wpb=704.2, bsz=704.2, num_updates=1750, lr=0.0004, gnorm=5.847, clip=0, train_wall=13, gb_free=71.2, wall=616 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:19:35]    INFO >> epoch 002:    265 / 1539 loss=4.253, wps=2749.4, ups=3.15, wpb=873.5, bsz=873.5, num_updates=1800, lr=0.0004, gnorm=6.843, clip=0, train_wall=15, gb_free=74.7, wall=632 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:19:49]    INFO >> epoch 002:    315 / 1539 loss=4.301, wps=2381.3, ups=3.69, wpb=645.1, bsz=645.1, num_updates=1850, lr=0.0004, gnorm=6.441, clip=0, train_wall=13, gb_free=71.9, wall=645 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:20:02]    INFO >> epoch 002:    365 / 1539 loss=4.265, wps=2361.7, ups=3.6, wpb=655.8, bsz=655.8, num_updates=1900, lr=0.0004, gnorm=6.912, clip=0, train_wall=13, gb_free=74, wall=659 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:20:19]    INFO >> epoch 002:    415 / 1539 loss=4.195, wps=2302.8, ups=3.23, wpb=713.2, bsz=713.2, num_updates=1950, lr=0.0004, gnorm=6.629, clip=0, train_wall=15, gb_free=76.2, wall=674 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:20:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 2.02 GiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 74.28 GiB memory in use. Of the allocated memory 70.77 GiB is allocated by PyTorch, and 3.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:20:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72062 MiB |  72463 MiB |  61909 GiB |  61839 GiB |
|       from large pool |  72045 MiB |  72445 MiB |  61561 GiB |  61491 GiB |
|       from small pool |     17 MiB |     21 MiB |    347 GiB |    347 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72062 MiB |  72463 MiB |  61909 GiB |  61839 GiB |
|       from large pool |  72045 MiB |  72445 MiB |  61561 GiB |  61491 GiB |
|       from small pool |     17 MiB |     21 MiB |    347 GiB |    347 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB |  61773 GiB |  61702 GiB |
|       from large pool |  72027 MiB |  72426 MiB |  61425 GiB |  61355 GiB |
|       from small pool |     17 MiB |     21 MiB |    347 GiB |    347 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  75552 MiB |  75738 MiB | 305576 MiB | 230024 MiB |
|       from large pool |  75528 MiB |  75528 MiB | 304712 MiB | 229184 MiB |
|       from small pool |     24 MiB |    210 MiB |    864 MiB |    840 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3489 MiB |   4990 MiB |  61025 GiB |  61021 GiB |
|       from large pool |   3482 MiB |   4982 MiB |  60629 GiB |  60626 GiB |
|       from small pool |      6 MiB |     27 MiB |    395 GiB |    395 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |    4085 K  |    4085 K  |
|       from large pool |     308    |     315    |    1969 K  |    1968 K  |
|       from small pool |     298    |     355    |    2116 K  |    2116 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |    4085 K  |    4085 K  |
|       from large pool |     308    |     315    |    1969 K  |    1968 K  |
|       from small pool |     298    |     355    |    2116 K  |    2116 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      86    |     179    |    1392    |    1306    |
|       from large pool |      74    |      74    |     960    |     886    |
|       from small pool |      12    |     105    |     432    |     420    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      96    |      96    |    2306 K  |    2306 K  |
|       from large pool |      73    |      73    |    1305 K  |    1304 K  |
|       from small pool |      23    |      51    |    1001 K  |    1001 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:20:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:20:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:20:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:20:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:20:34]    INFO >> epoch 002:    466 / 1539 loss=4.274, wps=2462.2, ups=3.38, wpb=729.4, bsz=729.4, num_updates=2000, lr=0.0004, gnorm=6.027, clip=0, train_wall=13, gb_free=71.6, wall=689 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:20:50]    INFO >> epoch 002:    516 / 1539 loss=4.282, wps=2488.5, ups=3.51, wpb=708.4, bsz=708.4, num_updates=2050, lr=0.0004, gnorm=6.797, clip=2, train_wall=14, gb_free=71.2, wall=703 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:21:03]    INFO >> epoch 002:    566 / 1539 loss=4.154, wps=2298.7, ups=3.69, wpb=622.3, bsz=622.3, num_updates=2100, lr=0.0004, gnorm=6.023, clip=0, train_wall=13, gb_free=74, wall=717 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:21:20]    INFO >> epoch 002:    616 / 1539 loss=4.202, wps=2843.2, ups=3.26, wpb=871.1, bsz=871.1, num_updates=2150, lr=0.0004, gnorm=6.107, clip=0, train_wall=15, gb_free=68.6, wall=732 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:21:35]    INFO >> epoch 002:    666 / 1539 loss=4.022, wps=2578.1, ups=3.34, wpb=772.9, bsz=772.9, num_updates=2200, lr=0.0004, gnorm=7.625, clip=2, train_wall=15, gb_free=68.4, wall=747 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:21:50]    INFO >> epoch 002:    716 / 1539 loss=3.971, wps=2577.6, ups=3.64, wpb=708.6, bsz=708.6, num_updates=2250, lr=0.0004, gnorm=6.664, clip=2, train_wall=13, gb_free=73.1, wall=761 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:22:04]    INFO >> epoch 002:    766 / 1539 loss=4.032, wps=2394.2, ups=3.6, wpb=665.2, bsz=665.2, num_updates=2300, lr=0.0004, gnorm=6.506, clip=2, train_wall=13, gb_free=75.7, wall=775 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:22:19]    INFO >> epoch 002:    816 / 1539 loss=4.287, wps=2235.1, ups=3.41, wpb=656, bsz=656, num_updates=2350, lr=0.0004, gnorm=6.203, clip=0, train_wall=14, gb_free=72.2, wall=790 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:22:35]    INFO >> epoch 002:    866 / 1539 loss=4.181, wps=2371.3, ups=3.29, wpb=719.8, bsz=719.8, num_updates=2400, lr=0.0004, gnorm=6.405, clip=2, train_wall=15, gb_free=75.8, wall=805 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:22:49]    INFO >> epoch 002:    916 / 1539 loss=4.142, wps=2336.1, ups=3.48, wpb=671.5, bsz=671.5, num_updates=2450, lr=0.0004, gnorm=5.72, clip=0, train_wall=14, gb_free=73.1, wall=819 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:23:05]    INFO >> epoch 002:    966 / 1539 loss=4.045, wps=2246.3, ups=3.52, wpb=637.9, bsz=637.9, num_updates=2500, lr=0.0004, gnorm=6.142, clip=0, train_wall=14, gb_free=69.2, wall=833 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:23:07] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 9.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.29 GiB memory in use. Of the allocated memory 74.46 GiB is allocated by PyTorch, and 1.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:23:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 11        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76192 MiB |  76252 MiB |  78094 GiB |  78020 GiB |
|       from large pool |  75826 MiB |  75886 MiB |  77657 GiB |  77583 GiB |
|       from small pool |    365 MiB |    367 MiB |    437 GiB |    436 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76192 MiB |  76252 MiB |  78094 GiB |  78020 GiB |
|       from large pool |  75826 MiB |  75886 MiB |  77657 GiB |  77583 GiB |
|       from small pool |    365 MiB |    367 MiB |    437 GiB |    436 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76156 MiB |  76215 MiB |  77925 GiB |  77851 GiB |
|       from large pool |  75792 MiB |  75851 MiB |  77488 GiB |  77414 GiB |
|       from small pool |    363 MiB |    365 MiB |    436 GiB |    436 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77612 MiB |  77612 MiB | 307636 MiB | 230024 MiB |
|       from large pool |  77208 MiB |  77208 MiB | 306392 MiB | 229184 MiB |
|       from small pool |    404 MiB |    404 MiB |   1244 MiB |    840 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1359 MiB |   4340 MiB |  79805 GiB |  79804 GiB |
|       from large pool |   1321 MiB |   4333 MiB |  79307 GiB |  79306 GiB |
|       from small pool |     38 MiB |     38 MiB |    498 GiB |    497 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6757    |    6760    |    5172 K  |    5165 K  |
|       from large pool |     884    |     885    |    2515 K  |    2514 K  |
|       from small pool |    5873    |    5876    |    2656 K  |    2650 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6757    |    6760    |    5172 K  |    5165 K  |
|       from large pool |     884    |     885    |    2515 K  |    2514 K  |
|       from small pool |    5873    |    5876    |    2656 K  |    2650 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     304    |     304    |    1610    |    1306    |
|       from large pool |     102    |     102    |     988    |     886    |
|       from small pool |     202    |     202    |     622    |     420    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     433    |     433    |    2893 K  |    2893 K  |
|       from large pool |      72    |      72    |    1646 K  |    1646 K  |
|       from small pool |     361    |     361    |    1247 K  |    1247 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:23:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:23:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:23:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:23:07] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-18 23:23:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 37.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.26 GiB memory in use. Of the allocated memory 74.60 GiB is allocated by PyTorch, and 1.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:23:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 13        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76328 MiB |  76388 MiB |  79053 GiB |  78978 GiB |
|       from large pool |  76266 MiB |  76326 MiB |  78610 GiB |  78536 GiB |
|       from small pool |     61 MiB |     62 MiB |    442 GiB |    442 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76328 MiB |  76388 MiB |  79053 GiB |  78978 GiB |
|       from large pool |  76266 MiB |  76326 MiB |  78610 GiB |  78536 GiB |
|       from small pool |     61 MiB |     62 MiB |    442 GiB |    442 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76298 MiB |  76358 MiB |  78882 GiB |  78807 GiB |
|       from large pool |  76237 MiB |  76296 MiB |  78440 GiB |  78366 GiB |
|       from small pool |     61 MiB |     62 MiB |    441 GiB |    441 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77584 MiB |  77612 MiB | 308062 MiB | 230478 MiB |
|       from large pool |  77520 MiB |  77520 MiB | 306812 MiB | 229292 MiB |
|       from small pool |     64 MiB |    404 MiB |   1250 MiB |   1186 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1195 MiB |   8183 MiB |  80822 GiB |  80821 GiB |
|       from large pool |   1193 MiB |   8174 MiB |  80318 GiB |  80317 GiB |
|       from small pool |      2 MiB |     29 MiB |    504 GiB |    504 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1444    |    1447    |    5233 K  |    5231 K  |
|       from large pool |     423    |     424    |    2545 K  |    2544 K  |
|       from small pool |    1021    |    1024    |    2688 K  |    2687 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1444    |    1447    |    5233 K  |    5231 K  |
|       from large pool |     423    |     424    |    2545 K  |    2544 K  |
|       from small pool |    1021    |    1024    |    2688 K  |    2687 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     137    |     304    |    1620    |    1483    |
|       from large pool |     105    |     105    |     995    |     890    |
|       from small pool |      32    |     202    |     625    |     593    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     128    |     129    |    2929 K  |    2929 K  |
|       from large pool |      90    |      95    |    1665 K  |    1665 K  |
|       from small pool |      38    |      59    |    1263 K  |    1263 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:23:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:23:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:23:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:23:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:23:20]    INFO >> epoch 002:   1018 / 1539 loss=4.034, wps=2254.5, ups=3.28, wpb=687.9, bsz=687.9, num_updates=2550, lr=0.0004, gnorm=6.422, clip=2, train_wall=13, gb_free=72.9, wall=849 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:23:37]    INFO >> epoch 002:   1068 / 1539 loss=3.842, wps=2562.8, ups=3.36, wpb=763.7, bsz=763.7, num_updates=2600, lr=0.0004, gnorm=6.84, clip=0, train_wall=14, gb_free=73.5, wall=864 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:23:52]    INFO >> epoch 002:   1118 / 1539 loss=3.71, wps=2563.6, ups=3.24, wpb=791.9, bsz=791.9, num_updates=2650, lr=0.0004, gnorm=7.136, clip=4, train_wall=15, gb_free=69.2, wall=879 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:24:08]    INFO >> epoch 002:   1168 / 1539 loss=4.132, wps=2597.3, ups=3.37, wpb=770.1, bsz=770.1, num_updates=2700, lr=0.0004, gnorm=5.94, clip=0, train_wall=14, gb_free=72.3, wall=894 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:24:23]    INFO >> epoch 002:   1218 / 1539 loss=4.072, wps=2458, ups=3.46, wpb=710, bsz=710, num_updates=2750, lr=0.0004, gnorm=6.149, clip=0, train_wall=14, gb_free=70.8, wall=908 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:24:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.17 GiB. GPU 0 has a total capacity of 79.14 GiB of which 527.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 75.79 GiB memory in use. Of the allocated memory 68.48 GiB is allocated by PyTorch, and 6.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:24:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 17        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  64342 MiB |  70749 MiB |  86281 GiB |  86219 GiB |
|       from large pool |  64329 MiB |  70736 MiB |  85799 GiB |  85736 GiB |
|       from small pool |     12 MiB |     14 MiB |    482 GiB |    482 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  64342 MiB |  70749 MiB |  86281 GiB |  86219 GiB |
|       from large pool |  64329 MiB |  70736 MiB |  85799 GiB |  85736 GiB |
|       from small pool |     12 MiB |     14 MiB |    482 GiB |    482 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  64321 MiB |  70724 MiB |  86095 GiB |  86032 GiB |
|       from large pool |  64308 MiB |  70711 MiB |  85613 GiB |  85550 GiB |
|       from small pool |     12 MiB |     14 MiB |    481 GiB |    481 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77094 MiB |  77544 MiB | 342274 MiB | 265180 MiB |
|       from large pool |  77070 MiB |  77486 MiB | 340862 MiB | 263792 MiB |
|       from small pool |     24 MiB |    226 MiB |   1412 MiB |   1388 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6423 MiB |   9444 MiB |  87778 GiB |  87771 GiB |
|       from large pool |   6412 MiB |   9431 MiB |  87227 GiB |  87221 GiB |
|       from small pool |     11 MiB |     25 MiB |    550 GiB |    550 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     584    |    5710 K  |    5709 K  |
|       from large pool |     275    |     293    |    2780 K  |    2780 K  |
|       from small pool |     291    |     356    |    2929 K  |    2929 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     584    |    5710 K  |    5709 K  |
|       from large pool |     275    |     293    |    2780 K  |    2780 K  |
|       from small pool |     291    |     356    |    2929 K  |    2929 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     165    |     282    |    1785    |    1620    |
|       from large pool |     153    |     169    |    1079    |     926    |
|       from small pool |      12    |     113    |     706    |     694    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     141    |     152    |    3201 K  |    3201 K  |
|       from large pool |     114    |     125    |    1823 K  |    1823 K  |
|       from small pool |      27    |      54    |    1378 K  |    1378 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:24:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:24:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:24:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:24:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:24:39]    INFO >> epoch 002:   1269 / 1539 loss=3.884, wps=2476.3, ups=3.24, wpb=765.3, bsz=765.3, num_updates=2800, lr=0.0004, gnorm=7.131, clip=4, train_wall=14, gb_free=70.5, wall=924 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:24:53]    INFO >> epoch 002:   1319 / 1539 loss=4.043, wps=2397.4, ups=3.62, wpb=661.5, bsz=661.5, num_updates=2850, lr=0.0004, gnorm=5.432, clip=0, train_wall=13, gb_free=75, wall=938 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:25:07]    INFO >> epoch 002:   1369 / 1539 loss=3.952, wps=2550.5, ups=3.5, wpb=728.5, bsz=728.5, num_updates=2900, lr=0.0004, gnorm=6.268, clip=0, train_wall=14, gb_free=70.5, wall=952 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:25:22]    INFO >> epoch 002:   1419 / 1539 loss=3.979, wps=2287.9, ups=3.5, wpb=653.6, bsz=653.6, num_updates=2950, lr=0.0004, gnorm=5.527, clip=0, train_wall=14, gb_free=64.8, wall=966 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:25:37]    INFO >> epoch 002:   1469 / 1539 loss=3.965, wps=2230.9, ups=3.18, wpb=702.5, bsz=702.5, num_updates=3000, lr=0.0004, gnorm=6.202, clip=0, train_wall=15, gb_free=70.3, wall=982 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:25:55]    INFO >> epoch 002:   1519 / 1539 loss=4.04, wps=2335.9, ups=3.47, wpb=673.3, bsz=673.3, num_updates=3050, lr=0.0004, gnorm=5.318, clip=0, train_wall=14, gb_free=74.2, wall=996 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:26:01]    INFO >> epoch 002 | loss 4.126 | wps 2308.8 | ups 3.24 | wpb 712.7 | bsz 712.7 | num_updates 3070 | lr 0.0004 | gnorm 6.334 | clip 0.7 | train_wall 430 | gb_free 72.4 | wall 1003 (progress_bar.py:267, print())[0m
[33m[2025-11-18 23:26:01] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-18 23:26:27]    INFO >> epoch 002 | valid on 'valid' subset | loss 3.934 | wps 6118.6 | wpb 5412.5 | bsz 5412.5 | num_updates 3070 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-18 23:26:28]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-18 23:26:28]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 2 @ 3070 updates, score 3.934) (writing took 0.013156 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-18 23:26:28] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-18 23:26:36]    INFO >> epoch 003:     30 / 1539 loss=3.908, wps=858.9, ups=1.26, wpb=681.1, bsz=681.1, num_updates=3100, lr=0.000392, gnorm=6.896, clip=0, train_wall=14, gb_free=70.7, wall=1036 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:26:52]    INFO >> epoch 003:     80 / 1539 loss=3.91, wps=2635.1, ups=3.41, wpb=772.8, bsz=772.8, num_updates=3150, lr=0.000392, gnorm=5.839, clip=0, train_wall=14, gb_free=73.4, wall=1051 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:26:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 2.02 GiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 74.28 GiB memory in use. Of the allocated memory 68.10 GiB is allocated by PyTorch, and 5.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:26:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 19        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69330 MiB |  70019 MiB | 102358 GiB | 102290 GiB |
|       from large pool |  69313 MiB |  70002 MiB | 101779 GiB | 101711 GiB |
|       from small pool |     17 MiB |     19 MiB |    579 GiB |    579 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69330 MiB |  70019 MiB | 102358 GiB | 102290 GiB |
|       from large pool |  69313 MiB |  70002 MiB | 101779 GiB | 101711 GiB |
|       from small pool |     17 MiB |     19 MiB |    579 GiB |    579 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69318 MiB |  70006 MiB | 102140 GiB | 102073 GiB |
|       from large pool |  69300 MiB |  69989 MiB | 101562 GiB | 101494 GiB |
|       from small pool |     17 MiB |     19 MiB |    578 GiB |    578 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  75550 MiB |  76134 MiB | 350370 MiB | 274820 MiB |
|       from large pool |  75524 MiB |  76010 MiB | 348858 MiB | 273334 MiB |
|       from small pool |     26 MiB |    124 MiB |   1512 MiB |   1486 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6219 MiB |   8497 MiB | 101281 GiB | 101275 GiB |
|       from large pool |   6210 MiB |   8488 MiB | 100625 GiB | 100619 GiB |
|       from small pool |      8 MiB |     29 MiB |    655 GiB |    655 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |    6736 K  |    6736 K  |
|       from large pool |     307    |     315    |    3208 K  |    3207 K  |
|       from small pool |     298    |     356    |    3528 K  |    3528 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |    6736 K  |    6736 K  |
|       from large pool |     307    |     315    |    3208 K  |    3207 K  |
|       from small pool |     298    |     356    |    3528 K  |    3528 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     152    |     207    |    1838    |    1686    |
|       from large pool |     139    |     145    |    1082    |     943    |
|       from small pool |      13    |      62    |     756    |     743    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     151    |     151    |    3801 K  |    3801 K  |
|       from large pool |     123    |     123    |    2110 K  |    2110 K  |
|       from small pool |      28    |      57    |    1691 K  |    1691 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:26:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:26:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:26:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:26:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:27:07]    INFO >> epoch 003:    131 / 1539 loss=3.895, wps=2689.8, ups=3.22, wpb=835, bsz=835, num_updates=3200, lr=0.000392, gnorm=5.823, clip=0, train_wall=14, gb_free=71.8, wall=1066 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:27:21]    INFO >> epoch 003:    181 / 1539 loss=4.09, wps=2447.6, ups=3.73, wpb=656.8, bsz=656.8, num_updates=3250, lr=0.000392, gnorm=4.937, clip=0, train_wall=13, gb_free=73, wall=1080 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:27:32] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 61.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.24 GiB memory in use. Of the allocated memory 71.03 GiB is allocated by PyTorch, and 4.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:27:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  70354 MiB |  72929 MiB | 105886 GiB | 105817 GiB |
|       from large pool |  70341 MiB |  72916 MiB | 105286 GiB | 105217 GiB |
|       from small pool |     12 MiB |     21 MiB |    600 GiB |    600 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  70354 MiB |  72929 MiB | 105886 GiB | 105817 GiB |
|       from large pool |  70341 MiB |  72916 MiB | 105286 GiB | 105217 GiB |
|       from small pool |     12 MiB |     21 MiB |    600 GiB |    600 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  70335 MiB |  72908 MiB | 105661 GiB | 105592 GiB |
|       from large pool |  70322 MiB |  72895 MiB | 105062 GiB | 104993 GiB |
|       from small pool |     12 MiB |     21 MiB |    599 GiB |    599 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77560 MiB |  77560 MiB | 352938 MiB | 275378 MiB |
|       from large pool |  77532 MiB |  77532 MiB | 351242 MiB | 273710 MiB |
|       from small pool |     28 MiB |    210 MiB |   1696 MiB |   1668 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5303 MiB |   8809 MiB | 104750 GiB | 104745 GiB |
|       from large pool |   5288 MiB |   8793 MiB | 104070 GiB | 104065 GiB |
|       from small pool |     15 MiB |     29 MiB |    679 GiB |    679 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     587    |     596    |    6976 K  |    6975 K  |
|       from large pool |     296    |     305    |    3322 K  |    3321 K  |
|       from small pool |     291    |     356    |    3654 K  |    3653 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     587    |     596    |    6976 K  |    6975 K  |
|       from large pool |     296    |     305    |    3322 K  |    3321 K  |
|       from small pool |     291    |     356    |    3654 K  |    3653 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     154    |     244    |    1932    |    1778    |
|       from large pool |     140    |     140    |    1084    |     944    |
|       from small pool |      14    |     105    |     848    |     834    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     149    |     149    |    3940 K  |    3939 K  |
|       from large pool |     118    |     118    |    2186 K  |    2186 K  |
|       from small pool |      31    |      53    |    1753 K  |    1753 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:27:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:27:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:27:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:27:32] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:27:38]    INFO >> epoch 003:    232 / 1539 loss=3.871, wps=2341.9, ups=3.05, wpb=767.2, bsz=767.2, num_updates=3300, lr=0.000392, gnorm=5.517, clip=0, train_wall=15, gb_free=74, wall=1096 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:27:53]    INFO >> epoch 003:    282 / 1539 loss=3.844, wps=2614.5, ups=3.48, wpb=751.9, bsz=751.9, num_updates=3350, lr=0.000392, gnorm=5.373, clip=0, train_wall=14, gb_free=70.6, wall=1110 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:28:10]    INFO >> epoch 003:    332 / 1539 loss=3.953, wps=2555.7, ups=3.23, wpb=790.4, bsz=790.4, num_updates=3400, lr=0.000392, gnorm=5.215, clip=0, train_wall=15, gb_free=73.8, wall=1126 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:28:25]    INFO >> epoch 003:    382 / 1539 loss=3.846, wps=2229.2, ups=3.24, wpb=688.4, bsz=688.4, num_updates=3450, lr=0.000392, gnorm=6.817, clip=0, train_wall=15, gb_free=72.5, wall=1141 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:28:41]    INFO >> epoch 003:    432 / 1539 loss=3.97, wps=2310.5, ups=3.51, wpb=657.8, bsz=657.8, num_updates=3500, lr=0.000392, gnorm=5.559, clip=0, train_wall=14, gb_free=66.2, wall=1155 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:28:56]    INFO >> epoch 003:    482 / 1539 loss=3.902, wps=2469.1, ups=3.27, wpb=754, bsz=754, num_updates=3550, lr=0.000392, gnorm=5.61, clip=0, train_wall=15, gb_free=73.1, wall=1171 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:29:11]    INFO >> epoch 003:    532 / 1539 loss=3.733, wps=2722.4, ups=3.53, wpb=770.5, bsz=770.5, num_updates=3600, lr=0.000392, gnorm=6.494, clip=2, train_wall=14, gb_free=73.8, wall=1185 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:29:27]    INFO >> epoch 003:    582 / 1539 loss=3.864, wps=2319.8, ups=3.22, wpb=720.1, bsz=720.1, num_updates=3650, lr=0.000392, gnorm=6.092, clip=0, train_wall=15, gb_free=71.6, wall=1200 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:29:42]    INFO >> epoch 003:    632 / 1539 loss=3.919, wps=2518.7, ups=3.73, wpb=674.5, bsz=674.5, num_updates=3700, lr=0.000392, gnorm=5.651, clip=0, train_wall=13, gb_free=66.5, wall=1214 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:29:56]    INFO >> epoch 003:    682 / 1539 loss=3.907, wps=2647.3, ups=3.5, wpb=756.5, bsz=756.5, num_updates=3750, lr=0.000392, gnorm=5.619, clip=0, train_wall=14, gb_free=75.1, wall=1228 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:30:12]    INFO >> epoch 003:    732 / 1539 loss=3.62, wps=2456.1, ups=3.03, wpb=810.3, bsz=810.3, num_updates=3800, lr=0.000392, gnorm=6.016, clip=2, train_wall=16, gb_free=73.9, wall=1245 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:30:27]    INFO >> epoch 003:    782 / 1539 loss=3.407, wps=2657.1, ups=3.34, wpb=795, bsz=795, num_updates=3850, lr=0.000392, gnorm=6.269, clip=2, train_wall=15, gb_free=73.7, wall=1260 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:30:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 77.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.22 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 5.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:30:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 23        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69008 MiB |  71765 MiB | 123385 GiB | 123318 GiB |
|       from large pool |  68991 MiB |  71748 MiB | 122689 GiB | 122622 GiB |
|       from small pool |     16 MiB |     18 MiB |    696 GiB |    696 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69008 MiB |  71765 MiB | 123385 GiB | 123318 GiB |
|       from large pool |  68991 MiB |  71748 MiB | 122689 GiB | 122622 GiB |
|       from small pool |     16 MiB |     18 MiB |    696 GiB |    696 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 123122 GiB | 123055 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 122427 GiB | 122360 GiB |
|       from small pool |     16 MiB |     18 MiB |    695 GiB |    695 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77544 MiB |  77544 MiB | 355046 MiB | 277502 MiB |
|       from large pool |  77520 MiB |  77520 MiB | 353132 MiB | 275612 MiB |
|       from small pool |     24 MiB |    246 MiB |   1914 MiB |   1890 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6645 MiB |   9885 MiB | 122145 GiB | 122139 GiB |
|       from large pool |   6638 MiB |   9878 MiB | 121354 GiB | 121348 GiB |
|       from small pool |      7 MiB |     29 MiB |    790 GiB |    790 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |    8130 K  |    8129 K  |
|       from large pool |     343    |     351    |    3898 K  |    3898 K  |
|       from small pool |     298    |     356    |    4231 K  |    4231 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |    8130 K  |    8129 K  |
|       from large pool |     343    |     351    |    3898 K  |    3898 K  |
|       from small pool |     298    |     356    |    4231 K  |    4231 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     152    |     262    |    2042    |    1890    |
|       from large pool |     140    |     140    |    1085    |     945    |
|       from small pool |      12    |     123    |     957    |     945    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     137    |     141    |    4589 K  |    4589 K  |
|       from large pool |     111    |     115    |    2566 K  |    2566 K  |
|       from small pool |      26    |      57    |    2023 K  |    2023 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:30:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:30:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:30:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:30:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:30:43]    INFO >> epoch 003:    833 / 1539 loss=3.793, wps=2402.3, ups=3.26, wpb=736, bsz=736, num_updates=3900, lr=0.000392, gnorm=6.835, clip=0, train_wall=14, gb_free=73, wall=1275 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:30:57]    INFO >> epoch 003:    883 / 1539 loss=4, wps=2569.7, ups=3.58, wpb=717, bsz=717, num_updates=3950, lr=0.000392, gnorm=6.665, clip=0, train_wall=14, gb_free=72.5, wall=1289 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:31:12]    INFO >> epoch 003:    933 / 1539 loss=3.774, wps=2411.9, ups=3.31, wpb=729.5, bsz=729.5, num_updates=4000, lr=0.000392, gnorm=6.234, clip=0, train_wall=15, gb_free=67.3, wall=1304 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:31:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.57 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.92 GiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 74.38 GiB memory in use. Of the allocated memory 66.03 GiB is allocated by PyTorch, and 7.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:31:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 24        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63501 MiB |  70133 MiB | 128251 GiB | 128189 GiB |
|       from large pool |  63481 MiB |  70112 MiB | 127530 GiB | 127468 GiB |
|       from small pool |     20 MiB |     60 MiB |    721 GiB |    721 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63501 MiB |  70133 MiB | 128251 GiB | 128189 GiB |
|       from large pool |  63481 MiB |  70112 MiB | 127530 GiB | 127468 GiB |
|       from small pool |     20 MiB |     60 MiB |    721 GiB |    721 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63481 MiB |  70111 MiB | 127977 GiB | 127915 GiB |
|       from large pool |  63461 MiB |  70091 MiB | 127257 GiB | 127195 GiB |
|       from small pool |     20 MiB |     60 MiB |    720 GiB |    720 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  75656 MiB |  75696 MiB | 355088 MiB | 279432 MiB |
|       from large pool |  75630 MiB |  75630 MiB | 353132 MiB | 277502 MiB |
|       from small pool |     26 MiB |     66 MiB |   1956 MiB |   1930 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6698 MiB |   9384 MiB | 126847 GiB | 126841 GiB |
|       from large pool |   6692 MiB |   9379 MiB | 126028 GiB | 126022 GiB |
|       from small pool |      5 MiB |     19 MiB |    819 GiB |    819 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |    1326    |    8439 K  |    8438 K  |
|       from large pool |     330    |     403    |    4057 K  |    4057 K  |
|       from small pool |     316    |     924    |    4381 K  |    4381 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |    1326    |    8439 K  |    8438 K  |
|       from large pool |     330    |     403    |    4057 K  |    4057 K  |
|       from small pool |     316    |     924    |    4381 K  |    4381 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     152    |     172    |    2063    |    1911    |
|       from large pool |     139    |     139    |    1085    |     946    |
|       from small pool |      13    |      33    |     978    |     965    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     141    |     160    |    4761 K  |    4761 K  |
|       from large pool |     109    |     113    |    2670 K  |    2670 K  |
|       from small pool |      32    |      53    |    2090 K  |    2090 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:31:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:31:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:31:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:31:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:31:28]    INFO >> epoch 003:    984 / 1539 loss=3.95, wps=1955.9, ups=3.11, wpb=628.7, bsz=628.7, num_updates=4050, lr=0.000392, gnorm=5.214, clip=0, train_wall=14, gb_free=70.1, wall=1320 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:31:42]    INFO >> epoch 003:   1034 / 1539 loss=3.855, wps=2244.8, ups=3.52, wpb=637.6, bsz=637.6, num_updates=4100, lr=0.000392, gnorm=5.116, clip=0, train_wall=14, gb_free=72.5, wall=1334 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:32:01]    INFO >> epoch 003:   1084 / 1539 loss=3.777, wps=2561.2, ups=3.77, wpb=679.5, bsz=679.5, num_updates=4150, lr=0.000392, gnorm=6.732, clip=2, train_wall=13, gb_free=69.8, wall=1348 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:32:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 35.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.27 GiB memory in use. Of the allocated memory 73.51 GiB is allocated by PyTorch, and 2.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:32:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 25        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75211 MiB |  75271 MiB | 133516 GiB | 133442 GiB |
|       from large pool |  74855 MiB |  74915 MiB | 132765 GiB | 132692 GiB |
|       from small pool |    355 MiB |    356 MiB |    751 GiB |    750 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75211 MiB |  75271 MiB | 133516 GiB | 133442 GiB |
|       from large pool |  74855 MiB |  74915 MiB | 132765 GiB | 132692 GiB |
|       from small pool |    355 MiB |    356 MiB |    751 GiB |    750 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75135 MiB |  75194 MiB | 133230 GiB | 133157 GiB |
|       from large pool |  74781 MiB |  74841 MiB | 132480 GiB | 132407 GiB |
|       from small pool |    353 MiB |    354 MiB |    749 GiB |    749 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77586 MiB |  77588 MiB | 362476 MiB | 284890 MiB |
|       from large pool |  77194 MiB |  77194 MiB | 360152 MiB | 282958 MiB |
|       from small pool |    392 MiB |    394 MiB |   2324 MiB |   1932 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2314 MiB |   5828 MiB | 132018 GiB | 132016 GiB |
|       from large pool |   2278 MiB |   5822 MiB | 131165 GiB | 131163 GiB |
|       from small pool |     36 MiB |     37 MiB |    853 GiB |    853 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6570    |    6573    |    8800 K  |    8793 K  |
|       from large pool |     867    |     868    |    4236 K  |    4235 K  |
|       from small pool |    5703    |    5706    |    4563 K  |    4557 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6570    |    6573    |    8800 K  |    8793 K  |
|       from large pool |     867    |     868    |    4236 K  |    4235 K  |
|       from small pool |    5703    |    5706    |    4563 K  |    4557 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     450    |     451    |    2364    |    1914    |
|       from large pool |     254    |     254    |    1202    |     948    |
|       from small pool |     196    |     197    |    1162    |     966    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     483    |     483    |    4962 K  |    4962 K  |
|       from large pool |     131    |     131    |    2789 K  |    2789 K  |
|       from small pool |     352    |     352    |    2173 K  |    2172 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:32:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:32:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:32:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:32:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:32:15]    INFO >> epoch 003:   1135 / 1539 loss=3.843, wps=2418, ups=3.53, wpb=685.9, bsz=685.9, num_updates=4200, lr=0.000392, gnorm=5.085, clip=0, train_wall=13, gb_free=75.1, wall=1362 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:32:31]    INFO >> epoch 003:   1185 / 1539 loss=3.743, wps=2246.1, ups=3.36, wpb=668, bsz=668, num_updates=4250, lr=0.000392, gnorm=5.537, clip=0, train_wall=14, gb_free=62.1, wall=1377 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:32:46]    INFO >> epoch 003:   1235 / 1539 loss=3.805, wps=2391.7, ups=3.33, wpb=717.9, bsz=717.9, num_updates=4300, lr=0.000392, gnorm=6.166, clip=0, train_wall=15, gb_free=73.2, wall=1392 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:32:59]    INFO >> epoch 003:   1285 / 1539 loss=3.8, wps=2332.7, ups=3.77, wpb=619.2, bsz=619.2, num_updates=4350, lr=0.000392, gnorm=4.994, clip=0, train_wall=13, gb_free=75, wall=1405 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:33:13]    INFO >> epoch 003:   1335 / 1539 loss=3.771, wps=2635.3, ups=3.65, wpb=721.1, bsz=721.1, num_updates=4400, lr=0.000392, gnorm=5.872, clip=2, train_wall=13, gb_free=67.6, wall=1419 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:33:27]    INFO >> epoch 003:   1385 / 1539 loss=3.739, wps=2488.1, ups=3.66, wpb=680.4, bsz=680.4, num_updates=4450, lr=0.000392, gnorm=5.908, clip=0, train_wall=13, gb_free=73.3, wall=1432 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:33:43]    INFO >> epoch 003:   1435 / 1539 loss=3.899, wps=2466.4, ups=3.75, wpb=657.2, bsz=657.2, num_updates=4500, lr=0.000392, gnorm=4.877, clip=0, train_wall=13, gb_free=73.3, wall=1446 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:33:57]    INFO >> epoch 003:   1485 / 1539 loss=3.759, wps=2384.7, ups=3.58, wpb=665.8, bsz=665.8, num_updates=4550, lr=0.000392, gnorm=5.167, clip=0, train_wall=14, gb_free=67.3, wall=1460 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:34:12]    INFO >> epoch 003:   1535 / 1539 loss=3.801, wps=2339.2, ups=3.55, wpb=658.1, bsz=658.1, num_updates=4600, lr=0.000392, gnorm=5.871, clip=2, train_wall=14, gb_free=71.4, wall=1474 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:34:13]    INFO >> epoch 003 | loss 3.832 | wps 2312 | ups 3.25 | wpb 711.1 | bsz 711.1 | num_updates 4604 | lr 0.000392 | gnorm 5.806 | clip 0.4 | train_wall 429 | gb_free 74.4 | wall 1475 (progress_bar.py:267, print())[0m
[33m[2025-11-18 23:34:13] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-18 23:34:38]    INFO >> epoch 003 | valid on 'valid' subset | loss 3.894 | wps 6095.4 | wpb 5412.5 | bsz 5412.5 | num_updates 4604 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-18 23:34:38]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-18 23:34:38]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 3 @ 4604 updates, score 3.894) (writing took 0.013379 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-18 23:34:38] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-18 23:34:54]    INFO >> epoch 004:     46 / 1539 loss=3.637, wps=924.3, ups=1.24, wpb=743.3, bsz=743.3, num_updates=4650, lr=0.000376, gnorm=5.495, clip=0, train_wall=15, gb_free=73.2, wall=1514 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:35:10]    INFO >> epoch 004:     96 / 1539 loss=3.726, wps=2690.1, ups=3.04, wpb=886.3, bsz=886.3, num_updates=4700, lr=0.000376, gnorm=5.335, clip=2, train_wall=16, gb_free=31.5, wall=1530 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:35:29]    INFO >> epoch 004:    146 / 1539 loss=3.382, wps=2621.7, ups=2.85, wpb=921.3, bsz=921.3, num_updates=4750, lr=0.000376, gnorm=6.706, clip=2, train_wall=17, gb_free=75.8, wall=1548 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:35:43]    INFO >> epoch 004:    196 / 1539 loss=3.782, wps=2524.9, ups=3.59, wpb=703.7, bsz=703.7, num_updates=4800, lr=0.000376, gnorm=5.242, clip=2, train_wall=14, gb_free=70.1, wall=1562 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:35:59]    INFO >> epoch 004:    246 / 1539 loss=3.772, wps=2468.9, ups=3.35, wpb=737.9, bsz=737.9, num_updates=4850, lr=0.000376, gnorm=5.254, clip=0, train_wall=15, gb_free=69.4, wall=1577 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:36:13]    INFO >> epoch 004:    296 / 1539 loss=3.823, wps=2345.5, ups=3.62, wpb=648.1, bsz=648.1, num_updates=4900, lr=0.000376, gnorm=5.346, clip=2, train_wall=13, gb_free=71.5, wall=1591 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:36:28]    INFO >> epoch 004:    346 / 1539 loss=3.741, wps=2547.6, ups=3.71, wpb=687, bsz=687, num_updates=4950, lr=0.000376, gnorm=4.497, clip=0, train_wall=13, gb_free=70.5, wall=1604 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:36:42]    INFO >> epoch 004:    396 / 1539 loss=3.549, wps=2639.8, ups=3.46, wpb=764, bsz=764, num_updates=5000, lr=0.000376, gnorm=5.66, clip=0, train_wall=14, gb_free=72.9, wall=1619 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:37:00]    INFO >> epoch 004:    446 / 1539 loss=3.844, wps=1977.4, ups=3.12, wpb=634, bsz=634, num_updates=5050, lr=0.000376, gnorm=5.302, clip=0, train_wall=15, gb_free=62.6, wall=1635 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:37:14]    INFO >> epoch 004:    496 / 1539 loss=3.785, wps=2535.9, ups=3.54, wpb=716.9, bsz=716.9, num_updates=5100, lr=0.000376, gnorm=5.445, clip=0, train_wall=14, gb_free=60.7, wall=1649 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:37:28]    INFO >> epoch 004:    546 / 1539 loss=3.778, wps=2501.6, ups=3.53, wpb=708.9, bsz=708.9, num_updates=5150, lr=0.000376, gnorm=5.538, clip=0, train_wall=14, gb_free=73.2, wall=1663 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:37:44]    INFO >> epoch 004:    596 / 1539 loss=3.814, wps=2175.5, ups=3.51, wpb=620.1, bsz=620.1, num_updates=5200, lr=0.000376, gnorm=4.079, clip=0, train_wall=14, gb_free=74.9, wall=1677 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:37:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 347.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 75.96 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 5.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:37:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 27        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69008 MiB |  71766 MiB | 167309 GiB | 167242 GiB |
|       from large pool |  68991 MiB |  71749 MiB | 166364 GiB | 166297 GiB |
|       from small pool |     16 MiB |     20 MiB |    945 GiB |    945 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69008 MiB |  71766 MiB | 167309 GiB | 167242 GiB |
|       from large pool |  68991 MiB |  71749 MiB | 166364 GiB | 166297 GiB |
|       from small pool |     16 MiB |     20 MiB |    945 GiB |    945 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 166952 GiB | 166884 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 166008 GiB | 165941 GiB |
|       from small pool |     16 MiB |     20 MiB |    943 GiB |    943 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77274 MiB |  77526 MiB | 368146 MiB | 290872 MiB |
|       from large pool |  77248 MiB |  77248 MiB | 365822 MiB | 288574 MiB |
|       from small pool |     26 MiB |    392 MiB |   2324 MiB |   2298 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6375 MiB |  10186 MiB | 161585 GiB | 161579 GiB |
|       from large pool |   6366 MiB |  10177 MiB | 160514 GiB | 160508 GiB |
|       from small pool |      9 MiB |     29 MiB |   1070 GiB |   1070 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   11020 K  |   11020 K  |
|       from large pool |     343    |     351    |    5268 K  |    5268 K  |
|       from small pool |     298    |     355    |    5751 K  |    5751 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   11020 K  |   11020 K  |
|       from large pool |     343    |     351    |    5268 K  |    5268 K  |
|       from small pool |     298    |     355    |    5751 K  |    5751 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     178    |     449    |    2367    |    2189    |
|       from large pool |     165    |     253    |    1205    |    1040    |
|       from small pool |      13    |     196    |    1162    |    1149    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     164    |     166    |    6237 K  |    6237 K  |
|       from large pool |     138    |     140    |    3483 K  |    3483 K  |
|       from small pool |      26    |      57    |    2753 K  |    2753 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:37:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:37:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:37:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:37:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:37:58]    INFO >> epoch 004:    647 / 1539 loss=3.569, wps=2390, ups=3.52, wpb=679.2, bsz=679.2, num_updates=5250, lr=0.000376, gnorm=5.099, clip=0, train_wall=13, gb_free=70.9, wall=1691 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:38:12] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.77 GiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 74.53 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 3.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:38:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 29        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72060 MiB |  72460 MiB | 169909 GiB | 169839 GiB |
|       from large pool |  72042 MiB |  72443 MiB | 168951 GiB | 168880 GiB |
|       from small pool |     17 MiB |     24 MiB |    958 GiB |    958 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72060 MiB |  72460 MiB | 169909 GiB | 169839 GiB |
|       from large pool |  72042 MiB |  72443 MiB | 168951 GiB | 168880 GiB |
|       from small pool |     17 MiB |     24 MiB |    958 GiB |    958 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 169546 GiB | 169475 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 168589 GiB | 168518 GiB |
|       from small pool |     17 MiB |     24 MiB |    956 GiB |    956 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  75810 MiB |  75810 MiB | 384002 MiB | 308192 MiB |
|       from large pool |  75786 MiB |  75786 MiB | 381648 MiB | 305862 MiB |
|       from small pool |     24 MiB |     56 MiB |   2354 MiB |   2330 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3749 MiB |   7475 MiB | 164119 GiB | 164115 GiB |
|       from large pool |   3743 MiB |   7464 MiB | 163033 GiB | 163029 GiB |
|       from small pool |      6 MiB |     23 MiB |   1086 GiB |   1086 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   11190 K  |   11189 K  |
|       from large pool |     308    |     315    |    5358 K  |    5358 K  |
|       from small pool |     298    |     356    |    5831 K  |    5831 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   11190 K  |   11189 K  |
|       from large pool |     308    |     315    |    5358 K  |    5358 K  |
|       from small pool |     298    |     356    |    5831 K  |    5831 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     146    |     192    |    2392    |    2246    |
|       from large pool |     134    |     164    |    1215    |    1081    |
|       from small pool |      12    |      28    |    1177    |    1165    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     139    |     139    |    6330 K  |    6329 K  |
|       from large pool |     112    |     112    |    3542 K  |    3542 K  |
|       from small pool |      27    |      49    |    2787 K  |    2787 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:38:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:38:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:38:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:38:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:38:14]    INFO >> epoch 004:    698 / 1539 loss=3.765, wps=2124.5, ups=3.36, wpb=631.8, bsz=631.8, num_updates=5300, lr=0.000376, gnorm=4.54, clip=0, train_wall=13, gb_free=73.1, wall=1706 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:38:31]    INFO >> epoch 004:    748 / 1539 loss=3.757, wps=2049.4, ups=3.02, wpb=679.3, bsz=679.3, num_updates=5350, lr=0.000376, gnorm=4.752, clip=0, train_wall=16, gb_free=73.3, wall=1723 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:38:46]    INFO >> epoch 004:    798 / 1539 loss=3.678, wps=2473.2, ups=3.48, wpb=710.2, bsz=710.2, num_updates=5400, lr=0.000376, gnorm=4.729, clip=0, train_wall=14, gb_free=72, wall=1737 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:39:01]    INFO >> epoch 004:    848 / 1539 loss=3.568, wps=2585.8, ups=3.5, wpb=738.3, bsz=738.3, num_updates=5450, lr=0.000376, gnorm=5.988, clip=0, train_wall=14, gb_free=74.5, wall=1751 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:39:17]    INFO >> epoch 004:    898 / 1539 loss=3.764, wps=2571.9, ups=3.29, wpb=781.1, bsz=781.1, num_updates=5500, lr=0.000376, gnorm=5.397, clip=0, train_wall=15, gb_free=73.7, wall=1767 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:39:31]    INFO >> epoch 004:    948 / 1539 loss=3.727, wps=2377.1, ups=3.67, wpb=647.5, bsz=647.5, num_updates=5550, lr=0.000376, gnorm=4.237, clip=0, train_wall=13, gb_free=73.1, wall=1780 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:39:48]    INFO >> epoch 004:    998 / 1539 loss=3.892, wps=2465.5, ups=3.26, wpb=756.2, bsz=756.2, num_updates=5600, lr=0.000376, gnorm=5.131, clip=0, train_wall=15, gb_free=74.2, wall=1796 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:40:02]    INFO >> epoch 004:   1048 / 1539 loss=3.814, wps=2272, ups=3.57, wpb=637.1, bsz=637.1, num_updates=5650, lr=0.000376, gnorm=4.562, clip=0, train_wall=14, gb_free=72.6, wall=1810 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:40:16]    INFO >> epoch 004:   1098 / 1539 loss=3.753, wps=2558.8, ups=3.52, wpb=726.8, bsz=726.8, num_updates=5700, lr=0.000376, gnorm=4.569, clip=0, train_wall=14, gb_free=73.3, wall=1824 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:40:31]    INFO >> epoch 004:   1148 / 1539 loss=3.649, wps=2451.8, ups=3.59, wpb=682.9, bsz=682.9, num_updates=5750, lr=0.000376, gnorm=4.529, clip=0, train_wall=13, gb_free=68, wall=1838 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:40:46]    INFO >> epoch 004:   1198 / 1539 loss=3.67, wps=2539.2, ups=3.38, wpb=750.3, bsz=750.3, num_updates=5800, lr=0.000376, gnorm=4.804, clip=0, train_wall=14, gb_free=65.5, wall=1853 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:41:00]    INFO >> epoch 004:   1248 / 1539 loss=3.651, wps=2433.9, ups=3.9, wpb=624.8, bsz=624.8, num_updates=5850, lr=0.000376, gnorm=4.574, clip=0, train_wall=12, gb_free=72, wall=1865 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:41:15]    INFO >> epoch 004:   1298 / 1539 loss=3.677, wps=2175.1, ups=3.34, wpb=651.5, bsz=651.5, num_updates=5900, lr=0.000376, gnorm=4.687, clip=2, train_wall=15, gb_free=70.1, wall=1880 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:41:30]    INFO >> epoch 004:   1348 / 1539 loss=3.557, wps=2749.4, ups=3.42, wpb=804.5, bsz=804.5, num_updates=5950, lr=0.000376, gnorm=5.751, clip=2, train_wall=14, gb_free=71.5, wall=1895 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:41:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 199.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.11 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 2.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:41:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 31        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72821 MiB |  75424 MiB | 190606 GiB | 190535 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 189538 GiB | 189467 GiB |
|       from small pool |     12 MiB |     13 MiB |   1067 GiB |   1067 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72821 MiB |  75424 MiB | 190606 GiB | 190535 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 189538 GiB | 189467 GiB |
|       from small pool |     12 MiB |     13 MiB |   1067 GiB |   1067 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 190196 GiB | 190125 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 189130 GiB | 189059 GiB |
|       from small pool |     12 MiB |     13 MiB |   1066 GiB |   1066 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77422 MiB |  77482 MiB | 388536 MiB | 311114 MiB |
|       from large pool |  77398 MiB |  77458 MiB | 385960 MiB | 308562 MiB |
|       from small pool |     24 MiB |    246 MiB |   2576 MiB |   2552 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2698 MiB |   7422 MiB | 185067 GiB | 185064 GiB |
|       from large pool |   2687 MiB |   7410 MiB | 183855 GiB | 183852 GiB |
|       from small pool |     11 MiB |     19 MiB |   1211 GiB |   1211 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   12540 K  |   12540 K  |
|       from large pool |     312    |     320    |    6050 K  |    6049 K  |
|       from small pool |     291    |     336    |    6490 K  |    6490 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   12540 K  |   12540 K  |
|       from large pool |     312    |     320    |    6050 K  |    6049 K  |
|       from small pool |     291    |     336    |    6490 K  |    6490 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     107    |     257    |    2509    |    2402    |
|       from large pool |      95    |     134    |    1221    |    1126    |
|       from small pool |      12    |     123    |    1288    |    1276    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     107    |     107    |    7072 K  |    7072 K  |
|       from large pool |      80    |      80    |    3991 K  |    3991 K  |
|       from small pool |      27    |      48    |    3081 K  |    3081 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:41:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:41:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:41:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:41:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:41:45]    INFO >> epoch 004:   1399 / 1539 loss=3.714, wps=2509, ups=3.2, wpb=783.3, bsz=783.3, num_updates=6000, lr=0.000376, gnorm=5.554, clip=0, train_wall=14, gb_free=73.6, wall=1911 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:42:02]    INFO >> epoch 004:   1449 / 1539 loss=3.706, wps=2484.3, ups=3.5, wpb=709.1, bsz=709.1, num_updates=6050, lr=0.000376, gnorm=4.594, clip=0, train_wall=14, gb_free=72.3, wall=1925 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:42:17]    INFO >> epoch 004:   1499 / 1539 loss=3.724, wps=2382.4, ups=3.48, wpb=685.1, bsz=685.1, num_updates=6100, lr=0.000376, gnorm=4.909, clip=0, train_wall=14, gb_free=71.8, wall=1939 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:42:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 43.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.26 GiB memory in use. Of the allocated memory 74.28 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:42:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76006 MiB |  76066 MiB | 194065 GiB | 193990 GiB |
|       from large pool |  75642 MiB |  75702 MiB | 192976 GiB | 192902 GiB |
|       from small pool |    364 MiB |    365 MiB |   1089 GiB |   1088 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76006 MiB |  76066 MiB | 194065 GiB | 193990 GiB |
|       from large pool |  75642 MiB |  75702 MiB | 192976 GiB | 192902 GiB |
|       from small pool |    364 MiB |    365 MiB |   1089 GiB |   1088 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75975 MiB |  76035 MiB | 193648 GiB | 193574 GiB |
|       from large pool |  75613 MiB |  75673 MiB | 192560 GiB | 192486 GiB |
|       from small pool |    362 MiB |    363 MiB |   1087 GiB |   1087 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77578 MiB |  77578 MiB | 390594 MiB | 313016 MiB |
|       from large pool |  77176 MiB |  77176 MiB | 387640 MiB | 310464 MiB |
|       from small pool |    402 MiB |    402 MiB |   2954 MiB |   2552 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1511 MiB |   5091 MiB | 189043 GiB | 189042 GiB |
|       from large pool |   1473 MiB |   5086 MiB | 187807 GiB | 187805 GiB |
|       from small pool |     37 MiB |     38 MiB |   1236 GiB |   1236 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6724    |    6727    |   12787 K  |   12781 K  |
|       from large pool |     881    |     882    |    6166 K  |    6165 K  |
|       from small pool |    5843    |    5846    |    6621 K  |    6615 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6724    |    6727    |   12787 K  |   12781 K  |
|       from large pool |     881    |     882    |    6166 K  |    6165 K  |
|       from small pool |    5843    |    5846    |    6621 K  |    6615 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     323    |     323    |    2726    |    2403    |
|       from large pool |     122    |     122    |    1249    |    1127    |
|       from small pool |     201    |     201    |    1477    |    1276    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     445    |     445    |    7207 K  |    7207 K  |
|       from large pool |      86    |      86    |    4063 K  |    4063 K  |
|       from small pool |     359    |     359    |    3143 K  |    3143 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:42:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:42:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:42:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:42:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:42:28]    INFO >> epoch 004 | loss 3.704 | wps 2299.6 | ups 3.23 | wpb 712.7 | bsz 712.7 | num_updates 6139 | lr 0.000376 | gnorm 5.077 | clip 0.4 | train_wall 433 | gb_free 70.1 | wall 1950 (progress_bar.py:267, print())[0m
[33m[2025-11-18 23:42:28] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-18 23:42:52]    INFO >> epoch 004 | valid on 'valid' subset | loss 3.842 | wps 6130.9 | wpb 5412.5 | bsz 5412.5 | num_updates 6139 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-18 23:42:53]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-18 23:42:53]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 4 @ 6139 updates, score 3.842) (writing took 0.013423 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-18 23:42:53] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-18 23:42:56]    INFO >> epoch 005:     11 / 1539 loss=3.748, wps=819.9, ups=1.27, wpb=644.5, bsz=644.5, num_updates=6150, lr=0.000354, gnorm=4.912, clip=0, train_wall=13, gb_free=70.3, wall=1979 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:43:14]    INFO >> epoch 005:     61 / 1539 loss=3.819, wps=2387.3, ups=3.36, wpb=710.6, bsz=710.6, num_updates=6200, lr=0.000354, gnorm=4.152, clip=0, train_wall=14, gb_free=72.2, wall=1993 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:43:27]    INFO >> epoch 005:    111 / 1539 loss=3.689, wps=2535.5, ups=3.64, wpb=696.3, bsz=696.3, num_updates=6250, lr=0.000354, gnorm=4.256, clip=0, train_wall=13, gb_free=74.8, wall=2007 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:43:40]    INFO >> epoch 005:    161 / 1539 loss=3.635, wps=2428.2, ups=3.81, wpb=637.8, bsz=637.8, num_updates=6300, lr=0.000354, gnorm=4.753, clip=0, train_wall=13, gb_free=75.2, wall=2020 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:43:55]    INFO >> epoch 005:    211 / 1539 loss=3.712, wps=2338.5, ups=3.48, wpb=672.9, bsz=672.9, num_updates=6350, lr=0.000354, gnorm=4.526, clip=0, train_wall=14, gb_free=73.3, wall=2035 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:44:11]    INFO >> epoch 005:    261 / 1539 loss=3.627, wps=2709.4, ups=3.02, wpb=898, bsz=898, num_updates=6400, lr=0.000354, gnorm=5.532, clip=0, train_wall=16, gb_free=73.4, wall=2051 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:44:28]    INFO >> epoch 005:    311 / 1539 loss=3.633, wps=2511.1, ups=3.53, wpb=712.2, bsz=712.2, num_updates=6450, lr=0.000354, gnorm=4.923, clip=0, train_wall=14, gb_free=66.9, wall=2065 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:44:43]    INFO >> epoch 005:    361 / 1539 loss=3.62, wps=2605.7, ups=3.43, wpb=760.5, bsz=760.5, num_updates=6500, lr=0.000354, gnorm=4.157, clip=0, train_wall=14, gb_free=73.1, wall=2080 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:44:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 49.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.25 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 1.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:44:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 33        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75508 MiB |  75568 MiB | 210880 GiB | 210806 GiB |
|       from large pool |  75149 MiB |  75209 MiB | 209681 GiB | 209607 GiB |
|       from small pool |    359 MiB |    360 MiB |   1199 GiB |   1198 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75508 MiB |  75568 MiB | 210880 GiB | 210806 GiB |
|       from large pool |  75149 MiB |  75209 MiB | 209681 GiB | 209607 GiB |
|       from small pool |    359 MiB |    360 MiB |   1199 GiB |   1198 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75495 MiB |  75555 MiB | 210431 GiB | 210357 GiB |
|       from large pool |  75138 MiB |  75197 MiB | 209234 GiB | 209160 GiB |
|       from small pool |    357 MiB |    358 MiB |   1197 GiB |   1197 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77572 MiB |  77578 MiB | 390654 MiB | 313082 MiB |
|       from large pool |  77176 MiB |  77176 MiB | 387700 MiB | 310524 MiB |
|       from small pool |    396 MiB |    402 MiB |   2954 MiB |   2558 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2003 MiB |   6445 MiB | 204917 GiB | 204915 GiB |
|       from large pool |   1966 MiB |   6439 MiB | 203558 GiB | 203556 GiB |
|       from small pool |     36 MiB |     38 MiB |   1358 GiB |   1358 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6636    |    6639    |   13939 K  |   13932 K  |
|       from large pool |     873    |     874    |    6633 K  |    6632 K  |
|       from small pool |    5763    |    5766    |    7305 K  |    7300 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6636    |    6639    |   13939 K  |   13932 K  |
|       from large pool |     873    |     874    |    6633 K  |    6632 K  |
|       from small pool |    5763    |    5766    |    7305 K  |    7300 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     320    |     323    |    2727    |    2407    |
|       from large pool |     122    |     122    |    1250    |    1128    |
|       from small pool |     198    |     201    |    1477    |    1279    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     467    |     467    |    7875 K  |    7874 K  |
|       from large pool |     112    |     112    |    4367 K  |    4366 K  |
|       from small pool |     355    |     355    |    3507 K  |    3507 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:44:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:44:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:44:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:44:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:44:58]    INFO >> epoch 005:    412 / 1539 loss=3.656, wps=2407.6, ups=3.54, wpb=680.3, bsz=680.3, num_updates=6550, lr=0.000354, gnorm=5.269, clip=0, train_wall=13, gb_free=75.6, wall=2094 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:45:12]    INFO >> epoch 005:    462 / 1539 loss=3.765, wps=2279.1, ups=3.72, wpb=612.2, bsz=612.2, num_updates=6600, lr=0.000354, gnorm=4.056, clip=0, train_wall=13, gb_free=74.3, wall=2108 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:45:28]    INFO >> epoch 005:    512 / 1539 loss=3.635, wps=2441.2, ups=3.34, wpb=731.5, bsz=731.5, num_updates=6650, lr=0.000354, gnorm=4.364, clip=0, train_wall=15, gb_free=74.8, wall=2123 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:45:42]    INFO >> epoch 005:    562 / 1539 loss=3.758, wps=2189.4, ups=3.53, wpb=619.7, bsz=619.7, num_updates=6700, lr=0.000354, gnorm=4.399, clip=0, train_wall=14, gb_free=75.6, wall=2137 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:45:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 475.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 75.84 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:45:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 34        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69004 MiB |  71761 MiB | 216269 GiB | 216202 GiB |
|       from large pool |  68988 MiB |  71744 MiB | 215043 GiB | 214976 GiB |
|       from small pool |     16 MiB |     17 MiB |   1226 GiB |   1226 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69004 MiB |  71761 MiB | 216269 GiB | 216202 GiB |
|       from large pool |  68988 MiB |  71744 MiB | 215043 GiB | 214976 GiB |
|       from small pool |     16 MiB |     17 MiB |   1226 GiB |   1226 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 215809 GiB | 215742 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 214585 GiB | 214517 GiB |
|       from small pool |     16 MiB |     17 MiB |   1224 GiB |   1224 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77146 MiB |  77512 MiB | 390654 MiB | 313508 MiB |
|       from large pool |  77116 MiB |  77116 MiB | 387700 MiB | 310584 MiB |
|       from small pool |     30 MiB |    396 MiB |   2954 MiB |   2924 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5413 MiB |   9813 MiB | 210644 GiB | 210639 GiB |
|       from large pool |   5399 MiB |   9799 MiB | 209254 GiB | 209249 GiB |
|       from small pool |     13 MiB |     27 MiB |   1389 GiB |   1389 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   14282 K  |   14282 K  |
|       from large pool |     343    |     351    |    6815 K  |    6814 K  |
|       from small pool |     298    |     356    |    7467 K  |    7467 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   14282 K  |   14282 K  |
|       from large pool |     343    |     351    |    6815 K  |    6814 K  |
|       from small pool |     298    |     356    |    7467 K  |    7467 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     136    |     319    |    2727    |    2591    |
|       from large pool |     121    |     121    |    1250    |    1129    |
|       from small pool |      15    |     198    |    1477    |    1462    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     126    |     134    |    8062 K  |    8061 K  |
|       from large pool |      97    |     105    |    4482 K  |    4482 K  |
|       from small pool |      29    |      59    |    3579 K  |    3579 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:39:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:39:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:39:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:39:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:39:16]    INFO >> epoch 005:    613 / 1539 loss=3.559, wps=3.8, ups=0.01, wpb=763.6, bsz=763.6, num_updates=6750, lr=0.000354, gnorm=5.701, clip=2, train_wall=10, gb_free=71.8, wall=12135 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:39:24]    INFO >> epoch 005:    663 / 1539 loss=3.776, wps=5168.3, ups=7.07, wpb=730.6, bsz=730.6, num_updates=6800, lr=0.000354, gnorm=5.097, clip=0, train_wall=7, gb_free=75.4, wall=12142 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:39:36]    INFO >> epoch 005:    713 / 1539 loss=3.728, wps=3172.4, ups=4.28, wpb=741, bsz=741, num_updates=6850, lr=0.000354, gnorm=5.269, clip=0, train_wall=11, gb_free=68.3, wall=12153 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:39:43]    INFO >> epoch 005:    763 / 1539 loss=3.693, wps=5213.9, ups=7.39, wpb=705.1, bsz=705.1, num_updates=6900, lr=0.000354, gnorm=4.523, clip=0, train_wall=6, gb_free=70, wall=12160 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:39:52]    INFO >> epoch 005:    813 / 1539 loss=3.642, wps=4446.4, ups=6.62, wpb=671.7, bsz=671.7, num_updates=6950, lr=0.000354, gnorm=4.911, clip=2, train_wall=7, gb_free=72.9, wall=12168 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:39:58]    INFO >> epoch 005:    863 / 1539 loss=3.553, wps=5009.7, ups=7.45, wpb=672.8, bsz=672.8, num_updates=7000, lr=0.000354, gnorm=4.356, clip=0, train_wall=6, gb_free=74.7, wall=12174 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:40:05]    INFO >> epoch 005:    913 / 1539 loss=3.573, wps=4822.8, ups=7.24, wpb=666.4, bsz=666.4, num_updates=7050, lr=0.000354, gnorm=4.578, clip=0, train_wall=7, gb_free=72.2, wall=12181 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:40:11] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 635.25 MiB is free. Including non-PyTorch memory, this process has 78.50 GiB memory in use. Of the allocated memory 74.09 GiB is allocated by PyTorch, and 3.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:40:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 35        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72058 MiB |  75867 MiB | 226912 GiB | 226842 GiB |
|       from large pool |  72040 MiB |  75849 MiB | 225632 GiB | 225561 GiB |
|       from small pool |     17 MiB |     24 MiB |   1280 GiB |   1280 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72058 MiB |  75867 MiB | 226912 GiB | 226842 GiB |
|       from large pool |  72040 MiB |  75849 MiB | 225632 GiB | 225561 GiB |
|       from small pool |     17 MiB |     24 MiB |   1280 GiB |   1280 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  75852 MiB | 226429 GiB | 226359 GiB |
|       from large pool |  72027 MiB |  75835 MiB | 225150 GiB | 225080 GiB |
|       from small pool |     17 MiB |     24 MiB |   1279 GiB |   1279 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79870 MiB |  79910 MiB | 396146 MiB | 316276 MiB |
|       from large pool |  79844 MiB |  79844 MiB | 393156 MiB | 313312 MiB |
|       from small pool |     26 MiB |     66 MiB |   2990 MiB |   2964 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5083 MiB |   7360 MiB | 221827 GiB | 221822 GiB |
|       from large pool |   5075 MiB |   7351 MiB | 220376 GiB | 220371 GiB |
|       from small pool |      8 MiB |     21 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     615    |   14965 K  |   14965 K  |
|       from large pool |     308    |     317    |    7169 K  |    7169 K  |
|       from small pool |     298    |     356    |    7795 K  |    7795 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     615    |   14965 K  |   14965 K  |
|       from large pool |     308    |     317    |    7169 K  |    7169 K  |
|       from small pool |     298    |     356    |    7795 K  |    7795 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     135    |     155    |    2747    |    2612    |
|       from large pool |     122    |     122    |    1252    |    1130    |
|       from small pool |      13    |      33    |    1495    |    1482    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     131    |     131    |    8433 K  |    8433 K  |
|       from large pool |     106    |     106    |    4710 K  |    4710 K  |
|       from small pool |      25    |      49    |    3723 K  |    3723 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:40:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:40:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:40:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:40:11] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:40:12]    INFO >> epoch 005:    964 / 1539 loss=3.658, wps=4667.4, ups=6.89, wpb=677.5, bsz=677.5, num_updates=7100, lr=0.000354, gnorm=4.166, clip=0, train_wall=6, gb_free=73.6, wall=12189 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:40:20]    INFO >> epoch 005:   1014 / 1539 loss=3.455, wps=6042.3, ups=6.31, wpb=957.3, bsz=957.3, num_updates=7150, lr=0.000354, gnorm=4.726, clip=0, train_wall=7, gb_free=71.9, wall=12196 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:40:29]    INFO >> epoch 005:   1064 / 1539 loss=3.539, wps=5256.1, ups=6.63, wpb=792.7, bsz=792.7, num_updates=7200, lr=0.000354, gnorm=5.55, clip=0, train_wall=7, gb_free=72.4, wall=12204 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:40:36]    INFO >> epoch 005:   1114 / 1539 loss=3.63, wps=4637.3, ups=7.47, wpb=620.6, bsz=620.6, num_updates=7250, lr=0.000354, gnorm=4.812, clip=0, train_wall=6, gb_free=73.3, wall=12211 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:40:38] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 551.25 MiB is free. Including non-PyTorch memory, this process has 78.58 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:40:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 37        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76956 MiB |  77479 MiB | 232438 GiB | 232362 GiB |
|       from large pool |  76943 MiB |  77466 MiB | 231126 GiB | 231050 GiB |
|       from small pool |     12 MiB |     21 MiB |   1312 GiB |   1311 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76956 MiB |  77479 MiB | 232438 GiB | 232362 GiB |
|       from large pool |  76943 MiB |  77466 MiB | 231126 GiB | 231050 GiB |
|       from small pool |     12 MiB |     21 MiB |   1312 GiB |   1311 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 231944 GiB | 231869 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 230634 GiB | 230558 GiB |
|       from small pool |     12 MiB |     21 MiB |   1310 GiB |   1310 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79954 MiB |  79954 MiB | 400658 MiB | 320704 MiB |
|       from large pool |  79928 MiB |  79928 MiB | 397468 MiB | 317540 MiB |
|       from small pool |     26 MiB |    226 MiB |   3190 MiB |   3164 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2997 MiB |   6919 MiB | 227729 GiB | 227726 GiB |
|       from large pool |   2984 MiB |   6905 MiB | 226241 GiB | 226238 GiB |
|       from small pool |     13 MiB |     31 MiB |   1488 GiB |   1488 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   15328 K  |   15327 K  |
|       from large pool |     315    |     322    |    7345 K  |    7345 K  |
|       from small pool |     291    |     356    |    7982 K  |    7982 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   15328 K  |   15327 K  |
|       from large pool |     315    |     322    |    7345 K  |    7345 K  |
|       from small pool |     291    |     356    |    7982 K  |    7982 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     115    |     239    |    2853    |    2738    |
|       from large pool |     102    |     126    |    1258    |    1156    |
|       from small pool |      13    |     113    |    1595    |    1582    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     114    |     114    |    8636 K  |    8636 K  |
|       from large pool |      86    |      86    |    4821 K  |    4821 K  |
|       from small pool |      28    |      65    |    3815 K  |    3815 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:40:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:40:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:40:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:40:38] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:40:44]    INFO >> epoch 005:   1165 / 1539 loss=3.686, wps=4972.2, ups=6.36, wpb=782.2, bsz=782.2, num_updates=7300, lr=0.000354, gnorm=4.497, clip=0, train_wall=7, gb_free=70.1, wall=12219 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:40:51]    INFO >> epoch 005:   1215 / 1539 loss=3.699, wps=4695, ups=7.19, wpb=653.4, bsz=653.4, num_updates=7350, lr=0.000354, gnorm=4.358, clip=0, train_wall=7, gb_free=73.5, wall=12225 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:40:57]    INFO >> epoch 005:   1265 / 1539 loss=3.548, wps=5052.7, ups=7.97, wpb=634.2, bsz=634.2, num_updates=7400, lr=0.000354, gnorm=5.064, clip=0, train_wall=6, gb_free=73.1, wall=12232 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:41:05]    INFO >> epoch 005:   1315 / 1539 loss=3.688, wps=4595.1, ups=7.07, wpb=650.3, bsz=650.3, num_updates=7450, lr=0.000354, gnorm=4.543, clip=0, train_wall=7, gb_free=70.6, wall=12239 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:41:13]    INFO >> epoch 005:   1365 / 1539 loss=3.634, wps=5110.1, ups=7.03, wpb=726.5, bsz=726.5, num_updates=7500, lr=0.000354, gnorm=4.997, clip=0, train_wall=7, gb_free=71.2, wall=12246 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:41:20]    INFO >> epoch 005:   1415 / 1539 loss=3.766, wps=4546.2, ups=7.17, wpb=633.8, bsz=633.8, num_updates=7550, lr=0.000354, gnorm=4.38, clip=0, train_wall=7, gb_free=71.9, wall=12253 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:41:27]    INFO >> epoch 005:   1465 / 1539 loss=3.546, wps=5160, ups=6.73, wpb=767.1, bsz=767.1, num_updates=7600, lr=0.000354, gnorm=5.127, clip=0, train_wall=7, gb_free=71.3, wall=12260 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:41:36]    INFO >> epoch 005:   1515 / 1539 loss=3.567, wps=5128.7, ups=6.38, wpb=804.1, bsz=804.1, num_updates=7650, lr=0.000354, gnorm=5.188, clip=0, train_wall=7, gb_free=68, wall=12268 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:41:40]    INFO >> epoch 005 | loss 3.647 | wps 106 | ups 0.15 | wpb 712.7 | bsz 712.7 | num_updates 7674 | lr 0.000354 | gnorm 4.746 | clip 0.1 | train_wall 294 | gb_free 63.5 | wall 12272 (progress_bar.py:267, print())[0m
[33m[2025-11-19 02:41:40] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:41:52]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.761 | wps 11953.9 | wpb 5412.5 | bsz 5412.5 | num_updates 7674 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 02:41:53]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 02:41:53]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 5 @ 7674 updates, score 3.761) (writing took 0.012259 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 02:41:53] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:41:56]    INFO >> epoch 006:     26 / 1539 loss=3.671, wps=1749.4, ups=2.48, wpb=706.1, bsz=706.1, num_updates=7700, lr=0.000327, gnorm=4.497, clip=0, train_wall=7, gb_free=74.2, wall=12288 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:42:04]    INFO >> epoch 006:     76 / 1539 loss=3.79, wps=5441.5, ups=6.35, wpb=857.5, bsz=857.5, num_updates=7750, lr=0.000327, gnorm=5.274, clip=0, train_wall=7, gb_free=74.6, wall=12296 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:42:13]    INFO >> epoch 006:    126 / 1539 loss=3.537, wps=4787.7, ups=6.97, wpb=687, bsz=687, num_updates=7800, lr=0.000327, gnorm=4.493, clip=0, train_wall=7, gb_free=67.5, wall=12303 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:42:20]    INFO >> epoch 006:    176 / 1539 loss=3.613, wps=4631.9, ups=6.85, wpb=675.9, bsz=675.9, num_updates=7850, lr=0.000327, gnorm=4.339, clip=0, train_wall=7, gb_free=68.7, wall=12311 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:42:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 611.25 MiB is free. Including non-PyTorch memory, this process has 78.52 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 4.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:42:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 38        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75054 MiB |  75997 MiB | 254432 GiB | 254358 GiB |
|       from large pool |  75041 MiB |  75984 MiB | 252990 GiB | 252917 GiB |
|       from small pool |     12 MiB |     15 MiB |   1441 GiB |   1441 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75054 MiB |  75997 MiB | 254432 GiB | 254358 GiB |
|       from large pool |  75041 MiB |  75984 MiB | 252990 GiB | 252917 GiB |
|       from small pool |     12 MiB |     15 MiB |   1441 GiB |   1441 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 253896 GiB | 253823 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 252456 GiB | 252383 GiB |
|       from small pool |     12 MiB |     15 MiB |   1439 GiB |   1439 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79894 MiB |  80026 MiB | 400730 MiB | 320836 MiB |
|       from large pool |  79868 MiB |  79928 MiB | 397468 MiB | 317600 MiB |
|       from small pool |     26 MiB |     98 MiB |   3262 MiB |   3236 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4779 MiB |   8403 MiB | 250061 GiB | 250056 GiB |
|       from large pool |   4766 MiB |   8389 MiB | 248429 GiB | 248425 GiB |
|       from small pool |     13 MiB |     25 MiB |   1631 GiB |   1631 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   16756 K  |   16756 K  |
|       from large pool |     314    |     322    |    7976 K  |    7976 K  |
|       from small pool |     291    |     356    |    8780 K  |    8780 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   16756 K  |   16756 K  |
|       from large pool |     314    |     322    |    7976 K  |    7976 K  |
|       from small pool |     291    |     356    |    8780 K  |    8780 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     114    |     151    |    2889    |    2775    |
|       from large pool |     101    |     102    |    1258    |    1157    |
|       from small pool |      13    |      49    |    1631    |    1618    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     110    |     110    |    9436 K  |    9436 K  |
|       from large pool |      84    |      84    |    5219 K  |    5219 K  |
|       from small pool |      26    |      57    |    4216 K  |    4216 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:42:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:42:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:42:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:42:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-19 02:42:28] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.47 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:42:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 42        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79271 MiB |  79331 MiB | 255609 GiB | 255531 GiB |
|       from large pool |  79179 MiB |  79239 MiB | 254161 GiB | 254084 GiB |
|       from small pool |     91 MiB |     92 MiB |   1447 GiB |   1447 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79271 MiB |  79331 MiB | 255609 GiB | 255531 GiB |
|       from large pool |  79179 MiB |  79239 MiB | 254161 GiB | 254084 GiB |
|       from small pool |     91 MiB |     92 MiB |   1447 GiB |   1447 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79244 MiB |  79303 MiB | 255071 GiB | 254993 GiB |
|       from large pool |  79153 MiB |  79212 MiB | 253625 GiB | 253548 GiB |
|       from small pool |     90 MiB |     92 MiB |   1445 GiB |   1445 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80502 MiB | 401462 MiB | 320960 MiB |
|       from large pool |  80408 MiB |  80408 MiB | 398128 MiB | 317720 MiB |
|       from small pool |     94 MiB |     96 MiB |   3334 MiB |   3240 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1170 MiB |   7797 MiB | 251337 GiB | 251336 GiB |
|       from large pool |   1168 MiB |   7788 MiB | 249700 GiB | 249698 GiB |
|       from small pool |      2 MiB |     27 MiB |   1637 GiB |   1637 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1983    |    1986    |   16826 K  |   16824 K  |
|       from large pool |     472    |     473    |    8010 K  |    8009 K  |
|       from small pool |    1511    |    1514    |    8815 K  |    8814 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1983    |    1986    |   16826 K  |   16824 K  |
|       from large pool |     472    |     473    |    8010 K  |    8009 K  |
|       from small pool |    1511    |    1514    |    8815 K  |    8814 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     157    |     157    |    2936    |    2779    |
|       from large pool |     110    |     110    |    1269    |    1159    |
|       from small pool |      47    |      48    |    1667    |    1620    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     131    |     132    |    9474 K  |    9474 K  |
|       from large pool |      83    |      89    |    5240 K  |    5240 K  |
|       from small pool |      48    |      61    |    4233 K  |    4233 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:42:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:42:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:42:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:42:28] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:42:29]    INFO >> epoch 006:    228 / 1539 loss=3.533, wps=4086.8, ups=5.55, wpb=736.4, bsz=736.4, num_updates=7900, lr=0.000327, gnorm=5.172, clip=0, train_wall=7, gb_free=70.3, wall=12320 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:42:36]    INFO >> epoch 006:    278 / 1539 loss=3.654, wps=4672, ups=7.46, wpb=626.3, bsz=626.3, num_updates=7950, lr=0.000327, gnorm=3.979, clip=0, train_wall=6, gb_free=68.3, wall=12326 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:42:45]    INFO >> epoch 006:    328 / 1539 loss=3.495, wps=5102.1, ups=6.23, wpb=819.2, bsz=819.2, num_updates=8000, lr=0.000327, gnorm=5.205, clip=0, train_wall=8, gb_free=61.8, wall=12334 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:42:52]    INFO >> epoch 006:    378 / 1539 loss=3.727, wps=4493.5, ups=6.82, wpb=658.6, bsz=658.6, num_updates=8050, lr=0.000327, gnorm=3.963, clip=0, train_wall=7, gb_free=73.6, wall=12342 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:43:00]    INFO >> epoch 006:    428 / 1539 loss=3.606, wps=5244.6, ups=6.74, wpb=777.8, bsz=777.8, num_updates=8100, lr=0.000327, gnorm=4.728, clip=0, train_wall=7, gb_free=72.2, wall=12349 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:43:02] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.85 GiB is free. Including non-PyTorch memory, this process has 77.27 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 6.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:43:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 44        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72056 MiB |  72456 MiB | 262033 GiB | 261963 GiB |
|       from large pool |  72039 MiB |  72438 MiB | 260551 GiB | 260480 GiB |
|       from small pool |     17 MiB |     21 MiB |   1482 GiB |   1482 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72056 MiB |  72456 MiB | 262033 GiB | 261963 GiB |
|       from large pool |  72039 MiB |  72438 MiB | 260551 GiB | 260480 GiB |
|       from small pool |     17 MiB |     21 MiB |   1482 GiB |   1482 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 261481 GiB | 261410 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 260000 GiB | 259930 GiB |
|       from small pool |     17 MiB |     21 MiB |   1480 GiB |   1480 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78612 MiB |  80222 MiB | 437782 MiB | 359170 MiB |
|       from large pool |  78586 MiB |  80012 MiB | 434332 MiB | 355746 MiB |
|       from small pool |     26 MiB |    210 MiB |   3450 MiB |   3424 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6431 MiB |   8503 MiB | 257687 GiB | 257681 GiB |
|       from large pool |   6422 MiB |   8494 MiB | 256009 GiB | 256002 GiB |
|       from small pool |      8 MiB |     29 MiB |   1678 GiB |   1678 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   17258 K  |   17258 K  |
|       from large pool |     308    |     315    |    8227 K  |    8226 K  |
|       from small pool |     298    |     356    |    9031 K  |    9031 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   17258 K  |   17258 K  |
|       from large pool |     308    |     315    |    8227 K  |    8226 K  |
|       from small pool |     298    |     356    |    9031 K  |    9031 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     181    |     296    |    3090    |    2909    |
|       from large pool |     168    |     191    |    1365    |    1197    |
|       from small pool |      13    |     105    |    1725    |    1712    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     168    |     170    |    9719 K  |    9719 K  |
|       from large pool |     145    |     147    |    5384 K  |    5384 K  |
|       from small pool |      23    |      56    |    4334 K  |    4334 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:43:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:43:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:43:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:43:02] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:43:08]    INFO >> epoch 006:    479 / 1539 loss=3.573, wps=4594.2, ups=6.49, wpb=707.8, bsz=707.8, num_updates=8150, lr=0.000327, gnorm=4.569, clip=0, train_wall=7, gb_free=69.8, wall=12357 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:43:17]    INFO >> epoch 006:    529 / 1539 loss=3.631, wps=4462.5, ups=6.39, wpb=697.9, bsz=697.9, num_updates=8200, lr=0.000327, gnorm=4.058, clip=0, train_wall=7, gb_free=73.7, wall=12365 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:43:24]    INFO >> epoch 006:    579 / 1539 loss=3.626, wps=5307.3, ups=6.65, wpb=798, bsz=798, num_updates=8250, lr=0.000327, gnorm=4.486, clip=0, train_wall=7, gb_free=75.1, wall=12372 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:43:31]    INFO >> epoch 006:    629 / 1539 loss=3.624, wps=4679.9, ups=7.24, wpb=646.3, bsz=646.3, num_updates=8300, lr=0.000327, gnorm=3.98, clip=0, train_wall=6, gb_free=71, wall=12379 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:43:39]    INFO >> epoch 006:    679 / 1539 loss=3.603, wps=4842.6, ups=6.54, wpb=740.2, bsz=740.2, num_updates=8350, lr=0.000327, gnorm=4.63, clip=0, train_wall=7, gb_free=69.7, wall=12387 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:43:47]    INFO >> epoch 006:    729 / 1539 loss=3.542, wps=4415.6, ups=7.29, wpb=605.6, bsz=605.6, num_updates=8400, lr=0.000327, gnorm=4.378, clip=0, train_wall=6, gb_free=70.9, wall=12394 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:43:48] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 2.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:43:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 47        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77744 MiB |  77804 MiB | 270505 GiB | 270429 GiB |
|       from large pool |  77363 MiB |  77423 MiB | 268972 GiB | 268896 GiB |
|       from small pool |    381 MiB |    382 MiB |   1532 GiB |   1532 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77744 MiB |  77804 MiB | 270505 GiB | 270429 GiB |
|       from large pool |  77363 MiB |  77423 MiB | 268972 GiB | 268896 GiB |
|       from small pool |    381 MiB |    382 MiB |   1532 GiB |   1532 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77717 MiB |  77776 MiB | 269933 GiB | 269857 GiB |
|       from large pool |  77337 MiB |  77397 MiB | 268402 GiB | 268327 GiB |
|       from small pool |    379 MiB |    380 MiB |   1530 GiB |   1530 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80502 MiB | 439858 MiB | 359356 MiB |
|       from large pool |  80082 MiB |  80082 MiB | 436012 MiB | 355930 MiB |
|       from small pool |    420 MiB |    422 MiB |   3846 MiB |   3426 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2697 MiB |   6955 MiB | 265773 GiB | 265770 GiB |
|       from large pool |   2658 MiB |   6949 MiB | 264036 GiB | 264033 GiB |
|       from small pool |     38 MiB |     40 MiB |   1736 GiB |   1736 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7043    |    7046    |   17853 K  |   17846 K  |
|       from large pool |     910    |     911    |    8517 K  |    8516 K  |
|       from small pool |    6133    |    6136    |    9336 K  |    9330 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7043    |    7046    |   17853 K  |   17846 K  |
|       from large pool |     910    |     911    |    8517 K  |    8516 K  |
|       from small pool |    6133    |    6136    |    9336 K  |    9330 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     403    |     403    |    3316    |    2913    |
|       from large pool |     193    |     193    |    1393    |    1200    |
|       from small pool |     210    |     211    |    1923    |    1713    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     532    |     533    |   10057 K  |   10057 K  |
|       from large pool |     158    |     158    |    5577 K  |    5577 K  |
|       from small pool |     374    |     375    |    4479 K  |    4479 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:43:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:43:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:43:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:43:48] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:43:54]    INFO >> epoch 006:    780 / 1539 loss=3.571, wps=4352, ups=6.74, wpb=646.1, bsz=646.1, num_updates=8450, lr=0.000327, gnorm=4.151, clip=0, train_wall=6, gb_free=71.4, wall=12401 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:44:02]    INFO >> epoch 006:    830 / 1539 loss=3.65, wps=4644.8, ups=6.85, wpb=678.1, bsz=678.1, num_updates=8500, lr=0.000327, gnorm=4.325, clip=0, train_wall=7, gb_free=63.5, wall=12408 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:44:09]    INFO >> epoch 006:    880 / 1539 loss=3.562, wps=4903.9, ups=6.97, wpb=703.3, bsz=703.3, num_updates=8550, lr=0.000327, gnorm=4.278, clip=0, train_wall=7, gb_free=70.8, wall=12416 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:44:17]    INFO >> epoch 006:    930 / 1539 loss=3.586, wps=5137, ups=6.17, wpb=832.2, bsz=832.2, num_updates=8600, lr=0.000327, gnorm=4.742, clip=0, train_wall=8, gb_free=74.6, wall=12424 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:44:26]    INFO >> epoch 006:    980 / 1539 loss=3.57, wps=4621.4, ups=6.65, wpb=695, bsz=695, num_updates=8650, lr=0.000327, gnorm=4.046, clip=0, train_wall=7, gb_free=70.8, wall=12431 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:44:35]    INFO >> epoch 006:   1030 / 1539 loss=3.597, wps=4488.4, ups=5.75, wpb=780.7, bsz=780.7, num_updates=8700, lr=0.000327, gnorm=5.246, clip=0, train_wall=8, gb_free=65.5, wall=12440 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:44:42]    INFO >> epoch 006:   1080 / 1539 loss=3.69, wps=4831.7, ups=6.68, wpb=723.5, bsz=723.5, num_updates=8750, lr=0.000327, gnorm=4.562, clip=0, train_wall=7, gb_free=72.4, wall=12447 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:44:50]    INFO >> epoch 006:   1130 / 1539 loss=3.525, wps=4634, ups=6.12, wpb=757.6, bsz=757.6, num_updates=8800, lr=0.000327, gnorm=4.401, clip=0, train_wall=8, gb_free=72.1, wall=12456 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:44:59]    INFO >> epoch 006:   1180 / 1539 loss=3.689, wps=4986.3, ups=6.73, wpb=740.5, bsz=740.5, num_updates=8850, lr=0.000327, gnorm=4.722, clip=0, train_wall=7, gb_free=69, wall=12463 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:45:07]    INFO >> epoch 006:   1230 / 1539 loss=3.594, wps=4261.6, ups=6.46, wpb=659.3, bsz=659.3, num_updates=8900, lr=0.000327, gnorm=4.59, clip=0, train_wall=7, gb_free=73.8, wall=12471 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:45:14]    INFO >> epoch 006:   1280 / 1539 loss=3.669, wps=4531.7, ups=6.97, wpb=650.6, bsz=650.6, num_updates=8950, lr=0.000327, gnorm=4.168, clip=0, train_wall=7, gb_free=75.3, wall=12478 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:45:21]    INFO >> epoch 006:   1330 / 1539 loss=3.479, wps=5006.2, ups=6.76, wpb=741, bsz=741, num_updates=9000, lr=0.000327, gnorm=4.614, clip=0, train_wall=7, gb_free=71.3, wall=12485 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:45:30]    INFO >> epoch 006:   1380 / 1539 loss=3.598, wps=4338.6, ups=7.13, wpb=608.9, bsz=608.9, num_updates=9050, lr=0.000327, gnorm=4.517, clip=0, train_wall=7, gb_free=68.9, wall=12492 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:45:37]    INFO >> epoch 006:   1430 / 1539 loss=3.547, wps=4963.3, ups=6.6, wpb=751.8, bsz=751.8, num_updates=9100, lr=0.000327, gnorm=4.863, clip=2, train_wall=7, gb_free=70.5, wall=12500 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:45:45]    INFO >> epoch 006:   1480 / 1539 loss=3.409, wps=4656, ups=6.49, wpb=717, bsz=717, num_updates=9150, lr=0.000327, gnorm=4.755, clip=0, train_wall=7, gb_free=69.4, wall=12508 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:45:52]    INFO >> epoch 006:   1530 / 1539 loss=3.511, wps=4869.7, ups=7.22, wpb=674.1, bsz=674.1, num_updates=9200, lr=0.000327, gnorm=4.191, clip=0, train_wall=7, gb_free=74.6, wall=12514 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:45:53]    INFO >> epoch 006 | loss 3.596 | wps 4482.5 | ups 6.29 | wpb 712.7 | bsz 712.7 | num_updates 9209 | lr 0.000327 | gnorm 4.501 | clip 0.1 | train_wall 216 | gb_free 72.3 | wall 12516 (progress_bar.py:267, print())[0m
[33m[2025-11-19 02:45:53] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:46:07]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.769 | wps 11815.8 | wpb 5412.5 | bsz 5412.5 | num_updates 9209 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 02:46:08]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 02:46:08]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 6 @ 9209 updates, score 3.769) (writing took 0.013186 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 02:46:08] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:46:14]    INFO >> epoch 007:     41 / 1539 loss=3.658, wps=1656.6, ups=2.35, wpb=704.3, bsz=704.3, num_updates=9250, lr=0.000295, gnorm=4.665, clip=0, train_wall=8, gb_free=71.6, wall=12536 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:46:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.90 GiB is free. Including non-PyTorch memory, this process has 77.22 GiB memory in use. Of the allocated memory 68.10 GiB is allocated by PyTorch, and 8.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:46:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 48        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69335 MiB |  70025 MiB | 300364 GiB | 300296 GiB |
|       from large pool |  69317 MiB |  70007 MiB | 298661 GiB | 298594 GiB |
|       from small pool |     17 MiB |     25 MiB |   1702 GiB |   1702 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69335 MiB |  70025 MiB | 300364 GiB | 300296 GiB |
|       from large pool |  69317 MiB |  70007 MiB | 298661 GiB | 298594 GiB |
|       from small pool |     17 MiB |     25 MiB |   1702 GiB |   1702 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69318 MiB |  70006 MiB | 299730 GiB | 299662 GiB |
|       from large pool |  69300 MiB |  69989 MiB | 298029 GiB | 297961 GiB |
|       from small pool |     17 MiB |     25 MiB |   1700 GiB |   1700 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78564 MiB |  80442 MiB | 439858 MiB | 361294 MiB |
|       from large pool |  78534 MiB |  80022 MiB | 436012 MiB | 357478 MiB |
|       from small pool |     30 MiB |    420 MiB |   3846 MiB |   3816 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8834 MiB |   8932 MiB | 291012 GiB | 291003 GiB |
|       from large pool |   8822 MiB |   8919 MiB | 289086 GiB | 289077 GiB |
|       from small pool |     12 MiB |     31 MiB |   1926 GiB |   1926 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   19776 K  |   19776 K  |
|       from large pool |     307    |     315    |    9398 K  |    9398 K  |
|       from small pool |     298    |     356    |   10377 K  |   10377 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   19776 K  |   19776 K  |
|       from large pool |     307    |     315    |    9398 K  |    9398 K  |
|       from small pool |     298    |     356    |   10377 K  |   10377 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     183    |     402    |    3316    |    3133    |
|       from large pool |     168    |     192    |    1393    |    1225    |
|       from small pool |      15    |     210    |    1923    |    1908    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     172    |     174    |   11179 K  |   11178 K  |
|       from large pool |     142    |     144    |    6179 K  |    6179 K  |
|       from small pool |      30    |      64    |    4999 K  |    4999 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:46:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:46:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:46:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:46:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:46:22]    INFO >> epoch 007:     92 / 1539 loss=3.536, wps=4519.3, ups=6.7, wpb=674.7, bsz=674.7, num_updates=9300, lr=0.000295, gnorm=4.523, clip=0, train_wall=6, gb_free=74.6, wall=12543 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:46:29]    INFO >> epoch 007:    142 / 1539 loss=3.627, wps=4457.1, ups=6.72, wpb=663.1, bsz=663.1, num_updates=9350, lr=0.000295, gnorm=4.699, clip=0, train_wall=7, gb_free=73.4, wall=12551 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:46:38]    INFO >> epoch 007:    192 / 1539 loss=3.592, wps=5718.1, ups=6.46, wpb=885.1, bsz=885.1, num_updates=9400, lr=0.000295, gnorm=4.306, clip=0, train_wall=7, gb_free=67.7, wall=12558 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:46:46]    INFO >> epoch 007:    242 / 1539 loss=3.582, wps=5064.6, ups=6.38, wpb=794.3, bsz=794.3, num_updates=9450, lr=0.000295, gnorm=4.469, clip=0, train_wall=7, gb_free=70.7, wall=12566 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:46:54]    INFO >> epoch 007:    292 / 1539 loss=3.61, wps=4734.3, ups=6.77, wpb=699.6, bsz=699.6, num_updates=9500, lr=0.000295, gnorm=4.397, clip=0, train_wall=7, gb_free=71.4, wall=12574 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:46:59] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 449.25 MiB is free. Including non-PyTorch memory, this process has 78.68 GiB memory in use. Of the allocated memory 72.94 GiB is allocated by PyTorch, and 5.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:46:59] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 49        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69870 MiB |  74868 MiB | 308571 GiB | 308502 GiB |
|       from large pool |  69853 MiB |  74850 MiB | 306823 GiB | 306755 GiB |
|       from small pool |     17 MiB |     18 MiB |   1747 GiB |   1747 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69870 MiB |  74868 MiB | 308571 GiB | 308502 GiB |
|       from large pool |  69853 MiB |  74850 MiB | 306823 GiB | 306755 GiB |
|       from small pool |     17 MiB |     18 MiB |   1747 GiB |   1747 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69846 MiB |  74842 MiB | 307917 GiB | 307849 GiB |
|       from large pool |  69829 MiB |  74825 MiB | 306172 GiB | 306104 GiB |
|       from small pool |     17 MiB |     17 MiB |   1745 GiB |   1744 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80056 MiB |  80256 MiB | 441944 MiB | 361888 MiB |
|       from large pool |  80030 MiB |  80030 MiB | 437902 MiB | 357872 MiB |
|       from small pool |     26 MiB |    226 MiB |   4042 MiB |   4016 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8295 MiB |   9852 MiB | 298878 GiB | 298870 GiB |
|       from large pool |   8286 MiB |   9842 MiB | 296901 GiB | 296893 GiB |
|       from small pool |      8 MiB |     23 MiB |   1977 GiB |   1977 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     643    |     661    |   20317 K  |   20316 K  |
|       from large pool |     344    |     362    |    9671 K  |    9671 K  |
|       from small pool |     299    |     342    |   10645 K  |   10645 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     643    |     661    |   20317 K  |   20316 K  |
|       from large pool |     344    |     362    |    9671 K  |    9671 K  |
|       from small pool |     299    |     342    |   10645 K  |   10645 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     179    |     279    |    3415    |    3236    |
|       from large pool |     166    |     166    |    1394    |    1228    |
|       from small pool |      13    |     113    |    2021    |    2008    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     158    |     158    |   11484 K  |   11484 K  |
|       from large pool |     131    |     131    |    6362 K  |    6362 K  |
|       from small pool |      27    |      49    |    5122 K  |    5122 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:46:59] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:46:59] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:46:59] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:46:59] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:47:02]    INFO >> epoch 007:    343 / 1539 loss=3.511, wps=3925.3, ups=5.71, wpb=687.1, bsz=687.1, num_updates=9550, lr=0.000295, gnorm=4.638, clip=0, train_wall=8, gb_free=69.5, wall=12582 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:47:11]    INFO >> epoch 007:    393 / 1539 loss=3.717, wps=4339.7, ups=7.26, wpb=598.1, bsz=598.1, num_updates=9600, lr=0.000295, gnorm=3.582, clip=0, train_wall=6, gb_free=62, wall=12589 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:47:18]    INFO >> epoch 007:    443 / 1539 loss=3.446, wps=4481, ups=6.61, wpb=677.8, bsz=677.8, num_updates=9650, lr=0.000295, gnorm=4.508, clip=0, train_wall=7, gb_free=74.5, wall=12597 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:47:23] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 369.25 MiB is free. Including non-PyTorch memory, this process has 78.76 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 4.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:47:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 51        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72821 MiB |  75424 MiB | 312994 GiB | 312923 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 311225 GiB | 311154 GiB |
|       from small pool |     12 MiB |     18 MiB |   1768 GiB |   1768 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72821 MiB |  75424 MiB | 312994 GiB | 312923 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 311225 GiB | 311154 GiB |
|       from small pool |     12 MiB |     18 MiB |   1768 GiB |   1768 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 312331 GiB | 312260 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 310565 GiB | 310494 GiB |
|       from small pool |     12 MiB |     18 MiB |   1765 GiB |   1765 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80136 MiB |  80136 MiB | 445820 MiB | 365684 MiB |
|       from large pool |  80110 MiB |  80110 MiB | 441732 MiB | 361622 MiB |
|       from small pool |     26 MiB |     72 MiB |   4088 MiB |   4062 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5412 MiB |  10270 MiB | 303036 GiB | 303031 GiB |
|       from large pool |   5399 MiB |  10257 MiB | 301035 GiB | 301030 GiB |
|       from small pool |     13 MiB |     27 MiB |   2000 GiB |   2000 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   20586 K  |   20586 K  |
|       from large pool |     312    |     320    |    9814 K  |    9813 K  |
|       from small pool |     291    |     356    |   10772 K  |   10772 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   20586 K  |   20586 K  |
|       from large pool |     312    |     320    |    9814 K  |    9813 K  |
|       from small pool |     291    |     356    |   10772 K  |   10772 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     152    |     202    |    3443    |    3291    |
|       from large pool |     139    |     166    |    1399    |    1260    |
|       from small pool |      13    |      36    |    2044    |    2031    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     146    |     148    |   11634 K  |   11634 K  |
|       from large pool |     116    |     118    |    6457 K  |    6457 K  |
|       from small pool |      30    |      52    |    5176 K  |    5176 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:47:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:47:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:47:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:47:23] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:47:27]    INFO >> epoch 007:    494 / 1539 loss=3.581, wps=4436.3, ups=5.97, wpb=742.8, bsz=742.8, num_updates=9700, lr=0.000295, gnorm=4.425, clip=0, train_wall=7, gb_free=72.2, wall=12605 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:47:34]    INFO >> epoch 007:    544 / 1539 loss=3.595, wps=4469.7, ups=7.08, wpb=631.2, bsz=631.2, num_updates=9750, lr=0.000295, gnorm=4.608, clip=0, train_wall=7, gb_free=74.3, wall=12612 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:47:42]    INFO >> epoch 007:    594 / 1539 loss=3.606, wps=4814.1, ups=7.03, wpb=684.3, bsz=684.3, num_updates=9800, lr=0.000295, gnorm=4.034, clip=0, train_wall=7, gb_free=75.1, wall=12619 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:47:50]    INFO >> epoch 007:    644 / 1539 loss=3.494, wps=4620, ups=6.62, wpb=697.8, bsz=697.8, num_updates=9850, lr=0.000295, gnorm=5.057, clip=2, train_wall=7, gb_free=70.6, wall=12627 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:47:57]    INFO >> epoch 007:    694 / 1539 loss=3.503, wps=5448, ups=6.52, wpb=835.9, bsz=835.9, num_updates=9900, lr=0.000295, gnorm=4.537, clip=0, train_wall=7, gb_free=73.9, wall=12635 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:48:05]    INFO >> epoch 007:    744 / 1539 loss=3.6, wps=4852.3, ups=6.81, wpb=712, bsz=712, num_updates=9950, lr=0.000295, gnorm=4.194, clip=0, train_wall=7, gb_free=72.8, wall=12642 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:48:12]    INFO >> epoch 007:    794 / 1539 loss=3.56, wps=4472, ups=6.83, wpb=654.6, bsz=654.6, num_updates=10000, lr=0.000295, gnorm=3.779, clip=0, train_wall=7, gb_free=72.1, wall=12649 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:48:21]    INFO >> epoch 007:    844 / 1539 loss=3.577, wps=4922.4, ups=6.81, wpb=722.7, bsz=722.7, num_updates=10050, lr=0.000295, gnorm=4.148, clip=0, train_wall=7, gb_free=70.6, wall=12657 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:48:28]    INFO >> epoch 007:    894 / 1539 loss=3.588, wps=4591.7, ups=6.67, wpb=688.1, bsz=688.1, num_updates=10100, lr=0.000295, gnorm=4.186, clip=0, train_wall=7, gb_free=73.8, wall=12664 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:48:35]    INFO >> epoch 007:    944 / 1539 loss=3.623, wps=4823.5, ups=6.9, wpb=699, bsz=699, num_updates=10150, lr=0.000295, gnorm=4.597, clip=0, train_wall=7, gb_free=71.9, wall=12671 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:48:43]    INFO >> epoch 007:    994 / 1539 loss=3.538, wps=4856.3, ups=6.6, wpb=736.1, bsz=736.1, num_updates=10200, lr=0.000295, gnorm=4.468, clip=0, train_wall=7, gb_free=70.3, wall=12679 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:48:52]    INFO >> epoch 007:   1044 / 1539 loss=3.47, wps=4827.5, ups=6.84, wpb=705.7, bsz=705.7, num_updates=10250, lr=0.000295, gnorm=4.501, clip=0, train_wall=7, gb_free=73, wall=12686 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:48:59]    INFO >> epoch 007:   1094 / 1539 loss=3.502, wps=4675.5, ups=6.74, wpb=693.4, bsz=693.4, num_updates=10300, lr=0.000295, gnorm=4.424, clip=0, train_wall=7, gb_free=71.6, wall=12694 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:49:07]    INFO >> epoch 007:   1144 / 1539 loss=3.461, wps=4895.4, ups=6.24, wpb=784.9, bsz=784.9, num_updates=10350, lr=0.000295, gnorm=4.232, clip=0, train_wall=8, gb_free=74, wall=12702 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:49:15]    INFO >> epoch 007:   1194 / 1539 loss=3.551, wps=5139, ups=6.33, wpb=811.5, bsz=811.5, num_updates=10400, lr=0.000295, gnorm=4.616, clip=0, train_wall=7, gb_free=72.8, wall=12710 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:49:24]    INFO >> epoch 007:   1244 / 1539 loss=3.493, wps=4731, ups=6.89, wpb=686.2, bsz=686.2, num_updates=10450, lr=0.000295, gnorm=4.038, clip=0, train_wall=7, gb_free=68.3, wall=12717 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:49:31]    INFO >> epoch 007:   1294 / 1539 loss=3.509, wps=4417.6, ups=6.9, wpb=640.4, bsz=640.4, num_updates=10500, lr=0.000295, gnorm=3.813, clip=0, train_wall=7, gb_free=70.7, wall=12724 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:49:38]    INFO >> epoch 007:   1344 / 1539 loss=3.542, wps=4851.9, ups=6.96, wpb=697.4, bsz=697.4, num_updates=10550, lr=0.000295, gnorm=4.27, clip=0, train_wall=7, gb_free=70.4, wall=12731 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:49:46]    INFO >> epoch 007:   1394 / 1539 loss=3.456, wps=4659.9, ups=6.67, wpb=699.1, bsz=699.1, num_updates=10600, lr=0.000295, gnorm=4.731, clip=0, train_wall=7, gb_free=67.1, wall=12739 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:49:51] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 15.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.16 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:49:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 52        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77927 MiB |  77987 MiB | 340862 GiB | 340786 GiB |
|       from large pool |  77544 MiB |  77604 MiB | 338940 GiB | 338865 GiB |
|       from small pool |    383 MiB |    384 MiB |   1921 GiB |   1921 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77927 MiB |  77987 MiB | 340862 GiB | 340786 GiB |
|       from large pool |  77544 MiB |  77604 MiB | 338940 GiB | 338865 GiB |
|       from small pool |    383 MiB |    384 MiB |   1921 GiB |   1921 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77897 MiB |  77956 MiB | 340137 GiB | 340061 GiB |
|       from large pool |  77516 MiB |  77575 MiB | 338218 GiB | 338142 GiB |
|       from small pool |    381 MiB |    382 MiB |   1918 GiB |   1918 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80490 MiB |  80492 MiB | 448078 MiB | 367588 MiB |
|       from large pool |  80068 MiB |  80068 MiB | 443592 MiB | 363524 MiB |
|       from small pool |    422 MiB |    424 MiB |   4486 MiB |   4064 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2502 MiB |   7537 MiB | 331243 GiB | 331240 GiB |
|       from large pool |   2463 MiB |   7531 MiB | 329067 GiB | 329065 GiB |
|       from small pool |     38 MiB |     40 MiB |   2175 GiB |   2175 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7076    |    7079    |   22457 K  |   22450 K  |
|       from large pool |     913    |     914    |   10758 K  |   10758 K  |
|       from small pool |    6163    |    6166    |   11698 K  |   11692 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7076    |    7079    |   22457 K  |   22450 K  |
|       from large pool |     913    |     914    |   10758 K  |   10758 K  |
|       from small pool |    6163    |    6166    |   11698 K  |   11692 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     380    |     381    |    3673    |    3293    |
|       from large pool |     169    |     169    |    1430    |    1261    |
|       from small pool |     211    |     212    |    2243    |    2032    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     510    |     511    |   12669 K  |   12669 K  |
|       from large pool |     133    |     133    |    7074 K  |    7073 K  |
|       from small pool |     377    |     378    |    5595 K  |    5595 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:49:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:49:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:49:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:49:51] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:49:54]    INFO >> epoch 007:   1445 / 1539 loss=3.554, wps=4371.8, ups=6.11, wpb=716, bsz=716, num_updates=10650, lr=0.000295, gnorm=4.392, clip=0, train_wall=7, gb_free=68.1, wall=12747 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:50:02]    INFO >> epoch 007:   1495 / 1539 loss=3.558, wps=5175.4, ups=7.25, wpb=713.5, bsz=713.5, num_updates=10700, lr=0.000295, gnorm=4.345, clip=0, train_wall=6, gb_free=70.4, wall=12754 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:50:09]    INFO >> epoch 007 | loss 3.55 | wps 4466.9 | ups 6.27 | wpb 712.7 | bsz 712.7 | num_updates 10744 | lr 0.000295 | gnorm 4.359 | clip 0.1 | train_wall 216 | gb_free 70.2 | wall 12761 (progress_bar.py:267, print())[0m
[33m[2025-11-19 02:50:09] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:50:22]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.752 | wps 11679.1 | wpb 5412.5 | bsz 5412.5 | num_updates 10744 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 02:50:22]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 02:50:22]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 7 @ 10744 updates, score 3.752) (writing took 0.014034 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 02:50:22] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:50:23]    INFO >> epoch 008:      6 / 1539 loss=3.426, wps=1737.5, ups=2.38, wpb=730.3, bsz=730.3, num_updates=10750, lr=0.000262, gnorm=4.125, clip=0, train_wall=7, gb_free=68.7, wall=12775 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:50:32]    INFO >> epoch 008:     56 / 1539 loss=3.405, wps=4973.8, ups=6.76, wpb=735.8, bsz=735.8, num_updates=10800, lr=0.000262, gnorm=4.335, clip=0, train_wall=7, gb_free=73.9, wall=12782 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:50:39]    INFO >> epoch 008:    106 / 1539 loss=3.443, wps=4977.1, ups=6.49, wpb=766.8, bsz=766.8, num_updates=10850, lr=0.000262, gnorm=4.195, clip=0, train_wall=7, gb_free=73.2, wall=12790 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:50:47]    INFO >> epoch 008:    156 / 1539 loss=3.497, wps=4534.2, ups=6.95, wpb=652.3, bsz=652.3, num_updates=10900, lr=0.000262, gnorm=3.933, clip=0, train_wall=7, gb_free=75.6, wall=12797 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:50:55]    INFO >> epoch 008:    206 / 1539 loss=3.435, wps=5225.1, ups=6.26, wpb=834.7, bsz=834.7, num_updates=10950, lr=0.000262, gnorm=4.146, clip=0, train_wall=8, gb_free=73.5, wall=12805 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:51:02]    INFO >> epoch 008:    256 / 1539 loss=3.488, wps=4925.9, ups=6.9, wpb=714.1, bsz=714.1, num_updates=11000, lr=0.000262, gnorm=4.296, clip=0, train_wall=7, gb_free=71.5, wall=12812 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:51:11]    INFO >> epoch 008:    306 / 1539 loss=3.623, wps=4773.3, ups=6.62, wpb=720.8, bsz=720.8, num_updates=11050, lr=0.000262, gnorm=4.633, clip=0, train_wall=7, gb_free=71.8, wall=12820 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:51:19]    INFO >> epoch 008:    356 / 1539 loss=3.536, wps=4260.8, ups=6.4, wpb=665.6, bsz=665.6, num_updates=11100, lr=0.000262, gnorm=4.051, clip=0, train_wall=7, gb_free=74.7, wall=12828 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:51:26]    INFO >> epoch 008:    406 / 1539 loss=3.565, wps=4735.7, ups=6.7, wpb=707, bsz=707, num_updates=11150, lr=0.000262, gnorm=3.615, clip=0, train_wall=7, gb_free=72.1, wall=12835 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:51:34]    INFO >> epoch 008:    456 / 1539 loss=3.558, wps=5709.4, ups=6.52, wpb=875.6, bsz=875.6, num_updates=11200, lr=0.000262, gnorm=4.396, clip=0, train_wall=7, gb_free=72.6, wall=12843 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:51:42]    INFO >> epoch 008:    506 / 1539 loss=3.473, wps=5085, ups=6.78, wpb=749.6, bsz=749.6, num_updates=11250, lr=0.000262, gnorm=4.369, clip=0, train_wall=7, gb_free=74.3, wall=12850 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:51:50]    INFO >> epoch 008:    556 / 1539 loss=3.48, wps=4359.3, ups=6.69, wpb=651.2, bsz=651.2, num_updates=11300, lr=0.000262, gnorm=3.662, clip=0, train_wall=7, gb_free=71.8, wall=12858 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:51:57]    INFO >> epoch 008:    606 / 1539 loss=3.544, wps=4629.3, ups=6.78, wpb=682.8, bsz=682.8, num_updates=11350, lr=0.000262, gnorm=4.127, clip=0, train_wall=7, gb_free=75.2, wall=12865 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:52:04]    INFO >> epoch 008:    656 / 1539 loss=3.582, wps=4610.6, ups=7.25, wpb=636.2, bsz=636.2, num_updates=11400, lr=0.000262, gnorm=3.6, clip=0, train_wall=6, gb_free=72.5, wall=12872 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:52:13]    INFO >> epoch 008:    706 / 1539 loss=3.489, wps=4801.7, ups=6.94, wpb=691.9, bsz=691.9, num_updates=11450, lr=0.000262, gnorm=3.813, clip=0, train_wall=7, gb_free=70, wall=12879 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:52:20]    INFO >> epoch 008:    756 / 1539 loss=3.528, wps=4906.4, ups=6.75, wpb=727, bsz=727, num_updates=11500, lr=0.000262, gnorm=3.82, clip=0, train_wall=7, gb_free=70.9, wall=12887 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:52:28]    INFO >> epoch 008:    806 / 1539 loss=3.443, wps=4642.4, ups=6.69, wpb=694.2, bsz=694.2, num_updates=11550, lr=0.000262, gnorm=4.419, clip=0, train_wall=7, gb_free=68.9, wall=12894 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:52:34]    INFO >> epoch 008:    856 / 1539 loss=3.543, wps=4390.2, ups=7.37, wpb=595.7, bsz=595.7, num_updates=11600, lr=0.000262, gnorm=3.564, clip=0, train_wall=6, gb_free=71.5, wall=12901 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:52:42]    INFO >> epoch 008:    906 / 1539 loss=3.406, wps=5097.2, ups=6.27, wpb=813.3, bsz=813.3, num_updates=11650, lr=0.000262, gnorm=4.342, clip=0, train_wall=8, gb_free=74, wall=12909 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:52:51]    INFO >> epoch 008:    956 / 1539 loss=3.479, wps=4733.1, ups=7.02, wpb=674.1, bsz=674.1, num_updates=11700, lr=0.000262, gnorm=4.446, clip=0, train_wall=7, gb_free=72.9, wall=12916 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:52:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 473.25 MiB is free. Including non-PyTorch memory, this process has 78.65 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 8.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:52:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 53        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69004 MiB |  71761 MiB | 377144 GiB | 377077 GiB |
|       from large pool |  68987 MiB |  71744 MiB | 375009 GiB | 374941 GiB |
|       from small pool |     16 MiB |     20 MiB |   2135 GiB |   2135 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69004 MiB |  71761 MiB | 377144 GiB | 377077 GiB |
|       from large pool |  68987 MiB |  71744 MiB | 375009 GiB | 374941 GiB |
|       from small pool |     16 MiB |     20 MiB |   2135 GiB |   2135 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 376341 GiB | 376273 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 374208 GiB | 374141 GiB |
|       from small pool |     16 MiB |     20 MiB |   2132 GiB |   2132 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80032 MiB |  80430 MiB | 448078 MiB | 368046 MiB |
|       from large pool |  80008 MiB |  80008 MiB | 443592 MiB | 363584 MiB |
|       from small pool |     24 MiB |    422 MiB |   4486 MiB |   4462 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8299 MiB |  11896 MiB | 364184 GiB | 364176 GiB |
|       from large pool |   8292 MiB |  11888 MiB | 361767 GiB | 361759 GiB |
|       from small pool |      7 MiB |     29 MiB |   2416 GiB |   2416 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   24894 K  |   24894 K  |
|       from large pool |     343    |     351    |   11887 K  |   11887 K  |
|       from small pool |     298    |     356    |   13007 K  |   13007 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   24894 K  |   24894 K  |
|       from large pool |     343    |     351    |   11887 K  |   11887 K  |
|       from small pool |     298    |     356    |   13007 K  |   13007 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     180    |     379    |    3673    |    3493    |
|       from large pool |     168    |     168    |    1430    |    1262    |
|       from small pool |      12    |     211    |    2243    |    2231    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     159    |     163    |   14069 K  |   14069 K  |
|       from large pool |     133    |     137    |    7829 K  |    7829 K  |
|       from small pool |      26    |      62    |    6240 K  |    6240 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:52:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:52:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:52:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:52:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:52:58]    INFO >> epoch 008:   1007 / 1539 loss=3.511, wps=4083.6, ups=6.77, wpb=602.8, bsz=602.8, num_updates=11750, lr=0.000262, gnorm=4.219, clip=0, train_wall=6, gb_free=70.6, wall=12923 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:53:07]    INFO >> epoch 008:   1057 / 1539 loss=3.388, wps=4788.7, ups=5.78, wpb=828.4, bsz=828.4, num_updates=11800, lr=0.000262, gnorm=3.925, clip=0, train_wall=8, gb_free=71.7, wall=12932 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:53:14]    INFO >> epoch 008:   1107 / 1539 loss=3.462, wps=4720.6, ups=7.14, wpb=661.2, bsz=661.2, num_updates=11850, lr=0.000262, gnorm=4.2, clip=0, train_wall=7, gb_free=72.8, wall=12939 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:53:22]    INFO >> epoch 008:   1157 / 1539 loss=3.684, wps=3968.6, ups=7.12, wpb=557.5, bsz=557.5, num_updates=11900, lr=0.000262, gnorm=3.753, clip=0, train_wall=7, gb_free=73.2, wall=12946 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:53:25] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 373.25 MiB is free. Including non-PyTorch memory, this process has 78.75 GiB memory in use. Of the allocated memory 73.45 GiB is allocated by PyTorch, and 4.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:53:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 55        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72820 MiB |  75423 MiB | 382592 GiB | 382521 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 380430 GiB | 380359 GiB |
|       from small pool |     12 MiB |     19 MiB |   2162 GiB |   2162 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72820 MiB |  75423 MiB | 382592 GiB | 382521 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 380430 GiB | 380359 GiB |
|       from small pool |     12 MiB |     19 MiB |   2162 GiB |   2162 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 381777 GiB | 381706 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 379617 GiB | 379546 GiB |
|       from small pool |     12 MiB |     19 MiB |   2159 GiB |   2159 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80132 MiB |  80462 MiB | 452682 MiB | 372550 MiB |
|       from large pool |  80102 MiB |  80396 MiB | 448154 MiB | 368052 MiB |
|       from small pool |     30 MiB |     66 MiB |   4528 MiB |   4498 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7251 MiB |  11832 MiB | 369246 GiB | 369239 GiB |
|       from large pool |   7233 MiB |  11815 MiB | 366798 GiB | 366791 GiB |
|       from small pool |     17 MiB |     31 MiB |   2447 GiB |   2447 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   25239 K  |   25239 K  |
|       from large pool |     312    |     320    |   12067 K  |   12067 K  |
|       from small pool |     291    |     348    |   13172 K  |   13172 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   25239 K  |   25239 K  |
|       from large pool |     312    |     320    |   12067 K  |   12067 K  |
|       from small pool |     291    |     348    |   13172 K  |   13172 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     158    |     202    |    3699    |    3541    |
|       from large pool |     143    |     169    |    1435    |    1292    |
|       from small pool |      15    |      33    |    2264    |    2249    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     152    |     154    |   14262 K  |   14262 K  |
|       from large pool |     123    |     125    |    7949 K  |    7949 K  |
|       from small pool |      29    |      61    |    6312 K  |    6312 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:25] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:53:31]    INFO >> epoch 008:   1208 / 1539 loss=3.504, wps=4247.8, ups=5.94, wpb=715, bsz=715, num_updates=11950, lr=0.000262, gnorm=4.048, clip=0, train_wall=7, gb_free=76, wall=12954 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:53:36] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 75.97 GiB is allocated by PyTorch, and 2.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:53:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 56        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77732 MiB |  77791 MiB | 384872 GiB | 384796 GiB |
|       from large pool |  77350 MiB |  77410 MiB | 382695 GiB | 382619 GiB |
|       from small pool |    381 MiB |    382 MiB |   2176 GiB |   2176 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77732 MiB |  77791 MiB | 384872 GiB | 384796 GiB |
|       from large pool |  77350 MiB |  77410 MiB | 382695 GiB | 382619 GiB |
|       from small pool |    381 MiB |    382 MiB |   2176 GiB |   2176 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77717 MiB |  77776 MiB | 384052 GiB | 383976 GiB |
|       from large pool |  77337 MiB |  77397 MiB | 381878 GiB | 381803 GiB |
|       from small pool |    379 MiB |    380 MiB |   2173 GiB |   2173 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80464 MiB | 453074 MiB | 372612 MiB |
|       from large pool |  80042 MiB |  80042 MiB | 448154 MiB | 368112 MiB |
|       from small pool |    420 MiB |    422 MiB |   4920 MiB |   4500 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2729 MiB |   7831 MiB | 371566 GiB | 371564 GiB |
|       from large pool |   2691 MiB |   7824 MiB | 369102 GiB | 369099 GiB |
|       from small pool |     38 MiB |     40 MiB |   2464 GiB |   2464 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7043    |    7046    |   25394 K  |   25387 K  |
|       from large pool |     910    |     911    |   12135 K  |   12134 K  |
|       from small pool |    6133    |    6136    |   13259 K  |   13253 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7043    |    7046    |   25394 K  |   25387 K  |
|       from large pool |     910    |     911    |   12135 K  |   12134 K  |
|       from small pool |    6133    |    6136    |   13259 K  |   13253 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     352    |     353    |    3895    |    3543    |
|       from large pool |     142    |     142    |    1435    |    1293    |
|       from small pool |     210    |     211    |    2460    |    2250    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     513    |     514    |   14352 K  |   14351 K  |
|       from large pool |     139    |     139    |    7994 K  |    7994 K  |
|       from small pool |     374    |     375    |    6357 K  |    6357 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:36] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:53:39]    INFO >> epoch 008:   1259 / 1539 loss=3.573, wps=5050, ups=5.94, wpb=850.7, bsz=850.7, num_updates=12000, lr=0.000262, gnorm=4.205, clip=0, train_wall=8, gb_free=73.2, wall=12963 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:53:46]    INFO >> epoch 008:   1309 / 1539 loss=3.512, wps=4757.4, ups=6.75, wpb=704.3, bsz=704.3, num_updates=12050, lr=0.000262, gnorm=4.172, clip=0, train_wall=7, gb_free=72.7, wall=12970 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:53:55]    INFO >> epoch 008:   1359 / 1539 loss=3.461, wps=4486.3, ups=6.75, wpb=664.7, bsz=664.7, num_updates=12100, lr=0.000262, gnorm=3.761, clip=0, train_wall=7, gb_free=69.7, wall=12978 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:53:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 682.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 439.25 MiB is free. Including non-PyTorch memory, this process has 78.69 GiB memory in use. Of the allocated memory 73.43 GiB is allocated by PyTorch, and 4.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:53:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 33           |        cudaMalloc retries: 57        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72060 MiB |  75188 MiB | 388514 GiB | 388443 GiB |
|       from large pool |  72042 MiB |  75170 MiB | 386318 GiB | 386248 GiB |
|       from small pool |     17 MiB |     18 MiB |   2195 GiB |   2195 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72060 MiB |  75188 MiB | 388514 GiB | 388443 GiB |
|       from large pool |  72042 MiB |  75170 MiB | 386318 GiB | 386248 GiB |
|       from small pool |     17 MiB |     18 MiB |   2195 GiB |   2195 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  75171 MiB | 387686 GiB | 387616 GiB |
|       from large pool |  72027 MiB |  75153 MiB | 385494 GiB | 385424 GiB |
|       from small pool |     17 MiB |     18 MiB |   2192 GiB |   2192 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80066 MiB |  80462 MiB | 453074 MiB | 373008 MiB |
|       from large pool |  80042 MiB |  80042 MiB | 448154 MiB | 368112 MiB |
|       from small pool |     24 MiB |    420 MiB |   4920 MiB |   4896 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5277 MiB |   8526 MiB | 375203 GiB | 375197 GiB |
|       from large pool |   5271 MiB |   8517 MiB | 372717 GiB | 372711 GiB |
|       from small pool |      6 MiB |     27 MiB |   2486 GiB |   2486 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     611    |   25621 K  |   25620 K  |
|       from large pool |     308    |     316    |   12253 K  |   12252 K  |
|       from small pool |     295    |     348    |   13368 K  |   13367 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     611    |   25621 K  |   25620 K  |
|       from large pool |     308    |     316    |   12253 K  |   12252 K  |
|       from small pool |     295    |     348    |   13368 K  |   13367 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     154    |     352    |    3895    |    3741    |
|       from large pool |     142    |     142    |    1435    |    1293    |
|       from small pool |      12    |     210    |    2460    |    2448    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     142    |     142    |   14479 K  |   14479 K  |
|       from large pool |     119    |     119    |    8071 K  |    8071 K  |
|       from small pool |      23    |      57    |    6407 K  |    6407 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:54:03]    INFO >> epoch 008:   1410 / 1539 loss=3.493, wps=5030.6, ups=6.1, wpb=824.4, bsz=824.4, num_updates=12150, lr=0.000262, gnorm=4.06, clip=0, train_wall=7, gb_free=73, wall=12986 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:54:12]    INFO >> epoch 008:   1460 / 1539 loss=3.487, wps=4227, ups=5.91, wpb=715.2, bsz=715.2, num_updates=12200, lr=0.000262, gnorm=3.914, clip=0, train_wall=8, gb_free=75, wall=12994 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:54:19]    INFO >> epoch 008:   1510 / 1539 loss=3.557, wps=4537.4, ups=6.81, wpb=666.4, bsz=666.4, num_updates=12250, lr=0.000262, gnorm=3.622, clip=0, train_wall=7, gb_free=67.3, wall=13002 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:54:23]    INFO >> epoch 008 | loss 3.501 | wps 4461.9 | ups 6.26 | wpb 712.7 | bsz 712.7 | num_updates 12279 | lr 0.000262 | gnorm 4.049 | clip 0 | train_wall 216 | gb_free 74.8 | wall 13006 (progress_bar.py:267, print())[0m
[33m[2025-11-19 02:54:23] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:54:38]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.784 | wps 11837 | wpb 5412.5 | bsz 5412.5 | num_updates 12279 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 02:54:38]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 02:54:38]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 8 @ 12279 updates, score 3.784) (writing took 0.013584 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 02:54:38] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:54:41]    INFO >> epoch 009:     21 / 1539 loss=3.553, wps=1659.3, ups=2.48, wpb=668.9, bsz=668.9, num_updates=12300, lr=0.000227, gnorm=3.736, clip=0, train_wall=7, gb_free=73.1, wall=13022 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:54:48]    INFO >> epoch 009:     71 / 1539 loss=3.484, wps=4583.3, ups=6.91, wpb=663.6, bsz=663.6, num_updates=12350, lr=0.000227, gnorm=4.164, clip=0, train_wall=7, gb_free=73.5, wall=13029 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:54:55]    INFO >> epoch 009:    121 / 1539 loss=3.53, wps=4825.3, ups=7.01, wpb=688.4, bsz=688.4, num_updates=12400, lr=0.000227, gnorm=4.054, clip=0, train_wall=7, gb_free=73.7, wall=13036 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:55:04]    INFO >> epoch 009:    171 / 1539 loss=3.406, wps=5418.2, ups=6.43, wpb=842.4, bsz=842.4, num_updates=12450, lr=0.000227, gnorm=4.268, clip=0, train_wall=7, gb_free=71.8, wall=13044 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:55:11]    INFO >> epoch 009:    221 / 1539 loss=3.548, wps=4814.3, ups=6.96, wpb=691.5, bsz=691.5, num_updates=12500, lr=0.000227, gnorm=3.942, clip=0, train_wall=7, gb_free=71.5, wall=13051 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:55:18]    INFO >> epoch 009:    271 / 1539 loss=3.488, wps=5119.5, ups=7.28, wpb=703.6, bsz=703.6, num_updates=12550, lr=0.000227, gnorm=3.826, clip=0, train_wall=6, gb_free=70, wall=13058 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:55:26]    INFO >> epoch 009:    321 / 1539 loss=3.498, wps=5123.4, ups=6.53, wpb=784.5, bsz=784.5, num_updates=12600, lr=0.000227, gnorm=4.27, clip=0, train_wall=7, gb_free=66.3, wall=13066 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:55:36]    INFO >> epoch 009:    371 / 1539 loss=3.562, wps=4231.2, ups=6.04, wpb=701, bsz=701, num_updates=12650, lr=0.000227, gnorm=3.899, clip=0, train_wall=8, gb_free=65.8, wall=13074 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:55:36] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 515.25 MiB is free. Including non-PyTorch memory, this process has 78.61 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 4.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:55:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 34           |        cudaMalloc retries: 58        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75054 MiB |  75997 MiB | 409339 GiB | 409266 GiB |
|       from large pool |  75041 MiB |  75984 MiB | 407021 GiB | 406947 GiB |
|       from small pool |     12 MiB |     16 MiB |   2318 GiB |   2318 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75054 MiB |  75997 MiB | 409339 GiB | 409266 GiB |
|       from large pool |  75041 MiB |  75984 MiB | 407021 GiB | 406947 GiB |
|       from small pool |     12 MiB |     16 MiB |   2318 GiB |   2318 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 408471 GiB | 408398 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 406156 GiB | 406083 GiB |
|       from small pool |     12 MiB |     16 MiB |   2315 GiB |   2315 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79990 MiB |  80090 MiB | 455826 MiB | 375836 MiB |
|       from large pool |  79966 MiB |  79966 MiB | 450806 MiB | 370840 MiB |
|       from small pool |     24 MiB |    124 MiB |   5020 MiB |   4996 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4935 MiB |  11872 MiB | 393887 GiB | 393882 GiB |
|       from large pool |   4924 MiB |  11860 MiB | 391265 GiB | 391260 GiB |
|       from small pool |     11 MiB |     21 MiB |   2621 GiB |   2621 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   26961 K  |   26960 K  |
|       from large pool |     314    |     322    |   12836 K  |   12835 K  |
|       from small pool |     291    |     341    |   14125 K  |   14124 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   26961 K  |   26960 K  |
|       from large pool |     314    |     322    |   12836 K  |   12835 K  |
|       from small pool |     291    |     341    |   14125 K  |   14124 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     159    |     209    |    3951    |    3792    |
|       from large pool |     147    |     147    |    1441    |    1294    |
|       from small pool |      12    |      62    |    2510    |    2498    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     154    |     154    |   15246 K  |   15246 K  |
|       from large pool |     130    |     130    |    8457 K  |    8457 K  |
|       from small pool |      24    |      45    |    6789 K  |    6789 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:55:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:55:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:55:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:55:36] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:55:44]    INFO >> epoch 009:    422 / 1539 loss=3.559, wps=4366.2, ups=6.09, wpb=716.4, bsz=716.4, num_updates=12700, lr=0.000227, gnorm=3.518, clip=0, train_wall=7, gb_free=73.1, wall=13082 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:55:51]    INFO >> epoch 009:    472 / 1539 loss=3.424, wps=4759.3, ups=6.88, wpb=691.3, bsz=691.3, num_updates=12750, lr=0.000227, gnorm=3.615, clip=0, train_wall=7, gb_free=74, wall=13089 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:55:59]    INFO >> epoch 009:    522 / 1539 loss=3.344, wps=5037, ups=6.62, wpb=760.8, bsz=760.8, num_updates=12800, lr=0.000227, gnorm=4.421, clip=0, train_wall=7, gb_free=72.7, wall=13097 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:56:08]    INFO >> epoch 009:    572 / 1539 loss=3.614, wps=4810.4, ups=6.41, wpb=750.9, bsz=750.9, num_updates=12850, lr=0.000227, gnorm=4.26, clip=0, train_wall=7, gb_free=74.2, wall=13105 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:56:09] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 513.25 MiB is free. Including non-PyTorch memory, this process has 78.62 GiB memory in use. Of the allocated memory 72.94 GiB is allocated by PyTorch, and 5.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:56:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 35           |        cudaMalloc retries: 59        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69870 MiB |  74868 MiB | 415417 GiB | 415349 GiB |
|       from large pool |  69853 MiB |  74851 MiB | 413068 GiB | 412999 GiB |
|       from small pool |     17 MiB |     17 MiB |   2349 GiB |   2349 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69870 MiB |  74868 MiB | 415417 GiB | 415349 GiB |
|       from large pool |  69853 MiB |  74851 MiB | 413068 GiB | 412999 GiB |
|       from small pool |     17 MiB |     17 MiB |   2349 GiB |   2349 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69846 MiB |  74842 MiB | 414536 GiB | 414468 GiB |
|       from large pool |  69829 MiB |  74825 MiB | 412190 GiB | 412122 GiB |
|       from small pool |     17 MiB |     17 MiB |   2346 GiB |   2346 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79992 MiB |  80030 MiB | 455866 MiB | 375874 MiB |
|       from large pool |  79966 MiB |  79966 MiB | 450806 MiB | 370840 MiB |
|       from small pool |     26 MiB |     64 MiB |   5060 MiB |   5034 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  10121 MiB |  11499 MiB | 399984 GiB | 399974 GiB |
|       from large pool |  10112 MiB |  11490 MiB | 397326 GiB | 397316 GiB |
|       from small pool |      8 MiB |     25 MiB |   2657 GiB |   2657 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     643    |     661    |   27345 K  |   27344 K  |
|       from large pool |     344    |     362    |   13035 K  |   13034 K  |
|       from small pool |     299    |     356    |   14310 K  |   14310 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     643    |     661    |   27345 K  |   27344 K  |
|       from large pool |     344    |     362    |   13035 K  |   13034 K  |
|       from small pool |     299    |     356    |   14310 K  |   14310 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     160    |     179    |    3971    |    3811    |
|       from large pool |     147    |     147    |    1441    |    1294    |
|       from small pool |      13    |      32    |    2530    |    2517    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     140    |     140    |   15459 K  |   15458 K  |
|       from large pool |     114    |     114    |    8587 K  |    8587 K  |
|       from small pool |      26    |      54    |    6871 K  |    6871 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:56:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:56:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:56:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:56:09] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:56:16]    INFO >> epoch 009:    623 / 1539 loss=3.54, wps=4394.6, ups=6.19, wpb=709.4, bsz=709.4, num_updates=12900, lr=0.000227, gnorm=3.62, clip=0, train_wall=7, gb_free=65.9, wall=13113 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:56:23]    INFO >> epoch 009:    673 / 1539 loss=3.494, wps=4649.1, ups=7.06, wpb=658.7, bsz=658.7, num_updates=12950, lr=0.000227, gnorm=4.266, clip=0, train_wall=7, gb_free=71.5, wall=13120 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:56:30]    INFO >> epoch 009:    723 / 1539 loss=3.545, wps=4638.3, ups=6.91, wpb=671.1, bsz=671.1, num_updates=13000, lr=0.000227, gnorm=3.824, clip=0, train_wall=7, gb_free=71.2, wall=13127 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:56:38]    INFO >> epoch 009:    773 / 1539 loss=3.438, wps=4575.7, ups=6.59, wpb=693.9, bsz=693.9, num_updates=13050, lr=0.000227, gnorm=4.444, clip=0, train_wall=7, gb_free=57.9, wall=13135 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:56:46] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 513.25 MiB is free. Including non-PyTorch memory, this process has 78.62 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 7.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:56:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 36           |        cudaMalloc retries: 60        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72060 MiB |  72460 MiB | 422558 GiB | 422488 GiB |
|       from large pool |  72043 MiB |  72442 MiB | 420172 GiB | 420101 GiB |
|       from small pool |     17 MiB |     18 MiB |   2386 GiB |   2386 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72060 MiB |  72460 MiB | 422558 GiB | 422488 GiB |
|       from large pool |  72043 MiB |  72442 MiB | 420172 GiB | 420101 GiB |
|       from small pool |     17 MiB |     18 MiB |   2386 GiB |   2386 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 421661 GiB | 421591 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 419278 GiB | 419208 GiB |
|       from small pool |     17 MiB |     18 MiB |   2382 GiB |   2382 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79992 MiB |  80040 MiB | 455914 MiB | 375922 MiB |
|       from large pool |  79966 MiB |  79966 MiB | 450806 MiB | 370840 MiB |
|       from small pool |     26 MiB |     74 MiB |   5108 MiB |   5082 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7931 MiB |   9277 MiB | 407074 GiB | 407066 GiB |
|       from large pool |   7922 MiB |   9266 MiB | 404375 GiB | 404367 GiB |
|       from small pool |      8 MiB |     21 MiB |   2699 GiB |   2699 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   27805 K  |   27804 K  |
|       from large pool |     308    |     315    |   13274 K  |   13274 K  |
|       from small pool |     298    |     348    |   14530 K  |   14530 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   27805 K  |   27804 K  |
|       from large pool |     308    |     315    |   13274 K  |   13274 K  |
|       from small pool |     298    |     348    |   14530 K  |   14530 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     160    |     184    |    3995    |    3835    |
|       from large pool |     147    |     147    |    1441    |    1294    |
|       from small pool |      13    |      37    |    2554    |    2541    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     155    |     155    |   15711 K  |   15711 K  |
|       from large pool |     129    |     129    |    8743 K  |    8742 K  |
|       from small pool |      26    |      52    |    6968 K  |    6968 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:56:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:56:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:56:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:56:46] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:56:47]    INFO >> epoch 009:    824 / 1539 loss=3.455, wps=4570.9, ups=6.35, wpb=719.6, bsz=719.6, num_updates=13100, lr=0.000227, gnorm=4.029, clip=0, train_wall=7, gb_free=70.5, wall=13143 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:56:54]    INFO >> epoch 009:    874 / 1539 loss=3.519, wps=4730.7, ups=7.34, wpb=644.4, bsz=644.4, num_updates=13150, lr=0.000227, gnorm=3.484, clip=0, train_wall=6, gb_free=74.8, wall=13149 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:57:01]    INFO >> epoch 009:    924 / 1539 loss=3.556, wps=4373.5, ups=7.07, wpb=618.7, bsz=618.7, num_updates=13200, lr=0.000227, gnorm=3.865, clip=0, train_wall=7, gb_free=73.5, wall=13157 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:57:10]    INFO >> epoch 009:    974 / 1539 loss=3.402, wps=4807.4, ups=5.59, wpb=859.7, bsz=859.7, num_updates=13250, lr=0.000227, gnorm=4.51, clip=0, train_wall=8, gb_free=70.8, wall=13165 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:57:18]    INFO >> epoch 009:   1024 / 1539 loss=3.444, wps=4832.3, ups=6.72, wpb=718.9, bsz=718.9, num_updates=13300, lr=0.000227, gnorm=3.825, clip=0, train_wall=7, gb_free=69.9, wall=13173 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:57:26]    INFO >> epoch 009:   1074 / 1539 loss=3.44, wps=4818.5, ups=6.64, wpb=725.8, bsz=725.8, num_updates=13350, lr=0.000227, gnorm=3.875, clip=0, train_wall=7, gb_free=65.3, wall=13180 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:57:34]    INFO >> epoch 009:   1124 / 1539 loss=3.454, wps=4995.2, ups=6.57, wpb=760.3, bsz=760.3, num_updates=13400, lr=0.000227, gnorm=4.017, clip=0, train_wall=7, gb_free=73.5, wall=13188 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:57:41]    INFO >> epoch 009:   1174 / 1539 loss=3.607, wps=4752.3, ups=7.1, wpb=669.6, bsz=669.6, num_updates=13450, lr=0.000227, gnorm=3.494, clip=0, train_wall=7, gb_free=72.7, wall=13195 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:57:50]    INFO >> epoch 009:   1224 / 1539 loss=3.08, wps=5295.4, ups=6.14, wpb=862, bsz=862, num_updates=13500, lr=0.000227, gnorm=4.322, clip=0, train_wall=8, gb_free=69, wall=13203 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:57:58]    INFO >> epoch 009:   1274 / 1539 loss=3.495, wps=5073.7, ups=6.54, wpb=776.3, bsz=776.3, num_updates=13550, lr=0.000227, gnorm=3.741, clip=0, train_wall=7, gb_free=72.7, wall=13211 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:58:06]    INFO >> epoch 009:   1324 / 1539 loss=3.56, wps=4555.3, ups=6.39, wpb=712.6, bsz=712.6, num_updates=13600, lr=0.000227, gnorm=3.515, clip=0, train_wall=7, gb_free=75.1, wall=13219 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:58:13]    INFO >> epoch 009:   1374 / 1539 loss=3.09, wps=4969.5, ups=6.63, wpb=749.6, bsz=749.6, num_updates=13650, lr=0.000227, gnorm=4.092, clip=2, train_wall=7, gb_free=69.9, wall=13226 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:58:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 59.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 75.97 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:58:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 37           |        cudaMalloc retries: 61        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77729 MiB |  77789 MiB | 440305 GiB | 440229 GiB |
|       from large pool |  77348 MiB |  77408 MiB | 437816 GiB | 437740 GiB |
|       from small pool |    381 MiB |    382 MiB |   2488 GiB |   2488 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77729 MiB |  77789 MiB | 440305 GiB | 440229 GiB |
|       from large pool |  77348 MiB |  77408 MiB | 437816 GiB | 437740 GiB |
|       from small pool |    381 MiB |    382 MiB |   2488 GiB |   2488 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77717 MiB |  77776 MiB | 439369 GiB | 439293 GiB |
|       from large pool |  77337 MiB |  77397 MiB | 436883 GiB | 436808 GiB |
|       from small pool |    379 MiB |    380 MiB |   2485 GiB |   2484 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80446 MiB |  80448 MiB | 456370 MiB | 375924 MiB |
|       from large pool |  80026 MiB |  80026 MiB | 450866 MiB | 370840 MiB |
|       from small pool |    420 MiB |    422 MiB |   5504 MiB |   5084 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2656 MiB |   7498 MiB | 424902 GiB | 424900 GiB |
|       from large pool |   2617 MiB |   7492 MiB | 422084 GiB | 422082 GiB |
|       from small pool |     38 MiB |     40 MiB |   2817 GiB |   2817 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7043    |    7046    |   29023 K  |   29016 K  |
|       from large pool |     910    |     911    |   13868 K  |   13868 K  |
|       from small pool |    6133    |    6136    |   15154 K  |   15148 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7043    |    7046    |   29023 K  |   29016 K  |
|       from large pool |     910    |     911    |   13868 K  |   13868 K  |
|       from small pool |    6133    |    6136    |   15154 K  |   15148 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     358    |     359    |    4194    |    3836    |
|       from large pool |     148    |     148    |    1442    |    1294    |
|       from small pool |     210    |     211    |    2752    |    2542    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     523    |     524    |   16395 K  |   16395 K  |
|       from large pool |     147    |     147    |    9130 K  |    9130 K  |
|       from small pool |     376    |     377    |    7264 K  |    7264 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:58:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:58:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:58:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:58:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:58:22]    INFO >> epoch 009:   1425 / 1539 loss=3.429, wps=4469.8, ups=6.6, wpb=676.7, bsz=676.7, num_updates=13700, lr=0.000227, gnorm=3.911, clip=0, train_wall=7, gb_free=72, wall=13234 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:58:29]    INFO >> epoch 009:   1475 / 1539 loss=3.58, wps=4425.5, ups=6.98, wpb=634.1, bsz=634.1, num_updates=13750, lr=0.000227, gnorm=3.653, clip=0, train_wall=7, gb_free=72.9, wall=13241 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:58:36]    INFO >> epoch 009:   1525 / 1539 loss=3.455, wps=4389, ups=7.04, wpb=623.3, bsz=623.3, num_updates=13800, lr=0.000227, gnorm=3.813, clip=0, train_wall=7, gb_free=67.9, wall=13248 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:58:38]    INFO >> epoch 009 | loss 3.466 | wps 4478.7 | ups 6.28 | wpb 712.7 | bsz 712.7 | num_updates 13814 | lr 0.000227 | gnorm 3.946 | clip 0.1 | train_wall 216 | gb_free 74.2 | wall 13250 (progress_bar.py:267, print())[0m
[33m[2025-11-19 02:58:38] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:58:51]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.621 | wps 11769.7 | wpb 5412.5 | bsz 5412.5 | num_updates 13814 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 02:58:52]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 02:58:52]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 9 @ 13814 updates, score 3.621) (writing took 0.013530 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 02:58:52] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:58:59]    INFO >> epoch 010:     36 / 1539 loss=3.485, wps=1643.6, ups=2.34, wpb=701.4, bsz=701.4, num_updates=13850, lr=0.000193, gnorm=3.567, clip=0, train_wall=8, gb_free=72.8, wall=13269 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:59:06]    INFO >> epoch 010:     86 / 1539 loss=3.375, wps=4990.6, ups=6.96, wpb=716.9, bsz=716.9, num_updates=13900, lr=0.000193, gnorm=4.239, clip=0, train_wall=7, gb_free=69.6, wall=13277 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:59:13]    INFO >> epoch 010:    136 / 1539 loss=3.445, wps=4405.1, ups=7.03, wpb=626.5, bsz=626.5, num_updates=13950, lr=0.000193, gnorm=4.081, clip=0, train_wall=7, gb_free=74.1, wall=13284 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:59:22]    INFO >> epoch 010:    186 / 1539 loss=3.388, wps=4536, ups=6.1, wpb=744.2, bsz=744.2, num_updates=14000, lr=0.000193, gnorm=4.556, clip=0, train_wall=8, gb_free=74.2, wall=13292 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:59:31]    INFO >> epoch 010:    236 / 1539 loss=3.333, wps=5258.9, ups=6.09, wpb=863.9, bsz=863.9, num_updates=14050, lr=0.000193, gnorm=4.12, clip=0, train_wall=8, gb_free=74.4, wall=13300 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:59:39]    INFO >> epoch 010:    286 / 1539 loss=3.538, wps=4453.5, ups=6.59, wpb=676, bsz=676, num_updates=14100, lr=0.000193, gnorm=3.868, clip=0, train_wall=7, gb_free=73.9, wall=13308 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:59:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 511.25 MiB is free. Including non-PyTorch memory, this process has 78.62 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 7.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:59:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 38           |        cudaMalloc retries: 62        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72060 MiB |  72460 MiB | 458001 GiB | 457930 GiB |
|       from large pool |  72043 MiB |  72442 MiB | 455406 GiB | 455335 GiB |
|       from small pool |     17 MiB |     18 MiB |   2594 GiB |   2594 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72060 MiB |  72460 MiB | 458001 GiB | 457930 GiB |
|       from large pool |  72043 MiB |  72442 MiB | 455406 GiB | 455335 GiB |
|       from small pool |     17 MiB |     18 MiB |   2594 GiB |   2594 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 457030 GiB | 456960 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 454439 GiB | 454368 GiB |
|       from small pool |     17 MiB |     18 MiB |   2591 GiB |   2591 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79994 MiB |  80386 MiB | 456370 MiB | 376376 MiB |
|       from large pool |  79966 MiB |  79966 MiB | 450866 MiB | 370900 MiB |
|       from small pool |     28 MiB |    420 MiB |   5504 MiB |   5476 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7933 MiB |   9279 MiB | 440248 GiB | 440240 GiB |
|       from large pool |   7922 MiB |   9266 MiB | 437313 GiB | 437305 GiB |
|       from small pool |     10 MiB |     25 MiB |   2934 GiB |   2934 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   30161 K  |   30161 K  |
|       from large pool |     308    |     315    |   14348 K  |   14348 K  |
|       from small pool |     298    |     342    |   15813 K  |   15812 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   30161 K  |   30161 K  |
|       from large pool |     308    |     315    |   14348 K  |   14348 K  |
|       from small pool |     298    |     342    |   15813 K  |   15812 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     161    |     357    |    4194    |    4033    |
|       from large pool |     147    |     147    |    1442    |    1295    |
|       from small pool |      14    |     210    |    2752    |    2738    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     155    |     155    |   17053 K  |   17053 K  |
|       from large pool |     129    |     129    |    9449 K  |    9448 K  |
|       from small pool |      26    |      50    |    7604 K  |    7604 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:59:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:59:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:59:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:59:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:59:46]    INFO >> epoch 010:    337 / 1539 loss=3.425, wps=4591.4, ups=6.52, wpb=703.9, bsz=703.9, num_updates=14150, lr=0.000193, gnorm=4.045, clip=0, train_wall=7, gb_free=67.2, wall=13315 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:59:54]    INFO >> epoch 010:    387 / 1539 loss=3.516, wps=4600.6, ups=6.36, wpb=723.5, bsz=723.5, num_updates=14200, lr=0.000193, gnorm=4.108, clip=0, train_wall=7, gb_free=73.3, wall=13323 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:00:03]    INFO >> epoch 010:    437 / 1539 loss=3.435, wps=4411.1, ups=6.99, wpb=631.4, bsz=631.4, num_updates=14250, lr=0.000193, gnorm=3.974, clip=0, train_wall=7, gb_free=72.6, wall=13330 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:00:10]    INFO >> epoch 010:    487 / 1539 loss=3.459, wps=4349.5, ups=6.51, wpb=668.5, bsz=668.5, num_updates=14300, lr=0.000193, gnorm=3.761, clip=0, train_wall=7, gb_free=72.5, wall=13338 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:00:17]    INFO >> epoch 010:    537 / 1539 loss=3.431, wps=4625.5, ups=7.16, wpb=645.7, bsz=645.7, num_updates=14350, lr=0.000193, gnorm=3.584, clip=0, train_wall=7, gb_free=75, wall=13345 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:00:25]    INFO >> epoch 010:    587 / 1539 loss=3.552, wps=4384.9, ups=6.59, wpb=665.2, bsz=665.2, num_updates=14400, lr=0.000193, gnorm=3.462, clip=0, train_wall=7, gb_free=71.4, wall=13353 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:00:32]    INFO >> epoch 010:    637 / 1539 loss=3.52, wps=5092.9, ups=7.08, wpb=719.7, bsz=719.7, num_updates=14450, lr=0.000193, gnorm=3.891, clip=0, train_wall=7, gb_free=69.2, wall=13360 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:00:41]    INFO >> epoch 010:    687 / 1539 loss=3.377, wps=5018.8, ups=6.91, wpb=726.6, bsz=726.6, num_updates=14500, lr=0.000193, gnorm=3.956, clip=0, train_wall=7, gb_free=73.3, wall=13367 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:00:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 59.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 75.97 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:00:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 39           |        cudaMalloc retries: 63        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77729 MiB |  77789 MiB | 469068 GiB | 468993 GiB |
|       from large pool |  77348 MiB |  77408 MiB | 466413 GiB | 466337 GiB |
|       from small pool |    381 MiB |    382 MiB |   2655 GiB |   2655 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77729 MiB |  77789 MiB | 469068 GiB | 468993 GiB |
|       from large pool |  77348 MiB |  77408 MiB | 466413 GiB | 466337 GiB |
|       from small pool |    381 MiB |    382 MiB |   2655 GiB |   2655 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77717 MiB |  77776 MiB | 468073 GiB | 467997 GiB |
|       from large pool |  77337 MiB |  77397 MiB | 465421 GiB | 465346 GiB |
|       from small pool |    379 MiB |    380 MiB |   2651 GiB |   2651 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80446 MiB |  80448 MiB | 456824 MiB | 376378 MiB |
|       from large pool |  80026 MiB |  80026 MiB | 450926 MiB | 370900 MiB |
|       from small pool |    420 MiB |    422 MiB |   5898 MiB |   5478 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2656 MiB |   7498 MiB | 451349 GiB | 451346 GiB |
|       from large pool |   2617 MiB |   7492 MiB | 448344 GiB | 448342 GiB |
|       from small pool |     38 MiB |     40 MiB |   3004 GiB |   3004 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7043    |    7046    |   30907 K  |   30900 K  |
|       from large pool |     910    |     911    |   14727 K  |   14726 K  |
|       from small pool |    6133    |    6136    |   16179 K  |   16173 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7043    |    7046    |   30907 K  |   30900 K  |
|       from large pool |     910    |     911    |   14727 K  |   14726 K  |
|       from small pool |    6133    |    6136    |   16179 K  |   16173 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     358    |     359    |    4392    |    4034    |
|       from large pool |     148    |     148    |    1443    |    1295    |
|       from small pool |     210    |     211    |    2949    |    2739    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     521    |     522    |   17466 K  |   17466 K  |
|       from large pool |     147    |     147    |    9697 K  |    9697 K  |
|       from small pool |     374    |     375    |    7768 K  |    7768 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:00:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:00:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:00:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:00:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:00:48]    INFO >> epoch 010:    738 / 1539 loss=3.628, wps=4269.5, ups=6.64, wpb=642.7, bsz=642.7, num_updates=14550, lr=0.000193, gnorm=3.473, clip=0, train_wall=7, gb_free=68.7, wall=13375 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:00:56]    INFO >> epoch 010:    788 / 1539 loss=3.438, wps=4697, ups=6.49, wpb=723.3, bsz=723.3, num_updates=14600, lr=0.000193, gnorm=4.032, clip=0, train_wall=7, gb_free=67.8, wall=13382 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:01:03]    INFO >> epoch 010:    838 / 1539 loss=3.561, wps=4484.6, ups=6.65, wpb=673.9, bsz=673.9, num_updates=14650, lr=0.000193, gnorm=3.584, clip=0, train_wall=7, gb_free=66.8, wall=13390 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:01:13]    INFO >> epoch 010:    888 / 1539 loss=3.246, wps=5187.4, ups=6.11, wpb=848.7, bsz=848.7, num_updates=14700, lr=0.000193, gnorm=4.415, clip=0, train_wall=8, gb_free=76.2, wall=13398 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:01:20]    INFO >> epoch 010:    938 / 1539 loss=3.463, wps=5003.9, ups=7.01, wpb=713.5, bsz=713.5, num_updates=14750, lr=0.000193, gnorm=4.17, clip=0, train_wall=7, gb_free=74.7, wall=13405 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:01:27]    INFO >> epoch 010:    988 / 1539 loss=3.607, wps=4822.3, ups=6.98, wpb=690.7, bsz=690.7, num_updates=14800, lr=0.000193, gnorm=3.375, clip=0, train_wall=7, gb_free=74.4, wall=13412 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:01:34]    INFO >> epoch 010:   1038 / 1539 loss=3.469, wps=4640.9, ups=6.91, wpb=671.7, bsz=671.7, num_updates=14850, lr=0.000193, gnorm=4.088, clip=0, train_wall=7, gb_free=70, wall=13419 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:01:43]    INFO >> epoch 010:   1088 / 1539 loss=3.399, wps=5401, ups=6.72, wpb=803.8, bsz=803.8, num_updates=14900, lr=0.000193, gnorm=4.35, clip=0, train_wall=7, gb_free=72.8, wall=13427 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:01:50]    INFO >> epoch 010:   1138 / 1539 loss=3.441, wps=4233.5, ups=6.91, wpb=612.7, bsz=612.7, num_updates=14950, lr=0.000193, gnorm=3.647, clip=0, train_wall=7, gb_free=71.8, wall=13434 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:01:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 513.25 MiB is free. Including non-PyTorch memory, this process has 78.62 GiB memory in use. Of the allocated memory 72.94 GiB is allocated by PyTorch, and 5.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:01:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 40           |        cudaMalloc retries: 64        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69870 MiB |  74868 MiB | 482884 GiB | 482815 GiB |
|       from large pool |  69853 MiB |  74851 MiB | 480153 GiB | 480085 GiB |
|       from small pool |     17 MiB |     17 MiB |   2730 GiB |   2730 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69870 MiB |  74868 MiB | 482884 GiB | 482815 GiB |
|       from large pool |  69853 MiB |  74851 MiB | 480153 GiB | 480085 GiB |
|       from small pool |     17 MiB |     17 MiB |   2730 GiB |   2730 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69846 MiB |  74842 MiB | 481857 GiB | 481789 GiB |
|       from large pool |  69829 MiB |  74825 MiB | 479130 GiB | 479062 GiB |
|       from small pool |     17 MiB |     17 MiB |   2726 GiB |   2726 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79992 MiB |  80386 MiB | 456824 MiB | 376832 MiB |
|       from large pool |  79966 MiB |  79966 MiB | 450926 MiB | 370960 MiB |
|       from small pool |     26 MiB |    420 MiB |   5898 MiB |   5872 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  10121 MiB |  11499 MiB | 465237 GiB | 465227 GiB |
|       from large pool |  10112 MiB |  11490 MiB | 462146 GiB | 462136 GiB |
|       from small pool |      8 MiB |     25 MiB |   3091 GiB |   3091 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     643    |     661    |   31823 K  |   31822 K  |
|       from large pool |     344    |     362    |   15191 K  |   15190 K  |
|       from small pool |     299    |     342    |   16631 K  |   16631 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     643    |     661    |   31823 K  |   31822 K  |
|       from large pool |     344    |     362    |   15191 K  |   15190 K  |
|       from small pool |     299    |     342    |   16631 K  |   16631 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     160    |     357    |    4392    |    4232    |
|       from large pool |     147    |     147    |    1443    |    1296    |
|       from small pool |      13    |     210    |    2949    |    2936    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     140    |     140    |   17975 K  |   17975 K  |
|       from large pool |     114    |     114    |    9999 K  |    9999 K  |
|       from small pool |      26    |      54    |    7975 K  |    7975 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:01:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:01:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:01:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:01:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:01:59]    INFO >> epoch 010:   1189 / 1539 loss=3.5, wps=4613.3, ups=5.85, wpb=787.9, bsz=787.9, num_updates=15000, lr=0.000193, gnorm=3.906, clip=0, train_wall=8, gb_free=70.1, wall=13443 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:02:07]    INFO >> epoch 010:   1239 / 1539 loss=3.372, wps=4972.9, ups=6.32, wpb=786.3, bsz=786.3, num_updates=15050, lr=0.000193, gnorm=4.431, clip=0, train_wall=7, gb_free=73.2, wall=13451 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:02:16]    INFO >> epoch 010:   1289 / 1539 loss=3.5, wps=4898.2, ups=6.39, wpb=766.5, bsz=766.5, num_updates=15100, lr=0.000193, gnorm=3.822, clip=0, train_wall=7, gb_free=72.7, wall=13458 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:02:23]    INFO >> epoch 010:   1339 / 1539 loss=3.533, wps=4595, ups=6.8, wpb=675.5, bsz=675.5, num_updates=15150, lr=0.000193, gnorm=3.904, clip=0, train_wall=7, gb_free=74.4, wall=13466 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:02:31]    INFO >> epoch 010:   1389 / 1539 loss=3.397, wps=4890.8, ups=6.55, wpb=746.1, bsz=746.1, num_updates=15200, lr=0.000193, gnorm=4.097, clip=0, train_wall=7, gb_free=73.6, wall=13473 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:02:38]    INFO >> epoch 010:   1439 / 1539 loss=3.459, wps=4671, ups=6.87, wpb=679.6, bsz=679.6, num_updates=15250, lr=0.000193, gnorm=3.916, clip=0, train_wall=7, gb_free=70.1, wall=13481 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:02:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 575.25 MiB is free. Including non-PyTorch memory, this process has 78.55 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 4.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:02:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 41           |        cudaMalloc retries: 65        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75058 MiB |  76001 MiB | 491622 GiB | 491549 GiB |
|       from large pool |  75045 MiB |  75988 MiB | 488845 GiB | 488771 GiB |
|       from small pool |     12 MiB |     24 MiB |   2777 GiB |   2777 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75058 MiB |  76001 MiB | 491622 GiB | 491549 GiB |
|       from large pool |  75045 MiB |  75988 MiB | 488845 GiB | 488771 GiB |
|       from small pool |     12 MiB |     24 MiB |   2777 GiB |   2777 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 490577 GiB | 490503 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 487803 GiB | 487729 GiB |
|       from small pool |     12 MiB |     24 MiB |   2773 GiB |   2773 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79930 MiB |  80176 MiB | 457008 MiB | 377078 MiB |
|       from large pool |  79906 MiB |  79966 MiB | 450926 MiB | 371020 MiB |
|       from small pool |     24 MiB |    210 MiB |   6082 MiB |   6058 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4811 MiB |  11162 MiB | 474057 GiB | 474052 GiB |
|       from large pool |   4800 MiB |  11151 MiB | 470911 GiB | 470907 GiB |
|       from small pool |     11 MiB |     31 MiB |   3145 GiB |   3145 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   32399 K  |   32398 K  |
|       from large pool |     314    |     322    |   15482 K  |   15482 K  |
|       from small pool |     291    |     356    |   16916 K  |   16916 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   32399 K  |   32398 K  |
|       from large pool |     314    |     322    |   15482 K  |   15482 K  |
|       from small pool |     291    |     356    |   16916 K  |   16916 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     158    |     252    |    4484    |    4326    |
|       from large pool |     146    |     147    |    1443    |    1297    |
|       from small pool |      12    |     105    |    3041    |    3029    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     148    |     153    |   18296 K  |   18296 K  |
|       from large pool |     119    |     124    |   10189 K  |   10189 K  |
|       from small pool |      29    |      60    |    8107 K  |    8107 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:02:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:02:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:02:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:02:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:02:46]    INFO >> epoch 010:   1490 / 1539 loss=3.599, wps=4429.4, ups=6.38, wpb=694.7, bsz=694.7, num_updates=15300, lr=0.000193, gnorm=3.99, clip=0, train_wall=7, gb_free=64.7, wall=13488 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:02:55]    INFO >> epoch 010 | loss 3.458 | wps 4453.9 | ups 6.25 | wpb 712.7 | bsz 712.7 | num_updates 15349 | lr 0.000193 | gnorm 3.955 | clip 0 | train_wall 217 | gb_free 70.8 | wall 13496 (progress_bar.py:267, print())[0m
[33m[2025-11-19 03:02:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:03:08]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.609 | wps 11399.3 | wpb 5412.5 | bsz 5412.5 | num_updates 15349 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 03:03:08]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:03:08]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 10 @ 15349 updates, score 3.609) (writing took 0.013777 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 03:03:08] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:03:09]    INFO >> epoch 011:      1 / 1539 loss=3.477, wps=1723.6, ups=2.37, wpb=728.6, bsz=728.6, num_updates=15350, lr=0.000161, gnorm=3.938, clip=0, train_wall=7, gb_free=68.7, wall=13510 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:03:16]    INFO >> epoch 011:     51 / 1539 loss=3.581, wps=4570.4, ups=7.06, wpb=647.3, bsz=647.3, num_updates=15400, lr=0.000161, gnorm=3.532, clip=0, train_wall=7, gb_free=75, wall=13517 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:03:25]    INFO >> epoch 011:    101 / 1539 loss=3.45, wps=4863.5, ups=6.67, wpb=729.7, bsz=729.7, num_updates=15450, lr=0.000161, gnorm=3.851, clip=0, train_wall=7, gb_free=74.4, wall=13524 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:03:31]    INFO >> epoch 011:    151 / 1539 loss=3.543, wps=4901.3, ups=7.36, wpb=666, bsz=666, num_updates=15500, lr=0.000161, gnorm=4.02, clip=0, train_wall=6, gb_free=71.4, wall=13531 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:03:39]    INFO >> epoch 011:    201 / 1539 loss=3.332, wps=5103.9, ups=6.63, wpb=769.4, bsz=769.4, num_updates=15550, lr=0.000161, gnorm=3.843, clip=0, train_wall=7, gb_free=71.9, wall=13539 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:03:46]    INFO >> epoch 011:    251 / 1539 loss=3.484, wps=5103.7, ups=7, wpb=729.4, bsz=729.4, num_updates=15600, lr=0.000161, gnorm=3.693, clip=0, train_wall=7, gb_free=73.3, wall=13546 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:03:53]    INFO >> epoch 011:    301 / 1539 loss=3.44, wps=4310.8, ups=6.92, wpb=623.3, bsz=623.3, num_updates=15650, lr=0.000161, gnorm=3.708, clip=0, train_wall=7, gb_free=74.6, wall=13553 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:04:02]    INFO >> epoch 011:    351 / 1539 loss=3.425, wps=4872.5, ups=6.49, wpb=751.3, bsz=751.3, num_updates=15700, lr=0.000161, gnorm=4.178, clip=0, train_wall=7, gb_free=72, wall=13561 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:04:09]    INFO >> epoch 011:    401 / 1539 loss=3.522, wps=4941.5, ups=7.29, wpb=677.5, bsz=677.5, num_updates=15750, lr=0.000161, gnorm=3.475, clip=0, train_wall=6, gb_free=72.3, wall=13567 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:04:16]    INFO >> epoch 011:    451 / 1539 loss=3.51, wps=4422.1, ups=7.09, wpb=624.1, bsz=624.1, num_updates=15800, lr=0.000161, gnorm=3.673, clip=0, train_wall=7, gb_free=72.8, wall=13575 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:04:25]    INFO >> epoch 011:    501 / 1539 loss=3.551, wps=3649.1, ups=6.01, wpb=606.7, bsz=606.7, num_updates=15850, lr=0.000161, gnorm=3.914, clip=0, train_wall=8, gb_free=75, wall=13583 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:04:33]    INFO >> epoch 011:    551 / 1539 loss=3.55, wps=4547.9, ups=7.04, wpb=645.8, bsz=645.8, num_updates=15900, lr=0.000161, gnorm=3.622, clip=0, train_wall=7, gb_free=73.9, wall=13590 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:04:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 59.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 76.03 GiB is allocated by PyTorch, and 2.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:04:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 42           |        cudaMalloc retries: 66        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77793 MiB |  77853 MiB | 513719 GiB | 513643 GiB |
|       from large pool |  77411 MiB |  77471 MiB | 510808 GiB | 510732 GiB |
|       from small pool |    382 MiB |    383 MiB |   2910 GiB |   2910 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77793 MiB |  77853 MiB | 513719 GiB | 513643 GiB |
|       from large pool |  77411 MiB |  77471 MiB | 510808 GiB | 510732 GiB |
|       from small pool |    382 MiB |    383 MiB |   2910 GiB |   2910 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77777 MiB |  77836 MiB | 512628 GiB | 512552 GiB |
|       from large pool |  77397 MiB |  77456 MiB | 509721 GiB | 509646 GiB |
|       from small pool |    380 MiB |    381 MiB |   2906 GiB |   2905 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80446 MiB |  80448 MiB | 457586 MiB | 377140 MiB |
|       from large pool |  80026 MiB |  80026 MiB | 451106 MiB | 371080 MiB |
|       from small pool |    420 MiB |    422 MiB |   6480 MiB |   6060 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2592 MiB |   7406 MiB | 494129 GiB | 494126 GiB |
|       from large pool |   2554 MiB |   7398 MiB | 490836 GiB | 490834 GiB |
|       from small pool |     37 MiB |     40 MiB |   3292 GiB |   3292 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7054    |    7057    |   33868 K  |   33861 K  |
|       from large pool |     911    |     912    |   16130 K  |   16129 K  |
|       from small pool |    6143    |    6146    |   17738 K  |   17732 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7054    |    7057    |   33868 K  |   33861 K  |
|       from large pool |     911    |     912    |   16130 K  |   16129 K  |
|       from small pool |    6143    |    6146    |   17738 K  |   17732 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     358    |     359    |    4686    |    4328    |
|       from large pool |     148    |     148    |    1446    |    1298    |
|       from small pool |     210    |     211    |    3240    |    3030    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     516    |     518    |   19139 K  |   19139 K  |
|       from large pool |     142    |     142    |   10618 K  |   10618 K  |
|       from small pool |     374    |     376    |    8521 K  |    8520 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:04:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:04:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:04:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:04:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:04:41]    INFO >> epoch 011:    602 / 1539 loss=3.412, wps=4294.8, ups=6.19, wpb=694.1, bsz=694.1, num_updates=15950, lr=0.000161, gnorm=3.728, clip=0, train_wall=7, gb_free=71.6, wall=13598 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:04:49]    INFO >> epoch 011:    652 / 1539 loss=3.5, wps=5195.5, ups=6.37, wpb=815.3, bsz=815.3, num_updates=16000, lr=0.000161, gnorm=3.114, clip=0, train_wall=7, gb_free=71.5, wall=13606 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:04:56]    INFO >> epoch 011:    702 / 1539 loss=3.453, wps=4595.4, ups=6.95, wpb=661.1, bsz=661.1, num_updates=16050, lr=0.000161, gnorm=3.545, clip=0, train_wall=7, gb_free=71, wall=13613 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:05:04]    INFO >> epoch 011:    752 / 1539 loss=3.359, wps=4436.4, ups=6.06, wpb=731.8, bsz=731.8, num_updates=16100, lr=0.000161, gnorm=3.985, clip=0, train_wall=8, gb_free=72.6, wall=13621 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:05:10] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 575.25 MiB is free. Including non-PyTorch memory, this process has 78.55 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 4.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:05:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 43           |        cudaMalloc retries: 67        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75058 MiB |  76001 MiB | 520726 GiB | 520653 GiB |
|       from large pool |  75045 MiB |  75988 MiB | 517779 GiB | 517705 GiB |
|       from small pool |     12 MiB |     21 MiB |   2947 GiB |   2947 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75058 MiB |  76001 MiB | 520726 GiB | 520653 GiB |
|       from large pool |  75045 MiB |  75988 MiB | 517779 GiB | 517705 GiB |
|       from small pool |     12 MiB |     21 MiB |   2947 GiB |   2947 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 519620 GiB | 519546 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 516676 GiB | 516603 GiB |
|       from small pool |     12 MiB |     21 MiB |   2943 GiB |   2943 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79930 MiB |  80386 MiB | 457586 MiB | 377656 MiB |
|       from large pool |  79906 MiB |  79966 MiB | 451106 MiB | 371200 MiB |
|       from small pool |     24 MiB |    420 MiB |   6480 MiB |   6456 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4811 MiB |  11162 MiB | 501099 GiB | 501094 GiB |
|       from large pool |   4800 MiB |  11151 MiB | 497763 GiB | 497758 GiB |
|       from small pool |     11 MiB |     29 MiB |   3335 GiB |   3335 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   34327 K  |   34326 K  |
|       from large pool |     314    |     322    |   16362 K  |   16361 K  |
|       from small pool |     291    |     356    |   17965 K  |   17965 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   34327 K  |   34326 K  |
|       from large pool |     314    |     322    |   16362 K  |   16361 K  |
|       from small pool |     291    |     356    |   17965 K  |   17965 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     158    |     357    |    4686    |    4528    |
|       from large pool |     146    |     147    |    1446    |    1300    |
|       from small pool |      12    |     210    |    3240    |    3228    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     146    |     151    |   19397 K  |   19397 K  |
|       from large pool |     119    |     124    |   10770 K  |   10770 K  |
|       from small pool |      27    |      57    |    8627 K  |    8627 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:05:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:05:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:05:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:05:10] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:05:12]    INFO >> epoch 011:    803 / 1539 loss=3.531, wps=3967.8, ups=6.77, wpb=586.3, bsz=586.3, num_updates=16150, lr=0.000161, gnorm=3.467, clip=0, train_wall=6, gb_free=62.8, wall=13629 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:05:19]    INFO >> epoch 011:    853 / 1539 loss=3.463, wps=4587.2, ups=6.98, wpb=657, bsz=657, num_updates=16200, lr=0.000161, gnorm=3.673, clip=0, train_wall=7, gb_free=74.8, wall=13636 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:05:26]    INFO >> epoch 011:    903 / 1539 loss=3.38, wps=5381.4, ups=6.9, wpb=779.8, bsz=779.8, num_updates=16250, lr=0.000161, gnorm=4.415, clip=0, train_wall=7, gb_free=72.7, wall=13643 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:05:34]    INFO >> epoch 011:    953 / 1539 loss=3.376, wps=4920, ups=6.59, wpb=746, bsz=746, num_updates=16300, lr=0.000161, gnorm=3.669, clip=0, train_wall=7, gb_free=72, wall=13651 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:05:44]    INFO >> epoch 011:   1003 / 1539 loss=3.413, wps=4926.9, ups=6.68, wpb=737.8, bsz=737.8, num_updates=16350, lr=0.000161, gnorm=3.648, clip=0, train_wall=7, gb_free=71.3, wall=13658 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:05:51]    INFO >> epoch 011:   1053 / 1539 loss=3.238, wps=4653.4, ups=6.72, wpb=692.9, bsz=692.9, num_updates=16400, lr=0.000161, gnorm=3.62, clip=0, train_wall=7, gb_free=73.7, wall=13666 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:05:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 633.25 MiB is free. Including non-PyTorch memory, this process has 78.50 GiB memory in use. Of the allocated memory 72.94 GiB is allocated by PyTorch, and 5.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:05:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 44           |        cudaMalloc retries: 68        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69869 MiB |  74866 MiB | 529293 GiB | 529224 GiB |
|       from large pool |  69852 MiB |  74849 MiB | 526299 GiB | 526231 GiB |
|       from small pool |     17 MiB |     17 MiB |   2993 GiB |   2993 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69869 MiB |  74866 MiB | 529293 GiB | 529224 GiB |
|       from large pool |  69852 MiB |  74849 MiB | 526299 GiB | 526231 GiB |
|       from small pool |     17 MiB |     17 MiB |   2993 GiB |   2993 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69846 MiB |  74842 MiB | 528167 GiB | 528099 GiB |
|       from large pool |  69829 MiB |  74825 MiB | 525178 GiB | 525110 GiB |
|       from small pool |     17 MiB |     17 MiB |   2989 GiB |   2989 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79872 MiB |  79970 MiB | 457686 MiB | 377814 MiB |
|       from large pool |  79846 MiB |  79846 MiB | 451106 MiB | 371260 MiB |
|       from small pool |     26 MiB |    124 MiB |   6580 MiB |   6554 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  10002 MiB |  11380 MiB | 509775 GiB | 509765 GiB |
|       from large pool |   9993 MiB |  11371 MiB | 506386 GiB | 506377 GiB |
|       from small pool |      8 MiB |     25 MiB |   3388 GiB |   3388 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     643    |     661    |   34885 K  |   34884 K  |
|       from large pool |     344    |     362    |   16646 K  |   16646 K  |
|       from small pool |     299    |     342    |   18238 K  |   18238 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     643    |     661    |   34885 K  |   34884 K  |
|       from large pool |     344    |     362    |   16646 K  |   16646 K  |
|       from small pool |     299    |     342    |   18238 K  |   18238 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     158    |     207    |    4736    |    4578    |
|       from large pool |     145    |     145    |    1446    |    1301    |
|       from small pool |      13    |      62    |    3290    |    3277    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     141    |     142    |   19706 K  |   19706 K  |
|       from large pool |     114    |     115    |   10955 K  |   10955 K  |
|       from small pool |      27    |      53    |    8750 K  |    8750 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:05:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:05:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:05:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:05:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:06:00]    INFO >> epoch 011:   1104 / 1539 loss=3.511, wps=4538.9, ups=5.94, wpb=764, bsz=764, num_updates=16450, lr=0.000161, gnorm=3.914, clip=0, train_wall=7, gb_free=75, wall=13674 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:06:08]    INFO >> epoch 011:   1154 / 1539 loss=3.322, wps=5625.9, ups=6.19, wpb=909, bsz=909, num_updates=16500, lr=0.000161, gnorm=4.066, clip=0, train_wall=8, gb_free=69.3, wall=13682 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:06:17]    INFO >> epoch 011:   1204 / 1539 loss=3.444, wps=4085.6, ups=6.72, wpb=608.2, bsz=608.2, num_updates=16550, lr=0.000161, gnorm=3.356, clip=0, train_wall=7, gb_free=68.8, wall=13690 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:06:25]    INFO >> epoch 011:   1254 / 1539 loss=3.452, wps=4951, ups=5.97, wpb=828.9, bsz=828.9, num_updates=16600, lr=0.000161, gnorm=4.211, clip=0, train_wall=8, gb_free=73.1, wall=13698 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:06:32]    INFO >> epoch 011:   1304 / 1539 loss=3.419, wps=4963.6, ups=6.72, wpb=739.1, bsz=739.1, num_updates=16650, lr=0.000161, gnorm=3.595, clip=0, train_wall=7, gb_free=71.9, wall=13705 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:06:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 633.25 MiB is free. Including non-PyTorch memory, this process has 78.50 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 7.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:06:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 45           |        cudaMalloc retries: 69        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72059 MiB |  72460 MiB | 536404 GiB | 536334 GiB |
|       from large pool |  72042 MiB |  72442 MiB | 533372 GiB | 533301 GiB |
|       from small pool |     17 MiB |     23 MiB |   3032 GiB |   3032 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72059 MiB |  72460 MiB | 536404 GiB | 536334 GiB |
|       from large pool |  72042 MiB |  72442 MiB | 533372 GiB | 533301 GiB |
|       from small pool |     17 MiB |     23 MiB |   3032 GiB |   3032 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 535265 GiB | 535194 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 532236 GiB | 532166 GiB |
|       from small pool |     17 MiB |     23 MiB |   3028 GiB |   3028 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79872 MiB |  80056 MiB | 457870 MiB | 377998 MiB |
|       from large pool |  79846 MiB |  79846 MiB | 451106 MiB | 371260 MiB |
|       from small pool |     26 MiB |    210 MiB |   6764 MiB |   6738 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7812 MiB |   9157 MiB | 516886 GiB | 516878 GiB |
|       from large pool |   7803 MiB |   9146 MiB | 513453 GiB | 513445 GiB |
|       from small pool |      8 MiB |     29 MiB |   3433 GiB |   3433 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   35349 K  |   35348 K  |
|       from large pool |     308    |     315    |   16876 K  |   16876 K  |
|       from small pool |     298    |     356    |   18472 K  |   18472 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   35349 K  |   35348 K  |
|       from large pool |     308    |     315    |   16876 K  |   16876 K  |
|       from small pool |     298    |     356    |   18472 K  |   18472 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     158    |     250    |    4828    |    4670    |
|       from large pool |     145    |     145    |    1446    |    1301    |
|       from small pool |      13    |     105    |    3382    |    3369    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     152    |     152    |   19966 K  |   19966 K  |
|       from large pool |     126    |     126    |   11105 K  |   11105 K  |
|       from small pool |      26    |      57    |    8860 K  |    8860 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:06:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:06:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:06:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:06:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:06:41]    INFO >> epoch 011:   1355 / 1539 loss=3.511, wps=4200.3, ups=6.1, wpb=689.1, bsz=689.1, num_updates=16700, lr=0.000161, gnorm=3.644, clip=0, train_wall=7, gb_free=56.8, wall=13714 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:06:50]    INFO >> epoch 011:   1405 / 1539 loss=3.547, wps=5058.4, ups=6.39, wpb=791.3, bsz=791.3, num_updates=16750, lr=0.000161, gnorm=4.018, clip=0, train_wall=7, gb_free=73.5, wall=13721 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:06:57]    INFO >> epoch 011:   1455 / 1539 loss=3.524, wps=4510.6, ups=6.78, wpb=665.3, bsz=665.3, num_updates=16800, lr=0.000161, gnorm=3.404, clip=0, train_wall=7, gb_free=65.3, wall=13729 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:07:05]    INFO >> epoch 011:   1505 / 1539 loss=3.124, wps=5083.7, ups=6.21, wpb=818.6, bsz=818.6, num_updates=16850, lr=0.000161, gnorm=4.157, clip=0, train_wall=8, gb_free=72.5, wall=13737 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:07:10]    INFO >> epoch 011 | loss 3.44 | wps 4444.8 | ups 6.24 | wpb 712.7 | bsz 712.7 | num_updates 16884 | lr 0.000161 | gnorm 3.761 | clip 0 | train_wall 217 | gb_free 73.9 | wall 13742 (progress_bar.py:267, print())[0m
[33m[2025-11-19 03:07:10] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:07:24]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.726 | wps 11792.1 | wpb 5412.5 | bsz 5412.5 | num_updates 16884 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 03:07:25]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:07:25]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 11 @ 16884 updates, score 3.726) (writing took 0.013304 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-19 03:07:25]    INFO >> æ—©åœ: éªŒè¯æ€§èƒ½å·²10è½®æœªæå‡ (train_enhanced.py:345, single_main())[0m
[32m[2025-11-19 03:07:25]    INFO >> è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ 13692.9 ç§’ (train_enhanced.py:355, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 03:07:25]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:07:25]    INFO >> æ‰€æœ‰æ—¥å¿—å·²ä¿å­˜åˆ°: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs (train_enhanced.py:359, single_main())[0m
