(base) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ conda aci[Ktivate naturalcc
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ export NCC=/mnt/data1/zhaojunzhang/typilus-data
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc[00m$ cd run/type_prediction/typilus/experiment_tools
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python run_experiments.py
============================================================
Typilus å‚æ•°è°ƒä¼˜å®éªŒç³»ç»Ÿ
============================================================

å…± 6 ä¸ªå®éªŒ:

1. baseline: åŸºçº¿å®éªŒ
2. exp_lr_1e-3: å­¦ä¹ ç‡1e-3
3. exp_lr_1e-4: å­¦ä¹ ç‡1e-4
4. exp_batch_64: æ‰¹é‡å¤§å°64
5. exp_hidden_128: éšè—å±‚128
6. exp_best: æ¨èé…ç½®

============================================================

æŒ‰ Enter å¼€å§‹å®éªŒ...

è¿›åº¦: 1/6

============================================================
å®éªŒ: baseline - åŸºçº¿å®éªŒ
æ—¶é—´: 2025-11-18 22:58:38
============================================================

Traceback (most recent call last):
  File "run_experiments.py", line 369, in <module>
    main()
  File "run_experiments.py", line 342, in main
    success = run_single_experiment(exp, str(base_config), str(train_script))
  File "run_experiments.py", line 100, in run_single_experiment
    create_config(base_config, exp_config, str(config_path))
  File "run_experiments.py", line 71, in create_config
    with open(base_config, 'r', encoding='utf-8') as f:
FileNotFoundError: [Errno 2] No such file or directory: 'config/typilus.yml'
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python run_experiments.py[C[Ccd run/type_prediction/typilus/experiment_tools[C[Cexport NCC=/mnt/data1/zhaojunzhang/typilus-data[C[C[23Pconda activate naturalcc[C[Cexport NCC=/mnt/data1/zhaojunzhang/typilus-data[C[Ccd run/type_prediction/typilus/experiment_tools[C[C[22Ppython run_experiments.py[C[C[K[K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ [K[A(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ git pull
remote: Enumerating objects: 13, done.[K
remote: Counting objects:   7% (1/13)[Kremote: Counting objects:  15% (2/13)[Kremote: Counting objects:  23% (3/13)[Kremote: Counting objects:  30% (4/13)[Kremote: Counting objects:  38% (5/13)[Kremote: Counting objects:  46% (6/13)[Kremote: Counting objects:  53% (7/13)[Kremote: Counting objects:  61% (8/13)[Kremote: Counting objects:  69% (9/13)[Kremote: Counting objects:  76% (10/13)[Kremote: Counting objects:  84% (11/13)[Kremote: Counting objects:  92% (12/13)[Kremote: Counting objects: 100% (13/13)[Kremote: Counting objects: 100% (13/13), done.[K
remote: Compressing objects: 100% (1/1)[Kremote: Compressing objects: 100% (1/1), done.[K
remote: Total 7 (delta 6), reused 7 (delta 6), pack-reused 0 (from 0)[K
Unpacking objects:  14% (1/7)Unpacking objects:  28% (2/7)Unpacking objects:  42% (3/7)Unpacking objects:  57% (4/7)Unpacking objects:  71% (5/7)Unpacking objects:  85% (6/7)Unpacking objects: 100% (7/7)Unpacking objects: 100% (7/7), 906 bytes | 906.00 KiB/s, done.
From https://github.com/Cicecoo/Type-Prediction
   fb9f138..19e037c  master     -> cicecoo/master
Updating fb9f138..19e037c
Fast-forward
 run/type_prediction/typilus/experiment_tools/run_experiments.py | 15 [32m++++++++++++++[m[31m-[m
 1 file changed, 14 insertions(+), 1 deletion(-)
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ ls -la ../config/typilus.yml
-rw-rw-r-- 1 zhaojunzhang zhaojunzhang 19265 11æœˆ 10 17:11 ../config/typilus.yml
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python run_experiments.py
é”™è¯¯: æ‰¾ä¸åˆ°é…ç½®æ–‡ä»¶ config/typilus.yml
å½“å‰ç›®å½•: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools
è„šæœ¬ç›®å½•: .
æœŸæœ›é…ç½®è·¯å¾„: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/config/typilus.yml
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python run_experiments.py[C[Cls -la ../config/typilus.yml[C[Cgit pull[K[C[Cpython run_experiments.py[C[Ccd run/type_prediction/typilus/experiment_tools[C[Cexport NCC=/mnt/data1/zhaojunzhang/typilus-data[C[C[23Pconda activate naturalcc[C[Cls[Kbash tools/run_experiment.sh exp_batch_8 [C[Cls[Kconda activate naturalcc[C[Cexport NCC=/mnt/data1/zhaojunzhang/typilus-data[C[Ccd run/type_prediction/typilus/experiment_tools[C[C[22Ppython run_experiments.py[C[Cgit pull[K
remote: Enumerating objects: 13, done.[K
remote: Counting objects:   7% (1/13)[Kremote: Counting objects:  15% (2/13)[Kremote: Counting objects:  23% (3/13)[Kremote: Counting objects:  30% (4/13)[Kremote: Counting objects:  38% (5/13)[Kremote: Counting objects:  46% (6/13)[Kremote: Counting objects:  53% (7/13)[Kremote: Counting objects:  61% (8/13)[Kremote: Counting objects:  69% (9/13)[Kremote: Counting objects:  76% (10/13)[Kremote: Counting objects:  84% (11/13)[Kremote: Counting objects:  92% (12/13)[Kremote: Counting objects: 100% (13/13)[Kremote: Counting objects: 100% (13/13), done.[K
remote: Compressing objects: 100% (1/1)[Kremote: Compressing objects: 100% (1/1), done.[K
remote: Total 7 (delta 6), reused 7 (delta 6), pack-reused 0 (from 0)[K
Unpacking objects:  14% (1/7)Unpacking objects:  28% (2/7)Unpacking objects:  42% (3/7)Unpacking objects:  57% (4/7)Unpacking objects:  71% (5/7)Unpacking objects:  85% (6/7)Unpacking objects: 100% (7/7)Unpacking objects: 100% (7/7), 3.00 KiB | 1.50 MiB/s, done.
From https://github.com/Cicecoo/Type-Prediction
   19e037c..4beb627  master     -> cicecoo/master
Updating 19e037c..4beb627
Fast-forward
 run/type_prediction/typilus/experiment_tools/run_experiments.py | 258 [32m+++++++++++++++++++++++++++++++++++++++++++++++[m[31m-----[m
 1 file changed, 236 insertions(+), 22 deletions(-)
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ git pull[C[Cpython run_experiments.py
============================================================
Typilus å‚æ•°è°ƒä¼˜å®éªŒç³»ç»Ÿ
============================================================

å…± 6 ä¸ªå®éªŒ:

1. baseline: åŸºçº¿å®éªŒ
2. exp_lr_1e-3: å­¦ä¹ ç‡1e-3
3. exp_lr_1e-4: å­¦ä¹ ç‡1e-4
4. exp_batch_64: æ‰¹é‡å¤§å°64
5. exp_hidden_128: éšè—å±‚128
6. exp_best: æ¨èé…ç½®

============================================================

æŒ‰ Enter å¼€å§‹å®éªŒ...

è¿›åº¦: 1/6

============================================================
å®éªŒ: baseline - åŸºçº¿å®éªŒ
æ—¶é—´: 2025-11-18 23:06:17
============================================================

[32m[2025-11-18 23:06:19]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/experiments/baseline/config.yml (train_enhanced.py:377, cli_main())[0m
Traceback (most recent call last):
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py", line 410, in <module>
    cli_main()
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py", line 378, in cli_main
    args = load_yaml(yaml_file)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/file_ops/yaml_io.py", line 54, in load_yaml
    with open(yaml_file, 'r', encoding='utf-8') as reader:
FileNotFoundError: [Errno 2] No such file or directory: '/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/experiments/baseline/config.yml'

âœ— baseline å¤±è´¥(1)

å®éªŒå¤±è´¥ï¼Œç»§ç»­? (y/n): ^CTraceback (most recent call last):
  File "run_experiments.py", line 596, in <module>
    main()
  File "run_experiments.py", line 573, in main
    response = input("\nå®éªŒå¤±è´¥ï¼Œç»§ç»­? (y/n): ")
KeyboardInterrupt

(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ python run_experiments.py[C[Cgit pull[K[C[Cpython run_experiments.py[C[Cgit pull[K
remote: Enumerating objects: 15, done.[K
remote: Counting objects:   6% (1/15)[Kremote: Counting objects:  13% (2/15)[Kremote: Counting objects:  20% (3/15)[Kremote: Counting objects:  26% (4/15)[Kremote: Counting objects:  33% (5/15)[Kremote: Counting objects:  40% (6/15)[Kremote: Counting objects:  46% (7/15)[Kremote: Counting objects:  53% (8/15)[Kremote: Counting objects:  60% (9/15)[Kremote: Counting objects:  66% (10/15)[Kremote: Counting objects:  73% (11/15)[Kremote: Counting objects:  80% (12/15)[Kremote: Counting objects:  86% (13/15)[Kremote: Counting objects:  93% (14/15)[Kremote: Counting objects: 100% (15/15)[Kremote: Counting objects: 100% (15/15), done.[K
remote: Compressing objects: 100% (1/1)[Kremote: Compressing objects: 100% (1/1), done.[K
remote: Total 8 (delta 7), reused 8 (delta 7), pack-reused 0 (from 0)[K
Unpacking objects:  12% (1/8)Unpacking objects:  25% (2/8)Unpacking objects:  37% (3/8)Unpacking objects:  50% (4/8)Unpacking objects:  62% (5/8)Unpacking objects:  75% (6/8)Unpacking objects:  87% (7/8)Unpacking objects: 100% (8/8)Unpacking objects: 100% (8/8), 945 bytes | 315.00 KiB/s, done.
From https://github.com/Cicecoo/Type-Prediction
   4beb627..5133140  master     -> cicecoo/master
Updating 4beb627..5133140
Fast-forward
 run/type_prediction/typilus/experiment_tools/run_experiments.py | 4 [32m+++[m[31m-[m
 run/type_prediction/typilus/experiment_tools/train_enhanced.py  | 9 [32m+++++++[m[31m--[m
 2 files changed, 10 insertions(+), 3 deletions(-)
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ git pull[C[Cpython run_experiments.py
============================================================
Typilus å‚æ•°è°ƒä¼˜å®éªŒç³»ç»Ÿ
============================================================

å…± 6 ä¸ªå®éªŒ:

1. baseline: åŸºçº¿å®éªŒ
2. exp_lr_1e-3: å­¦ä¹ ç‡1e-3
3. exp_lr_1e-4: å­¦ä¹ ç‡1e-4
4. exp_batch_64: æ‰¹é‡å¤§å°64
5. exp_hidden_128: éšè—å±‚128
6. exp_best: æ¨èé…ç½®

============================================================

æŒ‰ Enter å¼€å§‹å®éªŒ...

è¿›åº¦: 1/6

============================================================
å®éªŒ: baseline - åŸºçº¿å®éªŒ
æ—¶é—´: 2025-11-18 23:08:26
============================================================

[32m[2025-11-18 23:08:28]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/config.yml (train_enhanced.py:382, cli_main())[0m
[32m[2025-11-18 23:08:28]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:410, cli_main())[0m
[32m[2025-11-18 23:08:28]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs (train_enhanced.py:296, single_main())[0m
[32m[2025-11-18 23:08:28]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-18 23:08:28]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-18 23:08:28]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-18 23:08:36]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 64, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=64, out_features=64, bias=False)
      (2): Dropout(p=0.1, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(64, 64)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(128, 64)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=False)
      (1): Dropout(p=0.1, inplace=False)
      (2): Linear(in_features=64, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:303, single_main())[0m
[32m[2025-11-18 23:08:36]    INFO >> æ¨¡å‹: typilus, æŸå¤±å‡½æ•°: TypilusCriterion (train_enhanced.py:304, single_main())[0m
[32m[2025-11-18 23:08:36]    INFO >> æ¨¡å‹å‚æ•°: 847843 (å¯è®­ç»ƒ: 847843) (train_enhanced.py:305, single_main())[0m
[32m[2025-11-18 23:08:37]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-18 23:08:37]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 77695 MB ; used memory = 4224 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-18 23:08:37]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-18 23:08:37]    INFO >> ä½¿ç”¨ 1 ä¸ªGPUè®­ç»ƒ (train_enhanced.py:311, single_main())[0m
[32m[2025-11-18 23:08:37]    INFO >> no existing checkpoint found /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-18 23:08:37]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-18 23:09:41]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-18 23:09:41] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-18 23:09:57]    INFO >> epoch 001:     50 / 1539 loss=5.601, wps=2381.5, ups=3.29, wpb=720, bsz=720, num_updates=50, lr=0.0004, gnorm=6.502, clip=0, train_wall=15, gb_free=74.2, wall=78 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:10:01] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 11.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.29 GiB memory in use. Of the allocated memory 70.86 GiB is allocated by PyTorch, and 4.93 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:10:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72499 MiB |  72559 MiB |   1750 GiB |   1679 GiB |
|       from large pool |  72171 MiB |  72231 MiB |   1738 GiB |   1667 GiB |
|       from small pool |    327 MiB |    328 MiB |     12 GiB |     11 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72499 MiB |  72559 MiB |   1750 GiB |   1679 GiB |
|       from large pool |  72171 MiB |  72231 MiB |   1738 GiB |   1667 GiB |
|       from small pool |    327 MiB |    328 MiB |     12 GiB |     11 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72373 MiB |  72432 MiB |   1744 GiB |   1673 GiB |
|       from large pool |  72047 MiB |  72106 MiB |   1732 GiB |   1661 GiB |
|       from small pool |    326 MiB |    327 MiB |     12 GiB |     11 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77610 MiB |  77612 MiB |  89142 MiB |  11532 MiB |
|       from large pool |  77250 MiB |  77256 MiB |  88770 MiB |  11520 MiB |
|       from small pool |    360 MiB |    362 MiB |    372 MiB |     12 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5050 MiB |   5960 MiB |    874 GiB |    869 GiB |
|       from large pool |   5018 MiB |   5948 MiB |    859 GiB |    854 GiB |
|       from small pool |     32 MiB |     34 MiB |     14 GiB |     14 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6062    |    6065    |  134370    |  128308    |
|       from large pool |     821    |     822    |   57531    |   56710    |
|       from small pool |    5241    |    5244    |   76839    |   71598    |
|---------------------------------------------------------------------------|
| Active allocs         |    6062    |    6065    |  134370    |  128308    |
|       from large pool |     821    |     822    |   57531    |   56710    |
|       from small pool |    5241    |    5244    |   76839    |   71598    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     695    |     729    |     943    |     248    |
|       from large pool |     515    |     564    |     757    |     242    |
|       from small pool |     180    |     181    |     186    |       6    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     625    |     627    |   84540    |   83915    |
|       from large pool |     306    |     306    |   44162    |   43856    |
|       from small pool |     319    |     321    |   40378    |   40059    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:10:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:10:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:10:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:10:01] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:10:13]    INFO >> epoch 001:    101 / 1539 loss=5.711, wps=2077.9, ups=3.4, wpb=610.7, bsz=610.7, num_updates=100, lr=0.0004, gnorm=6.97, clip=0, train_wall=13, gb_free=75.6, wall=92 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:10:29]    INFO >> epoch 001:    151 / 1539 loss=5.93, wps=2551.8, ups=3.12, wpb=816.7, bsz=816.7, num_updates=150, lr=0.0004, gnorm=7.273, clip=0, train_wall=15, gb_free=74.2, wall=108 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:10:44]    INFO >> epoch 001:    201 / 1539 loss=5.873, wps=2365.1, ups=3.69, wpb=641.7, bsz=641.7, num_updates=200, lr=0.0004, gnorm=7.46, clip=0, train_wall=13, gb_free=74.8, wall=122 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:10:58]    INFO >> epoch 001:    251 / 1539 loss=5.999, wps=2295.2, ups=3.6, wpb=637.9, bsz=637.9, num_updates=250, lr=0.0004, gnorm=7.459, clip=0, train_wall=13, gb_free=71.5, wall=136 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:11:15]    INFO >> epoch 001:    301 / 1539 loss=5.747, wps=2507.1, ups=3.19, wpb=786.5, bsz=786.5, num_updates=300, lr=0.0004, gnorm=7.02, clip=0, train_wall=15, gb_free=73.8, wall=152 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:11:29]    INFO >> epoch 001:    351 / 1539 loss=5.925, wps=2470, ups=3.68, wpb=671.6, bsz=671.6, num_updates=350, lr=0.0004, gnorm=7.239, clip=0, train_wall=13, gb_free=72.4, wall=165 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:11:45]    INFO >> epoch 001:    401 / 1539 loss=5.594, wps=2750.6, ups=3.23, wpb=851.7, bsz=851.7, num_updates=400, lr=0.0004, gnorm=7.958, clip=2, train_wall=15, gb_free=73.2, wall=181 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:11:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 33.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.27 GiB memory in use. Of the allocated memory 74.97 GiB is allocated by PyTorch, and 822.35 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:11:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76705 MiB |  76765 MiB |  12407 GiB |  12332 GiB |
|       from large pool |  76640 MiB |  76700 MiB |  12334 GiB |  12260 GiB |
|       from small pool |     65 MiB |     66 MiB |     72 GiB |     72 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76705 MiB |  76765 MiB |  12407 GiB |  12332 GiB |
|       from large pool |  76640 MiB |  76700 MiB |  12334 GiB |  12260 GiB |
|       from small pool |     65 MiB |     66 MiB |     72 GiB |     72 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76659 MiB |  76718 MiB |  12368 GiB |  12293 GiB |
|       from large pool |  76594 MiB |  76653 MiB |  12295 GiB |  12220 GiB |
|       from small pool |     64 MiB |     66 MiB |     72 GiB |     72 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77588 MiB |  77590 MiB | 160960 MiB |  83372 MiB |
|       from large pool |  77520 MiB |  77520 MiB | 160542 MiB |  83022 MiB |
|       from small pool |     68 MiB |    360 MiB |    418 MiB |    350 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    822 MiB |   3119 MiB |   6159 GiB |   6158 GiB |
|       from large pool |    819 MiB |   3112 MiB |   6074 GiB |   6073 GiB |
|       from small pool |      2 MiB |     23 MiB |     85 GiB |     85 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1508    |    1511    |     864 K  |     862 K  |
|       from large pool |     429    |     430    |     421 K  |     420 K  |
|       from small pool |    1079    |    1082    |     443 K  |     442 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1508    |    1511    |     864 K  |     862 K  |
|       from large pool |     429    |     430    |     421 K  |     420 K  |
|       from small pool |    1079    |    1082    |     443 K  |     442 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     188    |     694    |    1075    |     887    |
|       from large pool |     154    |     514    |     866    |     712    |
|       from small pool |      34    |     180    |     209    |     175    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     120    |     121    |  536769    |  536649    |
|       from large pool |      79    |      81    |  324465    |  324386    |
|       from small pool |      41    |      55    |  212304    |  212263    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:11:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:11:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:11:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:11:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:12:06]    INFO >> epoch 001:    452 / 1539 loss=5.837, wps=1518, ups=2.4, wpb=631.6, bsz=631.6, num_updates=450, lr=0.0004, gnorm=6.824, clip=0, train_wall=13, gb_free=71.8, wall=201 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:12:25]    INFO >> epoch 001:    502 / 1539 loss=5.715, wps=2070.8, ups=2.79, wpb=743.1, bsz=743.1, num_updates=500, lr=0.0004, gnorm=7.261, clip=0, train_wall=17, gb_free=72.6, wall=219 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:12:40]    INFO >> epoch 001:    552 / 1539 loss=5.705, wps=2297.9, ups=3.5, wpb=657, bsz=657, num_updates=550, lr=0.0004, gnorm=7.486, clip=2, train_wall=14, gb_free=65.5, wall=234 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:12:56]    INFO >> epoch 001:    602 / 1539 loss=5.748, wps=2318, ups=3.47, wpb=668.6, bsz=668.6, num_updates=600, lr=0.0004, gnorm=7.298, clip=0, train_wall=14, gb_free=73.2, wall=248 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:13:11]    INFO >> epoch 001:    652 / 1539 loss=5.603, wps=2325.7, ups=3.27, wpb=712.2, bsz=712.2, num_updates=650, lr=0.0004, gnorm=7.014, clip=0, train_wall=15, gb_free=73.5, wall=263 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:13:28]    INFO >> epoch 001:    702 / 1539 loss=5.63, wps=2094.9, ups=3.12, wpb=672.5, bsz=672.5, num_updates=700, lr=0.0004, gnorm=5.923, clip=0, train_wall=15, gb_free=74.1, wall=280 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:13:43]    INFO >> epoch 001:    752 / 1539 loss=5.494, wps=2507.1, ups=3.31, wpb=757.4, bsz=757.4, num_updates=750, lr=0.0004, gnorm=7.159, clip=2, train_wall=14, gb_free=73.7, wall=295 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:13:59]    INFO >> epoch 001:    802 / 1539 loss=5.523, wps=2584.8, ups=3.56, wpb=725.2, bsz=725.2, num_updates=800, lr=0.0004, gnorm=7.026, clip=0, train_wall=13, gb_free=73.4, wall=309 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:14:14]    INFO >> epoch 001:    852 / 1539 loss=5.552, wps=2166.9, ups=3.38, wpb=641.1, bsz=641.1, num_updates=850, lr=0.0004, gnorm=6.835, clip=0, train_wall=14, gb_free=71.8, wall=323 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:14:27]    INFO >> epoch 001:    902 / 1539 loss=5.533, wps=2391.9, ups=3.62, wpb=660.1, bsz=660.1, num_updates=900, lr=0.0004, gnorm=5.928, clip=0, train_wall=13, gb_free=72.1, wall=337 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:14:43]    INFO >> epoch 001:    952 / 1539 loss=5.455, wps=2414.8, ups=3.39, wpb=713.2, bsz=713.2, num_updates=950, lr=0.0004, gnorm=6.051, clip=0, train_wall=14, gb_free=71.8, wall=352 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:14:49] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 697.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 75.62 GiB memory in use. Of the allocated memory 74.09 GiB is allocated by PyTorch, and 1.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:14:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72061 MiB |  75870 MiB |  28387 GiB |  28317 GiB |
|       from large pool |  72043 MiB |  75853 MiB |  28234 GiB |  28163 GiB |
|       from small pool |     17 MiB |     18 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72061 MiB |  75870 MiB |  28387 GiB |  28317 GiB |
|       from large pool |  72043 MiB |  75853 MiB |  28234 GiB |  28163 GiB |
|       from small pool |     17 MiB |     18 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  75852 MiB |  28314 GiB |  28244 GiB |
|       from large pool |  72027 MiB |  75835 MiB |  28161 GiB |  28091 GiB |
|       from small pool |     17 MiB |     18 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  76924 MiB |  76924 MiB | 297094 MiB | 220170 MiB |
|       from large pool |  76900 MiB |  76900 MiB | 296602 MiB | 219702 MiB |
|       from small pool |     24 MiB |     98 MiB |    492 MiB |    468 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1452 MiB |   3665 MiB |  24376 GiB |  24374 GiB |
|       from large pool |   1446 MiB |   3649 MiB |  24199 GiB |  24197 GiB |
|       from small pool |      6 MiB |     27 MiB |    176 GiB |    176 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     604    |     613    |    1876 K  |    1875 K  |
|       from large pool |     308    |     317    |     948 K  |     948 K  |
|       from small pool |     296    |     354    |     927 K  |     927 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     604    |     613    |    1876 K  |    1875 K  |
|       from large pool |     308    |     317    |     948 K  |     948 K  |
|       from small pool |     296    |     354    |     927 K  |     927 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     122    |    1201    |    1105    |
|       from large pool |      84    |      84    |     955    |     871    |
|       from small pool |      12    |      49    |     246    |     234    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     110    |     111    |    1083 K  |    1083 K  |
|       from large pool |      88    |      89    |     660 K  |     660 K  |
|       from small pool |      22    |      56    |     422 K  |     422 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:14:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:14:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:14:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:14:49] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:15:01]    INFO >> epoch 001:   1003 / 1539 loss=5.35, wps=1894.1, ups=2.92, wpb=649.4, bsz=649.4, num_updates=1000, lr=0.0004, gnorm=7.002, clip=0, train_wall=14, gb_free=72.1, wall=369 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:15:17]    INFO >> epoch 001:   1053 / 1539 loss=5.426, wps=2656.4, ups=3.23, wpb=822.6, bsz=822.6, num_updates=1050, lr=0.0004, gnorm=7.285, clip=0, train_wall=15, gb_free=68, wall=385 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:15:33] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 167.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.14 GiB memory in use. Of the allocated memory 72.89 GiB is allocated by PyTorch, and 2.75 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:15:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 9         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72223 MiB |  74872 MiB |  32726 GiB |  32655 GiB |
|       from large pool |  72210 MiB |  74859 MiB |  32548 GiB |  32477 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72223 MiB |  74872 MiB |  32726 GiB |  32655 GiB |
|       from large pool |  72210 MiB |  74859 MiB |  32548 GiB |  32477 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72206 MiB |  74855 MiB |  32645 GiB |  32574 GiB |
|       from large pool |  72194 MiB |  74842 MiB |  32467 GiB |  32396 GiB |
|       from small pool |     12 MiB |     15 MiB |    177 GiB |    177 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77454 MiB |  77504 MiB | 305390 MiB | 227936 MiB |
|       from large pool |  77430 MiB |  77430 MiB | 304712 MiB | 227282 MiB |
|       from small pool |     24 MiB |    210 MiB |    678 MiB |    654 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3328 MiB |   7876 MiB |  29293 GiB |  29290 GiB |
|       from large pool |   3317 MiB |   7864 MiB |  29088 GiB |  29085 GiB |
|       from small pool |     11 MiB |     23 MiB |    205 GiB |    205 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     597    |     606    |    2161 K  |    2160 K  |
|       from large pool |     308    |     316    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1076 K  |    1075 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     597    |     606    |    2161 K  |    2160 K  |
|       from large pool |     308    |     316    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1076 K  |    1075 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      87    |     189    |    1299    |    1212    |
|       from large pool |      75    |      84    |     960    |     885    |
|       from small pool |      12    |     105    |     339    |     327    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      85    |      85    |    1241 K  |    1241 K  |
|       from large pool |      61    |      61    |     745 K  |     745 K  |
|       from small pool |      24    |      53    |     495 K  |     495 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:15:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:15:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:15:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:15:33] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:15:35]    INFO >> epoch 001:   1104 / 1539 loss=5.507, wps=2576.3, ups=2.77, wpb=929.6, bsz=929.6, num_updates=1100, lr=0.0004, gnorm=6.525, clip=0, train_wall=16, gb_free=72.8, wall=403 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:15:51]    INFO >> epoch 001:   1154 / 1539 loss=5.305, wps=2448.1, ups=3.53, wpb=692.6, bsz=692.6, num_updates=1150, lr=0.0004, gnorm=6.978, clip=0, train_wall=14, gb_free=73.1, wall=417 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:16:05]    INFO >> epoch 001:   1204 / 1539 loss=5.269, wps=2372.5, ups=3.5, wpb=678.4, bsz=678.4, num_updates=1200, lr=0.0004, gnorm=7.014, clip=0, train_wall=14, gb_free=71.2, wall=431 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:16:21]    INFO >> epoch 001:   1254 / 1539 loss=5.169, wps=2577.2, ups=3.53, wpb=730.9, bsz=730.9, num_updates=1250, lr=0.0004, gnorm=6.734, clip=0, train_wall=14, gb_free=71.3, wall=445 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:16:36]    INFO >> epoch 001:   1304 / 1539 loss=5.009, wps=2420.9, ups=3.29, wpb=735.4, bsz=735.4, num_updates=1300, lr=0.0004, gnorm=5.702, clip=0, train_wall=15, gb_free=74.3, wall=461 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:16:52]    INFO >> epoch 001:   1354 / 1539 loss=4.96, wps=2265.7, ups=3.45, wpb=657.2, bsz=657.2, num_updates=1350, lr=0.0004, gnorm=6.761, clip=2, train_wall=14, gb_free=73.4, wall=475 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:17:07]    INFO >> epoch 001:   1404 / 1539 loss=4.794, wps=2366.3, ups=3.32, wpb=712.5, bsz=712.5, num_updates=1400, lr=0.0004, gnorm=6.15, clip=0, train_wall=14, gb_free=73.3, wall=490 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:17:23]    INFO >> epoch 001:   1454 / 1539 loss=4.72, wps=2462.3, ups=3.51, wpb=702.2, bsz=702.2, num_updates=1450, lr=0.0004, gnorm=6.786, clip=2, train_wall=14, gb_free=71.6, wall=504 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:17:37]    INFO >> epoch 001:   1504 / 1539 loss=4.527, wps=2433, ups=3.5, wpb=695.9, bsz=695.9, num_updates=1500, lr=0.0004, gnorm=6.468, clip=0, train_wall=14, gb_free=70.8, wall=519 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:17:47]    INFO >> epoch 001 | loss 5.45 | wps 2345.7 | ups 3.29 | wpb 712.7 | bsz 712.7 | num_updates 1535 | lr 0.0004 | gnorm 6.856 | clip 0.3 | train_wall 436 | gb_free 76.6 | wall 529 (progress_bar.py:267, print())[0m
[33m[2025-11-18 23:17:47] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-18 23:18:14]    INFO >> epoch 001 | valid on 'valid' subset | loss 4.404 | wps 5796.6 | wpb 5412.5 | bsz 5412.5 | num_updates 1535 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-18 23:18:15]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-18 23:18:15]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_best.pt (epoch 1 @ 1535 updates, score 4.404) (writing took 0.016764 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-18 23:18:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-18 23:18:19]    INFO >> epoch 002:     15 / 1539 loss=4.547, wps=895.9, ups=1.22, wpb=731.3, bsz=731.3, num_updates=1550, lr=0.0004, gnorm=6.276, clip=0, train_wall=14, gb_free=74.4, wall=559 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:18:33]    INFO >> epoch 002:     65 / 1539 loss=4.369, wps=2542.8, ups=3.86, wpb=658.8, bsz=658.8, num_updates=1600, lr=0.0004, gnorm=6.504, clip=0, train_wall=13, gb_free=73.4, wall=572 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:18:48]    INFO >> epoch 002:    115 / 1539 loss=4.455, wps=2411.4, ups=3.37, wpb=714.6, bsz=714.6, num_updates=1650, lr=0.0004, gnorm=6.544, clip=0, train_wall=14, gb_free=65.7, wall=587 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:19:04]    INFO >> epoch 002:    165 / 1539 loss=4.258, wps=2534, ups=3.43, wpb=738.8, bsz=738.8, num_updates=1700, lr=0.0004, gnorm=5.905, clip=0, train_wall=14, gb_free=73.6, wall=602 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:19:18]    INFO >> epoch 002:    215 / 1539 loss=4.498, wps=2555.8, ups=3.63, wpb=704.2, bsz=704.2, num_updates=1750, lr=0.0004, gnorm=5.847, clip=0, train_wall=13, gb_free=71.2, wall=616 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:19:35]    INFO >> epoch 002:    265 / 1539 loss=4.253, wps=2749.4, ups=3.15, wpb=873.5, bsz=873.5, num_updates=1800, lr=0.0004, gnorm=6.843, clip=0, train_wall=15, gb_free=74.7, wall=632 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:19:49]    INFO >> epoch 002:    315 / 1539 loss=4.301, wps=2381.3, ups=3.69, wpb=645.1, bsz=645.1, num_updates=1850, lr=0.0004, gnorm=6.441, clip=0, train_wall=13, gb_free=71.9, wall=645 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:20:02]    INFO >> epoch 002:    365 / 1539 loss=4.265, wps=2361.7, ups=3.6, wpb=655.8, bsz=655.8, num_updates=1900, lr=0.0004, gnorm=6.912, clip=0, train_wall=13, gb_free=74, wall=659 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:20:19]    INFO >> epoch 002:    415 / 1539 loss=4.195, wps=2302.8, ups=3.23, wpb=713.2, bsz=713.2, num_updates=1950, lr=0.0004, gnorm=6.629, clip=0, train_wall=15, gb_free=76.2, wall=674 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:20:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 2.02 GiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 74.28 GiB memory in use. Of the allocated memory 70.77 GiB is allocated by PyTorch, and 3.02 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:20:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72062 MiB |  72463 MiB |  61909 GiB |  61839 GiB |
|       from large pool |  72045 MiB |  72445 MiB |  61561 GiB |  61491 GiB |
|       from small pool |     17 MiB |     21 MiB |    347 GiB |    347 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72062 MiB |  72463 MiB |  61909 GiB |  61839 GiB |
|       from large pool |  72045 MiB |  72445 MiB |  61561 GiB |  61491 GiB |
|       from small pool |     17 MiB |     21 MiB |    347 GiB |    347 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB |  61773 GiB |  61702 GiB |
|       from large pool |  72027 MiB |  72426 MiB |  61425 GiB |  61355 GiB |
|       from small pool |     17 MiB |     21 MiB |    347 GiB |    347 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  75552 MiB |  75738 MiB | 305576 MiB | 230024 MiB |
|       from large pool |  75528 MiB |  75528 MiB | 304712 MiB | 229184 MiB |
|       from small pool |     24 MiB |    210 MiB |    864 MiB |    840 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3489 MiB |   4990 MiB |  61025 GiB |  61021 GiB |
|       from large pool |   3482 MiB |   4982 MiB |  60629 GiB |  60626 GiB |
|       from small pool |      6 MiB |     27 MiB |    395 GiB |    395 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |    4085 K  |    4085 K  |
|       from large pool |     308    |     315    |    1969 K  |    1968 K  |
|       from small pool |     298    |     355    |    2116 K  |    2116 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |    4085 K  |    4085 K  |
|       from large pool |     308    |     315    |    1969 K  |    1968 K  |
|       from small pool |     298    |     355    |    2116 K  |    2116 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      86    |     179    |    1392    |    1306    |
|       from large pool |      74    |      74    |     960    |     886    |
|       from small pool |      12    |     105    |     432    |     420    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      96    |      96    |    2306 K  |    2306 K  |
|       from large pool |      73    |      73    |    1305 K  |    1304 K  |
|       from small pool |      23    |      51    |    1001 K  |    1001 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:20:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:20:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:20:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:20:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:20:34]    INFO >> epoch 002:    466 / 1539 loss=4.274, wps=2462.2, ups=3.38, wpb=729.4, bsz=729.4, num_updates=2000, lr=0.0004, gnorm=6.027, clip=0, train_wall=13, gb_free=71.6, wall=689 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:20:50]    INFO >> epoch 002:    516 / 1539 loss=4.282, wps=2488.5, ups=3.51, wpb=708.4, bsz=708.4, num_updates=2050, lr=0.0004, gnorm=6.797, clip=2, train_wall=14, gb_free=71.2, wall=703 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:21:03]    INFO >> epoch 002:    566 / 1539 loss=4.154, wps=2298.7, ups=3.69, wpb=622.3, bsz=622.3, num_updates=2100, lr=0.0004, gnorm=6.023, clip=0, train_wall=13, gb_free=74, wall=717 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:21:20]    INFO >> epoch 002:    616 / 1539 loss=4.202, wps=2843.2, ups=3.26, wpb=871.1, bsz=871.1, num_updates=2150, lr=0.0004, gnorm=6.107, clip=0, train_wall=15, gb_free=68.6, wall=732 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:21:35]    INFO >> epoch 002:    666 / 1539 loss=4.022, wps=2578.1, ups=3.34, wpb=772.9, bsz=772.9, num_updates=2200, lr=0.0004, gnorm=7.625, clip=2, train_wall=15, gb_free=68.4, wall=747 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:21:50]    INFO >> epoch 002:    716 / 1539 loss=3.971, wps=2577.6, ups=3.64, wpb=708.6, bsz=708.6, num_updates=2250, lr=0.0004, gnorm=6.664, clip=2, train_wall=13, gb_free=73.1, wall=761 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:22:04]    INFO >> epoch 002:    766 / 1539 loss=4.032, wps=2394.2, ups=3.6, wpb=665.2, bsz=665.2, num_updates=2300, lr=0.0004, gnorm=6.506, clip=2, train_wall=13, gb_free=75.7, wall=775 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:22:19]    INFO >> epoch 002:    816 / 1539 loss=4.287, wps=2235.1, ups=3.41, wpb=656, bsz=656, num_updates=2350, lr=0.0004, gnorm=6.203, clip=0, train_wall=14, gb_free=72.2, wall=790 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:22:35]    INFO >> epoch 002:    866 / 1539 loss=4.181, wps=2371.3, ups=3.29, wpb=719.8, bsz=719.8, num_updates=2400, lr=0.0004, gnorm=6.405, clip=2, train_wall=15, gb_free=75.8, wall=805 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:22:49]    INFO >> epoch 002:    916 / 1539 loss=4.142, wps=2336.1, ups=3.48, wpb=671.5, bsz=671.5, num_updates=2450, lr=0.0004, gnorm=5.72, clip=0, train_wall=14, gb_free=73.1, wall=819 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:23:05]    INFO >> epoch 002:    966 / 1539 loss=4.045, wps=2246.3, ups=3.52, wpb=637.9, bsz=637.9, num_updates=2500, lr=0.0004, gnorm=6.142, clip=0, train_wall=14, gb_free=69.2, wall=833 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:23:07] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 9.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.29 GiB memory in use. Of the allocated memory 74.46 GiB is allocated by PyTorch, and 1.33 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:23:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 11        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76192 MiB |  76252 MiB |  78094 GiB |  78020 GiB |
|       from large pool |  75826 MiB |  75886 MiB |  77657 GiB |  77583 GiB |
|       from small pool |    365 MiB |    367 MiB |    437 GiB |    436 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76192 MiB |  76252 MiB |  78094 GiB |  78020 GiB |
|       from large pool |  75826 MiB |  75886 MiB |  77657 GiB |  77583 GiB |
|       from small pool |    365 MiB |    367 MiB |    437 GiB |    436 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76156 MiB |  76215 MiB |  77925 GiB |  77851 GiB |
|       from large pool |  75792 MiB |  75851 MiB |  77488 GiB |  77414 GiB |
|       from small pool |    363 MiB |    365 MiB |    436 GiB |    436 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77612 MiB |  77612 MiB | 307636 MiB | 230024 MiB |
|       from large pool |  77208 MiB |  77208 MiB | 306392 MiB | 229184 MiB |
|       from small pool |    404 MiB |    404 MiB |   1244 MiB |    840 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1359 MiB |   4340 MiB |  79805 GiB |  79804 GiB |
|       from large pool |   1321 MiB |   4333 MiB |  79307 GiB |  79306 GiB |
|       from small pool |     38 MiB |     38 MiB |    498 GiB |    497 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6757    |    6760    |    5172 K  |    5165 K  |
|       from large pool |     884    |     885    |    2515 K  |    2514 K  |
|       from small pool |    5873    |    5876    |    2656 K  |    2650 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6757    |    6760    |    5172 K  |    5165 K  |
|       from large pool |     884    |     885    |    2515 K  |    2514 K  |
|       from small pool |    5873    |    5876    |    2656 K  |    2650 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     304    |     304    |    1610    |    1306    |
|       from large pool |     102    |     102    |     988    |     886    |
|       from small pool |     202    |     202    |     622    |     420    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     433    |     433    |    2893 K  |    2893 K  |
|       from large pool |      72    |      72    |    1646 K  |    1646 K  |
|       from small pool |     361    |     361    |    1247 K  |    1247 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:23:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:23:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:23:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:23:07] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-18 23:23:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 37.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.26 GiB memory in use. Of the allocated memory 74.60 GiB is allocated by PyTorch, and 1.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:23:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 13        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76328 MiB |  76388 MiB |  79053 GiB |  78978 GiB |
|       from large pool |  76266 MiB |  76326 MiB |  78610 GiB |  78536 GiB |
|       from small pool |     61 MiB |     62 MiB |    442 GiB |    442 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76328 MiB |  76388 MiB |  79053 GiB |  78978 GiB |
|       from large pool |  76266 MiB |  76326 MiB |  78610 GiB |  78536 GiB |
|       from small pool |     61 MiB |     62 MiB |    442 GiB |    442 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76298 MiB |  76358 MiB |  78882 GiB |  78807 GiB |
|       from large pool |  76237 MiB |  76296 MiB |  78440 GiB |  78366 GiB |
|       from small pool |     61 MiB |     62 MiB |    441 GiB |    441 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77584 MiB |  77612 MiB | 308062 MiB | 230478 MiB |
|       from large pool |  77520 MiB |  77520 MiB | 306812 MiB | 229292 MiB |
|       from small pool |     64 MiB |    404 MiB |   1250 MiB |   1186 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1195 MiB |   8183 MiB |  80822 GiB |  80821 GiB |
|       from large pool |   1193 MiB |   8174 MiB |  80318 GiB |  80317 GiB |
|       from small pool |      2 MiB |     29 MiB |    504 GiB |    504 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1444    |    1447    |    5233 K  |    5231 K  |
|       from large pool |     423    |     424    |    2545 K  |    2544 K  |
|       from small pool |    1021    |    1024    |    2688 K  |    2687 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1444    |    1447    |    5233 K  |    5231 K  |
|       from large pool |     423    |     424    |    2545 K  |    2544 K  |
|       from small pool |    1021    |    1024    |    2688 K  |    2687 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     137    |     304    |    1620    |    1483    |
|       from large pool |     105    |     105    |     995    |     890    |
|       from small pool |      32    |     202    |     625    |     593    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     128    |     129    |    2929 K  |    2929 K  |
|       from large pool |      90    |      95    |    1665 K  |    1665 K  |
|       from small pool |      38    |      59    |    1263 K  |    1263 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:23:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:23:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:23:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:23:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:23:20]    INFO >> epoch 002:   1018 / 1539 loss=4.034, wps=2254.5, ups=3.28, wpb=687.9, bsz=687.9, num_updates=2550, lr=0.0004, gnorm=6.422, clip=2, train_wall=13, gb_free=72.9, wall=849 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:23:37]    INFO >> epoch 002:   1068 / 1539 loss=3.842, wps=2562.8, ups=3.36, wpb=763.7, bsz=763.7, num_updates=2600, lr=0.0004, gnorm=6.84, clip=0, train_wall=14, gb_free=73.5, wall=864 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:23:52]    INFO >> epoch 002:   1118 / 1539 loss=3.71, wps=2563.6, ups=3.24, wpb=791.9, bsz=791.9, num_updates=2650, lr=0.0004, gnorm=7.136, clip=4, train_wall=15, gb_free=69.2, wall=879 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:24:08]    INFO >> epoch 002:   1168 / 1539 loss=4.132, wps=2597.3, ups=3.37, wpb=770.1, bsz=770.1, num_updates=2700, lr=0.0004, gnorm=5.94, clip=0, train_wall=14, gb_free=72.3, wall=894 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:24:23]    INFO >> epoch 002:   1218 / 1539 loss=4.072, wps=2458, ups=3.46, wpb=710, bsz=710, num_updates=2750, lr=0.0004, gnorm=6.149, clip=0, train_wall=14, gb_free=70.8, wall=908 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:24:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.17 GiB. GPU 0 has a total capacity of 79.14 GiB of which 527.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 75.79 GiB memory in use. Of the allocated memory 68.48 GiB is allocated by PyTorch, and 6.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:24:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 17        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  64342 MiB |  70749 MiB |  86281 GiB |  86219 GiB |
|       from large pool |  64329 MiB |  70736 MiB |  85799 GiB |  85736 GiB |
|       from small pool |     12 MiB |     14 MiB |    482 GiB |    482 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  64342 MiB |  70749 MiB |  86281 GiB |  86219 GiB |
|       from large pool |  64329 MiB |  70736 MiB |  85799 GiB |  85736 GiB |
|       from small pool |     12 MiB |     14 MiB |    482 GiB |    482 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  64321 MiB |  70724 MiB |  86095 GiB |  86032 GiB |
|       from large pool |  64308 MiB |  70711 MiB |  85613 GiB |  85550 GiB |
|       from small pool |     12 MiB |     14 MiB |    481 GiB |    481 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77094 MiB |  77544 MiB | 342274 MiB | 265180 MiB |
|       from large pool |  77070 MiB |  77486 MiB | 340862 MiB | 263792 MiB |
|       from small pool |     24 MiB |    226 MiB |   1412 MiB |   1388 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6423 MiB |   9444 MiB |  87778 GiB |  87771 GiB |
|       from large pool |   6412 MiB |   9431 MiB |  87227 GiB |  87221 GiB |
|       from small pool |     11 MiB |     25 MiB |    550 GiB |    550 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     584    |    5710 K  |    5709 K  |
|       from large pool |     275    |     293    |    2780 K  |    2780 K  |
|       from small pool |     291    |     356    |    2929 K  |    2929 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     584    |    5710 K  |    5709 K  |
|       from large pool |     275    |     293    |    2780 K  |    2780 K  |
|       from small pool |     291    |     356    |    2929 K  |    2929 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     165    |     282    |    1785    |    1620    |
|       from large pool |     153    |     169    |    1079    |     926    |
|       from small pool |      12    |     113    |     706    |     694    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     141    |     152    |    3201 K  |    3201 K  |
|       from large pool |     114    |     125    |    1823 K  |    1823 K  |
|       from small pool |      27    |      54    |    1378 K  |    1378 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:24:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:24:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:24:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:24:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:24:39]    INFO >> epoch 002:   1269 / 1539 loss=3.884, wps=2476.3, ups=3.24, wpb=765.3, bsz=765.3, num_updates=2800, lr=0.0004, gnorm=7.131, clip=4, train_wall=14, gb_free=70.5, wall=924 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:24:53]    INFO >> epoch 002:   1319 / 1539 loss=4.043, wps=2397.4, ups=3.62, wpb=661.5, bsz=661.5, num_updates=2850, lr=0.0004, gnorm=5.432, clip=0, train_wall=13, gb_free=75, wall=938 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:25:07]    INFO >> epoch 002:   1369 / 1539 loss=3.952, wps=2550.5, ups=3.5, wpb=728.5, bsz=728.5, num_updates=2900, lr=0.0004, gnorm=6.268, clip=0, train_wall=14, gb_free=70.5, wall=952 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:25:22]    INFO >> epoch 002:   1419 / 1539 loss=3.979, wps=2287.9, ups=3.5, wpb=653.6, bsz=653.6, num_updates=2950, lr=0.0004, gnorm=5.527, clip=0, train_wall=14, gb_free=64.8, wall=966 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:25:37]    INFO >> epoch 002:   1469 / 1539 loss=3.965, wps=2230.9, ups=3.18, wpb=702.5, bsz=702.5, num_updates=3000, lr=0.0004, gnorm=6.202, clip=0, train_wall=15, gb_free=70.3, wall=982 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:25:55]    INFO >> epoch 002:   1519 / 1539 loss=4.04, wps=2335.9, ups=3.47, wpb=673.3, bsz=673.3, num_updates=3050, lr=0.0004, gnorm=5.318, clip=0, train_wall=14, gb_free=74.2, wall=996 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:26:01]    INFO >> epoch 002 | loss 4.126 | wps 2308.8 | ups 3.24 | wpb 712.7 | bsz 712.7 | num_updates 3070 | lr 0.0004 | gnorm 6.334 | clip 0.7 | train_wall 430 | gb_free 72.4 | wall 1003 (progress_bar.py:267, print())[0m
[33m[2025-11-18 23:26:01] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-18 23:26:27]    INFO >> epoch 002 | valid on 'valid' subset | loss 3.934 | wps 6118.6 | wpb 5412.5 | bsz 5412.5 | num_updates 3070 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-18 23:26:28]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-18 23:26:28]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 2 @ 3070 updates, score 3.934) (writing took 0.013156 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-18 23:26:28] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-18 23:26:36]    INFO >> epoch 003:     30 / 1539 loss=3.908, wps=858.9, ups=1.26, wpb=681.1, bsz=681.1, num_updates=3100, lr=0.000392, gnorm=6.896, clip=0, train_wall=14, gb_free=70.7, wall=1036 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:26:52]    INFO >> epoch 003:     80 / 1539 loss=3.91, wps=2635.1, ups=3.41, wpb=772.8, bsz=772.8, num_updates=3150, lr=0.000392, gnorm=5.839, clip=0, train_wall=14, gb_free=73.4, wall=1051 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:26:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 2.02 GiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 74.28 GiB memory in use. Of the allocated memory 68.10 GiB is allocated by PyTorch, and 5.68 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:26:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 19        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69330 MiB |  70019 MiB | 102358 GiB | 102290 GiB |
|       from large pool |  69313 MiB |  70002 MiB | 101779 GiB | 101711 GiB |
|       from small pool |     17 MiB |     19 MiB |    579 GiB |    579 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69330 MiB |  70019 MiB | 102358 GiB | 102290 GiB |
|       from large pool |  69313 MiB |  70002 MiB | 101779 GiB | 101711 GiB |
|       from small pool |     17 MiB |     19 MiB |    579 GiB |    579 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69318 MiB |  70006 MiB | 102140 GiB | 102073 GiB |
|       from large pool |  69300 MiB |  69989 MiB | 101562 GiB | 101494 GiB |
|       from small pool |     17 MiB |     19 MiB |    578 GiB |    578 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  75550 MiB |  76134 MiB | 350370 MiB | 274820 MiB |
|       from large pool |  75524 MiB |  76010 MiB | 348858 MiB | 273334 MiB |
|       from small pool |     26 MiB |    124 MiB |   1512 MiB |   1486 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6219 MiB |   8497 MiB | 101281 GiB | 101275 GiB |
|       from large pool |   6210 MiB |   8488 MiB | 100625 GiB | 100619 GiB |
|       from small pool |      8 MiB |     29 MiB |    655 GiB |    655 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |    6736 K  |    6736 K  |
|       from large pool |     307    |     315    |    3208 K  |    3207 K  |
|       from small pool |     298    |     356    |    3528 K  |    3528 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |    6736 K  |    6736 K  |
|       from large pool |     307    |     315    |    3208 K  |    3207 K  |
|       from small pool |     298    |     356    |    3528 K  |    3528 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     152    |     207    |    1838    |    1686    |
|       from large pool |     139    |     145    |    1082    |     943    |
|       from small pool |      13    |      62    |     756    |     743    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     151    |     151    |    3801 K  |    3801 K  |
|       from large pool |     123    |     123    |    2110 K  |    2110 K  |
|       from small pool |      28    |      57    |    1691 K  |    1691 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:26:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:26:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:26:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:26:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:27:07]    INFO >> epoch 003:    131 / 1539 loss=3.895, wps=2689.8, ups=3.22, wpb=835, bsz=835, num_updates=3200, lr=0.000392, gnorm=5.823, clip=0, train_wall=14, gb_free=71.8, wall=1066 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:27:21]    INFO >> epoch 003:    181 / 1539 loss=4.09, wps=2447.6, ups=3.73, wpb=656.8, bsz=656.8, num_updates=3250, lr=0.000392, gnorm=4.937, clip=0, train_wall=13, gb_free=73, wall=1080 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:27:32] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 61.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.24 GiB memory in use. Of the allocated memory 71.03 GiB is allocated by PyTorch, and 4.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:27:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  70354 MiB |  72929 MiB | 105886 GiB | 105817 GiB |
|       from large pool |  70341 MiB |  72916 MiB | 105286 GiB | 105217 GiB |
|       from small pool |     12 MiB |     21 MiB |    600 GiB |    600 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  70354 MiB |  72929 MiB | 105886 GiB | 105817 GiB |
|       from large pool |  70341 MiB |  72916 MiB | 105286 GiB | 105217 GiB |
|       from small pool |     12 MiB |     21 MiB |    600 GiB |    600 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  70335 MiB |  72908 MiB | 105661 GiB | 105592 GiB |
|       from large pool |  70322 MiB |  72895 MiB | 105062 GiB | 104993 GiB |
|       from small pool |     12 MiB |     21 MiB |    599 GiB |    599 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77560 MiB |  77560 MiB | 352938 MiB | 275378 MiB |
|       from large pool |  77532 MiB |  77532 MiB | 351242 MiB | 273710 MiB |
|       from small pool |     28 MiB |    210 MiB |   1696 MiB |   1668 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5303 MiB |   8809 MiB | 104750 GiB | 104745 GiB |
|       from large pool |   5288 MiB |   8793 MiB | 104070 GiB | 104065 GiB |
|       from small pool |     15 MiB |     29 MiB |    679 GiB |    679 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     587    |     596    |    6976 K  |    6975 K  |
|       from large pool |     296    |     305    |    3322 K  |    3321 K  |
|       from small pool |     291    |     356    |    3654 K  |    3653 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     587    |     596    |    6976 K  |    6975 K  |
|       from large pool |     296    |     305    |    3322 K  |    3321 K  |
|       from small pool |     291    |     356    |    3654 K  |    3653 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     154    |     244    |    1932    |    1778    |
|       from large pool |     140    |     140    |    1084    |     944    |
|       from small pool |      14    |     105    |     848    |     834    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     149    |     149    |    3940 K  |    3939 K  |
|       from large pool |     118    |     118    |    2186 K  |    2186 K  |
|       from small pool |      31    |      53    |    1753 K  |    1753 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:27:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:27:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:27:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:27:32] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:27:38]    INFO >> epoch 003:    232 / 1539 loss=3.871, wps=2341.9, ups=3.05, wpb=767.2, bsz=767.2, num_updates=3300, lr=0.000392, gnorm=5.517, clip=0, train_wall=15, gb_free=74, wall=1096 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:27:53]    INFO >> epoch 003:    282 / 1539 loss=3.844, wps=2614.5, ups=3.48, wpb=751.9, bsz=751.9, num_updates=3350, lr=0.000392, gnorm=5.373, clip=0, train_wall=14, gb_free=70.6, wall=1110 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:28:10]    INFO >> epoch 003:    332 / 1539 loss=3.953, wps=2555.7, ups=3.23, wpb=790.4, bsz=790.4, num_updates=3400, lr=0.000392, gnorm=5.215, clip=0, train_wall=15, gb_free=73.8, wall=1126 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:28:25]    INFO >> epoch 003:    382 / 1539 loss=3.846, wps=2229.2, ups=3.24, wpb=688.4, bsz=688.4, num_updates=3450, lr=0.000392, gnorm=6.817, clip=0, train_wall=15, gb_free=72.5, wall=1141 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:28:41]    INFO >> epoch 003:    432 / 1539 loss=3.97, wps=2310.5, ups=3.51, wpb=657.8, bsz=657.8, num_updates=3500, lr=0.000392, gnorm=5.559, clip=0, train_wall=14, gb_free=66.2, wall=1155 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:28:56]    INFO >> epoch 003:    482 / 1539 loss=3.902, wps=2469.1, ups=3.27, wpb=754, bsz=754, num_updates=3550, lr=0.000392, gnorm=5.61, clip=0, train_wall=15, gb_free=73.1, wall=1171 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:29:11]    INFO >> epoch 003:    532 / 1539 loss=3.733, wps=2722.4, ups=3.53, wpb=770.5, bsz=770.5, num_updates=3600, lr=0.000392, gnorm=6.494, clip=2, train_wall=14, gb_free=73.8, wall=1185 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:29:27]    INFO >> epoch 003:    582 / 1539 loss=3.864, wps=2319.8, ups=3.22, wpb=720.1, bsz=720.1, num_updates=3650, lr=0.000392, gnorm=6.092, clip=0, train_wall=15, gb_free=71.6, wall=1200 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:29:42]    INFO >> epoch 003:    632 / 1539 loss=3.919, wps=2518.7, ups=3.73, wpb=674.5, bsz=674.5, num_updates=3700, lr=0.000392, gnorm=5.651, clip=0, train_wall=13, gb_free=66.5, wall=1214 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:29:56]    INFO >> epoch 003:    682 / 1539 loss=3.907, wps=2647.3, ups=3.5, wpb=756.5, bsz=756.5, num_updates=3750, lr=0.000392, gnorm=5.619, clip=0, train_wall=14, gb_free=75.1, wall=1228 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:30:12]    INFO >> epoch 003:    732 / 1539 loss=3.62, wps=2456.1, ups=3.03, wpb=810.3, bsz=810.3, num_updates=3800, lr=0.000392, gnorm=6.016, clip=2, train_wall=16, gb_free=73.9, wall=1245 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:30:27]    INFO >> epoch 003:    782 / 1539 loss=3.407, wps=2657.1, ups=3.34, wpb=795, bsz=795, num_updates=3850, lr=0.000392, gnorm=6.269, clip=2, train_wall=15, gb_free=73.7, wall=1260 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:30:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 77.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.22 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 5.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:30:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 23        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69008 MiB |  71765 MiB | 123385 GiB | 123318 GiB |
|       from large pool |  68991 MiB |  71748 MiB | 122689 GiB | 122622 GiB |
|       from small pool |     16 MiB |     18 MiB |    696 GiB |    696 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69008 MiB |  71765 MiB | 123385 GiB | 123318 GiB |
|       from large pool |  68991 MiB |  71748 MiB | 122689 GiB | 122622 GiB |
|       from small pool |     16 MiB |     18 MiB |    696 GiB |    696 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 123122 GiB | 123055 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 122427 GiB | 122360 GiB |
|       from small pool |     16 MiB |     18 MiB |    695 GiB |    695 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77544 MiB |  77544 MiB | 355046 MiB | 277502 MiB |
|       from large pool |  77520 MiB |  77520 MiB | 353132 MiB | 275612 MiB |
|       from small pool |     24 MiB |    246 MiB |   1914 MiB |   1890 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6645 MiB |   9885 MiB | 122145 GiB | 122139 GiB |
|       from large pool |   6638 MiB |   9878 MiB | 121354 GiB | 121348 GiB |
|       from small pool |      7 MiB |     29 MiB |    790 GiB |    790 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |    8130 K  |    8129 K  |
|       from large pool |     343    |     351    |    3898 K  |    3898 K  |
|       from small pool |     298    |     356    |    4231 K  |    4231 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |    8130 K  |    8129 K  |
|       from large pool |     343    |     351    |    3898 K  |    3898 K  |
|       from small pool |     298    |     356    |    4231 K  |    4231 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     152    |     262    |    2042    |    1890    |
|       from large pool |     140    |     140    |    1085    |     945    |
|       from small pool |      12    |     123    |     957    |     945    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     137    |     141    |    4589 K  |    4589 K  |
|       from large pool |     111    |     115    |    2566 K  |    2566 K  |
|       from small pool |      26    |      57    |    2023 K  |    2023 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:30:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:30:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:30:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:30:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:30:43]    INFO >> epoch 003:    833 / 1539 loss=3.793, wps=2402.3, ups=3.26, wpb=736, bsz=736, num_updates=3900, lr=0.000392, gnorm=6.835, clip=0, train_wall=14, gb_free=73, wall=1275 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:30:57]    INFO >> epoch 003:    883 / 1539 loss=4, wps=2569.7, ups=3.58, wpb=717, bsz=717, num_updates=3950, lr=0.000392, gnorm=6.665, clip=0, train_wall=14, gb_free=72.5, wall=1289 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:31:12]    INFO >> epoch 003:    933 / 1539 loss=3.774, wps=2411.9, ups=3.31, wpb=729.5, bsz=729.5, num_updates=4000, lr=0.000392, gnorm=6.234, clip=0, train_wall=15, gb_free=67.3, wall=1304 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:31:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.57 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.92 GiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 74.38 GiB memory in use. Of the allocated memory 66.03 GiB is allocated by PyTorch, and 7.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:31:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 24        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63501 MiB |  70133 MiB | 128251 GiB | 128189 GiB |
|       from large pool |  63481 MiB |  70112 MiB | 127530 GiB | 127468 GiB |
|       from small pool |     20 MiB |     60 MiB |    721 GiB |    721 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63501 MiB |  70133 MiB | 128251 GiB | 128189 GiB |
|       from large pool |  63481 MiB |  70112 MiB | 127530 GiB | 127468 GiB |
|       from small pool |     20 MiB |     60 MiB |    721 GiB |    721 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63481 MiB |  70111 MiB | 127977 GiB | 127915 GiB |
|       from large pool |  63461 MiB |  70091 MiB | 127257 GiB | 127195 GiB |
|       from small pool |     20 MiB |     60 MiB |    720 GiB |    720 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  75656 MiB |  75696 MiB | 355088 MiB | 279432 MiB |
|       from large pool |  75630 MiB |  75630 MiB | 353132 MiB | 277502 MiB |
|       from small pool |     26 MiB |     66 MiB |   1956 MiB |   1930 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6698 MiB |   9384 MiB | 126847 GiB | 126841 GiB |
|       from large pool |   6692 MiB |   9379 MiB | 126028 GiB | 126022 GiB |
|       from small pool |      5 MiB |     19 MiB |    819 GiB |    819 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |    1326    |    8439 K  |    8438 K  |
|       from large pool |     330    |     403    |    4057 K  |    4057 K  |
|       from small pool |     316    |     924    |    4381 K  |    4381 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |    1326    |    8439 K  |    8438 K  |
|       from large pool |     330    |     403    |    4057 K  |    4057 K  |
|       from small pool |     316    |     924    |    4381 K  |    4381 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     152    |     172    |    2063    |    1911    |
|       from large pool |     139    |     139    |    1085    |     946    |
|       from small pool |      13    |      33    |     978    |     965    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     141    |     160    |    4761 K  |    4761 K  |
|       from large pool |     109    |     113    |    2670 K  |    2670 K  |
|       from small pool |      32    |      53    |    2090 K  |    2090 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:31:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:31:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:31:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:31:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:31:28]    INFO >> epoch 003:    984 / 1539 loss=3.95, wps=1955.9, ups=3.11, wpb=628.7, bsz=628.7, num_updates=4050, lr=0.000392, gnorm=5.214, clip=0, train_wall=14, gb_free=70.1, wall=1320 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:31:42]    INFO >> epoch 003:   1034 / 1539 loss=3.855, wps=2244.8, ups=3.52, wpb=637.6, bsz=637.6, num_updates=4100, lr=0.000392, gnorm=5.116, clip=0, train_wall=14, gb_free=72.5, wall=1334 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:32:01]    INFO >> epoch 003:   1084 / 1539 loss=3.777, wps=2561.2, ups=3.77, wpb=679.5, bsz=679.5, num_updates=4150, lr=0.000392, gnorm=6.732, clip=2, train_wall=13, gb_free=69.8, wall=1348 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:32:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 35.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.27 GiB memory in use. Of the allocated memory 73.51 GiB is allocated by PyTorch, and 2.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:32:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 25        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75211 MiB |  75271 MiB | 133516 GiB | 133442 GiB |
|       from large pool |  74855 MiB |  74915 MiB | 132765 GiB | 132692 GiB |
|       from small pool |    355 MiB |    356 MiB |    751 GiB |    750 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75211 MiB |  75271 MiB | 133516 GiB | 133442 GiB |
|       from large pool |  74855 MiB |  74915 MiB | 132765 GiB | 132692 GiB |
|       from small pool |    355 MiB |    356 MiB |    751 GiB |    750 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75135 MiB |  75194 MiB | 133230 GiB | 133157 GiB |
|       from large pool |  74781 MiB |  74841 MiB | 132480 GiB | 132407 GiB |
|       from small pool |    353 MiB |    354 MiB |    749 GiB |    749 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77586 MiB |  77588 MiB | 362476 MiB | 284890 MiB |
|       from large pool |  77194 MiB |  77194 MiB | 360152 MiB | 282958 MiB |
|       from small pool |    392 MiB |    394 MiB |   2324 MiB |   1932 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2314 MiB |   5828 MiB | 132018 GiB | 132016 GiB |
|       from large pool |   2278 MiB |   5822 MiB | 131165 GiB | 131163 GiB |
|       from small pool |     36 MiB |     37 MiB |    853 GiB |    853 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6570    |    6573    |    8800 K  |    8793 K  |
|       from large pool |     867    |     868    |    4236 K  |    4235 K  |
|       from small pool |    5703    |    5706    |    4563 K  |    4557 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6570    |    6573    |    8800 K  |    8793 K  |
|       from large pool |     867    |     868    |    4236 K  |    4235 K  |
|       from small pool |    5703    |    5706    |    4563 K  |    4557 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     450    |     451    |    2364    |    1914    |
|       from large pool |     254    |     254    |    1202    |     948    |
|       from small pool |     196    |     197    |    1162    |     966    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     483    |     483    |    4962 K  |    4962 K  |
|       from large pool |     131    |     131    |    2789 K  |    2789 K  |
|       from small pool |     352    |     352    |    2173 K  |    2172 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:32:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:32:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:32:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:32:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:32:15]    INFO >> epoch 003:   1135 / 1539 loss=3.843, wps=2418, ups=3.53, wpb=685.9, bsz=685.9, num_updates=4200, lr=0.000392, gnorm=5.085, clip=0, train_wall=13, gb_free=75.1, wall=1362 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:32:31]    INFO >> epoch 003:   1185 / 1539 loss=3.743, wps=2246.1, ups=3.36, wpb=668, bsz=668, num_updates=4250, lr=0.000392, gnorm=5.537, clip=0, train_wall=14, gb_free=62.1, wall=1377 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:32:46]    INFO >> epoch 003:   1235 / 1539 loss=3.805, wps=2391.7, ups=3.33, wpb=717.9, bsz=717.9, num_updates=4300, lr=0.000392, gnorm=6.166, clip=0, train_wall=15, gb_free=73.2, wall=1392 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:32:59]    INFO >> epoch 003:   1285 / 1539 loss=3.8, wps=2332.7, ups=3.77, wpb=619.2, bsz=619.2, num_updates=4350, lr=0.000392, gnorm=4.994, clip=0, train_wall=13, gb_free=75, wall=1405 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:33:13]    INFO >> epoch 003:   1335 / 1539 loss=3.771, wps=2635.3, ups=3.65, wpb=721.1, bsz=721.1, num_updates=4400, lr=0.000392, gnorm=5.872, clip=2, train_wall=13, gb_free=67.6, wall=1419 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:33:27]    INFO >> epoch 003:   1385 / 1539 loss=3.739, wps=2488.1, ups=3.66, wpb=680.4, bsz=680.4, num_updates=4450, lr=0.000392, gnorm=5.908, clip=0, train_wall=13, gb_free=73.3, wall=1432 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:33:43]    INFO >> epoch 003:   1435 / 1539 loss=3.899, wps=2466.4, ups=3.75, wpb=657.2, bsz=657.2, num_updates=4500, lr=0.000392, gnorm=4.877, clip=0, train_wall=13, gb_free=73.3, wall=1446 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:33:57]    INFO >> epoch 003:   1485 / 1539 loss=3.759, wps=2384.7, ups=3.58, wpb=665.8, bsz=665.8, num_updates=4550, lr=0.000392, gnorm=5.167, clip=0, train_wall=14, gb_free=67.3, wall=1460 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:34:12]    INFO >> epoch 003:   1535 / 1539 loss=3.801, wps=2339.2, ups=3.55, wpb=658.1, bsz=658.1, num_updates=4600, lr=0.000392, gnorm=5.871, clip=2, train_wall=14, gb_free=71.4, wall=1474 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:34:13]    INFO >> epoch 003 | loss 3.832 | wps 2312 | ups 3.25 | wpb 711.1 | bsz 711.1 | num_updates 4604 | lr 0.000392 | gnorm 5.806 | clip 0.4 | train_wall 429 | gb_free 74.4 | wall 1475 (progress_bar.py:267, print())[0m
[33m[2025-11-18 23:34:13] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-18 23:34:38]    INFO >> epoch 003 | valid on 'valid' subset | loss 3.894 | wps 6095.4 | wpb 5412.5 | bsz 5412.5 | num_updates 4604 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-18 23:34:38]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-18 23:34:38]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 3 @ 4604 updates, score 3.894) (writing took 0.013379 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-18 23:34:38] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-18 23:34:54]    INFO >> epoch 004:     46 / 1539 loss=3.637, wps=924.3, ups=1.24, wpb=743.3, bsz=743.3, num_updates=4650, lr=0.000376, gnorm=5.495, clip=0, train_wall=15, gb_free=73.2, wall=1514 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:35:10]    INFO >> epoch 004:     96 / 1539 loss=3.726, wps=2690.1, ups=3.04, wpb=886.3, bsz=886.3, num_updates=4700, lr=0.000376, gnorm=5.335, clip=2, train_wall=16, gb_free=31.5, wall=1530 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:35:29]    INFO >> epoch 004:    146 / 1539 loss=3.382, wps=2621.7, ups=2.85, wpb=921.3, bsz=921.3, num_updates=4750, lr=0.000376, gnorm=6.706, clip=2, train_wall=17, gb_free=75.8, wall=1548 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:35:43]    INFO >> epoch 004:    196 / 1539 loss=3.782, wps=2524.9, ups=3.59, wpb=703.7, bsz=703.7, num_updates=4800, lr=0.000376, gnorm=5.242, clip=2, train_wall=14, gb_free=70.1, wall=1562 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:35:59]    INFO >> epoch 004:    246 / 1539 loss=3.772, wps=2468.9, ups=3.35, wpb=737.9, bsz=737.9, num_updates=4850, lr=0.000376, gnorm=5.254, clip=0, train_wall=15, gb_free=69.4, wall=1577 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:36:13]    INFO >> epoch 004:    296 / 1539 loss=3.823, wps=2345.5, ups=3.62, wpb=648.1, bsz=648.1, num_updates=4900, lr=0.000376, gnorm=5.346, clip=2, train_wall=13, gb_free=71.5, wall=1591 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:36:28]    INFO >> epoch 004:    346 / 1539 loss=3.741, wps=2547.6, ups=3.71, wpb=687, bsz=687, num_updates=4950, lr=0.000376, gnorm=4.497, clip=0, train_wall=13, gb_free=70.5, wall=1604 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:36:42]    INFO >> epoch 004:    396 / 1539 loss=3.549, wps=2639.8, ups=3.46, wpb=764, bsz=764, num_updates=5000, lr=0.000376, gnorm=5.66, clip=0, train_wall=14, gb_free=72.9, wall=1619 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:37:00]    INFO >> epoch 004:    446 / 1539 loss=3.844, wps=1977.4, ups=3.12, wpb=634, bsz=634, num_updates=5050, lr=0.000376, gnorm=5.302, clip=0, train_wall=15, gb_free=62.6, wall=1635 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:37:14]    INFO >> epoch 004:    496 / 1539 loss=3.785, wps=2535.9, ups=3.54, wpb=716.9, bsz=716.9, num_updates=5100, lr=0.000376, gnorm=5.445, clip=0, train_wall=14, gb_free=60.7, wall=1649 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:37:28]    INFO >> epoch 004:    546 / 1539 loss=3.778, wps=2501.6, ups=3.53, wpb=708.9, bsz=708.9, num_updates=5150, lr=0.000376, gnorm=5.538, clip=0, train_wall=14, gb_free=73.2, wall=1663 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:37:44]    INFO >> epoch 004:    596 / 1539 loss=3.814, wps=2175.5, ups=3.51, wpb=620.1, bsz=620.1, num_updates=5200, lr=0.000376, gnorm=4.079, clip=0, train_wall=14, gb_free=74.9, wall=1677 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:37:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 347.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 75.96 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 5.38 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:37:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 27        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69008 MiB |  71766 MiB | 167309 GiB | 167242 GiB |
|       from large pool |  68991 MiB |  71749 MiB | 166364 GiB | 166297 GiB |
|       from small pool |     16 MiB |     20 MiB |    945 GiB |    945 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69008 MiB |  71766 MiB | 167309 GiB | 167242 GiB |
|       from large pool |  68991 MiB |  71749 MiB | 166364 GiB | 166297 GiB |
|       from small pool |     16 MiB |     20 MiB |    945 GiB |    945 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 166952 GiB | 166884 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 166008 GiB | 165941 GiB |
|       from small pool |     16 MiB |     20 MiB |    943 GiB |    943 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77274 MiB |  77526 MiB | 368146 MiB | 290872 MiB |
|       from large pool |  77248 MiB |  77248 MiB | 365822 MiB | 288574 MiB |
|       from small pool |     26 MiB |    392 MiB |   2324 MiB |   2298 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6375 MiB |  10186 MiB | 161585 GiB | 161579 GiB |
|       from large pool |   6366 MiB |  10177 MiB | 160514 GiB | 160508 GiB |
|       from small pool |      9 MiB |     29 MiB |   1070 GiB |   1070 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   11020 K  |   11020 K  |
|       from large pool |     343    |     351    |    5268 K  |    5268 K  |
|       from small pool |     298    |     355    |    5751 K  |    5751 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   11020 K  |   11020 K  |
|       from large pool |     343    |     351    |    5268 K  |    5268 K  |
|       from small pool |     298    |     355    |    5751 K  |    5751 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     178    |     449    |    2367    |    2189    |
|       from large pool |     165    |     253    |    1205    |    1040    |
|       from small pool |      13    |     196    |    1162    |    1149    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     164    |     166    |    6237 K  |    6237 K  |
|       from large pool |     138    |     140    |    3483 K  |    3483 K  |
|       from small pool |      26    |      57    |    2753 K  |    2753 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:37:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:37:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:37:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:37:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:37:58]    INFO >> epoch 004:    647 / 1539 loss=3.569, wps=2390, ups=3.52, wpb=679.2, bsz=679.2, num_updates=5250, lr=0.000376, gnorm=5.099, clip=0, train_wall=13, gb_free=70.9, wall=1691 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:38:12] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.77 GiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 74.53 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 3.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:38:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 29        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72060 MiB |  72460 MiB | 169909 GiB | 169839 GiB |
|       from large pool |  72042 MiB |  72443 MiB | 168951 GiB | 168880 GiB |
|       from small pool |     17 MiB |     24 MiB |    958 GiB |    958 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72060 MiB |  72460 MiB | 169909 GiB | 169839 GiB |
|       from large pool |  72042 MiB |  72443 MiB | 168951 GiB | 168880 GiB |
|       from small pool |     17 MiB |     24 MiB |    958 GiB |    958 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 169546 GiB | 169475 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 168589 GiB | 168518 GiB |
|       from small pool |     17 MiB |     24 MiB |    956 GiB |    956 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  75810 MiB |  75810 MiB | 384002 MiB | 308192 MiB |
|       from large pool |  75786 MiB |  75786 MiB | 381648 MiB | 305862 MiB |
|       from small pool |     24 MiB |     56 MiB |   2354 MiB |   2330 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3749 MiB |   7475 MiB | 164119 GiB | 164115 GiB |
|       from large pool |   3743 MiB |   7464 MiB | 163033 GiB | 163029 GiB |
|       from small pool |      6 MiB |     23 MiB |   1086 GiB |   1086 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   11190 K  |   11189 K  |
|       from large pool |     308    |     315    |    5358 K  |    5358 K  |
|       from small pool |     298    |     356    |    5831 K  |    5831 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   11190 K  |   11189 K  |
|       from large pool |     308    |     315    |    5358 K  |    5358 K  |
|       from small pool |     298    |     356    |    5831 K  |    5831 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     146    |     192    |    2392    |    2246    |
|       from large pool |     134    |     164    |    1215    |    1081    |
|       from small pool |      12    |      28    |    1177    |    1165    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     139    |     139    |    6330 K  |    6329 K  |
|       from large pool |     112    |     112    |    3542 K  |    3542 K  |
|       from small pool |      27    |      49    |    2787 K  |    2787 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:38:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:38:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:38:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:38:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:38:14]    INFO >> epoch 004:    698 / 1539 loss=3.765, wps=2124.5, ups=3.36, wpb=631.8, bsz=631.8, num_updates=5300, lr=0.000376, gnorm=4.54, clip=0, train_wall=13, gb_free=73.1, wall=1706 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:38:31]    INFO >> epoch 004:    748 / 1539 loss=3.757, wps=2049.4, ups=3.02, wpb=679.3, bsz=679.3, num_updates=5350, lr=0.000376, gnorm=4.752, clip=0, train_wall=16, gb_free=73.3, wall=1723 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:38:46]    INFO >> epoch 004:    798 / 1539 loss=3.678, wps=2473.2, ups=3.48, wpb=710.2, bsz=710.2, num_updates=5400, lr=0.000376, gnorm=4.729, clip=0, train_wall=14, gb_free=72, wall=1737 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:39:01]    INFO >> epoch 004:    848 / 1539 loss=3.568, wps=2585.8, ups=3.5, wpb=738.3, bsz=738.3, num_updates=5450, lr=0.000376, gnorm=5.988, clip=0, train_wall=14, gb_free=74.5, wall=1751 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:39:17]    INFO >> epoch 004:    898 / 1539 loss=3.764, wps=2571.9, ups=3.29, wpb=781.1, bsz=781.1, num_updates=5500, lr=0.000376, gnorm=5.397, clip=0, train_wall=15, gb_free=73.7, wall=1767 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:39:31]    INFO >> epoch 004:    948 / 1539 loss=3.727, wps=2377.1, ups=3.67, wpb=647.5, bsz=647.5, num_updates=5550, lr=0.000376, gnorm=4.237, clip=0, train_wall=13, gb_free=73.1, wall=1780 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:39:48]    INFO >> epoch 004:    998 / 1539 loss=3.892, wps=2465.5, ups=3.26, wpb=756.2, bsz=756.2, num_updates=5600, lr=0.000376, gnorm=5.131, clip=0, train_wall=15, gb_free=74.2, wall=1796 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:40:02]    INFO >> epoch 004:   1048 / 1539 loss=3.814, wps=2272, ups=3.57, wpb=637.1, bsz=637.1, num_updates=5650, lr=0.000376, gnorm=4.562, clip=0, train_wall=14, gb_free=72.6, wall=1810 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:40:16]    INFO >> epoch 004:   1098 / 1539 loss=3.753, wps=2558.8, ups=3.52, wpb=726.8, bsz=726.8, num_updates=5700, lr=0.000376, gnorm=4.569, clip=0, train_wall=14, gb_free=73.3, wall=1824 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:40:31]    INFO >> epoch 004:   1148 / 1539 loss=3.649, wps=2451.8, ups=3.59, wpb=682.9, bsz=682.9, num_updates=5750, lr=0.000376, gnorm=4.529, clip=0, train_wall=13, gb_free=68, wall=1838 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:40:46]    INFO >> epoch 004:   1198 / 1539 loss=3.67, wps=2539.2, ups=3.38, wpb=750.3, bsz=750.3, num_updates=5800, lr=0.000376, gnorm=4.804, clip=0, train_wall=14, gb_free=65.5, wall=1853 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:41:00]    INFO >> epoch 004:   1248 / 1539 loss=3.651, wps=2433.9, ups=3.9, wpb=624.8, bsz=624.8, num_updates=5850, lr=0.000376, gnorm=4.574, clip=0, train_wall=12, gb_free=72, wall=1865 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:41:15]    INFO >> epoch 004:   1298 / 1539 loss=3.677, wps=2175.1, ups=3.34, wpb=651.5, bsz=651.5, num_updates=5900, lr=0.000376, gnorm=4.687, clip=2, train_wall=15, gb_free=70.1, wall=1880 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:41:30]    INFO >> epoch 004:   1348 / 1539 loss=3.557, wps=2749.4, ups=3.42, wpb=804.5, bsz=804.5, num_updates=5950, lr=0.000376, gnorm=5.751, clip=2, train_wall=14, gb_free=71.5, wall=1895 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:41:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 199.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.11 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 2.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:41:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 31        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72821 MiB |  75424 MiB | 190606 GiB | 190535 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 189538 GiB | 189467 GiB |
|       from small pool |     12 MiB |     13 MiB |   1067 GiB |   1067 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72821 MiB |  75424 MiB | 190606 GiB | 190535 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 189538 GiB | 189467 GiB |
|       from small pool |     12 MiB |     13 MiB |   1067 GiB |   1067 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 190196 GiB | 190125 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 189130 GiB | 189059 GiB |
|       from small pool |     12 MiB |     13 MiB |   1066 GiB |   1066 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77422 MiB |  77482 MiB | 388536 MiB | 311114 MiB |
|       from large pool |  77398 MiB |  77458 MiB | 385960 MiB | 308562 MiB |
|       from small pool |     24 MiB |    246 MiB |   2576 MiB |   2552 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2698 MiB |   7422 MiB | 185067 GiB | 185064 GiB |
|       from large pool |   2687 MiB |   7410 MiB | 183855 GiB | 183852 GiB |
|       from small pool |     11 MiB |     19 MiB |   1211 GiB |   1211 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   12540 K  |   12540 K  |
|       from large pool |     312    |     320    |    6050 K  |    6049 K  |
|       from small pool |     291    |     336    |    6490 K  |    6490 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   12540 K  |   12540 K  |
|       from large pool |     312    |     320    |    6050 K  |    6049 K  |
|       from small pool |     291    |     336    |    6490 K  |    6490 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     107    |     257    |    2509    |    2402    |
|       from large pool |      95    |     134    |    1221    |    1126    |
|       from small pool |      12    |     123    |    1288    |    1276    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     107    |     107    |    7072 K  |    7072 K  |
|       from large pool |      80    |      80    |    3991 K  |    3991 K  |
|       from small pool |      27    |      48    |    3081 K  |    3081 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:41:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:41:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:41:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:41:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:41:45]    INFO >> epoch 004:   1399 / 1539 loss=3.714, wps=2509, ups=3.2, wpb=783.3, bsz=783.3, num_updates=6000, lr=0.000376, gnorm=5.554, clip=0, train_wall=14, gb_free=73.6, wall=1911 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:42:02]    INFO >> epoch 004:   1449 / 1539 loss=3.706, wps=2484.3, ups=3.5, wpb=709.1, bsz=709.1, num_updates=6050, lr=0.000376, gnorm=4.594, clip=0, train_wall=14, gb_free=72.3, wall=1925 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:42:17]    INFO >> epoch 004:   1499 / 1539 loss=3.724, wps=2382.4, ups=3.48, wpb=685.1, bsz=685.1, num_updates=6100, lr=0.000376, gnorm=4.909, clip=0, train_wall=14, gb_free=71.8, wall=1939 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:42:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 43.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.26 GiB memory in use. Of the allocated memory 74.28 GiB is allocated by PyTorch, and 1.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:42:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76006 MiB |  76066 MiB | 194065 GiB | 193990 GiB |
|       from large pool |  75642 MiB |  75702 MiB | 192976 GiB | 192902 GiB |
|       from small pool |    364 MiB |    365 MiB |   1089 GiB |   1088 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76006 MiB |  76066 MiB | 194065 GiB | 193990 GiB |
|       from large pool |  75642 MiB |  75702 MiB | 192976 GiB | 192902 GiB |
|       from small pool |    364 MiB |    365 MiB |   1089 GiB |   1088 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75975 MiB |  76035 MiB | 193648 GiB | 193574 GiB |
|       from large pool |  75613 MiB |  75673 MiB | 192560 GiB | 192486 GiB |
|       from small pool |    362 MiB |    363 MiB |   1087 GiB |   1087 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77578 MiB |  77578 MiB | 390594 MiB | 313016 MiB |
|       from large pool |  77176 MiB |  77176 MiB | 387640 MiB | 310464 MiB |
|       from small pool |    402 MiB |    402 MiB |   2954 MiB |   2552 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1511 MiB |   5091 MiB | 189043 GiB | 189042 GiB |
|       from large pool |   1473 MiB |   5086 MiB | 187807 GiB | 187805 GiB |
|       from small pool |     37 MiB |     38 MiB |   1236 GiB |   1236 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6724    |    6727    |   12787 K  |   12781 K  |
|       from large pool |     881    |     882    |    6166 K  |    6165 K  |
|       from small pool |    5843    |    5846    |    6621 K  |    6615 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6724    |    6727    |   12787 K  |   12781 K  |
|       from large pool |     881    |     882    |    6166 K  |    6165 K  |
|       from small pool |    5843    |    5846    |    6621 K  |    6615 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     323    |     323    |    2726    |    2403    |
|       from large pool |     122    |     122    |    1249    |    1127    |
|       from small pool |     201    |     201    |    1477    |    1276    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     445    |     445    |    7207 K  |    7207 K  |
|       from large pool |      86    |      86    |    4063 K  |    4063 K  |
|       from small pool |     359    |     359    |    3143 K  |    3143 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:42:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:42:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:42:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:42:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:42:28]    INFO >> epoch 004 | loss 3.704 | wps 2299.6 | ups 3.23 | wpb 712.7 | bsz 712.7 | num_updates 6139 | lr 0.000376 | gnorm 5.077 | clip 0.4 | train_wall 433 | gb_free 70.1 | wall 1950 (progress_bar.py:267, print())[0m
[33m[2025-11-18 23:42:28] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-18 23:42:52]    INFO >> epoch 004 | valid on 'valid' subset | loss 3.842 | wps 6130.9 | wpb 5412.5 | bsz 5412.5 | num_updates 6139 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-18 23:42:53]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-18 23:42:53]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 4 @ 6139 updates, score 3.842) (writing took 0.013423 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-18 23:42:53] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-18 23:42:56]    INFO >> epoch 005:     11 / 1539 loss=3.748, wps=819.9, ups=1.27, wpb=644.5, bsz=644.5, num_updates=6150, lr=0.000354, gnorm=4.912, clip=0, train_wall=13, gb_free=70.3, wall=1979 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:43:14]    INFO >> epoch 005:     61 / 1539 loss=3.819, wps=2387.3, ups=3.36, wpb=710.6, bsz=710.6, num_updates=6200, lr=0.000354, gnorm=4.152, clip=0, train_wall=14, gb_free=72.2, wall=1993 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:43:27]    INFO >> epoch 005:    111 / 1539 loss=3.689, wps=2535.5, ups=3.64, wpb=696.3, bsz=696.3, num_updates=6250, lr=0.000354, gnorm=4.256, clip=0, train_wall=13, gb_free=74.8, wall=2007 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:43:40]    INFO >> epoch 005:    161 / 1539 loss=3.635, wps=2428.2, ups=3.81, wpb=637.8, bsz=637.8, num_updates=6300, lr=0.000354, gnorm=4.753, clip=0, train_wall=13, gb_free=75.2, wall=2020 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:43:55]    INFO >> epoch 005:    211 / 1539 loss=3.712, wps=2338.5, ups=3.48, wpb=672.9, bsz=672.9, num_updates=6350, lr=0.000354, gnorm=4.526, clip=0, train_wall=14, gb_free=73.3, wall=2035 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:44:11]    INFO >> epoch 005:    261 / 1539 loss=3.627, wps=2709.4, ups=3.02, wpb=898, bsz=898, num_updates=6400, lr=0.000354, gnorm=5.532, clip=0, train_wall=16, gb_free=73.4, wall=2051 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:44:28]    INFO >> epoch 005:    311 / 1539 loss=3.633, wps=2511.1, ups=3.53, wpb=712.2, bsz=712.2, num_updates=6450, lr=0.000354, gnorm=4.923, clip=0, train_wall=14, gb_free=66.9, wall=2065 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:44:43]    INFO >> epoch 005:    361 / 1539 loss=3.62, wps=2605.7, ups=3.43, wpb=760.5, bsz=760.5, num_updates=6500, lr=0.000354, gnorm=4.157, clip=0, train_wall=14, gb_free=73.1, wall=2080 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:44:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 49.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 76.25 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 1.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:44:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 33        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75508 MiB |  75568 MiB | 210880 GiB | 210806 GiB |
|       from large pool |  75149 MiB |  75209 MiB | 209681 GiB | 209607 GiB |
|       from small pool |    359 MiB |    360 MiB |   1199 GiB |   1198 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75508 MiB |  75568 MiB | 210880 GiB | 210806 GiB |
|       from large pool |  75149 MiB |  75209 MiB | 209681 GiB | 209607 GiB |
|       from small pool |    359 MiB |    360 MiB |   1199 GiB |   1198 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75495 MiB |  75555 MiB | 210431 GiB | 210357 GiB |
|       from large pool |  75138 MiB |  75197 MiB | 209234 GiB | 209160 GiB |
|       from small pool |    357 MiB |    358 MiB |   1197 GiB |   1197 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77572 MiB |  77578 MiB | 390654 MiB | 313082 MiB |
|       from large pool |  77176 MiB |  77176 MiB | 387700 MiB | 310524 MiB |
|       from small pool |    396 MiB |    402 MiB |   2954 MiB |   2558 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2003 MiB |   6445 MiB | 204917 GiB | 204915 GiB |
|       from large pool |   1966 MiB |   6439 MiB | 203558 GiB | 203556 GiB |
|       from small pool |     36 MiB |     38 MiB |   1358 GiB |   1358 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6636    |    6639    |   13939 K  |   13932 K  |
|       from large pool |     873    |     874    |    6633 K  |    6632 K  |
|       from small pool |    5763    |    5766    |    7305 K  |    7300 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    6636    |    6639    |   13939 K  |   13932 K  |
|       from large pool |     873    |     874    |    6633 K  |    6632 K  |
|       from small pool |    5763    |    5766    |    7305 K  |    7300 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     320    |     323    |    2727    |    2407    |
|       from large pool |     122    |     122    |    1250    |    1128    |
|       from small pool |     198    |     201    |    1477    |    1279    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     467    |     467    |    7875 K  |    7874 K  |
|       from large pool |     112    |     112    |    4367 K  |    4366 K  |
|       from small pool |     355    |     355    |    3507 K  |    3507 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:44:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:44:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:44:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-18 23:44:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-18 23:44:58]    INFO >> epoch 005:    412 / 1539 loss=3.656, wps=2407.6, ups=3.54, wpb=680.3, bsz=680.3, num_updates=6550, lr=0.000354, gnorm=5.269, clip=0, train_wall=13, gb_free=75.6, wall=2094 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:45:12]    INFO >> epoch 005:    462 / 1539 loss=3.765, wps=2279.1, ups=3.72, wpb=612.2, bsz=612.2, num_updates=6600, lr=0.000354, gnorm=4.056, clip=0, train_wall=13, gb_free=74.3, wall=2108 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:45:28]    INFO >> epoch 005:    512 / 1539 loss=3.635, wps=2441.2, ups=3.34, wpb=731.5, bsz=731.5, num_updates=6650, lr=0.000354, gnorm=4.364, clip=0, train_wall=15, gb_free=74.8, wall=2123 (progress_bar.py:258, log())[0m
[32m[2025-11-18 23:45:42]    INFO >> epoch 005:    562 / 1539 loss=3.758, wps=2189.4, ups=3.53, wpb=619.7, bsz=619.7, num_updates=6700, lr=0.000354, gnorm=4.399, clip=0, train_wall=14, gb_free=75.6, wall=2137 (progress_bar.py:258, log())[0m
[33m[2025-11-18 23:45:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 475.69 MiB is free. Process 2419889 has 2.81 GiB memory in use. Including non-PyTorch memory, this process has 75.84 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 5.26 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-18 23:45:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 34        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69004 MiB |  71761 MiB | 216269 GiB | 216202 GiB |
|       from large pool |  68988 MiB |  71744 MiB | 215043 GiB | 214976 GiB |
|       from small pool |     16 MiB |     17 MiB |   1226 GiB |   1226 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69004 MiB |  71761 MiB | 216269 GiB | 216202 GiB |
|       from large pool |  68988 MiB |  71744 MiB | 215043 GiB | 214976 GiB |
|       from small pool |     16 MiB |     17 MiB |   1226 GiB |   1226 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 215809 GiB | 215742 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 214585 GiB | 214517 GiB |
|       from small pool |     16 MiB |     17 MiB |   1224 GiB |   1224 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  77146 MiB |  77512 MiB | 390654 MiB | 313508 MiB |
|       from large pool |  77116 MiB |  77116 MiB | 387700 MiB | 310584 MiB |
|       from small pool |     30 MiB |    396 MiB |   2954 MiB |   2924 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5413 MiB |   9813 MiB | 210644 GiB | 210639 GiB |
|       from large pool |   5399 MiB |   9799 MiB | 209254 GiB | 209249 GiB |
|       from small pool |     13 MiB |     27 MiB |   1389 GiB |   1389 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   14282 K  |   14282 K  |
|       from large pool |     343    |     351    |    6815 K  |    6814 K  |
|       from small pool |     298    |     356    |    7467 K  |    7467 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   14282 K  |   14282 K  |
|       from large pool |     343    |     351    |    6815 K  |    6814 K  |
|       from small pool |     298    |     356    |    7467 K  |    7467 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     136    |     319    |    2727    |    2591    |
|       from large pool |     121    |     121    |    1250    |    1129    |
|       from small pool |      15    |     198    |    1477    |    1462    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     126    |     134    |    8062 K  |    8061 K  |
|       from large pool |      97    |     105    |    4482 K  |    4482 K  |
|       from small pool |      29    |      59    |    3579 K  |    3579 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:39:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:39:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:39:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:39:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:39:16]    INFO >> epoch 005:    613 / 1539 loss=3.559, wps=3.8, ups=0.01, wpb=763.6, bsz=763.6, num_updates=6750, lr=0.000354, gnorm=5.701, clip=2, train_wall=10, gb_free=71.8, wall=12135 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:39:24]    INFO >> epoch 005:    663 / 1539 loss=3.776, wps=5168.3, ups=7.07, wpb=730.6, bsz=730.6, num_updates=6800, lr=0.000354, gnorm=5.097, clip=0, train_wall=7, gb_free=75.4, wall=12142 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:39:36]    INFO >> epoch 005:    713 / 1539 loss=3.728, wps=3172.4, ups=4.28, wpb=741, bsz=741, num_updates=6850, lr=0.000354, gnorm=5.269, clip=0, train_wall=11, gb_free=68.3, wall=12153 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:39:43]    INFO >> epoch 005:    763 / 1539 loss=3.693, wps=5213.9, ups=7.39, wpb=705.1, bsz=705.1, num_updates=6900, lr=0.000354, gnorm=4.523, clip=0, train_wall=6, gb_free=70, wall=12160 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:39:52]    INFO >> epoch 005:    813 / 1539 loss=3.642, wps=4446.4, ups=6.62, wpb=671.7, bsz=671.7, num_updates=6950, lr=0.000354, gnorm=4.911, clip=2, train_wall=7, gb_free=72.9, wall=12168 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:39:58]    INFO >> epoch 005:    863 / 1539 loss=3.553, wps=5009.7, ups=7.45, wpb=672.8, bsz=672.8, num_updates=7000, lr=0.000354, gnorm=4.356, clip=0, train_wall=6, gb_free=74.7, wall=12174 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:40:05]    INFO >> epoch 005:    913 / 1539 loss=3.573, wps=4822.8, ups=7.24, wpb=666.4, bsz=666.4, num_updates=7050, lr=0.000354, gnorm=4.578, clip=0, train_wall=7, gb_free=72.2, wall=12181 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:40:11] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 635.25 MiB is free. Including non-PyTorch memory, this process has 78.50 GiB memory in use. Of the allocated memory 74.09 GiB is allocated by PyTorch, and 3.91 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:40:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 35        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72058 MiB |  75867 MiB | 226912 GiB | 226842 GiB |
|       from large pool |  72040 MiB |  75849 MiB | 225632 GiB | 225561 GiB |
|       from small pool |     17 MiB |     24 MiB |   1280 GiB |   1280 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72058 MiB |  75867 MiB | 226912 GiB | 226842 GiB |
|       from large pool |  72040 MiB |  75849 MiB | 225632 GiB | 225561 GiB |
|       from small pool |     17 MiB |     24 MiB |   1280 GiB |   1280 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  75852 MiB | 226429 GiB | 226359 GiB |
|       from large pool |  72027 MiB |  75835 MiB | 225150 GiB | 225080 GiB |
|       from small pool |     17 MiB |     24 MiB |   1279 GiB |   1279 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79870 MiB |  79910 MiB | 396146 MiB | 316276 MiB |
|       from large pool |  79844 MiB |  79844 MiB | 393156 MiB | 313312 MiB |
|       from small pool |     26 MiB |     66 MiB |   2990 MiB |   2964 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5083 MiB |   7360 MiB | 221827 GiB | 221822 GiB |
|       from large pool |   5075 MiB |   7351 MiB | 220376 GiB | 220371 GiB |
|       from small pool |      8 MiB |     21 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     615    |   14965 K  |   14965 K  |
|       from large pool |     308    |     317    |    7169 K  |    7169 K  |
|       from small pool |     298    |     356    |    7795 K  |    7795 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     615    |   14965 K  |   14965 K  |
|       from large pool |     308    |     317    |    7169 K  |    7169 K  |
|       from small pool |     298    |     356    |    7795 K  |    7795 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     135    |     155    |    2747    |    2612    |
|       from large pool |     122    |     122    |    1252    |    1130    |
|       from small pool |      13    |      33    |    1495    |    1482    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     131    |     131    |    8433 K  |    8433 K  |
|       from large pool |     106    |     106    |    4710 K  |    4710 K  |
|       from small pool |      25    |      49    |    3723 K  |    3723 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:40:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:40:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:40:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:40:11] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:40:12]    INFO >> epoch 005:    964 / 1539 loss=3.658, wps=4667.4, ups=6.89, wpb=677.5, bsz=677.5, num_updates=7100, lr=0.000354, gnorm=4.166, clip=0, train_wall=6, gb_free=73.6, wall=12189 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:40:20]    INFO >> epoch 005:   1014 / 1539 loss=3.455, wps=6042.3, ups=6.31, wpb=957.3, bsz=957.3, num_updates=7150, lr=0.000354, gnorm=4.726, clip=0, train_wall=7, gb_free=71.9, wall=12196 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:40:29]    INFO >> epoch 005:   1064 / 1539 loss=3.539, wps=5256.1, ups=6.63, wpb=792.7, bsz=792.7, num_updates=7200, lr=0.000354, gnorm=5.55, clip=0, train_wall=7, gb_free=72.4, wall=12204 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:40:36]    INFO >> epoch 005:   1114 / 1539 loss=3.63, wps=4637.3, ups=7.47, wpb=620.6, bsz=620.6, num_updates=7250, lr=0.000354, gnorm=4.812, clip=0, train_wall=6, gb_free=73.3, wall=12211 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:40:38] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 551.25 MiB is free. Including non-PyTorch memory, this process has 78.58 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:40:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 37        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76956 MiB |  77479 MiB | 232438 GiB | 232362 GiB |
|       from large pool |  76943 MiB |  77466 MiB | 231126 GiB | 231050 GiB |
|       from small pool |     12 MiB |     21 MiB |   1312 GiB |   1311 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76956 MiB |  77479 MiB | 232438 GiB | 232362 GiB |
|       from large pool |  76943 MiB |  77466 MiB | 231126 GiB | 231050 GiB |
|       from small pool |     12 MiB |     21 MiB |   1312 GiB |   1311 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 231944 GiB | 231869 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 230634 GiB | 230558 GiB |
|       from small pool |     12 MiB |     21 MiB |   1310 GiB |   1310 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79954 MiB |  79954 MiB | 400658 MiB | 320704 MiB |
|       from large pool |  79928 MiB |  79928 MiB | 397468 MiB | 317540 MiB |
|       from small pool |     26 MiB |    226 MiB |   3190 MiB |   3164 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2997 MiB |   6919 MiB | 227729 GiB | 227726 GiB |
|       from large pool |   2984 MiB |   6905 MiB | 226241 GiB | 226238 GiB |
|       from small pool |     13 MiB |     31 MiB |   1488 GiB |   1488 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   15328 K  |   15327 K  |
|       from large pool |     315    |     322    |    7345 K  |    7345 K  |
|       from small pool |     291    |     356    |    7982 K  |    7982 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   15328 K  |   15327 K  |
|       from large pool |     315    |     322    |    7345 K  |    7345 K  |
|       from small pool |     291    |     356    |    7982 K  |    7982 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     115    |     239    |    2853    |    2738    |
|       from large pool |     102    |     126    |    1258    |    1156    |
|       from small pool |      13    |     113    |    1595    |    1582    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     114    |     114    |    8636 K  |    8636 K  |
|       from large pool |      86    |      86    |    4821 K  |    4821 K  |
|       from small pool |      28    |      65    |    3815 K  |    3815 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:40:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:40:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:40:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:40:38] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:40:44]    INFO >> epoch 005:   1165 / 1539 loss=3.686, wps=4972.2, ups=6.36, wpb=782.2, bsz=782.2, num_updates=7300, lr=0.000354, gnorm=4.497, clip=0, train_wall=7, gb_free=70.1, wall=12219 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:40:51]    INFO >> epoch 005:   1215 / 1539 loss=3.699, wps=4695, ups=7.19, wpb=653.4, bsz=653.4, num_updates=7350, lr=0.000354, gnorm=4.358, clip=0, train_wall=7, gb_free=73.5, wall=12225 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:40:57]    INFO >> epoch 005:   1265 / 1539 loss=3.548, wps=5052.7, ups=7.97, wpb=634.2, bsz=634.2, num_updates=7400, lr=0.000354, gnorm=5.064, clip=0, train_wall=6, gb_free=73.1, wall=12232 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:41:05]    INFO >> epoch 005:   1315 / 1539 loss=3.688, wps=4595.1, ups=7.07, wpb=650.3, bsz=650.3, num_updates=7450, lr=0.000354, gnorm=4.543, clip=0, train_wall=7, gb_free=70.6, wall=12239 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:41:13]    INFO >> epoch 005:   1365 / 1539 loss=3.634, wps=5110.1, ups=7.03, wpb=726.5, bsz=726.5, num_updates=7500, lr=0.000354, gnorm=4.997, clip=0, train_wall=7, gb_free=71.2, wall=12246 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:41:20]    INFO >> epoch 005:   1415 / 1539 loss=3.766, wps=4546.2, ups=7.17, wpb=633.8, bsz=633.8, num_updates=7550, lr=0.000354, gnorm=4.38, clip=0, train_wall=7, gb_free=71.9, wall=12253 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:41:27]    INFO >> epoch 005:   1465 / 1539 loss=3.546, wps=5160, ups=6.73, wpb=767.1, bsz=767.1, num_updates=7600, lr=0.000354, gnorm=5.127, clip=0, train_wall=7, gb_free=71.3, wall=12260 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:41:36]    INFO >> epoch 005:   1515 / 1539 loss=3.567, wps=5128.7, ups=6.38, wpb=804.1, bsz=804.1, num_updates=7650, lr=0.000354, gnorm=5.188, clip=0, train_wall=7, gb_free=68, wall=12268 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:41:40]    INFO >> epoch 005 | loss 3.647 | wps 106 | ups 0.15 | wpb 712.7 | bsz 712.7 | num_updates 7674 | lr 0.000354 | gnorm 4.746 | clip 0.1 | train_wall 294 | gb_free 63.5 | wall 12272 (progress_bar.py:267, print())[0m
[33m[2025-11-19 02:41:40] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:41:52]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.761 | wps 11953.9 | wpb 5412.5 | bsz 5412.5 | num_updates 7674 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 02:41:53]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 02:41:53]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 5 @ 7674 updates, score 3.761) (writing took 0.012259 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 02:41:53] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:41:56]    INFO >> epoch 006:     26 / 1539 loss=3.671, wps=1749.4, ups=2.48, wpb=706.1, bsz=706.1, num_updates=7700, lr=0.000327, gnorm=4.497, clip=0, train_wall=7, gb_free=74.2, wall=12288 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:42:04]    INFO >> epoch 006:     76 / 1539 loss=3.79, wps=5441.5, ups=6.35, wpb=857.5, bsz=857.5, num_updates=7750, lr=0.000327, gnorm=5.274, clip=0, train_wall=7, gb_free=74.6, wall=12296 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:42:13]    INFO >> epoch 006:    126 / 1539 loss=3.537, wps=4787.7, ups=6.97, wpb=687, bsz=687, num_updates=7800, lr=0.000327, gnorm=4.493, clip=0, train_wall=7, gb_free=67.5, wall=12303 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:42:20]    INFO >> epoch 006:    176 / 1539 loss=3.613, wps=4631.9, ups=6.85, wpb=675.9, bsz=675.9, num_updates=7850, lr=0.000327, gnorm=4.339, clip=0, train_wall=7, gb_free=68.7, wall=12311 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:42:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 611.25 MiB is free. Including non-PyTorch memory, this process has 78.52 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 4.22 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:42:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 38        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75054 MiB |  75997 MiB | 254432 GiB | 254358 GiB |
|       from large pool |  75041 MiB |  75984 MiB | 252990 GiB | 252917 GiB |
|       from small pool |     12 MiB |     15 MiB |   1441 GiB |   1441 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75054 MiB |  75997 MiB | 254432 GiB | 254358 GiB |
|       from large pool |  75041 MiB |  75984 MiB | 252990 GiB | 252917 GiB |
|       from small pool |     12 MiB |     15 MiB |   1441 GiB |   1441 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 253896 GiB | 253823 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 252456 GiB | 252383 GiB |
|       from small pool |     12 MiB |     15 MiB |   1439 GiB |   1439 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79894 MiB |  80026 MiB | 400730 MiB | 320836 MiB |
|       from large pool |  79868 MiB |  79928 MiB | 397468 MiB | 317600 MiB |
|       from small pool |     26 MiB |     98 MiB |   3262 MiB |   3236 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4779 MiB |   8403 MiB | 250061 GiB | 250056 GiB |
|       from large pool |   4766 MiB |   8389 MiB | 248429 GiB | 248425 GiB |
|       from small pool |     13 MiB |     25 MiB |   1631 GiB |   1631 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   16756 K  |   16756 K  |
|       from large pool |     314    |     322    |    7976 K  |    7976 K  |
|       from small pool |     291    |     356    |    8780 K  |    8780 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   16756 K  |   16756 K  |
|       from large pool |     314    |     322    |    7976 K  |    7976 K  |
|       from small pool |     291    |     356    |    8780 K  |    8780 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     114    |     151    |    2889    |    2775    |
|       from large pool |     101    |     102    |    1258    |    1157    |
|       from small pool |      13    |      49    |    1631    |    1618    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     110    |     110    |    9436 K  |    9436 K  |
|       from large pool |      84    |      84    |    5219 K  |    5219 K  |
|       from small pool |      26    |      57    |    4216 K  |    4216 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:42:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:42:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:42:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:42:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-19 02:42:28] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.47 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:42:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 42        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79271 MiB |  79331 MiB | 255609 GiB | 255531 GiB |
|       from large pool |  79179 MiB |  79239 MiB | 254161 GiB | 254084 GiB |
|       from small pool |     91 MiB |     92 MiB |   1447 GiB |   1447 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79271 MiB |  79331 MiB | 255609 GiB | 255531 GiB |
|       from large pool |  79179 MiB |  79239 MiB | 254161 GiB | 254084 GiB |
|       from small pool |     91 MiB |     92 MiB |   1447 GiB |   1447 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79244 MiB |  79303 MiB | 255071 GiB | 254993 GiB |
|       from large pool |  79153 MiB |  79212 MiB | 253625 GiB | 253548 GiB |
|       from small pool |     90 MiB |     92 MiB |   1445 GiB |   1445 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80502 MiB | 401462 MiB | 320960 MiB |
|       from large pool |  80408 MiB |  80408 MiB | 398128 MiB | 317720 MiB |
|       from small pool |     94 MiB |     96 MiB |   3334 MiB |   3240 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1170 MiB |   7797 MiB | 251337 GiB | 251336 GiB |
|       from large pool |   1168 MiB |   7788 MiB | 249700 GiB | 249698 GiB |
|       from small pool |      2 MiB |     27 MiB |   1637 GiB |   1637 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1983    |    1986    |   16826 K  |   16824 K  |
|       from large pool |     472    |     473    |    8010 K  |    8009 K  |
|       from small pool |    1511    |    1514    |    8815 K  |    8814 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1983    |    1986    |   16826 K  |   16824 K  |
|       from large pool |     472    |     473    |    8010 K  |    8009 K  |
|       from small pool |    1511    |    1514    |    8815 K  |    8814 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     157    |     157    |    2936    |    2779    |
|       from large pool |     110    |     110    |    1269    |    1159    |
|       from small pool |      47    |      48    |    1667    |    1620    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     131    |     132    |    9474 K  |    9474 K  |
|       from large pool |      83    |      89    |    5240 K  |    5240 K  |
|       from small pool |      48    |      61    |    4233 K  |    4233 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:42:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:42:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:42:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:42:28] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:42:29]    INFO >> epoch 006:    228 / 1539 loss=3.533, wps=4086.8, ups=5.55, wpb=736.4, bsz=736.4, num_updates=7900, lr=0.000327, gnorm=5.172, clip=0, train_wall=7, gb_free=70.3, wall=12320 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:42:36]    INFO >> epoch 006:    278 / 1539 loss=3.654, wps=4672, ups=7.46, wpb=626.3, bsz=626.3, num_updates=7950, lr=0.000327, gnorm=3.979, clip=0, train_wall=6, gb_free=68.3, wall=12326 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:42:45]    INFO >> epoch 006:    328 / 1539 loss=3.495, wps=5102.1, ups=6.23, wpb=819.2, bsz=819.2, num_updates=8000, lr=0.000327, gnorm=5.205, clip=0, train_wall=8, gb_free=61.8, wall=12334 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:42:52]    INFO >> epoch 006:    378 / 1539 loss=3.727, wps=4493.5, ups=6.82, wpb=658.6, bsz=658.6, num_updates=8050, lr=0.000327, gnorm=3.963, clip=0, train_wall=7, gb_free=73.6, wall=12342 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:43:00]    INFO >> epoch 006:    428 / 1539 loss=3.606, wps=5244.6, ups=6.74, wpb=777.8, bsz=777.8, num_updates=8100, lr=0.000327, gnorm=4.728, clip=0, train_wall=7, gb_free=72.2, wall=12349 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:43:02] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.85 GiB is free. Including non-PyTorch memory, this process has 77.27 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 6.01 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:43:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 44        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72056 MiB |  72456 MiB | 262033 GiB | 261963 GiB |
|       from large pool |  72039 MiB |  72438 MiB | 260551 GiB | 260480 GiB |
|       from small pool |     17 MiB |     21 MiB |   1482 GiB |   1482 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72056 MiB |  72456 MiB | 262033 GiB | 261963 GiB |
|       from large pool |  72039 MiB |  72438 MiB | 260551 GiB | 260480 GiB |
|       from small pool |     17 MiB |     21 MiB |   1482 GiB |   1482 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 261481 GiB | 261410 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 260000 GiB | 259930 GiB |
|       from small pool |     17 MiB |     21 MiB |   1480 GiB |   1480 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78612 MiB |  80222 MiB | 437782 MiB | 359170 MiB |
|       from large pool |  78586 MiB |  80012 MiB | 434332 MiB | 355746 MiB |
|       from small pool |     26 MiB |    210 MiB |   3450 MiB |   3424 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6431 MiB |   8503 MiB | 257687 GiB | 257681 GiB |
|       from large pool |   6422 MiB |   8494 MiB | 256009 GiB | 256002 GiB |
|       from small pool |      8 MiB |     29 MiB |   1678 GiB |   1678 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   17258 K  |   17258 K  |
|       from large pool |     308    |     315    |    8227 K  |    8226 K  |
|       from small pool |     298    |     356    |    9031 K  |    9031 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   17258 K  |   17258 K  |
|       from large pool |     308    |     315    |    8227 K  |    8226 K  |
|       from small pool |     298    |     356    |    9031 K  |    9031 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     181    |     296    |    3090    |    2909    |
|       from large pool |     168    |     191    |    1365    |    1197    |
|       from small pool |      13    |     105    |    1725    |    1712    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     168    |     170    |    9719 K  |    9719 K  |
|       from large pool |     145    |     147    |    5384 K  |    5384 K  |
|       from small pool |      23    |      56    |    4334 K  |    4334 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:43:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:43:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:43:02] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:43:02] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:43:08]    INFO >> epoch 006:    479 / 1539 loss=3.573, wps=4594.2, ups=6.49, wpb=707.8, bsz=707.8, num_updates=8150, lr=0.000327, gnorm=4.569, clip=0, train_wall=7, gb_free=69.8, wall=12357 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:43:17]    INFO >> epoch 006:    529 / 1539 loss=3.631, wps=4462.5, ups=6.39, wpb=697.9, bsz=697.9, num_updates=8200, lr=0.000327, gnorm=4.058, clip=0, train_wall=7, gb_free=73.7, wall=12365 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:43:24]    INFO >> epoch 006:    579 / 1539 loss=3.626, wps=5307.3, ups=6.65, wpb=798, bsz=798, num_updates=8250, lr=0.000327, gnorm=4.486, clip=0, train_wall=7, gb_free=75.1, wall=12372 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:43:31]    INFO >> epoch 006:    629 / 1539 loss=3.624, wps=4679.9, ups=7.24, wpb=646.3, bsz=646.3, num_updates=8300, lr=0.000327, gnorm=3.98, clip=0, train_wall=6, gb_free=71, wall=12379 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:43:39]    INFO >> epoch 006:    679 / 1539 loss=3.603, wps=4842.6, ups=6.54, wpb=740.2, bsz=740.2, num_updates=8350, lr=0.000327, gnorm=4.63, clip=0, train_wall=7, gb_free=69.7, wall=12387 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:43:47]    INFO >> epoch 006:    729 / 1539 loss=3.542, wps=4415.6, ups=7.29, wpb=605.6, bsz=605.6, num_updates=8400, lr=0.000327, gnorm=4.378, clip=0, train_wall=6, gb_free=70.9, wall=12394 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:43:48] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 75.98 GiB is allocated by PyTorch, and 2.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:43:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 47        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77744 MiB |  77804 MiB | 270505 GiB | 270429 GiB |
|       from large pool |  77363 MiB |  77423 MiB | 268972 GiB | 268896 GiB |
|       from small pool |    381 MiB |    382 MiB |   1532 GiB |   1532 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77744 MiB |  77804 MiB | 270505 GiB | 270429 GiB |
|       from large pool |  77363 MiB |  77423 MiB | 268972 GiB | 268896 GiB |
|       from small pool |    381 MiB |    382 MiB |   1532 GiB |   1532 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77717 MiB |  77776 MiB | 269933 GiB | 269857 GiB |
|       from large pool |  77337 MiB |  77397 MiB | 268402 GiB | 268327 GiB |
|       from small pool |    379 MiB |    380 MiB |   1530 GiB |   1530 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80502 MiB | 439858 MiB | 359356 MiB |
|       from large pool |  80082 MiB |  80082 MiB | 436012 MiB | 355930 MiB |
|       from small pool |    420 MiB |    422 MiB |   3846 MiB |   3426 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2697 MiB |   6955 MiB | 265773 GiB | 265770 GiB |
|       from large pool |   2658 MiB |   6949 MiB | 264036 GiB | 264033 GiB |
|       from small pool |     38 MiB |     40 MiB |   1736 GiB |   1736 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7043    |    7046    |   17853 K  |   17846 K  |
|       from large pool |     910    |     911    |    8517 K  |    8516 K  |
|       from small pool |    6133    |    6136    |    9336 K  |    9330 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7043    |    7046    |   17853 K  |   17846 K  |
|       from large pool |     910    |     911    |    8517 K  |    8516 K  |
|       from small pool |    6133    |    6136    |    9336 K  |    9330 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     403    |     403    |    3316    |    2913    |
|       from large pool |     193    |     193    |    1393    |    1200    |
|       from small pool |     210    |     211    |    1923    |    1713    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     532    |     533    |   10057 K  |   10057 K  |
|       from large pool |     158    |     158    |    5577 K  |    5577 K  |
|       from small pool |     374    |     375    |    4479 K  |    4479 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:43:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:43:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:43:48] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:43:48] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:43:54]    INFO >> epoch 006:    780 / 1539 loss=3.571, wps=4352, ups=6.74, wpb=646.1, bsz=646.1, num_updates=8450, lr=0.000327, gnorm=4.151, clip=0, train_wall=6, gb_free=71.4, wall=12401 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:44:02]    INFO >> epoch 006:    830 / 1539 loss=3.65, wps=4644.8, ups=6.85, wpb=678.1, bsz=678.1, num_updates=8500, lr=0.000327, gnorm=4.325, clip=0, train_wall=7, gb_free=63.5, wall=12408 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:44:09]    INFO >> epoch 006:    880 / 1539 loss=3.562, wps=4903.9, ups=6.97, wpb=703.3, bsz=703.3, num_updates=8550, lr=0.000327, gnorm=4.278, clip=0, train_wall=7, gb_free=70.8, wall=12416 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:44:17]    INFO >> epoch 006:    930 / 1539 loss=3.586, wps=5137, ups=6.17, wpb=832.2, bsz=832.2, num_updates=8600, lr=0.000327, gnorm=4.742, clip=0, train_wall=8, gb_free=74.6, wall=12424 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:44:26]    INFO >> epoch 006:    980 / 1539 loss=3.57, wps=4621.4, ups=6.65, wpb=695, bsz=695, num_updates=8650, lr=0.000327, gnorm=4.046, clip=0, train_wall=7, gb_free=70.8, wall=12431 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:44:35]    INFO >> epoch 006:   1030 / 1539 loss=3.597, wps=4488.4, ups=5.75, wpb=780.7, bsz=780.7, num_updates=8700, lr=0.000327, gnorm=5.246, clip=0, train_wall=8, gb_free=65.5, wall=12440 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:44:42]    INFO >> epoch 006:   1080 / 1539 loss=3.69, wps=4831.7, ups=6.68, wpb=723.5, bsz=723.5, num_updates=8750, lr=0.000327, gnorm=4.562, clip=0, train_wall=7, gb_free=72.4, wall=12447 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:44:50]    INFO >> epoch 006:   1130 / 1539 loss=3.525, wps=4634, ups=6.12, wpb=757.6, bsz=757.6, num_updates=8800, lr=0.000327, gnorm=4.401, clip=0, train_wall=8, gb_free=72.1, wall=12456 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:44:59]    INFO >> epoch 006:   1180 / 1539 loss=3.689, wps=4986.3, ups=6.73, wpb=740.5, bsz=740.5, num_updates=8850, lr=0.000327, gnorm=4.722, clip=0, train_wall=7, gb_free=69, wall=12463 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:45:07]    INFO >> epoch 006:   1230 / 1539 loss=3.594, wps=4261.6, ups=6.46, wpb=659.3, bsz=659.3, num_updates=8900, lr=0.000327, gnorm=4.59, clip=0, train_wall=7, gb_free=73.8, wall=12471 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:45:14]    INFO >> epoch 006:   1280 / 1539 loss=3.669, wps=4531.7, ups=6.97, wpb=650.6, bsz=650.6, num_updates=8950, lr=0.000327, gnorm=4.168, clip=0, train_wall=7, gb_free=75.3, wall=12478 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:45:21]    INFO >> epoch 006:   1330 / 1539 loss=3.479, wps=5006.2, ups=6.76, wpb=741, bsz=741, num_updates=9000, lr=0.000327, gnorm=4.614, clip=0, train_wall=7, gb_free=71.3, wall=12485 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:45:30]    INFO >> epoch 006:   1380 / 1539 loss=3.598, wps=4338.6, ups=7.13, wpb=608.9, bsz=608.9, num_updates=9050, lr=0.000327, gnorm=4.517, clip=0, train_wall=7, gb_free=68.9, wall=12492 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:45:37]    INFO >> epoch 006:   1430 / 1539 loss=3.547, wps=4963.3, ups=6.6, wpb=751.8, bsz=751.8, num_updates=9100, lr=0.000327, gnorm=4.863, clip=2, train_wall=7, gb_free=70.5, wall=12500 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:45:45]    INFO >> epoch 006:   1480 / 1539 loss=3.409, wps=4656, ups=6.49, wpb=717, bsz=717, num_updates=9150, lr=0.000327, gnorm=4.755, clip=0, train_wall=7, gb_free=69.4, wall=12508 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:45:52]    INFO >> epoch 006:   1530 / 1539 loss=3.511, wps=4869.7, ups=7.22, wpb=674.1, bsz=674.1, num_updates=9200, lr=0.000327, gnorm=4.191, clip=0, train_wall=7, gb_free=74.6, wall=12514 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:45:53]    INFO >> epoch 006 | loss 3.596 | wps 4482.5 | ups 6.29 | wpb 712.7 | bsz 712.7 | num_updates 9209 | lr 0.000327 | gnorm 4.501 | clip 0.1 | train_wall 216 | gb_free 72.3 | wall 12516 (progress_bar.py:267, print())[0m
[33m[2025-11-19 02:45:53] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:46:07]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.769 | wps 11815.8 | wpb 5412.5 | bsz 5412.5 | num_updates 9209 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 02:46:08]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 02:46:08]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 6 @ 9209 updates, score 3.769) (writing took 0.013186 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 02:46:08] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:46:14]    INFO >> epoch 007:     41 / 1539 loss=3.658, wps=1656.6, ups=2.35, wpb=704.3, bsz=704.3, num_updates=9250, lr=0.000295, gnorm=4.665, clip=0, train_wall=8, gb_free=71.6, wall=12536 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:46:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.90 GiB is free. Including non-PyTorch memory, this process has 77.22 GiB memory in use. Of the allocated memory 68.10 GiB is allocated by PyTorch, and 8.62 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:46:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 48        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69335 MiB |  70025 MiB | 300364 GiB | 300296 GiB |
|       from large pool |  69317 MiB |  70007 MiB | 298661 GiB | 298594 GiB |
|       from small pool |     17 MiB |     25 MiB |   1702 GiB |   1702 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69335 MiB |  70025 MiB | 300364 GiB | 300296 GiB |
|       from large pool |  69317 MiB |  70007 MiB | 298661 GiB | 298594 GiB |
|       from small pool |     17 MiB |     25 MiB |   1702 GiB |   1702 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69318 MiB |  70006 MiB | 299730 GiB | 299662 GiB |
|       from large pool |  69300 MiB |  69989 MiB | 298029 GiB | 297961 GiB |
|       from small pool |     17 MiB |     25 MiB |   1700 GiB |   1700 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78564 MiB |  80442 MiB | 439858 MiB | 361294 MiB |
|       from large pool |  78534 MiB |  80022 MiB | 436012 MiB | 357478 MiB |
|       from small pool |     30 MiB |    420 MiB |   3846 MiB |   3816 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8834 MiB |   8932 MiB | 291012 GiB | 291003 GiB |
|       from large pool |   8822 MiB |   8919 MiB | 289086 GiB | 289077 GiB |
|       from small pool |     12 MiB |     31 MiB |   1926 GiB |   1926 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   19776 K  |   19776 K  |
|       from large pool |     307    |     315    |    9398 K  |    9398 K  |
|       from small pool |     298    |     356    |   10377 K  |   10377 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   19776 K  |   19776 K  |
|       from large pool |     307    |     315    |    9398 K  |    9398 K  |
|       from small pool |     298    |     356    |   10377 K  |   10377 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     183    |     402    |    3316    |    3133    |
|       from large pool |     168    |     192    |    1393    |    1225    |
|       from small pool |      15    |     210    |    1923    |    1908    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     172    |     174    |   11179 K  |   11178 K  |
|       from large pool |     142    |     144    |    6179 K  |    6179 K  |
|       from small pool |      30    |      64    |    4999 K  |    4999 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:46:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:46:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:46:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:46:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:46:22]    INFO >> epoch 007:     92 / 1539 loss=3.536, wps=4519.3, ups=6.7, wpb=674.7, bsz=674.7, num_updates=9300, lr=0.000295, gnorm=4.523, clip=0, train_wall=6, gb_free=74.6, wall=12543 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:46:29]    INFO >> epoch 007:    142 / 1539 loss=3.627, wps=4457.1, ups=6.72, wpb=663.1, bsz=663.1, num_updates=9350, lr=0.000295, gnorm=4.699, clip=0, train_wall=7, gb_free=73.4, wall=12551 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:46:38]    INFO >> epoch 007:    192 / 1539 loss=3.592, wps=5718.1, ups=6.46, wpb=885.1, bsz=885.1, num_updates=9400, lr=0.000295, gnorm=4.306, clip=0, train_wall=7, gb_free=67.7, wall=12558 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:46:46]    INFO >> epoch 007:    242 / 1539 loss=3.582, wps=5064.6, ups=6.38, wpb=794.3, bsz=794.3, num_updates=9450, lr=0.000295, gnorm=4.469, clip=0, train_wall=7, gb_free=70.7, wall=12566 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:46:54]    INFO >> epoch 007:    292 / 1539 loss=3.61, wps=4734.3, ups=6.77, wpb=699.6, bsz=699.6, num_updates=9500, lr=0.000295, gnorm=4.397, clip=0, train_wall=7, gb_free=71.4, wall=12574 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:46:59] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 449.25 MiB is free. Including non-PyTorch memory, this process has 78.68 GiB memory in use. Of the allocated memory 72.94 GiB is allocated by PyTorch, and 5.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:46:59] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 49        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69870 MiB |  74868 MiB | 308571 GiB | 308502 GiB |
|       from large pool |  69853 MiB |  74850 MiB | 306823 GiB | 306755 GiB |
|       from small pool |     17 MiB |     18 MiB |   1747 GiB |   1747 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69870 MiB |  74868 MiB | 308571 GiB | 308502 GiB |
|       from large pool |  69853 MiB |  74850 MiB | 306823 GiB | 306755 GiB |
|       from small pool |     17 MiB |     18 MiB |   1747 GiB |   1747 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69846 MiB |  74842 MiB | 307917 GiB | 307849 GiB |
|       from large pool |  69829 MiB |  74825 MiB | 306172 GiB | 306104 GiB |
|       from small pool |     17 MiB |     17 MiB |   1745 GiB |   1744 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80056 MiB |  80256 MiB | 441944 MiB | 361888 MiB |
|       from large pool |  80030 MiB |  80030 MiB | 437902 MiB | 357872 MiB |
|       from small pool |     26 MiB |    226 MiB |   4042 MiB |   4016 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8295 MiB |   9852 MiB | 298878 GiB | 298870 GiB |
|       from large pool |   8286 MiB |   9842 MiB | 296901 GiB | 296893 GiB |
|       from small pool |      8 MiB |     23 MiB |   1977 GiB |   1977 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     643    |     661    |   20317 K  |   20316 K  |
|       from large pool |     344    |     362    |    9671 K  |    9671 K  |
|       from small pool |     299    |     342    |   10645 K  |   10645 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     643    |     661    |   20317 K  |   20316 K  |
|       from large pool |     344    |     362    |    9671 K  |    9671 K  |
|       from small pool |     299    |     342    |   10645 K  |   10645 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     179    |     279    |    3415    |    3236    |
|       from large pool |     166    |     166    |    1394    |    1228    |
|       from small pool |      13    |     113    |    2021    |    2008    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     158    |     158    |   11484 K  |   11484 K  |
|       from large pool |     131    |     131    |    6362 K  |    6362 K  |
|       from small pool |      27    |      49    |    5122 K  |    5122 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:46:59] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:46:59] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:46:59] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:46:59] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:47:02]    INFO >> epoch 007:    343 / 1539 loss=3.511, wps=3925.3, ups=5.71, wpb=687.1, bsz=687.1, num_updates=9550, lr=0.000295, gnorm=4.638, clip=0, train_wall=8, gb_free=69.5, wall=12582 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:47:11]    INFO >> epoch 007:    393 / 1539 loss=3.717, wps=4339.7, ups=7.26, wpb=598.1, bsz=598.1, num_updates=9600, lr=0.000295, gnorm=3.582, clip=0, train_wall=6, gb_free=62, wall=12589 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:47:18]    INFO >> epoch 007:    443 / 1539 loss=3.446, wps=4481, ups=6.61, wpb=677.8, bsz=677.8, num_updates=9650, lr=0.000295, gnorm=4.508, clip=0, train_wall=7, gb_free=74.5, wall=12597 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:47:23] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 369.25 MiB is free. Including non-PyTorch memory, this process has 78.76 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 4.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:47:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 51        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72821 MiB |  75424 MiB | 312994 GiB | 312923 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 311225 GiB | 311154 GiB |
|       from small pool |     12 MiB |     18 MiB |   1768 GiB |   1768 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72821 MiB |  75424 MiB | 312994 GiB | 312923 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 311225 GiB | 311154 GiB |
|       from small pool |     12 MiB |     18 MiB |   1768 GiB |   1768 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 312331 GiB | 312260 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 310565 GiB | 310494 GiB |
|       from small pool |     12 MiB |     18 MiB |   1765 GiB |   1765 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80136 MiB |  80136 MiB | 445820 MiB | 365684 MiB |
|       from large pool |  80110 MiB |  80110 MiB | 441732 MiB | 361622 MiB |
|       from small pool |     26 MiB |     72 MiB |   4088 MiB |   4062 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5412 MiB |  10270 MiB | 303036 GiB | 303031 GiB |
|       from large pool |   5399 MiB |  10257 MiB | 301035 GiB | 301030 GiB |
|       from small pool |     13 MiB |     27 MiB |   2000 GiB |   2000 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   20586 K  |   20586 K  |
|       from large pool |     312    |     320    |    9814 K  |    9813 K  |
|       from small pool |     291    |     356    |   10772 K  |   10772 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   20586 K  |   20586 K  |
|       from large pool |     312    |     320    |    9814 K  |    9813 K  |
|       from small pool |     291    |     356    |   10772 K  |   10772 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     152    |     202    |    3443    |    3291    |
|       from large pool |     139    |     166    |    1399    |    1260    |
|       from small pool |      13    |      36    |    2044    |    2031    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     146    |     148    |   11634 K  |   11634 K  |
|       from large pool |     116    |     118    |    6457 K  |    6457 K  |
|       from small pool |      30    |      52    |    5176 K  |    5176 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:47:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:47:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:47:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:47:23] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:47:27]    INFO >> epoch 007:    494 / 1539 loss=3.581, wps=4436.3, ups=5.97, wpb=742.8, bsz=742.8, num_updates=9700, lr=0.000295, gnorm=4.425, clip=0, train_wall=7, gb_free=72.2, wall=12605 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:47:34]    INFO >> epoch 007:    544 / 1539 loss=3.595, wps=4469.7, ups=7.08, wpb=631.2, bsz=631.2, num_updates=9750, lr=0.000295, gnorm=4.608, clip=0, train_wall=7, gb_free=74.3, wall=12612 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:47:42]    INFO >> epoch 007:    594 / 1539 loss=3.606, wps=4814.1, ups=7.03, wpb=684.3, bsz=684.3, num_updates=9800, lr=0.000295, gnorm=4.034, clip=0, train_wall=7, gb_free=75.1, wall=12619 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:47:50]    INFO >> epoch 007:    644 / 1539 loss=3.494, wps=4620, ups=6.62, wpb=697.8, bsz=697.8, num_updates=9850, lr=0.000295, gnorm=5.057, clip=2, train_wall=7, gb_free=70.6, wall=12627 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:47:57]    INFO >> epoch 007:    694 / 1539 loss=3.503, wps=5448, ups=6.52, wpb=835.9, bsz=835.9, num_updates=9900, lr=0.000295, gnorm=4.537, clip=0, train_wall=7, gb_free=73.9, wall=12635 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:48:05]    INFO >> epoch 007:    744 / 1539 loss=3.6, wps=4852.3, ups=6.81, wpb=712, bsz=712, num_updates=9950, lr=0.000295, gnorm=4.194, clip=0, train_wall=7, gb_free=72.8, wall=12642 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:48:12]    INFO >> epoch 007:    794 / 1539 loss=3.56, wps=4472, ups=6.83, wpb=654.6, bsz=654.6, num_updates=10000, lr=0.000295, gnorm=3.779, clip=0, train_wall=7, gb_free=72.1, wall=12649 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:48:21]    INFO >> epoch 007:    844 / 1539 loss=3.577, wps=4922.4, ups=6.81, wpb=722.7, bsz=722.7, num_updates=10050, lr=0.000295, gnorm=4.148, clip=0, train_wall=7, gb_free=70.6, wall=12657 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:48:28]    INFO >> epoch 007:    894 / 1539 loss=3.588, wps=4591.7, ups=6.67, wpb=688.1, bsz=688.1, num_updates=10100, lr=0.000295, gnorm=4.186, clip=0, train_wall=7, gb_free=73.8, wall=12664 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:48:35]    INFO >> epoch 007:    944 / 1539 loss=3.623, wps=4823.5, ups=6.9, wpb=699, bsz=699, num_updates=10150, lr=0.000295, gnorm=4.597, clip=0, train_wall=7, gb_free=71.9, wall=12671 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:48:43]    INFO >> epoch 007:    994 / 1539 loss=3.538, wps=4856.3, ups=6.6, wpb=736.1, bsz=736.1, num_updates=10200, lr=0.000295, gnorm=4.468, clip=0, train_wall=7, gb_free=70.3, wall=12679 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:48:52]    INFO >> epoch 007:   1044 / 1539 loss=3.47, wps=4827.5, ups=6.84, wpb=705.7, bsz=705.7, num_updates=10250, lr=0.000295, gnorm=4.501, clip=0, train_wall=7, gb_free=73, wall=12686 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:48:59]    INFO >> epoch 007:   1094 / 1539 loss=3.502, wps=4675.5, ups=6.74, wpb=693.4, bsz=693.4, num_updates=10300, lr=0.000295, gnorm=4.424, clip=0, train_wall=7, gb_free=71.6, wall=12694 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:49:07]    INFO >> epoch 007:   1144 / 1539 loss=3.461, wps=4895.4, ups=6.24, wpb=784.9, bsz=784.9, num_updates=10350, lr=0.000295, gnorm=4.232, clip=0, train_wall=8, gb_free=74, wall=12702 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:49:15]    INFO >> epoch 007:   1194 / 1539 loss=3.551, wps=5139, ups=6.33, wpb=811.5, bsz=811.5, num_updates=10400, lr=0.000295, gnorm=4.616, clip=0, train_wall=7, gb_free=72.8, wall=12710 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:49:24]    INFO >> epoch 007:   1244 / 1539 loss=3.493, wps=4731, ups=6.89, wpb=686.2, bsz=686.2, num_updates=10450, lr=0.000295, gnorm=4.038, clip=0, train_wall=7, gb_free=68.3, wall=12717 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:49:31]    INFO >> epoch 007:   1294 / 1539 loss=3.509, wps=4417.6, ups=6.9, wpb=640.4, bsz=640.4, num_updates=10500, lr=0.000295, gnorm=3.813, clip=0, train_wall=7, gb_free=70.7, wall=12724 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:49:38]    INFO >> epoch 007:   1344 / 1539 loss=3.542, wps=4851.9, ups=6.96, wpb=697.4, bsz=697.4, num_updates=10550, lr=0.000295, gnorm=4.27, clip=0, train_wall=7, gb_free=70.4, wall=12731 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:49:46]    INFO >> epoch 007:   1394 / 1539 loss=3.456, wps=4659.9, ups=6.67, wpb=699.1, bsz=699.1, num_updates=10600, lr=0.000295, gnorm=4.731, clip=0, train_wall=7, gb_free=67.1, wall=12739 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:49:51] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 15.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.16 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:49:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 52        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77927 MiB |  77987 MiB | 340862 GiB | 340786 GiB |
|       from large pool |  77544 MiB |  77604 MiB | 338940 GiB | 338865 GiB |
|       from small pool |    383 MiB |    384 MiB |   1921 GiB |   1921 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77927 MiB |  77987 MiB | 340862 GiB | 340786 GiB |
|       from large pool |  77544 MiB |  77604 MiB | 338940 GiB | 338865 GiB |
|       from small pool |    383 MiB |    384 MiB |   1921 GiB |   1921 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77897 MiB |  77956 MiB | 340137 GiB | 340061 GiB |
|       from large pool |  77516 MiB |  77575 MiB | 338218 GiB | 338142 GiB |
|       from small pool |    381 MiB |    382 MiB |   1918 GiB |   1918 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80490 MiB |  80492 MiB | 448078 MiB | 367588 MiB |
|       from large pool |  80068 MiB |  80068 MiB | 443592 MiB | 363524 MiB |
|       from small pool |    422 MiB |    424 MiB |   4486 MiB |   4064 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2502 MiB |   7537 MiB | 331243 GiB | 331240 GiB |
|       from large pool |   2463 MiB |   7531 MiB | 329067 GiB | 329065 GiB |
|       from small pool |     38 MiB |     40 MiB |   2175 GiB |   2175 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7076    |    7079    |   22457 K  |   22450 K  |
|       from large pool |     913    |     914    |   10758 K  |   10758 K  |
|       from small pool |    6163    |    6166    |   11698 K  |   11692 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7076    |    7079    |   22457 K  |   22450 K  |
|       from large pool |     913    |     914    |   10758 K  |   10758 K  |
|       from small pool |    6163    |    6166    |   11698 K  |   11692 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     380    |     381    |    3673    |    3293    |
|       from large pool |     169    |     169    |    1430    |    1261    |
|       from small pool |     211    |     212    |    2243    |    2032    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     510    |     511    |   12669 K  |   12669 K  |
|       from large pool |     133    |     133    |    7074 K  |    7073 K  |
|       from small pool |     377    |     378    |    5595 K  |    5595 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:49:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:49:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:49:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:49:51] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:49:54]    INFO >> epoch 007:   1445 / 1539 loss=3.554, wps=4371.8, ups=6.11, wpb=716, bsz=716, num_updates=10650, lr=0.000295, gnorm=4.392, clip=0, train_wall=7, gb_free=68.1, wall=12747 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:50:02]    INFO >> epoch 007:   1495 / 1539 loss=3.558, wps=5175.4, ups=7.25, wpb=713.5, bsz=713.5, num_updates=10700, lr=0.000295, gnorm=4.345, clip=0, train_wall=6, gb_free=70.4, wall=12754 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:50:09]    INFO >> epoch 007 | loss 3.55 | wps 4466.9 | ups 6.27 | wpb 712.7 | bsz 712.7 | num_updates 10744 | lr 0.000295 | gnorm 4.359 | clip 0.1 | train_wall 216 | gb_free 70.2 | wall 12761 (progress_bar.py:267, print())[0m
[33m[2025-11-19 02:50:09] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:50:22]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.752 | wps 11679.1 | wpb 5412.5 | bsz 5412.5 | num_updates 10744 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 02:50:22]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 02:50:22]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 7 @ 10744 updates, score 3.752) (writing took 0.014034 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 02:50:22] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:50:23]    INFO >> epoch 008:      6 / 1539 loss=3.426, wps=1737.5, ups=2.38, wpb=730.3, bsz=730.3, num_updates=10750, lr=0.000262, gnorm=4.125, clip=0, train_wall=7, gb_free=68.7, wall=12775 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:50:32]    INFO >> epoch 008:     56 / 1539 loss=3.405, wps=4973.8, ups=6.76, wpb=735.8, bsz=735.8, num_updates=10800, lr=0.000262, gnorm=4.335, clip=0, train_wall=7, gb_free=73.9, wall=12782 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:50:39]    INFO >> epoch 008:    106 / 1539 loss=3.443, wps=4977.1, ups=6.49, wpb=766.8, bsz=766.8, num_updates=10850, lr=0.000262, gnorm=4.195, clip=0, train_wall=7, gb_free=73.2, wall=12790 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:50:47]    INFO >> epoch 008:    156 / 1539 loss=3.497, wps=4534.2, ups=6.95, wpb=652.3, bsz=652.3, num_updates=10900, lr=0.000262, gnorm=3.933, clip=0, train_wall=7, gb_free=75.6, wall=12797 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:50:55]    INFO >> epoch 008:    206 / 1539 loss=3.435, wps=5225.1, ups=6.26, wpb=834.7, bsz=834.7, num_updates=10950, lr=0.000262, gnorm=4.146, clip=0, train_wall=8, gb_free=73.5, wall=12805 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:51:02]    INFO >> epoch 008:    256 / 1539 loss=3.488, wps=4925.9, ups=6.9, wpb=714.1, bsz=714.1, num_updates=11000, lr=0.000262, gnorm=4.296, clip=0, train_wall=7, gb_free=71.5, wall=12812 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:51:11]    INFO >> epoch 008:    306 / 1539 loss=3.623, wps=4773.3, ups=6.62, wpb=720.8, bsz=720.8, num_updates=11050, lr=0.000262, gnorm=4.633, clip=0, train_wall=7, gb_free=71.8, wall=12820 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:51:19]    INFO >> epoch 008:    356 / 1539 loss=3.536, wps=4260.8, ups=6.4, wpb=665.6, bsz=665.6, num_updates=11100, lr=0.000262, gnorm=4.051, clip=0, train_wall=7, gb_free=74.7, wall=12828 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:51:26]    INFO >> epoch 008:    406 / 1539 loss=3.565, wps=4735.7, ups=6.7, wpb=707, bsz=707, num_updates=11150, lr=0.000262, gnorm=3.615, clip=0, train_wall=7, gb_free=72.1, wall=12835 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:51:34]    INFO >> epoch 008:    456 / 1539 loss=3.558, wps=5709.4, ups=6.52, wpb=875.6, bsz=875.6, num_updates=11200, lr=0.000262, gnorm=4.396, clip=0, train_wall=7, gb_free=72.6, wall=12843 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:51:42]    INFO >> epoch 008:    506 / 1539 loss=3.473, wps=5085, ups=6.78, wpb=749.6, bsz=749.6, num_updates=11250, lr=0.000262, gnorm=4.369, clip=0, train_wall=7, gb_free=74.3, wall=12850 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:51:50]    INFO >> epoch 008:    556 / 1539 loss=3.48, wps=4359.3, ups=6.69, wpb=651.2, bsz=651.2, num_updates=11300, lr=0.000262, gnorm=3.662, clip=0, train_wall=7, gb_free=71.8, wall=12858 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:51:57]    INFO >> epoch 008:    606 / 1539 loss=3.544, wps=4629.3, ups=6.78, wpb=682.8, bsz=682.8, num_updates=11350, lr=0.000262, gnorm=4.127, clip=0, train_wall=7, gb_free=75.2, wall=12865 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:52:04]    INFO >> epoch 008:    656 / 1539 loss=3.582, wps=4610.6, ups=7.25, wpb=636.2, bsz=636.2, num_updates=11400, lr=0.000262, gnorm=3.6, clip=0, train_wall=6, gb_free=72.5, wall=12872 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:52:13]    INFO >> epoch 008:    706 / 1539 loss=3.489, wps=4801.7, ups=6.94, wpb=691.9, bsz=691.9, num_updates=11450, lr=0.000262, gnorm=3.813, clip=0, train_wall=7, gb_free=70, wall=12879 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:52:20]    INFO >> epoch 008:    756 / 1539 loss=3.528, wps=4906.4, ups=6.75, wpb=727, bsz=727, num_updates=11500, lr=0.000262, gnorm=3.82, clip=0, train_wall=7, gb_free=70.9, wall=12887 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:52:28]    INFO >> epoch 008:    806 / 1539 loss=3.443, wps=4642.4, ups=6.69, wpb=694.2, bsz=694.2, num_updates=11550, lr=0.000262, gnorm=4.419, clip=0, train_wall=7, gb_free=68.9, wall=12894 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:52:34]    INFO >> epoch 008:    856 / 1539 loss=3.543, wps=4390.2, ups=7.37, wpb=595.7, bsz=595.7, num_updates=11600, lr=0.000262, gnorm=3.564, clip=0, train_wall=6, gb_free=71.5, wall=12901 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:52:42]    INFO >> epoch 008:    906 / 1539 loss=3.406, wps=5097.2, ups=6.27, wpb=813.3, bsz=813.3, num_updates=11650, lr=0.000262, gnorm=4.342, clip=0, train_wall=8, gb_free=74, wall=12909 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:52:51]    INFO >> epoch 008:    956 / 1539 loss=3.479, wps=4733.1, ups=7.02, wpb=674.1, bsz=674.1, num_updates=11700, lr=0.000262, gnorm=4.446, clip=0, train_wall=7, gb_free=72.9, wall=12916 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:52:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 473.25 MiB is free. Including non-PyTorch memory, this process has 78.65 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 8.08 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:52:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 53        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69004 MiB |  71761 MiB | 377144 GiB | 377077 GiB |
|       from large pool |  68987 MiB |  71744 MiB | 375009 GiB | 374941 GiB |
|       from small pool |     16 MiB |     20 MiB |   2135 GiB |   2135 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69004 MiB |  71761 MiB | 377144 GiB | 377077 GiB |
|       from large pool |  68987 MiB |  71744 MiB | 375009 GiB | 374941 GiB |
|       from small pool |     16 MiB |     20 MiB |   2135 GiB |   2135 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 376341 GiB | 376273 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 374208 GiB | 374141 GiB |
|       from small pool |     16 MiB |     20 MiB |   2132 GiB |   2132 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80032 MiB |  80430 MiB | 448078 MiB | 368046 MiB |
|       from large pool |  80008 MiB |  80008 MiB | 443592 MiB | 363584 MiB |
|       from small pool |     24 MiB |    422 MiB |   4486 MiB |   4462 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8299 MiB |  11896 MiB | 364184 GiB | 364176 GiB |
|       from large pool |   8292 MiB |  11888 MiB | 361767 GiB | 361759 GiB |
|       from small pool |      7 MiB |     29 MiB |   2416 GiB |   2416 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   24894 K  |   24894 K  |
|       from large pool |     343    |     351    |   11887 K  |   11887 K  |
|       from small pool |     298    |     356    |   13007 K  |   13007 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   24894 K  |   24894 K  |
|       from large pool |     343    |     351    |   11887 K  |   11887 K  |
|       from small pool |     298    |     356    |   13007 K  |   13007 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     180    |     379    |    3673    |    3493    |
|       from large pool |     168    |     168    |    1430    |    1262    |
|       from small pool |      12    |     211    |    2243    |    2231    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     159    |     163    |   14069 K  |   14069 K  |
|       from large pool |     133    |     137    |    7829 K  |    7829 K  |
|       from small pool |      26    |      62    |    6240 K  |    6240 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:52:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:52:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:52:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:52:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:52:58]    INFO >> epoch 008:   1007 / 1539 loss=3.511, wps=4083.6, ups=6.77, wpb=602.8, bsz=602.8, num_updates=11750, lr=0.000262, gnorm=4.219, clip=0, train_wall=6, gb_free=70.6, wall=12923 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:53:07]    INFO >> epoch 008:   1057 / 1539 loss=3.388, wps=4788.7, ups=5.78, wpb=828.4, bsz=828.4, num_updates=11800, lr=0.000262, gnorm=3.925, clip=0, train_wall=8, gb_free=71.7, wall=12932 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:53:14]    INFO >> epoch 008:   1107 / 1539 loss=3.462, wps=4720.6, ups=7.14, wpb=661.2, bsz=661.2, num_updates=11850, lr=0.000262, gnorm=4.2, clip=0, train_wall=7, gb_free=72.8, wall=12939 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:53:22]    INFO >> epoch 008:   1157 / 1539 loss=3.684, wps=3968.6, ups=7.12, wpb=557.5, bsz=557.5, num_updates=11900, lr=0.000262, gnorm=3.753, clip=0, train_wall=7, gb_free=73.2, wall=12946 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:53:25] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 373.25 MiB is free. Including non-PyTorch memory, this process has 78.75 GiB memory in use. Of the allocated memory 73.45 GiB is allocated by PyTorch, and 4.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:53:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 55        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72820 MiB |  75423 MiB | 382592 GiB | 382521 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 380430 GiB | 380359 GiB |
|       from small pool |     12 MiB |     19 MiB |   2162 GiB |   2162 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72820 MiB |  75423 MiB | 382592 GiB | 382521 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 380430 GiB | 380359 GiB |
|       from small pool |     12 MiB |     19 MiB |   2162 GiB |   2162 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 381777 GiB | 381706 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 379617 GiB | 379546 GiB |
|       from small pool |     12 MiB |     19 MiB |   2159 GiB |   2159 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80132 MiB |  80462 MiB | 452682 MiB | 372550 MiB |
|       from large pool |  80102 MiB |  80396 MiB | 448154 MiB | 368052 MiB |
|       from small pool |     30 MiB |     66 MiB |   4528 MiB |   4498 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7251 MiB |  11832 MiB | 369246 GiB | 369239 GiB |
|       from large pool |   7233 MiB |  11815 MiB | 366798 GiB | 366791 GiB |
|       from small pool |     17 MiB |     31 MiB |   2447 GiB |   2447 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   25239 K  |   25239 K  |
|       from large pool |     312    |     320    |   12067 K  |   12067 K  |
|       from small pool |     291    |     348    |   13172 K  |   13172 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   25239 K  |   25239 K  |
|       from large pool |     312    |     320    |   12067 K  |   12067 K  |
|       from small pool |     291    |     348    |   13172 K  |   13172 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     158    |     202    |    3699    |    3541    |
|       from large pool |     143    |     169    |    1435    |    1292    |
|       from small pool |      15    |      33    |    2264    |    2249    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     152    |     154    |   14262 K  |   14262 K  |
|       from large pool |     123    |     125    |    7949 K  |    7949 K  |
|       from small pool |      29    |      61    |    6312 K  |    6312 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:25] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:53:31]    INFO >> epoch 008:   1208 / 1539 loss=3.504, wps=4247.8, ups=5.94, wpb=715, bsz=715, num_updates=11950, lr=0.000262, gnorm=4.048, clip=0, train_wall=7, gb_free=76, wall=12954 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:53:36] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 75.97 GiB is allocated by PyTorch, and 2.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:53:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 56        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77732 MiB |  77791 MiB | 384872 GiB | 384796 GiB |
|       from large pool |  77350 MiB |  77410 MiB | 382695 GiB | 382619 GiB |
|       from small pool |    381 MiB |    382 MiB |   2176 GiB |   2176 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77732 MiB |  77791 MiB | 384872 GiB | 384796 GiB |
|       from large pool |  77350 MiB |  77410 MiB | 382695 GiB | 382619 GiB |
|       from small pool |    381 MiB |    382 MiB |   2176 GiB |   2176 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77717 MiB |  77776 MiB | 384052 GiB | 383976 GiB |
|       from large pool |  77337 MiB |  77397 MiB | 381878 GiB | 381803 GiB |
|       from small pool |    379 MiB |    380 MiB |   2173 GiB |   2173 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80464 MiB | 453074 MiB | 372612 MiB |
|       from large pool |  80042 MiB |  80042 MiB | 448154 MiB | 368112 MiB |
|       from small pool |    420 MiB |    422 MiB |   4920 MiB |   4500 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2729 MiB |   7831 MiB | 371566 GiB | 371564 GiB |
|       from large pool |   2691 MiB |   7824 MiB | 369102 GiB | 369099 GiB |
|       from small pool |     38 MiB |     40 MiB |   2464 GiB |   2464 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7043    |    7046    |   25394 K  |   25387 K  |
|       from large pool |     910    |     911    |   12135 K  |   12134 K  |
|       from small pool |    6133    |    6136    |   13259 K  |   13253 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7043    |    7046    |   25394 K  |   25387 K  |
|       from large pool |     910    |     911    |   12135 K  |   12134 K  |
|       from small pool |    6133    |    6136    |   13259 K  |   13253 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     352    |     353    |    3895    |    3543    |
|       from large pool |     142    |     142    |    1435    |    1293    |
|       from small pool |     210    |     211    |    2460    |    2250    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     513    |     514    |   14352 K  |   14351 K  |
|       from large pool |     139    |     139    |    7994 K  |    7994 K  |
|       from small pool |     374    |     375    |    6357 K  |    6357 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:36] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:53:39]    INFO >> epoch 008:   1259 / 1539 loss=3.573, wps=5050, ups=5.94, wpb=850.7, bsz=850.7, num_updates=12000, lr=0.000262, gnorm=4.205, clip=0, train_wall=8, gb_free=73.2, wall=12963 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:53:46]    INFO >> epoch 008:   1309 / 1539 loss=3.512, wps=4757.4, ups=6.75, wpb=704.3, bsz=704.3, num_updates=12050, lr=0.000262, gnorm=4.172, clip=0, train_wall=7, gb_free=72.7, wall=12970 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:53:55]    INFO >> epoch 008:   1359 / 1539 loss=3.461, wps=4486.3, ups=6.75, wpb=664.7, bsz=664.7, num_updates=12100, lr=0.000262, gnorm=3.761, clip=0, train_wall=7, gb_free=69.7, wall=12978 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:53:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 682.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 439.25 MiB is free. Including non-PyTorch memory, this process has 78.69 GiB memory in use. Of the allocated memory 73.43 GiB is allocated by PyTorch, and 4.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:53:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 33           |        cudaMalloc retries: 57        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72060 MiB |  75188 MiB | 388514 GiB | 388443 GiB |
|       from large pool |  72042 MiB |  75170 MiB | 386318 GiB | 386248 GiB |
|       from small pool |     17 MiB |     18 MiB |   2195 GiB |   2195 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72060 MiB |  75188 MiB | 388514 GiB | 388443 GiB |
|       from large pool |  72042 MiB |  75170 MiB | 386318 GiB | 386248 GiB |
|       from small pool |     17 MiB |     18 MiB |   2195 GiB |   2195 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  75171 MiB | 387686 GiB | 387616 GiB |
|       from large pool |  72027 MiB |  75153 MiB | 385494 GiB | 385424 GiB |
|       from small pool |     17 MiB |     18 MiB |   2192 GiB |   2192 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80066 MiB |  80462 MiB | 453074 MiB | 373008 MiB |
|       from large pool |  80042 MiB |  80042 MiB | 448154 MiB | 368112 MiB |
|       from small pool |     24 MiB |    420 MiB |   4920 MiB |   4896 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5277 MiB |   8526 MiB | 375203 GiB | 375197 GiB |
|       from large pool |   5271 MiB |   8517 MiB | 372717 GiB | 372711 GiB |
|       from small pool |      6 MiB |     27 MiB |   2486 GiB |   2486 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     611    |   25621 K  |   25620 K  |
|       from large pool |     308    |     316    |   12253 K  |   12252 K  |
|       from small pool |     295    |     348    |   13368 K  |   13367 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     611    |   25621 K  |   25620 K  |
|       from large pool |     308    |     316    |   12253 K  |   12252 K  |
|       from small pool |     295    |     348    |   13368 K  |   13367 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     154    |     352    |    3895    |    3741    |
|       from large pool |     142    |     142    |    1435    |    1293    |
|       from small pool |      12    |     210    |    2460    |    2448    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     142    |     142    |   14479 K  |   14479 K  |
|       from large pool |     119    |     119    |    8071 K  |    8071 K  |
|       from small pool |      23    |      57    |    6407 K  |    6407 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:53:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:54:03]    INFO >> epoch 008:   1410 / 1539 loss=3.493, wps=5030.6, ups=6.1, wpb=824.4, bsz=824.4, num_updates=12150, lr=0.000262, gnorm=4.06, clip=0, train_wall=7, gb_free=73, wall=12986 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:54:12]    INFO >> epoch 008:   1460 / 1539 loss=3.487, wps=4227, ups=5.91, wpb=715.2, bsz=715.2, num_updates=12200, lr=0.000262, gnorm=3.914, clip=0, train_wall=8, gb_free=75, wall=12994 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:54:19]    INFO >> epoch 008:   1510 / 1539 loss=3.557, wps=4537.4, ups=6.81, wpb=666.4, bsz=666.4, num_updates=12250, lr=0.000262, gnorm=3.622, clip=0, train_wall=7, gb_free=67.3, wall=13002 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:54:23]    INFO >> epoch 008 | loss 3.501 | wps 4461.9 | ups 6.26 | wpb 712.7 | bsz 712.7 | num_updates 12279 | lr 0.000262 | gnorm 4.049 | clip 0 | train_wall 216 | gb_free 74.8 | wall 13006 (progress_bar.py:267, print())[0m
[33m[2025-11-19 02:54:23] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:54:38]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.784 | wps 11837 | wpb 5412.5 | bsz 5412.5 | num_updates 12279 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 02:54:38]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 02:54:38]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 8 @ 12279 updates, score 3.784) (writing took 0.013584 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 02:54:38] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:54:41]    INFO >> epoch 009:     21 / 1539 loss=3.553, wps=1659.3, ups=2.48, wpb=668.9, bsz=668.9, num_updates=12300, lr=0.000227, gnorm=3.736, clip=0, train_wall=7, gb_free=73.1, wall=13022 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:54:48]    INFO >> epoch 009:     71 / 1539 loss=3.484, wps=4583.3, ups=6.91, wpb=663.6, bsz=663.6, num_updates=12350, lr=0.000227, gnorm=4.164, clip=0, train_wall=7, gb_free=73.5, wall=13029 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:54:55]    INFO >> epoch 009:    121 / 1539 loss=3.53, wps=4825.3, ups=7.01, wpb=688.4, bsz=688.4, num_updates=12400, lr=0.000227, gnorm=4.054, clip=0, train_wall=7, gb_free=73.7, wall=13036 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:55:04]    INFO >> epoch 009:    171 / 1539 loss=3.406, wps=5418.2, ups=6.43, wpb=842.4, bsz=842.4, num_updates=12450, lr=0.000227, gnorm=4.268, clip=0, train_wall=7, gb_free=71.8, wall=13044 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:55:11]    INFO >> epoch 009:    221 / 1539 loss=3.548, wps=4814.3, ups=6.96, wpb=691.5, bsz=691.5, num_updates=12500, lr=0.000227, gnorm=3.942, clip=0, train_wall=7, gb_free=71.5, wall=13051 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:55:18]    INFO >> epoch 009:    271 / 1539 loss=3.488, wps=5119.5, ups=7.28, wpb=703.6, bsz=703.6, num_updates=12550, lr=0.000227, gnorm=3.826, clip=0, train_wall=6, gb_free=70, wall=13058 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:55:26]    INFO >> epoch 009:    321 / 1539 loss=3.498, wps=5123.4, ups=6.53, wpb=784.5, bsz=784.5, num_updates=12600, lr=0.000227, gnorm=4.27, clip=0, train_wall=7, gb_free=66.3, wall=13066 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:55:36]    INFO >> epoch 009:    371 / 1539 loss=3.562, wps=4231.2, ups=6.04, wpb=701, bsz=701, num_updates=12650, lr=0.000227, gnorm=3.899, clip=0, train_wall=8, gb_free=65.8, wall=13074 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:55:36] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 515.25 MiB is free. Including non-PyTorch memory, this process has 78.61 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 4.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:55:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 34           |        cudaMalloc retries: 58        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75054 MiB |  75997 MiB | 409339 GiB | 409266 GiB |
|       from large pool |  75041 MiB |  75984 MiB | 407021 GiB | 406947 GiB |
|       from small pool |     12 MiB |     16 MiB |   2318 GiB |   2318 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75054 MiB |  75997 MiB | 409339 GiB | 409266 GiB |
|       from large pool |  75041 MiB |  75984 MiB | 407021 GiB | 406947 GiB |
|       from small pool |     12 MiB |     16 MiB |   2318 GiB |   2318 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 408471 GiB | 408398 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 406156 GiB | 406083 GiB |
|       from small pool |     12 MiB |     16 MiB |   2315 GiB |   2315 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79990 MiB |  80090 MiB | 455826 MiB | 375836 MiB |
|       from large pool |  79966 MiB |  79966 MiB | 450806 MiB | 370840 MiB |
|       from small pool |     24 MiB |    124 MiB |   5020 MiB |   4996 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4935 MiB |  11872 MiB | 393887 GiB | 393882 GiB |
|       from large pool |   4924 MiB |  11860 MiB | 391265 GiB | 391260 GiB |
|       from small pool |     11 MiB |     21 MiB |   2621 GiB |   2621 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   26961 K  |   26960 K  |
|       from large pool |     314    |     322    |   12836 K  |   12835 K  |
|       from small pool |     291    |     341    |   14125 K  |   14124 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   26961 K  |   26960 K  |
|       from large pool |     314    |     322    |   12836 K  |   12835 K  |
|       from small pool |     291    |     341    |   14125 K  |   14124 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     159    |     209    |    3951    |    3792    |
|       from large pool |     147    |     147    |    1441    |    1294    |
|       from small pool |      12    |      62    |    2510    |    2498    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     154    |     154    |   15246 K  |   15246 K  |
|       from large pool |     130    |     130    |    8457 K  |    8457 K  |
|       from small pool |      24    |      45    |    6789 K  |    6789 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:55:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:55:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:55:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:55:36] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:55:44]    INFO >> epoch 009:    422 / 1539 loss=3.559, wps=4366.2, ups=6.09, wpb=716.4, bsz=716.4, num_updates=12700, lr=0.000227, gnorm=3.518, clip=0, train_wall=7, gb_free=73.1, wall=13082 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:55:51]    INFO >> epoch 009:    472 / 1539 loss=3.424, wps=4759.3, ups=6.88, wpb=691.3, bsz=691.3, num_updates=12750, lr=0.000227, gnorm=3.615, clip=0, train_wall=7, gb_free=74, wall=13089 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:55:59]    INFO >> epoch 009:    522 / 1539 loss=3.344, wps=5037, ups=6.62, wpb=760.8, bsz=760.8, num_updates=12800, lr=0.000227, gnorm=4.421, clip=0, train_wall=7, gb_free=72.7, wall=13097 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:56:08]    INFO >> epoch 009:    572 / 1539 loss=3.614, wps=4810.4, ups=6.41, wpb=750.9, bsz=750.9, num_updates=12850, lr=0.000227, gnorm=4.26, clip=0, train_wall=7, gb_free=74.2, wall=13105 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:56:09] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 513.25 MiB is free. Including non-PyTorch memory, this process has 78.62 GiB memory in use. Of the allocated memory 72.94 GiB is allocated by PyTorch, and 5.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:56:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 35           |        cudaMalloc retries: 59        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69870 MiB |  74868 MiB | 415417 GiB | 415349 GiB |
|       from large pool |  69853 MiB |  74851 MiB | 413068 GiB | 412999 GiB |
|       from small pool |     17 MiB |     17 MiB |   2349 GiB |   2349 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69870 MiB |  74868 MiB | 415417 GiB | 415349 GiB |
|       from large pool |  69853 MiB |  74851 MiB | 413068 GiB | 412999 GiB |
|       from small pool |     17 MiB |     17 MiB |   2349 GiB |   2349 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69846 MiB |  74842 MiB | 414536 GiB | 414468 GiB |
|       from large pool |  69829 MiB |  74825 MiB | 412190 GiB | 412122 GiB |
|       from small pool |     17 MiB |     17 MiB |   2346 GiB |   2346 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79992 MiB |  80030 MiB | 455866 MiB | 375874 MiB |
|       from large pool |  79966 MiB |  79966 MiB | 450806 MiB | 370840 MiB |
|       from small pool |     26 MiB |     64 MiB |   5060 MiB |   5034 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  10121 MiB |  11499 MiB | 399984 GiB | 399974 GiB |
|       from large pool |  10112 MiB |  11490 MiB | 397326 GiB | 397316 GiB |
|       from small pool |      8 MiB |     25 MiB |   2657 GiB |   2657 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     643    |     661    |   27345 K  |   27344 K  |
|       from large pool |     344    |     362    |   13035 K  |   13034 K  |
|       from small pool |     299    |     356    |   14310 K  |   14310 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     643    |     661    |   27345 K  |   27344 K  |
|       from large pool |     344    |     362    |   13035 K  |   13034 K  |
|       from small pool |     299    |     356    |   14310 K  |   14310 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     160    |     179    |    3971    |    3811    |
|       from large pool |     147    |     147    |    1441    |    1294    |
|       from small pool |      13    |      32    |    2530    |    2517    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     140    |     140    |   15459 K  |   15458 K  |
|       from large pool |     114    |     114    |    8587 K  |    8587 K  |
|       from small pool |      26    |      54    |    6871 K  |    6871 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:56:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:56:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:56:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:56:09] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:56:16]    INFO >> epoch 009:    623 / 1539 loss=3.54, wps=4394.6, ups=6.19, wpb=709.4, bsz=709.4, num_updates=12900, lr=0.000227, gnorm=3.62, clip=0, train_wall=7, gb_free=65.9, wall=13113 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:56:23]    INFO >> epoch 009:    673 / 1539 loss=3.494, wps=4649.1, ups=7.06, wpb=658.7, bsz=658.7, num_updates=12950, lr=0.000227, gnorm=4.266, clip=0, train_wall=7, gb_free=71.5, wall=13120 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:56:30]    INFO >> epoch 009:    723 / 1539 loss=3.545, wps=4638.3, ups=6.91, wpb=671.1, bsz=671.1, num_updates=13000, lr=0.000227, gnorm=3.824, clip=0, train_wall=7, gb_free=71.2, wall=13127 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:56:38]    INFO >> epoch 009:    773 / 1539 loss=3.438, wps=4575.7, ups=6.59, wpb=693.9, bsz=693.9, num_updates=13050, lr=0.000227, gnorm=4.444, clip=0, train_wall=7, gb_free=57.9, wall=13135 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:56:46] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 513.25 MiB is free. Including non-PyTorch memory, this process has 78.62 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 7.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:56:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 36           |        cudaMalloc retries: 60        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72060 MiB |  72460 MiB | 422558 GiB | 422488 GiB |
|       from large pool |  72043 MiB |  72442 MiB | 420172 GiB | 420101 GiB |
|       from small pool |     17 MiB |     18 MiB |   2386 GiB |   2386 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72060 MiB |  72460 MiB | 422558 GiB | 422488 GiB |
|       from large pool |  72043 MiB |  72442 MiB | 420172 GiB | 420101 GiB |
|       from small pool |     17 MiB |     18 MiB |   2386 GiB |   2386 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 421661 GiB | 421591 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 419278 GiB | 419208 GiB |
|       from small pool |     17 MiB |     18 MiB |   2382 GiB |   2382 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79992 MiB |  80040 MiB | 455914 MiB | 375922 MiB |
|       from large pool |  79966 MiB |  79966 MiB | 450806 MiB | 370840 MiB |
|       from small pool |     26 MiB |     74 MiB |   5108 MiB |   5082 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7931 MiB |   9277 MiB | 407074 GiB | 407066 GiB |
|       from large pool |   7922 MiB |   9266 MiB | 404375 GiB | 404367 GiB |
|       from small pool |      8 MiB |     21 MiB |   2699 GiB |   2699 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   27805 K  |   27804 K  |
|       from large pool |     308    |     315    |   13274 K  |   13274 K  |
|       from small pool |     298    |     348    |   14530 K  |   14530 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   27805 K  |   27804 K  |
|       from large pool |     308    |     315    |   13274 K  |   13274 K  |
|       from small pool |     298    |     348    |   14530 K  |   14530 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     160    |     184    |    3995    |    3835    |
|       from large pool |     147    |     147    |    1441    |    1294    |
|       from small pool |      13    |      37    |    2554    |    2541    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     155    |     155    |   15711 K  |   15711 K  |
|       from large pool |     129    |     129    |    8743 K  |    8742 K  |
|       from small pool |      26    |      52    |    6968 K  |    6968 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:56:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:56:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:56:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:56:46] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:56:47]    INFO >> epoch 009:    824 / 1539 loss=3.455, wps=4570.9, ups=6.35, wpb=719.6, bsz=719.6, num_updates=13100, lr=0.000227, gnorm=4.029, clip=0, train_wall=7, gb_free=70.5, wall=13143 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:56:54]    INFO >> epoch 009:    874 / 1539 loss=3.519, wps=4730.7, ups=7.34, wpb=644.4, bsz=644.4, num_updates=13150, lr=0.000227, gnorm=3.484, clip=0, train_wall=6, gb_free=74.8, wall=13149 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:57:01]    INFO >> epoch 009:    924 / 1539 loss=3.556, wps=4373.5, ups=7.07, wpb=618.7, bsz=618.7, num_updates=13200, lr=0.000227, gnorm=3.865, clip=0, train_wall=7, gb_free=73.5, wall=13157 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:57:10]    INFO >> epoch 009:    974 / 1539 loss=3.402, wps=4807.4, ups=5.59, wpb=859.7, bsz=859.7, num_updates=13250, lr=0.000227, gnorm=4.51, clip=0, train_wall=8, gb_free=70.8, wall=13165 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:57:18]    INFO >> epoch 009:   1024 / 1539 loss=3.444, wps=4832.3, ups=6.72, wpb=718.9, bsz=718.9, num_updates=13300, lr=0.000227, gnorm=3.825, clip=0, train_wall=7, gb_free=69.9, wall=13173 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:57:26]    INFO >> epoch 009:   1074 / 1539 loss=3.44, wps=4818.5, ups=6.64, wpb=725.8, bsz=725.8, num_updates=13350, lr=0.000227, gnorm=3.875, clip=0, train_wall=7, gb_free=65.3, wall=13180 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:57:34]    INFO >> epoch 009:   1124 / 1539 loss=3.454, wps=4995.2, ups=6.57, wpb=760.3, bsz=760.3, num_updates=13400, lr=0.000227, gnorm=4.017, clip=0, train_wall=7, gb_free=73.5, wall=13188 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:57:41]    INFO >> epoch 009:   1174 / 1539 loss=3.607, wps=4752.3, ups=7.1, wpb=669.6, bsz=669.6, num_updates=13450, lr=0.000227, gnorm=3.494, clip=0, train_wall=7, gb_free=72.7, wall=13195 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:57:50]    INFO >> epoch 009:   1224 / 1539 loss=3.08, wps=5295.4, ups=6.14, wpb=862, bsz=862, num_updates=13500, lr=0.000227, gnorm=4.322, clip=0, train_wall=8, gb_free=69, wall=13203 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:57:58]    INFO >> epoch 009:   1274 / 1539 loss=3.495, wps=5073.7, ups=6.54, wpb=776.3, bsz=776.3, num_updates=13550, lr=0.000227, gnorm=3.741, clip=0, train_wall=7, gb_free=72.7, wall=13211 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:58:06]    INFO >> epoch 009:   1324 / 1539 loss=3.56, wps=4555.3, ups=6.39, wpb=712.6, bsz=712.6, num_updates=13600, lr=0.000227, gnorm=3.515, clip=0, train_wall=7, gb_free=75.1, wall=13219 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:58:13]    INFO >> epoch 009:   1374 / 1539 loss=3.09, wps=4969.5, ups=6.63, wpb=749.6, bsz=749.6, num_updates=13650, lr=0.000227, gnorm=4.092, clip=2, train_wall=7, gb_free=69.9, wall=13226 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:58:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 59.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 75.97 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:58:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 37           |        cudaMalloc retries: 61        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77729 MiB |  77789 MiB | 440305 GiB | 440229 GiB |
|       from large pool |  77348 MiB |  77408 MiB | 437816 GiB | 437740 GiB |
|       from small pool |    381 MiB |    382 MiB |   2488 GiB |   2488 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77729 MiB |  77789 MiB | 440305 GiB | 440229 GiB |
|       from large pool |  77348 MiB |  77408 MiB | 437816 GiB | 437740 GiB |
|       from small pool |    381 MiB |    382 MiB |   2488 GiB |   2488 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77717 MiB |  77776 MiB | 439369 GiB | 439293 GiB |
|       from large pool |  77337 MiB |  77397 MiB | 436883 GiB | 436808 GiB |
|       from small pool |    379 MiB |    380 MiB |   2485 GiB |   2484 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80446 MiB |  80448 MiB | 456370 MiB | 375924 MiB |
|       from large pool |  80026 MiB |  80026 MiB | 450866 MiB | 370840 MiB |
|       from small pool |    420 MiB |    422 MiB |   5504 MiB |   5084 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2656 MiB |   7498 MiB | 424902 GiB | 424900 GiB |
|       from large pool |   2617 MiB |   7492 MiB | 422084 GiB | 422082 GiB |
|       from small pool |     38 MiB |     40 MiB |   2817 GiB |   2817 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7043    |    7046    |   29023 K  |   29016 K  |
|       from large pool |     910    |     911    |   13868 K  |   13868 K  |
|       from small pool |    6133    |    6136    |   15154 K  |   15148 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7043    |    7046    |   29023 K  |   29016 K  |
|       from large pool |     910    |     911    |   13868 K  |   13868 K  |
|       from small pool |    6133    |    6136    |   15154 K  |   15148 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     358    |     359    |    4194    |    3836    |
|       from large pool |     148    |     148    |    1442    |    1294    |
|       from small pool |     210    |     211    |    2752    |    2542    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     523    |     524    |   16395 K  |   16395 K  |
|       from large pool |     147    |     147    |    9130 K  |    9130 K  |
|       from small pool |     376    |     377    |    7264 K  |    7264 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:58:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:58:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:58:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:58:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:58:22]    INFO >> epoch 009:   1425 / 1539 loss=3.429, wps=4469.8, ups=6.6, wpb=676.7, bsz=676.7, num_updates=13700, lr=0.000227, gnorm=3.911, clip=0, train_wall=7, gb_free=72, wall=13234 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:58:29]    INFO >> epoch 009:   1475 / 1539 loss=3.58, wps=4425.5, ups=6.98, wpb=634.1, bsz=634.1, num_updates=13750, lr=0.000227, gnorm=3.653, clip=0, train_wall=7, gb_free=72.9, wall=13241 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:58:36]    INFO >> epoch 009:   1525 / 1539 loss=3.455, wps=4389, ups=7.04, wpb=623.3, bsz=623.3, num_updates=13800, lr=0.000227, gnorm=3.813, clip=0, train_wall=7, gb_free=67.9, wall=13248 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:58:38]    INFO >> epoch 009 | loss 3.466 | wps 4478.7 | ups 6.28 | wpb 712.7 | bsz 712.7 | num_updates 13814 | lr 0.000227 | gnorm 3.946 | clip 0.1 | train_wall 216 | gb_free 74.2 | wall 13250 (progress_bar.py:267, print())[0m
[33m[2025-11-19 02:58:38] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:58:51]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.621 | wps 11769.7 | wpb 5412.5 | bsz 5412.5 | num_updates 13814 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 02:58:52]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 02:58:52]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 9 @ 13814 updates, score 3.621) (writing took 0.013530 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 02:58:52] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 02:58:59]    INFO >> epoch 010:     36 / 1539 loss=3.485, wps=1643.6, ups=2.34, wpb=701.4, bsz=701.4, num_updates=13850, lr=0.000193, gnorm=3.567, clip=0, train_wall=8, gb_free=72.8, wall=13269 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:59:06]    INFO >> epoch 010:     86 / 1539 loss=3.375, wps=4990.6, ups=6.96, wpb=716.9, bsz=716.9, num_updates=13900, lr=0.000193, gnorm=4.239, clip=0, train_wall=7, gb_free=69.6, wall=13277 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:59:13]    INFO >> epoch 010:    136 / 1539 loss=3.445, wps=4405.1, ups=7.03, wpb=626.5, bsz=626.5, num_updates=13950, lr=0.000193, gnorm=4.081, clip=0, train_wall=7, gb_free=74.1, wall=13284 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:59:22]    INFO >> epoch 010:    186 / 1539 loss=3.388, wps=4536, ups=6.1, wpb=744.2, bsz=744.2, num_updates=14000, lr=0.000193, gnorm=4.556, clip=0, train_wall=8, gb_free=74.2, wall=13292 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:59:31]    INFO >> epoch 010:    236 / 1539 loss=3.333, wps=5258.9, ups=6.09, wpb=863.9, bsz=863.9, num_updates=14050, lr=0.000193, gnorm=4.12, clip=0, train_wall=8, gb_free=74.4, wall=13300 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:59:39]    INFO >> epoch 010:    286 / 1539 loss=3.538, wps=4453.5, ups=6.59, wpb=676, bsz=676, num_updates=14100, lr=0.000193, gnorm=3.868, clip=0, train_wall=7, gb_free=73.9, wall=13308 (progress_bar.py:258, log())[0m
[33m[2025-11-19 02:59:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 511.25 MiB is free. Including non-PyTorch memory, this process has 78.62 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 7.36 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 02:59:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 38           |        cudaMalloc retries: 62        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72060 MiB |  72460 MiB | 458001 GiB | 457930 GiB |
|       from large pool |  72043 MiB |  72442 MiB | 455406 GiB | 455335 GiB |
|       from small pool |     17 MiB |     18 MiB |   2594 GiB |   2594 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72060 MiB |  72460 MiB | 458001 GiB | 457930 GiB |
|       from large pool |  72043 MiB |  72442 MiB | 455406 GiB | 455335 GiB |
|       from small pool |     17 MiB |     18 MiB |   2594 GiB |   2594 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 457030 GiB | 456960 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 454439 GiB | 454368 GiB |
|       from small pool |     17 MiB |     18 MiB |   2591 GiB |   2591 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79994 MiB |  80386 MiB | 456370 MiB | 376376 MiB |
|       from large pool |  79966 MiB |  79966 MiB | 450866 MiB | 370900 MiB |
|       from small pool |     28 MiB |    420 MiB |   5504 MiB |   5476 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7933 MiB |   9279 MiB | 440248 GiB | 440240 GiB |
|       from large pool |   7922 MiB |   9266 MiB | 437313 GiB | 437305 GiB |
|       from small pool |     10 MiB |     25 MiB |   2934 GiB |   2934 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   30161 K  |   30161 K  |
|       from large pool |     308    |     315    |   14348 K  |   14348 K  |
|       from small pool |     298    |     342    |   15813 K  |   15812 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   30161 K  |   30161 K  |
|       from large pool |     308    |     315    |   14348 K  |   14348 K  |
|       from small pool |     298    |     342    |   15813 K  |   15812 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     161    |     357    |    4194    |    4033    |
|       from large pool |     147    |     147    |    1442    |    1295    |
|       from small pool |      14    |     210    |    2752    |    2738    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     155    |     155    |   17053 K  |   17053 K  |
|       from large pool |     129    |     129    |    9449 K  |    9448 K  |
|       from small pool |      26    |      50    |    7604 K  |    7604 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:59:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:59:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:59:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 02:59:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 02:59:46]    INFO >> epoch 010:    337 / 1539 loss=3.425, wps=4591.4, ups=6.52, wpb=703.9, bsz=703.9, num_updates=14150, lr=0.000193, gnorm=4.045, clip=0, train_wall=7, gb_free=67.2, wall=13315 (progress_bar.py:258, log())[0m
[32m[2025-11-19 02:59:54]    INFO >> epoch 010:    387 / 1539 loss=3.516, wps=4600.6, ups=6.36, wpb=723.5, bsz=723.5, num_updates=14200, lr=0.000193, gnorm=4.108, clip=0, train_wall=7, gb_free=73.3, wall=13323 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:00:03]    INFO >> epoch 010:    437 / 1539 loss=3.435, wps=4411.1, ups=6.99, wpb=631.4, bsz=631.4, num_updates=14250, lr=0.000193, gnorm=3.974, clip=0, train_wall=7, gb_free=72.6, wall=13330 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:00:10]    INFO >> epoch 010:    487 / 1539 loss=3.459, wps=4349.5, ups=6.51, wpb=668.5, bsz=668.5, num_updates=14300, lr=0.000193, gnorm=3.761, clip=0, train_wall=7, gb_free=72.5, wall=13338 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:00:17]    INFO >> epoch 010:    537 / 1539 loss=3.431, wps=4625.5, ups=7.16, wpb=645.7, bsz=645.7, num_updates=14350, lr=0.000193, gnorm=3.584, clip=0, train_wall=7, gb_free=75, wall=13345 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:00:25]    INFO >> epoch 010:    587 / 1539 loss=3.552, wps=4384.9, ups=6.59, wpb=665.2, bsz=665.2, num_updates=14400, lr=0.000193, gnorm=3.462, clip=0, train_wall=7, gb_free=71.4, wall=13353 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:00:32]    INFO >> epoch 010:    637 / 1539 loss=3.52, wps=5092.9, ups=7.08, wpb=719.7, bsz=719.7, num_updates=14450, lr=0.000193, gnorm=3.891, clip=0, train_wall=7, gb_free=69.2, wall=13360 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:00:41]    INFO >> epoch 010:    687 / 1539 loss=3.377, wps=5018.8, ups=6.91, wpb=726.6, bsz=726.6, num_updates=14500, lr=0.000193, gnorm=3.956, clip=0, train_wall=7, gb_free=73.3, wall=13367 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:00:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 59.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 75.97 GiB is allocated by PyTorch, and 2.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:00:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 39           |        cudaMalloc retries: 63        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77729 MiB |  77789 MiB | 469068 GiB | 468993 GiB |
|       from large pool |  77348 MiB |  77408 MiB | 466413 GiB | 466337 GiB |
|       from small pool |    381 MiB |    382 MiB |   2655 GiB |   2655 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77729 MiB |  77789 MiB | 469068 GiB | 468993 GiB |
|       from large pool |  77348 MiB |  77408 MiB | 466413 GiB | 466337 GiB |
|       from small pool |    381 MiB |    382 MiB |   2655 GiB |   2655 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77717 MiB |  77776 MiB | 468073 GiB | 467997 GiB |
|       from large pool |  77337 MiB |  77397 MiB | 465421 GiB | 465346 GiB |
|       from small pool |    379 MiB |    380 MiB |   2651 GiB |   2651 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80446 MiB |  80448 MiB | 456824 MiB | 376378 MiB |
|       from large pool |  80026 MiB |  80026 MiB | 450926 MiB | 370900 MiB |
|       from small pool |    420 MiB |    422 MiB |   5898 MiB |   5478 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2656 MiB |   7498 MiB | 451349 GiB | 451346 GiB |
|       from large pool |   2617 MiB |   7492 MiB | 448344 GiB | 448342 GiB |
|       from small pool |     38 MiB |     40 MiB |   3004 GiB |   3004 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7043    |    7046    |   30907 K  |   30900 K  |
|       from large pool |     910    |     911    |   14727 K  |   14726 K  |
|       from small pool |    6133    |    6136    |   16179 K  |   16173 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7043    |    7046    |   30907 K  |   30900 K  |
|       from large pool |     910    |     911    |   14727 K  |   14726 K  |
|       from small pool |    6133    |    6136    |   16179 K  |   16173 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     358    |     359    |    4392    |    4034    |
|       from large pool |     148    |     148    |    1443    |    1295    |
|       from small pool |     210    |     211    |    2949    |    2739    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     521    |     522    |   17466 K  |   17466 K  |
|       from large pool |     147    |     147    |    9697 K  |    9697 K  |
|       from small pool |     374    |     375    |    7768 K  |    7768 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:00:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:00:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:00:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:00:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:00:48]    INFO >> epoch 010:    738 / 1539 loss=3.628, wps=4269.5, ups=6.64, wpb=642.7, bsz=642.7, num_updates=14550, lr=0.000193, gnorm=3.473, clip=0, train_wall=7, gb_free=68.7, wall=13375 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:00:56]    INFO >> epoch 010:    788 / 1539 loss=3.438, wps=4697, ups=6.49, wpb=723.3, bsz=723.3, num_updates=14600, lr=0.000193, gnorm=4.032, clip=0, train_wall=7, gb_free=67.8, wall=13382 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:01:03]    INFO >> epoch 010:    838 / 1539 loss=3.561, wps=4484.6, ups=6.65, wpb=673.9, bsz=673.9, num_updates=14650, lr=0.000193, gnorm=3.584, clip=0, train_wall=7, gb_free=66.8, wall=13390 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:01:13]    INFO >> epoch 010:    888 / 1539 loss=3.246, wps=5187.4, ups=6.11, wpb=848.7, bsz=848.7, num_updates=14700, lr=0.000193, gnorm=4.415, clip=0, train_wall=8, gb_free=76.2, wall=13398 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:01:20]    INFO >> epoch 010:    938 / 1539 loss=3.463, wps=5003.9, ups=7.01, wpb=713.5, bsz=713.5, num_updates=14750, lr=0.000193, gnorm=4.17, clip=0, train_wall=7, gb_free=74.7, wall=13405 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:01:27]    INFO >> epoch 010:    988 / 1539 loss=3.607, wps=4822.3, ups=6.98, wpb=690.7, bsz=690.7, num_updates=14800, lr=0.000193, gnorm=3.375, clip=0, train_wall=7, gb_free=74.4, wall=13412 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:01:34]    INFO >> epoch 010:   1038 / 1539 loss=3.469, wps=4640.9, ups=6.91, wpb=671.7, bsz=671.7, num_updates=14850, lr=0.000193, gnorm=4.088, clip=0, train_wall=7, gb_free=70, wall=13419 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:01:43]    INFO >> epoch 010:   1088 / 1539 loss=3.399, wps=5401, ups=6.72, wpb=803.8, bsz=803.8, num_updates=14900, lr=0.000193, gnorm=4.35, clip=0, train_wall=7, gb_free=72.8, wall=13427 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:01:50]    INFO >> epoch 010:   1138 / 1539 loss=3.441, wps=4233.5, ups=6.91, wpb=612.7, bsz=612.7, num_updates=14950, lr=0.000193, gnorm=3.647, clip=0, train_wall=7, gb_free=71.8, wall=13434 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:01:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 513.25 MiB is free. Including non-PyTorch memory, this process has 78.62 GiB memory in use. Of the allocated memory 72.94 GiB is allocated by PyTorch, and 5.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:01:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 40           |        cudaMalloc retries: 64        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69870 MiB |  74868 MiB | 482884 GiB | 482815 GiB |
|       from large pool |  69853 MiB |  74851 MiB | 480153 GiB | 480085 GiB |
|       from small pool |     17 MiB |     17 MiB |   2730 GiB |   2730 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69870 MiB |  74868 MiB | 482884 GiB | 482815 GiB |
|       from large pool |  69853 MiB |  74851 MiB | 480153 GiB | 480085 GiB |
|       from small pool |     17 MiB |     17 MiB |   2730 GiB |   2730 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69846 MiB |  74842 MiB | 481857 GiB | 481789 GiB |
|       from large pool |  69829 MiB |  74825 MiB | 479130 GiB | 479062 GiB |
|       from small pool |     17 MiB |     17 MiB |   2726 GiB |   2726 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79992 MiB |  80386 MiB | 456824 MiB | 376832 MiB |
|       from large pool |  79966 MiB |  79966 MiB | 450926 MiB | 370960 MiB |
|       from small pool |     26 MiB |    420 MiB |   5898 MiB |   5872 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  10121 MiB |  11499 MiB | 465237 GiB | 465227 GiB |
|       from large pool |  10112 MiB |  11490 MiB | 462146 GiB | 462136 GiB |
|       from small pool |      8 MiB |     25 MiB |   3091 GiB |   3091 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     643    |     661    |   31823 K  |   31822 K  |
|       from large pool |     344    |     362    |   15191 K  |   15190 K  |
|       from small pool |     299    |     342    |   16631 K  |   16631 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     643    |     661    |   31823 K  |   31822 K  |
|       from large pool |     344    |     362    |   15191 K  |   15190 K  |
|       from small pool |     299    |     342    |   16631 K  |   16631 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     160    |     357    |    4392    |    4232    |
|       from large pool |     147    |     147    |    1443    |    1296    |
|       from small pool |      13    |     210    |    2949    |    2936    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     140    |     140    |   17975 K  |   17975 K  |
|       from large pool |     114    |     114    |    9999 K  |    9999 K  |
|       from small pool |      26    |      54    |    7975 K  |    7975 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:01:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:01:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:01:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:01:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:01:59]    INFO >> epoch 010:   1189 / 1539 loss=3.5, wps=4613.3, ups=5.85, wpb=787.9, bsz=787.9, num_updates=15000, lr=0.000193, gnorm=3.906, clip=0, train_wall=8, gb_free=70.1, wall=13443 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:02:07]    INFO >> epoch 010:   1239 / 1539 loss=3.372, wps=4972.9, ups=6.32, wpb=786.3, bsz=786.3, num_updates=15050, lr=0.000193, gnorm=4.431, clip=0, train_wall=7, gb_free=73.2, wall=13451 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:02:16]    INFO >> epoch 010:   1289 / 1539 loss=3.5, wps=4898.2, ups=6.39, wpb=766.5, bsz=766.5, num_updates=15100, lr=0.000193, gnorm=3.822, clip=0, train_wall=7, gb_free=72.7, wall=13458 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:02:23]    INFO >> epoch 010:   1339 / 1539 loss=3.533, wps=4595, ups=6.8, wpb=675.5, bsz=675.5, num_updates=15150, lr=0.000193, gnorm=3.904, clip=0, train_wall=7, gb_free=74.4, wall=13466 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:02:31]    INFO >> epoch 010:   1389 / 1539 loss=3.397, wps=4890.8, ups=6.55, wpb=746.1, bsz=746.1, num_updates=15200, lr=0.000193, gnorm=4.097, clip=0, train_wall=7, gb_free=73.6, wall=13473 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:02:38]    INFO >> epoch 010:   1439 / 1539 loss=3.459, wps=4671, ups=6.87, wpb=679.6, bsz=679.6, num_updates=15250, lr=0.000193, gnorm=3.916, clip=0, train_wall=7, gb_free=70.1, wall=13481 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:02:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 575.25 MiB is free. Including non-PyTorch memory, this process has 78.55 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 4.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:02:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 41           |        cudaMalloc retries: 65        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75058 MiB |  76001 MiB | 491622 GiB | 491549 GiB |
|       from large pool |  75045 MiB |  75988 MiB | 488845 GiB | 488771 GiB |
|       from small pool |     12 MiB |     24 MiB |   2777 GiB |   2777 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75058 MiB |  76001 MiB | 491622 GiB | 491549 GiB |
|       from large pool |  75045 MiB |  75988 MiB | 488845 GiB | 488771 GiB |
|       from small pool |     12 MiB |     24 MiB |   2777 GiB |   2777 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 490577 GiB | 490503 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 487803 GiB | 487729 GiB |
|       from small pool |     12 MiB |     24 MiB |   2773 GiB |   2773 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79930 MiB |  80176 MiB | 457008 MiB | 377078 MiB |
|       from large pool |  79906 MiB |  79966 MiB | 450926 MiB | 371020 MiB |
|       from small pool |     24 MiB |    210 MiB |   6082 MiB |   6058 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4811 MiB |  11162 MiB | 474057 GiB | 474052 GiB |
|       from large pool |   4800 MiB |  11151 MiB | 470911 GiB | 470907 GiB |
|       from small pool |     11 MiB |     31 MiB |   3145 GiB |   3145 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   32399 K  |   32398 K  |
|       from large pool |     314    |     322    |   15482 K  |   15482 K  |
|       from small pool |     291    |     356    |   16916 K  |   16916 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   32399 K  |   32398 K  |
|       from large pool |     314    |     322    |   15482 K  |   15482 K  |
|       from small pool |     291    |     356    |   16916 K  |   16916 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     158    |     252    |    4484    |    4326    |
|       from large pool |     146    |     147    |    1443    |    1297    |
|       from small pool |      12    |     105    |    3041    |    3029    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     148    |     153    |   18296 K  |   18296 K  |
|       from large pool |     119    |     124    |   10189 K  |   10189 K  |
|       from small pool |      29    |      60    |    8107 K  |    8107 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:02:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:02:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:02:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:02:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:02:46]    INFO >> epoch 010:   1490 / 1539 loss=3.599, wps=4429.4, ups=6.38, wpb=694.7, bsz=694.7, num_updates=15300, lr=0.000193, gnorm=3.99, clip=0, train_wall=7, gb_free=64.7, wall=13488 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:02:55]    INFO >> epoch 010 | loss 3.458 | wps 4453.9 | ups 6.25 | wpb 712.7 | bsz 712.7 | num_updates 15349 | lr 0.000193 | gnorm 3.955 | clip 0 | train_wall 217 | gb_free 70.8 | wall 13496 (progress_bar.py:267, print())[0m
[33m[2025-11-19 03:02:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:03:08]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.609 | wps 11399.3 | wpb 5412.5 | bsz 5412.5 | num_updates 15349 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 03:03:08]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:03:08]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 10 @ 15349 updates, score 3.609) (writing took 0.013777 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 03:03:08] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:03:09]    INFO >> epoch 011:      1 / 1539 loss=3.477, wps=1723.6, ups=2.37, wpb=728.6, bsz=728.6, num_updates=15350, lr=0.000161, gnorm=3.938, clip=0, train_wall=7, gb_free=68.7, wall=13510 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:03:16]    INFO >> epoch 011:     51 / 1539 loss=3.581, wps=4570.4, ups=7.06, wpb=647.3, bsz=647.3, num_updates=15400, lr=0.000161, gnorm=3.532, clip=0, train_wall=7, gb_free=75, wall=13517 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:03:25]    INFO >> epoch 011:    101 / 1539 loss=3.45, wps=4863.5, ups=6.67, wpb=729.7, bsz=729.7, num_updates=15450, lr=0.000161, gnorm=3.851, clip=0, train_wall=7, gb_free=74.4, wall=13524 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:03:31]    INFO >> epoch 011:    151 / 1539 loss=3.543, wps=4901.3, ups=7.36, wpb=666, bsz=666, num_updates=15500, lr=0.000161, gnorm=4.02, clip=0, train_wall=6, gb_free=71.4, wall=13531 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:03:39]    INFO >> epoch 011:    201 / 1539 loss=3.332, wps=5103.9, ups=6.63, wpb=769.4, bsz=769.4, num_updates=15550, lr=0.000161, gnorm=3.843, clip=0, train_wall=7, gb_free=71.9, wall=13539 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:03:46]    INFO >> epoch 011:    251 / 1539 loss=3.484, wps=5103.7, ups=7, wpb=729.4, bsz=729.4, num_updates=15600, lr=0.000161, gnorm=3.693, clip=0, train_wall=7, gb_free=73.3, wall=13546 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:03:53]    INFO >> epoch 011:    301 / 1539 loss=3.44, wps=4310.8, ups=6.92, wpb=623.3, bsz=623.3, num_updates=15650, lr=0.000161, gnorm=3.708, clip=0, train_wall=7, gb_free=74.6, wall=13553 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:04:02]    INFO >> epoch 011:    351 / 1539 loss=3.425, wps=4872.5, ups=6.49, wpb=751.3, bsz=751.3, num_updates=15700, lr=0.000161, gnorm=4.178, clip=0, train_wall=7, gb_free=72, wall=13561 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:04:09]    INFO >> epoch 011:    401 / 1539 loss=3.522, wps=4941.5, ups=7.29, wpb=677.5, bsz=677.5, num_updates=15750, lr=0.000161, gnorm=3.475, clip=0, train_wall=6, gb_free=72.3, wall=13567 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:04:16]    INFO >> epoch 011:    451 / 1539 loss=3.51, wps=4422.1, ups=7.09, wpb=624.1, bsz=624.1, num_updates=15800, lr=0.000161, gnorm=3.673, clip=0, train_wall=7, gb_free=72.8, wall=13575 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:04:25]    INFO >> epoch 011:    501 / 1539 loss=3.551, wps=3649.1, ups=6.01, wpb=606.7, bsz=606.7, num_updates=15850, lr=0.000161, gnorm=3.914, clip=0, train_wall=8, gb_free=75, wall=13583 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:04:33]    INFO >> epoch 011:    551 / 1539 loss=3.55, wps=4547.9, ups=7.04, wpb=645.8, bsz=645.8, num_updates=15900, lr=0.000161, gnorm=3.622, clip=0, train_wall=7, gb_free=73.9, wall=13590 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:04:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 59.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 76.03 GiB is allocated by PyTorch, and 2.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:04:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 42           |        cudaMalloc retries: 66        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77793 MiB |  77853 MiB | 513719 GiB | 513643 GiB |
|       from large pool |  77411 MiB |  77471 MiB | 510808 GiB | 510732 GiB |
|       from small pool |    382 MiB |    383 MiB |   2910 GiB |   2910 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77793 MiB |  77853 MiB | 513719 GiB | 513643 GiB |
|       from large pool |  77411 MiB |  77471 MiB | 510808 GiB | 510732 GiB |
|       from small pool |    382 MiB |    383 MiB |   2910 GiB |   2910 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77777 MiB |  77836 MiB | 512628 GiB | 512552 GiB |
|       from large pool |  77397 MiB |  77456 MiB | 509721 GiB | 509646 GiB |
|       from small pool |    380 MiB |    381 MiB |   2906 GiB |   2905 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80446 MiB |  80448 MiB | 457586 MiB | 377140 MiB |
|       from large pool |  80026 MiB |  80026 MiB | 451106 MiB | 371080 MiB |
|       from small pool |    420 MiB |    422 MiB |   6480 MiB |   6060 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2592 MiB |   7406 MiB | 494129 GiB | 494126 GiB |
|       from large pool |   2554 MiB |   7398 MiB | 490836 GiB | 490834 GiB |
|       from small pool |     37 MiB |     40 MiB |   3292 GiB |   3292 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7054    |    7057    |   33868 K  |   33861 K  |
|       from large pool |     911    |     912    |   16130 K  |   16129 K  |
|       from small pool |    6143    |    6146    |   17738 K  |   17732 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7054    |    7057    |   33868 K  |   33861 K  |
|       from large pool |     911    |     912    |   16130 K  |   16129 K  |
|       from small pool |    6143    |    6146    |   17738 K  |   17732 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     358    |     359    |    4686    |    4328    |
|       from large pool |     148    |     148    |    1446    |    1298    |
|       from small pool |     210    |     211    |    3240    |    3030    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     516    |     518    |   19139 K  |   19139 K  |
|       from large pool |     142    |     142    |   10618 K  |   10618 K  |
|       from small pool |     374    |     376    |    8521 K  |    8520 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:04:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:04:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:04:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:04:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:04:41]    INFO >> epoch 011:    602 / 1539 loss=3.412, wps=4294.8, ups=6.19, wpb=694.1, bsz=694.1, num_updates=15950, lr=0.000161, gnorm=3.728, clip=0, train_wall=7, gb_free=71.6, wall=13598 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:04:49]    INFO >> epoch 011:    652 / 1539 loss=3.5, wps=5195.5, ups=6.37, wpb=815.3, bsz=815.3, num_updates=16000, lr=0.000161, gnorm=3.114, clip=0, train_wall=7, gb_free=71.5, wall=13606 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:04:56]    INFO >> epoch 011:    702 / 1539 loss=3.453, wps=4595.4, ups=6.95, wpb=661.1, bsz=661.1, num_updates=16050, lr=0.000161, gnorm=3.545, clip=0, train_wall=7, gb_free=71, wall=13613 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:05:04]    INFO >> epoch 011:    752 / 1539 loss=3.359, wps=4436.4, ups=6.06, wpb=731.8, bsz=731.8, num_updates=16100, lr=0.000161, gnorm=3.985, clip=0, train_wall=8, gb_free=72.6, wall=13621 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:05:10] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 575.25 MiB is free. Including non-PyTorch memory, this process has 78.55 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 4.25 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:05:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 43           |        cudaMalloc retries: 67        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75058 MiB |  76001 MiB | 520726 GiB | 520653 GiB |
|       from large pool |  75045 MiB |  75988 MiB | 517779 GiB | 517705 GiB |
|       from small pool |     12 MiB |     21 MiB |   2947 GiB |   2947 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75058 MiB |  76001 MiB | 520726 GiB | 520653 GiB |
|       from large pool |  75045 MiB |  75988 MiB | 517779 GiB | 517705 GiB |
|       from small pool |     12 MiB |     21 MiB |   2947 GiB |   2947 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 519620 GiB | 519546 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 516676 GiB | 516603 GiB |
|       from small pool |     12 MiB |     21 MiB |   2943 GiB |   2943 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79930 MiB |  80386 MiB | 457586 MiB | 377656 MiB |
|       from large pool |  79906 MiB |  79966 MiB | 451106 MiB | 371200 MiB |
|       from small pool |     24 MiB |    420 MiB |   6480 MiB |   6456 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4811 MiB |  11162 MiB | 501099 GiB | 501094 GiB |
|       from large pool |   4800 MiB |  11151 MiB | 497763 GiB | 497758 GiB |
|       from small pool |     11 MiB |     29 MiB |   3335 GiB |   3335 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   34327 K  |   34326 K  |
|       from large pool |     314    |     322    |   16362 K  |   16361 K  |
|       from small pool |     291    |     356    |   17965 K  |   17965 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   34327 K  |   34326 K  |
|       from large pool |     314    |     322    |   16362 K  |   16361 K  |
|       from small pool |     291    |     356    |   17965 K  |   17965 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     158    |     357    |    4686    |    4528    |
|       from large pool |     146    |     147    |    1446    |    1300    |
|       from small pool |      12    |     210    |    3240    |    3228    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     146    |     151    |   19397 K  |   19397 K  |
|       from large pool |     119    |     124    |   10770 K  |   10770 K  |
|       from small pool |      27    |      57    |    8627 K  |    8627 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:05:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:05:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:05:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:05:10] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:05:12]    INFO >> epoch 011:    803 / 1539 loss=3.531, wps=3967.8, ups=6.77, wpb=586.3, bsz=586.3, num_updates=16150, lr=0.000161, gnorm=3.467, clip=0, train_wall=6, gb_free=62.8, wall=13629 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:05:19]    INFO >> epoch 011:    853 / 1539 loss=3.463, wps=4587.2, ups=6.98, wpb=657, bsz=657, num_updates=16200, lr=0.000161, gnorm=3.673, clip=0, train_wall=7, gb_free=74.8, wall=13636 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:05:26]    INFO >> epoch 011:    903 / 1539 loss=3.38, wps=5381.4, ups=6.9, wpb=779.8, bsz=779.8, num_updates=16250, lr=0.000161, gnorm=4.415, clip=0, train_wall=7, gb_free=72.7, wall=13643 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:05:34]    INFO >> epoch 011:    953 / 1539 loss=3.376, wps=4920, ups=6.59, wpb=746, bsz=746, num_updates=16300, lr=0.000161, gnorm=3.669, clip=0, train_wall=7, gb_free=72, wall=13651 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:05:44]    INFO >> epoch 011:   1003 / 1539 loss=3.413, wps=4926.9, ups=6.68, wpb=737.8, bsz=737.8, num_updates=16350, lr=0.000161, gnorm=3.648, clip=0, train_wall=7, gb_free=71.3, wall=13658 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:05:51]    INFO >> epoch 011:   1053 / 1539 loss=3.238, wps=4653.4, ups=6.72, wpb=692.9, bsz=692.9, num_updates=16400, lr=0.000161, gnorm=3.62, clip=0, train_wall=7, gb_free=73.7, wall=13666 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:05:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 633.25 MiB is free. Including non-PyTorch memory, this process has 78.50 GiB memory in use. Of the allocated memory 72.94 GiB is allocated by PyTorch, and 5.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:05:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 44           |        cudaMalloc retries: 68        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69869 MiB |  74866 MiB | 529293 GiB | 529224 GiB |
|       from large pool |  69852 MiB |  74849 MiB | 526299 GiB | 526231 GiB |
|       from small pool |     17 MiB |     17 MiB |   2993 GiB |   2993 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69869 MiB |  74866 MiB | 529293 GiB | 529224 GiB |
|       from large pool |  69852 MiB |  74849 MiB | 526299 GiB | 526231 GiB |
|       from small pool |     17 MiB |     17 MiB |   2993 GiB |   2993 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69846 MiB |  74842 MiB | 528167 GiB | 528099 GiB |
|       from large pool |  69829 MiB |  74825 MiB | 525178 GiB | 525110 GiB |
|       from small pool |     17 MiB |     17 MiB |   2989 GiB |   2989 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79872 MiB |  79970 MiB | 457686 MiB | 377814 MiB |
|       from large pool |  79846 MiB |  79846 MiB | 451106 MiB | 371260 MiB |
|       from small pool |     26 MiB |    124 MiB |   6580 MiB |   6554 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  10002 MiB |  11380 MiB | 509775 GiB | 509765 GiB |
|       from large pool |   9993 MiB |  11371 MiB | 506386 GiB | 506377 GiB |
|       from small pool |      8 MiB |     25 MiB |   3388 GiB |   3388 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     643    |     661    |   34885 K  |   34884 K  |
|       from large pool |     344    |     362    |   16646 K  |   16646 K  |
|       from small pool |     299    |     342    |   18238 K  |   18238 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     643    |     661    |   34885 K  |   34884 K  |
|       from large pool |     344    |     362    |   16646 K  |   16646 K  |
|       from small pool |     299    |     342    |   18238 K  |   18238 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     158    |     207    |    4736    |    4578    |
|       from large pool |     145    |     145    |    1446    |    1301    |
|       from small pool |      13    |      62    |    3290    |    3277    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     141    |     142    |   19706 K  |   19706 K  |
|       from large pool |     114    |     115    |   10955 K  |   10955 K  |
|       from small pool |      27    |      53    |    8750 K  |    8750 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:05:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:05:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:05:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:05:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:06:00]    INFO >> epoch 011:   1104 / 1539 loss=3.511, wps=4538.9, ups=5.94, wpb=764, bsz=764, num_updates=16450, lr=0.000161, gnorm=3.914, clip=0, train_wall=7, gb_free=75, wall=13674 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:06:08]    INFO >> epoch 011:   1154 / 1539 loss=3.322, wps=5625.9, ups=6.19, wpb=909, bsz=909, num_updates=16500, lr=0.000161, gnorm=4.066, clip=0, train_wall=8, gb_free=69.3, wall=13682 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:06:17]    INFO >> epoch 011:   1204 / 1539 loss=3.444, wps=4085.6, ups=6.72, wpb=608.2, bsz=608.2, num_updates=16550, lr=0.000161, gnorm=3.356, clip=0, train_wall=7, gb_free=68.8, wall=13690 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:06:25]    INFO >> epoch 011:   1254 / 1539 loss=3.452, wps=4951, ups=5.97, wpb=828.9, bsz=828.9, num_updates=16600, lr=0.000161, gnorm=4.211, clip=0, train_wall=8, gb_free=73.1, wall=13698 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:06:32]    INFO >> epoch 011:   1304 / 1539 loss=3.419, wps=4963.6, ups=6.72, wpb=739.1, bsz=739.1, num_updates=16650, lr=0.000161, gnorm=3.595, clip=0, train_wall=7, gb_free=71.9, wall=13705 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:06:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 633.25 MiB is free. Including non-PyTorch memory, this process has 78.50 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 7.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:06:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 45           |        cudaMalloc retries: 69        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72059 MiB |  72460 MiB | 536404 GiB | 536334 GiB |
|       from large pool |  72042 MiB |  72442 MiB | 533372 GiB | 533301 GiB |
|       from small pool |     17 MiB |     23 MiB |   3032 GiB |   3032 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72059 MiB |  72460 MiB | 536404 GiB | 536334 GiB |
|       from large pool |  72042 MiB |  72442 MiB | 533372 GiB | 533301 GiB |
|       from small pool |     17 MiB |     23 MiB |   3032 GiB |   3032 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 535265 GiB | 535194 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 532236 GiB | 532166 GiB |
|       from small pool |     17 MiB |     23 MiB |   3028 GiB |   3028 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79872 MiB |  80056 MiB | 457870 MiB | 377998 MiB |
|       from large pool |  79846 MiB |  79846 MiB | 451106 MiB | 371260 MiB |
|       from small pool |     26 MiB |    210 MiB |   6764 MiB |   6738 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7812 MiB |   9157 MiB | 516886 GiB | 516878 GiB |
|       from large pool |   7803 MiB |   9146 MiB | 513453 GiB | 513445 GiB |
|       from small pool |      8 MiB |     29 MiB |   3433 GiB |   3433 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   35349 K  |   35348 K  |
|       from large pool |     308    |     315    |   16876 K  |   16876 K  |
|       from small pool |     298    |     356    |   18472 K  |   18472 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   35349 K  |   35348 K  |
|       from large pool |     308    |     315    |   16876 K  |   16876 K  |
|       from small pool |     298    |     356    |   18472 K  |   18472 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     158    |     250    |    4828    |    4670    |
|       from large pool |     145    |     145    |    1446    |    1301    |
|       from small pool |      13    |     105    |    3382    |    3369    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     152    |     152    |   19966 K  |   19966 K  |
|       from large pool |     126    |     126    |   11105 K  |   11105 K  |
|       from small pool |      26    |      57    |    8860 K  |    8860 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:06:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:06:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:06:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:06:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:06:41]    INFO >> epoch 011:   1355 / 1539 loss=3.511, wps=4200.3, ups=6.1, wpb=689.1, bsz=689.1, num_updates=16700, lr=0.000161, gnorm=3.644, clip=0, train_wall=7, gb_free=56.8, wall=13714 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:06:50]    INFO >> epoch 011:   1405 / 1539 loss=3.547, wps=5058.4, ups=6.39, wpb=791.3, bsz=791.3, num_updates=16750, lr=0.000161, gnorm=4.018, clip=0, train_wall=7, gb_free=73.5, wall=13721 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:06:57]    INFO >> epoch 011:   1455 / 1539 loss=3.524, wps=4510.6, ups=6.78, wpb=665.3, bsz=665.3, num_updates=16800, lr=0.000161, gnorm=3.404, clip=0, train_wall=7, gb_free=65.3, wall=13729 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:07:05]    INFO >> epoch 011:   1505 / 1539 loss=3.124, wps=5083.7, ups=6.21, wpb=818.6, bsz=818.6, num_updates=16850, lr=0.000161, gnorm=4.157, clip=0, train_wall=8, gb_free=72.5, wall=13737 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:07:10]    INFO >> epoch 011 | loss 3.44 | wps 4444.8 | ups 6.24 | wpb 712.7 | bsz 712.7 | num_updates 16884 | lr 0.000161 | gnorm 3.761 | clip 0 | train_wall 217 | gb_free 73.9 | wall 13742 (progress_bar.py:267, print())[0m
[33m[2025-11-19 03:07:10] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:07:24]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.726 | wps 11792.1 | wpb 5412.5 | bsz 5412.5 | num_updates 16884 | best_loss 4.404 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 03:07:25]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:07:25]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/checkpoints/checkpoint_last.pt (epoch 11 @ 16884 updates, score 3.726) (writing took 0.013304 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-19 03:07:25]    INFO >> æ—©åœ: éªŒè¯æ€§èƒ½å·²10è½®æœªæå‡ (train_enhanced.py:345, single_main())[0m
[32m[2025-11-19 03:07:25]    INFO >> è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ 13692.9 ç§’ (train_enhanced.py:355, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 03:07:25]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:07:25]    INFO >> æ‰€æœ‰æ—¥å¿—å·²ä¿å­˜åˆ°: /home/zhaojunzhang/naturalcc/typilus/experiments/baseline/logs (train_enhanced.py:359, single_main())[0m

âœ“ baseline æˆåŠŸ

ç­‰å¾…3ç§’...

è¿›åº¦: 2/6

============================================================
å®éªŒ: exp_lr_1e-3 - å­¦ä¹ ç‡1e-3
æ—¶é—´: 2025-11-19 03:08:06
============================================================

[32m[2025-11-19 03:08:08]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/config.yml (train_enhanced.py:382, cli_main())[0m
[32m[2025-11-19 03:08:08]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:410, cli_main())[0m
[32m[2025-11-19 03:08:08]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/logs (train_enhanced.py:296, single_main())[0m
[32m[2025-11-19 03:08:08]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-19 03:08:08]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-19 03:08:08]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-19 03:08:16]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 64, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=64, out_features=64, bias=False)
      (2): Dropout(p=0.1, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(64, 64)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(128, 64)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=False)
      (1): Dropout(p=0.1, inplace=False)
      (2): Linear(in_features=64, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:303, single_main())[0m
[32m[2025-11-19 03:08:16]    INFO >> æ¨¡å‹: typilus, æŸå¤±å‡½æ•°: TypilusCriterion (train_enhanced.py:304, single_main())[0m
[32m[2025-11-19 03:08:16]    INFO >> æ¨¡å‹å‚æ•°: 847843 (å¯è®­ç»ƒ: 847843) (train_enhanced.py:305, single_main())[0m
[32m[2025-11-19 03:08:16]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-19 03:08:16]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80579 MB ; used memory = 1340 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-19 03:08:16]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-19 03:08:16]    INFO >> ä½¿ç”¨ 1 ä¸ªGPUè®­ç»ƒ (train_enhanced.py:311, single_main())[0m
[32m[2025-11-19 03:08:16]    INFO >> no existing checkpoint found /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-19 03:08:16]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-19 03:09:15]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-19 03:09:16] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-19 03:09:23]    INFO >> epoch 001:     50 / 1539 loss=5.73, wps=5341.9, ups=7.39, wpb=720, bsz=720, num_updates=50, lr=0.001, gnorm=7.492, clip=0, train_wall=6, gb_free=74.2, wall=64 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:09:24] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 73.64 GiB is allocated by PyTorch, and 4.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:09:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75347 MiB |  75407 MiB |   1756 GiB |   1682 GiB |
|       from large pool |  74991 MiB |  75051 MiB |   1743 GiB |   1670 GiB |
|       from small pool |    356 MiB |    357 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75347 MiB |  75407 MiB |   1756 GiB |   1682 GiB |
|       from large pool |  74991 MiB |  75051 MiB |   1743 GiB |   1670 GiB |
|       from small pool |    356 MiB |    357 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75195 MiB |  75254 MiB |   1750 GiB |   1676 GiB |
|       from large pool |  74841 MiB |  74900 MiB |   1737 GiB |   1664 GiB |
|       from small pool |    354 MiB |    355 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80498 MiB |  91994 MiB |  11532 MiB |
|       from large pool |  80070 MiB |  80136 MiB |  91590 MiB |  11520 MiB |
|       from small pool |    392 MiB |    394 MiB |    404 MiB |     12 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5054 MiB |   5960 MiB |    875 GiB |    870 GiB |
|       from large pool |   5018 MiB |   5948 MiB |    859 GiB |    855 GiB |
|       from small pool |     35 MiB |     37 MiB |     15 GiB |     14 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6579    |    6582    |  136626    |  130047    |
|       from large pool |     868    |     869    |   57719    |   56851    |
|       from small pool |    5711    |    5714    |   78907    |   73196    |
|---------------------------------------------------------------------------|
| Active allocs         |    6579    |    6582    |  136626    |  130047    |
|       from large pool |     868    |     869    |   57719    |   56851    |
|       from small pool |    5711    |    5714    |   78907    |   73196    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     758    |     793    |    1006    |     248    |
|       from large pool |     562    |     612    |     804    |     242    |
|       from small pool |     196    |     197    |     202    |       6    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     654    |     656    |   86039    |   85385    |
|       from large pool |     306    |     306    |   44256    |   43950    |
|       from small pool |     348    |     350    |   41783    |   41435    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:09:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:09:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:09:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:09:24] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:09:29]    INFO >> epoch 001:    101 / 1539 loss=6.083, wps=4691.2, ups=7.68, wpb=610.7, bsz=610.7, num_updates=100, lr=0.001, gnorm=7.85, clip=0, train_wall=5, gb_free=75.6, wall=70 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:09:38]    INFO >> epoch 001:    151 / 1539 loss=6.008, wps=5713.5, ups=7, wpb=816.7, bsz=816.7, num_updates=150, lr=0.001, gnorm=8.526, clip=2, train_wall=7, gb_free=74.2, wall=78 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:09:44]    INFO >> epoch 001:    201 / 1539 loss=5.894, wps=5198.9, ups=8.1, wpb=641.7, bsz=641.7, num_updates=200, lr=0.001, gnorm=6.637, clip=0, train_wall=6, gb_free=74.8, wall=84 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:09:50]    INFO >> epoch 001:    251 / 1539 loss=5.785, wps=4981.3, ups=7.81, wpb=637.9, bsz=637.9, num_updates=250, lr=0.001, gnorm=6.565, clip=0, train_wall=6, gb_free=71.5, wall=90 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:09:57]    INFO >> epoch 001:    301 / 1539 loss=5.686, wps=5490.1, ups=6.98, wpb=786.5, bsz=786.5, num_updates=300, lr=0.001, gnorm=6.412, clip=0, train_wall=7, gb_free=73.8, wall=97 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:10:04]    INFO >> epoch 001:    351 / 1539 loss=5.577, wps=5135, ups=7.65, wpb=671.6, bsz=671.6, num_updates=350, lr=0.001, gnorm=7.321, clip=4, train_wall=6, gb_free=72.4, wall=104 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:10:12]    INFO >> epoch 001:    401 / 1539 loss=5.537, wps=5915.5, ups=6.95, wpb=851.7, bsz=851.7, num_updates=400, lr=0.001, gnorm=6.835, clip=0, train_wall=7, gb_free=73.2, wall=111 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:10:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.81 GiB is allocated by PyTorch, and 823.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:10:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79614 MiB |  79674 MiB |  12419 GiB |  12342 GiB |
|       from large pool |  79520 MiB |  79580 MiB |  12346 GiB |  12268 GiB |
|       from small pool |     94 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79614 MiB |  79674 MiB |  12419 GiB |  12342 GiB |
|       from large pool |  79520 MiB |  79580 MiB |  12346 GiB |  12268 GiB |
|       from small pool |     94 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79544 MiB |  79604 MiB |  12379 GiB |  12302 GiB |
|       from large pool |  79450 MiB |  79510 MiB |  12306 GiB |  12228 GiB |
|       from small pool |     93 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 166722 MiB |  86224 MiB |
|       from large pool |  80400 MiB |  80400 MiB | 166242 MiB |  85842 MiB |
|       from small pool |     98 MiB |    392 MiB |    480 MiB |    382 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    823 MiB |   3119 MiB |   6143 GiB |   6143 GiB |
|       from large pool |    819 MiB |   3112 MiB |   6057 GiB |   6057 GiB |
|       from small pool |      3 MiB |     23 MiB |     85 GiB |     85 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2036    |    2039    |     869 K  |     867 K  |
|       from large pool |     477    |     478    |     421 K  |     421 K  |
|       from small pool |    1559    |    1562    |     447 K  |     445 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2036    |    2039    |     869 K  |     867 K  |
|       from large pool |     477    |     478    |     421 K  |     421 K  |
|       from small pool |    1559    |    1562    |     447 K  |     445 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     251    |     757    |    1201    |     950    |
|       from large pool |     202    |     561    |     961    |     759    |
|       from small pool |      49    |     196    |     240    |     191    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     132    |     133    |  540020    |  539888    |
|       from large pool |      79    |      81    |  324754    |  324675    |
|       from small pool |      53    |      55    |  215266    |  215213    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:10:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:10:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:10:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:10:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:10:25]    INFO >> epoch 001:    452 / 1539 loss=5.35, wps=2447.5, ups=3.88, wpb=631.6, bsz=631.6, num_updates=450, lr=0.001, gnorm=6.812, clip=2, train_wall=6, gb_free=71.8, wall=124 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:10:35]    INFO >> epoch 001:    502 / 1539 loss=5.131, wps=3968.2, ups=5.34, wpb=743.1, bsz=743.1, num_updates=500, lr=0.001, gnorm=7.041, clip=0, train_wall=9, gb_free=72.6, wall=133 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:10:43]    INFO >> epoch 001:    552 / 1539 loss=5, wps=4918, ups=7.49, wpb=657, bsz=657, num_updates=550, lr=0.001, gnorm=6.411, clip=0, train_wall=6, gb_free=65.5, wall=140 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:10:49]    INFO >> epoch 001:    602 / 1539 loss=4.748, wps=5049.2, ups=7.55, wpb=668.6, bsz=668.6, num_updates=600, lr=0.001, gnorm=6.384, clip=0, train_wall=6, gb_free=73.2, wall=147 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:10:57]    INFO >> epoch 001:    652 / 1539 loss=4.38, wps=4965.1, ups=6.97, wpb=712.2, bsz=712.2, num_updates=650, lr=0.001, gnorm=7.086, clip=0, train_wall=7, gb_free=73.5, wall=154 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:11:05]    INFO >> epoch 001:    702 / 1539 loss=4.397, wps=4242.5, ups=6.31, wpb=672.5, bsz=672.5, num_updates=700, lr=0.001, gnorm=6.474, clip=0, train_wall=7, gb_free=74.1, wall=162 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:11:13]    INFO >> epoch 001:    752 / 1539 loss=4.373, wps=5146.7, ups=6.79, wpb=757.4, bsz=757.4, num_updates=750, lr=0.001, gnorm=6.671, clip=0, train_wall=7, gb_free=73.7, wall=169 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:11:20]    INFO >> epoch 001:    802 / 1539 loss=4.379, wps=5240.4, ups=7.23, wpb=725.2, bsz=725.2, num_updates=800, lr=0.001, gnorm=7.535, clip=0, train_wall=6, gb_free=73.4, wall=176 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:11:27]    INFO >> epoch 001:    852 / 1539 loss=4.296, wps=4379.6, ups=6.83, wpb=641.1, bsz=641.1, num_updates=850, lr=0.001, gnorm=6.676, clip=0, train_wall=7, gb_free=71.8, wall=183 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:11:34]    INFO >> epoch 001:    902 / 1539 loss=4.256, wps=4764.6, ups=7.22, wpb=660.1, bsz=660.1, num_updates=900, lr=0.001, gnorm=6.391, clip=0, train_wall=6, gb_free=72.1, wall=190 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:11:42]    INFO >> epoch 001:    952 / 1539 loss=4.092, wps=4998.9, ups=7.01, wpb=713.2, bsz=713.2, num_updates=950, lr=0.001, gnorm=7.044, clip=0, train_wall=7, gb_free=71.8, wall=197 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:11:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 901.25 MiB is free. Including non-PyTorch memory, this process has 78.24 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:11:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76322 MiB |  78597 MiB |  28415 GiB |  28340 GiB |
|       from large pool |  76303 MiB |  78579 MiB |  28261 GiB |  28187 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76322 MiB |  78597 MiB |  28415 GiB |  28340 GiB |
|       from large pool |  76303 MiB |  78579 MiB |  28261 GiB |  28187 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB |  28342 GiB |  28268 GiB |
|       from large pool |  76285 MiB |  78562 MiB |  28189 GiB |  28114 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79604 MiB |  79604 MiB | 305536 MiB | 225932 MiB |
|       from large pool |  79580 MiB |  79580 MiB | 304982 MiB | 225402 MiB |
|       from small pool |     24 MiB |     98 MiB |    554 MiB |    530 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3281 MiB |   4789 MiB |  24281 GiB |  24277 GiB |
|       from large pool |   3276 MiB |   4783 MiB |  24103 GiB |  24100 GiB |
|       from small pool |      5 MiB |     27 MiB |    177 GiB |    177 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     638    |     644    |    1881 K  |    1880 K  |
|       from large pool |     340    |     346    |     949 K  |     948 K  |
|       from small pool |     298    |     354    |     931 K  |     931 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     638    |     644    |    1881 K  |    1880 K  |
|       from large pool |     340    |     346    |     949 K  |     948 K  |
|       from small pool |     298    |     354    |     931 K  |     931 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     123    |    1327    |    1231    |
|       from large pool |      84    |      84    |    1050    |     966    |
|       from small pool |      12    |      49    |     277    |     265    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     115    |     117    |    1087 K  |    1087 K  |
|       from large pool |      93    |      95    |     661 K  |     661 K  |
|       from small pool |      22    |      56    |     426 K  |     426 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:11:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:11:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:11:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:11:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:11:52]    INFO >> epoch 001:   1003 / 1539 loss=4.192, wps=3577.7, ups=5.51, wpb=649.4, bsz=649.4, num_updates=1000, lr=0.001, gnorm=5.869, clip=0, train_wall=7, gb_free=72.1, wall=206 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:12:00]    INFO >> epoch 001:   1053 / 1539 loss=4.009, wps=5368.8, ups=6.53, wpb=822.6, bsz=822.6, num_updates=1050, lr=0.001, gnorm=7.67, clip=0, train_wall=7, gb_free=68, wall=214 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:12:07] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 197.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:12:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 9         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76955 MiB |  77477 MiB |  32758 GiB |  32683 GiB |
|       from large pool |  76942 MiB |  77464 MiB |  32579 GiB |  32504 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76955 MiB |  77477 MiB |  32758 GiB |  32683 GiB |
|       from large pool |  76942 MiB |  77464 MiB |  32579 GiB |  32504 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB |  32677 GiB |  32602 GiB |
|       from large pool |  76923 MiB |  77445 MiB |  32498 GiB |  32423 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80308 MiB |  80308 MiB | 310972 MiB | 230664 MiB |
|       from large pool |  80284 MiB |  80284 MiB | 310232 MiB | 229948 MiB |
|       from small pool |     24 MiB |    210 MiB |    740 MiB |    716 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3352 MiB |   9143 MiB |  29238 GiB |  29235 GiB |
|       from large pool |   3341 MiB |   9131 MiB |  29032 GiB |  29029 GiB |
|       from small pool |     11 MiB |     23 MiB |    206 GiB |    206 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     604    |     611    |    2166 K  |    2165 K  |
|       from large pool |     315    |     322    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1080 K  |    1079 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     604    |     611    |    2166 K  |    2165 K  |
|       from large pool |     315    |     322    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1080 K  |    1079 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     189    |    1425    |    1337    |
|       from large pool |      76    |      84    |    1055    |     979    |
|       from small pool |      12    |     105    |     370    |     358    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      84    |      84    |    1245 K  |    1245 K  |
|       from large pool |      60    |      60    |     746 K  |     746 K  |
|       from small pool |      24    |      53    |     498 K  |     498 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:12:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:12:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:12:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:12:07] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:12:08]    INFO >> epoch 001:   1104 / 1539 loss=3.833, wps=5218.5, ups=5.61, wpb=929.6, bsz=929.6, num_updates=1100, lr=0.001, gnorm=7.467, clip=2, train_wall=8, gb_free=72.8, wall=223 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:12:16]    INFO >> epoch 001:   1154 / 1539 loss=4.105, wps=4769.3, ups=6.89, wpb=692.6, bsz=692.6, num_updates=1150, lr=0.001, gnorm=7.244, clip=0, train_wall=7, gb_free=73.1, wall=230 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:12:24]    INFO >> epoch 001:   1204 / 1539 loss=3.939, wps=4826.1, ups=7.11, wpb=678.4, bsz=678.4, num_updates=1200, lr=0.001, gnorm=6.72, clip=0, train_wall=6, gb_free=71.2, wall=237 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:12:31]    INFO >> epoch 001:   1254 / 1539 loss=4.14, wps=5048.5, ups=6.91, wpb=730.9, bsz=730.9, num_updates=1250, lr=0.001, gnorm=6.338, clip=0, train_wall=7, gb_free=71.3, wall=245 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:12:39]    INFO >> epoch 001:   1304 / 1539 loss=3.977, wps=4872.3, ups=6.63, wpb=735.4, bsz=735.4, num_updates=1300, lr=0.001, gnorm=6.112, clip=0, train_wall=7, gb_free=74.3, wall=252 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:12:46]    INFO >> epoch 001:   1354 / 1539 loss=4.007, wps=4529.1, ups=6.89, wpb=657.2, bsz=657.2, num_updates=1350, lr=0.001, gnorm=5.424, clip=0, train_wall=7, gb_free=73.4, wall=259 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:12:55]    INFO >> epoch 001:   1404 / 1539 loss=3.93, wps=4829.9, ups=6.78, wpb=712.5, bsz=712.5, num_updates=1400, lr=0.001, gnorm=5.881, clip=0, train_wall=7, gb_free=73.3, wall=267 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:13:02]    INFO >> epoch 001:   1454 / 1539 loss=4.071, wps=4854.7, ups=6.91, wpb=702.2, bsz=702.2, num_updates=1450, lr=0.001, gnorm=6.145, clip=0, train_wall=7, gb_free=71.6, wall=274 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:13:10]    INFO >> epoch 001:   1504 / 1539 loss=3.824, wps=4644, ups=6.67, wpb=695.9, bsz=695.9, num_updates=1500, lr=0.001, gnorm=6.131, clip=0, train_wall=7, gb_free=70.8, wall=282 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:13:15]    INFO >> epoch 001 | loss 4.66 | wps 4770 | ups 6.69 | wpb 712.7 | bsz 712.7 | num_updates 1535 | lr 0.001 | gnorm 6.816 | clip 0.4 | train_wall 202 | gb_free 76.6 | wall 287 (progress_bar.py:267, print())[0m
[33m[2025-11-19 03:13:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:13:30]    INFO >> epoch 001 | valid on 'valid' subset | loss 3.943 | wps 10849.8 | wpb 5412.5 | bsz 5412.5 | num_updates 1535 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-19 03:13:30]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:13:31]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/checkpoints/checkpoint_best.pt (epoch 1 @ 1535 updates, score 3.943) (writing took 0.014477 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 03:13:31] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-19 03:13:32]    INFO >> epoch 002:     15 / 1539 loss=3.855, wps=1704.9, ups=2.33, wpb=731.3, bsz=731.3, num_updates=1550, lr=0.001, gnorm=7.89, clip=2, train_wall=6, gb_free=74.4, wall=303 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:13:39]    INFO >> epoch 002:     65 / 1539 loss=3.944, wps=5017.9, ups=7.62, wpb=658.8, bsz=658.8, num_updates=1600, lr=0.001, gnorm=5.512, clip=0, train_wall=6, gb_free=73.4, wall=310 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:13:46]    INFO >> epoch 002:    115 / 1539 loss=3.945, wps=4917.5, ups=6.88, wpb=714.6, bsz=714.6, num_updates=1650, lr=0.001, gnorm=5.153, clip=0, train_wall=7, gb_free=65.7, wall=317 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:13:54]    INFO >> epoch 002:    165 / 1539 loss=3.673, wps=5014.9, ups=6.79, wpb=738.8, bsz=738.8, num_updates=1700, lr=0.001, gnorm=6.126, clip=0, train_wall=7, gb_free=73.6, wall=324 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:14:02]    INFO >> epoch 002:    215 / 1539 loss=4.035, wps=4960.6, ups=7.04, wpb=704.2, bsz=704.2, num_updates=1750, lr=0.001, gnorm=4.969, clip=0, train_wall=7, gb_free=71.2, wall=331 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:14:10]    INFO >> epoch 002:    265 / 1539 loss=3.782, wps=5393, ups=6.17, wpb=873.5, bsz=873.5, num_updates=1800, lr=0.001, gnorm=6.055, clip=0, train_wall=8, gb_free=74.7, wall=339 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:14:17]    INFO >> epoch 002:    315 / 1539 loss=3.836, wps=4702.4, ups=7.29, wpb=645.1, bsz=645.1, num_updates=1850, lr=0.001, gnorm=6.206, clip=0, train_wall=6, gb_free=71.9, wall=346 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:14:24]    INFO >> epoch 002:    365 / 1539 loss=3.856, wps=4730.2, ups=7.21, wpb=655.8, bsz=655.8, num_updates=1900, lr=0.001, gnorm=6.705, clip=0, train_wall=7, gb_free=74, wall=353 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:14:32]    INFO >> epoch 002:    415 / 1539 loss=3.759, wps=4698.3, ups=6.59, wpb=713.2, bsz=713.2, num_updates=1950, lr=0.001, gnorm=6.207, clip=0, train_wall=7, gb_free=76.2, wall=361 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:14:32] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 197.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 3.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:14:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76327 MiB |  78599 MiB |  61960 GiB |  61886 GiB |
|       from large pool |  76308 MiB |  78581 MiB |  61611 GiB |  61537 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76327 MiB |  78599 MiB |  61960 GiB |  61886 GiB |
|       from large pool |  76308 MiB |  78581 MiB |  61611 GiB |  61537 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB |  61824 GiB |  61750 GiB |
|       from large pool |  76285 MiB |  78562 MiB |  61476 GiB |  61402 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80308 MiB |  80494 MiB | 311158 MiB | 230850 MiB |
|       from large pool |  80284 MiB |  80284 MiB | 310232 MiB | 229948 MiB |
|       from small pool |     24 MiB |    210 MiB |    926 MiB |    902 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3980 MiB |   5490 MiB |  61034 GiB |  61030 GiB |
|       from large pool |   3975 MiB |   5484 MiB |  60638 GiB |  60634 GiB |
|       from small pool |      5 MiB |     27 MiB |    395 GiB |    395 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |    4090 K  |    4090 K  |
|       from large pool |     340    |     346    |    1969 K  |    1969 K  |
|       from small pool |     300    |     355    |    2121 K  |    2120 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |    4090 K  |    4090 K  |
|       from large pool |     340    |     346    |    1969 K  |    1969 K  |
|       from small pool |     300    |     355    |    2121 K  |    2120 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     181    |    1518    |    1430    |
|       from large pool |      76    |      76    |    1055    |     979    |
|       from small pool |      12    |     105    |     463    |     451    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      93    |      99    |    2309 K  |    2309 K  |
|       from large pool |      70    |      76    |    1305 K  |    1305 K  |
|       from small pool |      23    |      51    |    1004 K  |    1004 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:14:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:14:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:14:32] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:14:32] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:14:41]    INFO >> epoch 002:    466 / 1539 loss=3.91, wps=4605.6, ups=6.31, wpb=729.4, bsz=729.4, num_updates=2000, lr=0.001, gnorm=5.512, clip=0, train_wall=7, gb_free=71.6, wall=369 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:14:48]    INFO >> epoch 002:    516 / 1539 loss=3.933, wps=4847.2, ups=6.84, wpb=708.4, bsz=708.4, num_updates=2050, lr=0.001, gnorm=5.626, clip=0, train_wall=7, gb_free=71.2, wall=376 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:14:55]    INFO >> epoch 002:    566 / 1539 loss=3.862, wps=4521, ups=7.27, wpb=622.3, bsz=622.3, num_updates=2100, lr=0.001, gnorm=4.619, clip=0, train_wall=7, gb_free=74, wall=383 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:15:03]    INFO >> epoch 002:    616 / 1539 loss=3.75, wps=5560.9, ups=6.38, wpb=871.1, bsz=871.1, num_updates=2150, lr=0.001, gnorm=5.102, clip=0, train_wall=7, gb_free=68.6, wall=391 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:15:12]    INFO >> epoch 002:    666 / 1539 loss=3.693, wps=5043.8, ups=6.53, wpb=772.9, bsz=772.9, num_updates=2200, lr=0.001, gnorm=5.008, clip=0, train_wall=7, gb_free=68.4, wall=398 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:15:19]    INFO >> epoch 002:    716 / 1539 loss=3.823, wps=4935.1, ups=6.96, wpb=708.6, bsz=708.6, num_updates=2250, lr=0.001, gnorm=4.627, clip=0, train_wall=7, gb_free=73.1, wall=406 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:15:26]    INFO >> epoch 002:    766 / 1539 loss=3.728, wps=4596, ups=6.91, wpb=665.2, bsz=665.2, num_updates=2300, lr=0.001, gnorm=4.327, clip=0, train_wall=7, gb_free=75.7, wall=413 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:15:34]    INFO >> epoch 002:    816 / 1539 loss=3.904, wps=4482.3, ups=6.83, wpb=656, bsz=656, num_updates=2350, lr=0.001, gnorm=5.022, clip=0, train_wall=7, gb_free=72.2, wall=420 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:15:42]    INFO >> epoch 002:    866 / 1539 loss=3.813, wps=4821.1, ups=6.7, wpb=719.8, bsz=719.8, num_updates=2400, lr=0.001, gnorm=5.468, clip=0, train_wall=7, gb_free=75.7, wall=428 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:15:50]    INFO >> epoch 002:    916 / 1539 loss=3.848, wps=4656.7, ups=6.94, wpb=671.5, bsz=671.5, num_updates=2450, lr=0.001, gnorm=4.905, clip=0, train_wall=7, gb_free=73.1, wall=435 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:15:57]    INFO >> epoch 002:    966 / 1539 loss=3.666, wps=4409.5, ups=6.91, wpb=637.9, bsz=637.9, num_updates=2500, lr=0.001, gnorm=4.839, clip=0, train_wall=7, gb_free=69.2, wall=442 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:15:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 37.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.54 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:15:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79337 MiB |  79397 MiB |  78153 GiB |  78075 GiB |
|       from large pool |  78941 MiB |  79001 MiB |  77714 GiB |  77637 GiB |
|       from small pool |    395 MiB |    396 MiB |    438 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79337 MiB |  79397 MiB |  78153 GiB |  78075 GiB |
|       from large pool |  78941 MiB |  79001 MiB |  77714 GiB |  77637 GiB |
|       from small pool |    395 MiB |    396 MiB |    438 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79098 MiB |  79157 MiB |  77983 GiB |  77905 GiB |
|       from large pool |  78705 MiB |  78764 MiB |  77545 GiB |  77468 GiB |
|       from small pool |    393 MiB |    394 MiB |    437 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80468 MiB |  80470 MiB | 335872 MiB | 255404 MiB |
|       from large pool |  80032 MiB |  80032 MiB | 334532 MiB | 254500 MiB |
|       from small pool |    436 MiB |    438 MiB |   1340 MiB |    904 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1070 MiB |   4610 MiB |  79130 GiB |  79129 GiB |
|       from large pool |   1030 MiB |   4603 MiB |  78631 GiB |  78630 GiB |
|       from small pool |     40 MiB |     41 MiB |    498 GiB |    498 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7296    |    7299    |    5179 K  |    5172 K  |
|       from large pool |     933    |     934    |    2516 K  |    2515 K  |
|       from small pool |    6363    |    6366    |    2663 K  |    2656 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7296    |    7299    |    5179 K  |    5172 K  |
|       from large pool |     933    |     934    |    2516 K  |    2515 K  |
|       from small pool |    6363    |    6366    |    2663 K  |    2656 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     690    |     691    |    2130    |    1440    |
|       from large pool |     472    |     472    |    1460    |     988    |
|       from small pool |     218    |     219    |     670    |     452    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     454    |     454    |    2903 K  |    2902 K  |
|       from large pool |      63    |      63    |    1651 K  |    1651 K  |
|       from small pool |     391    |     391    |    1251 K  |    1251 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:15:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:15:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:15:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:15:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-19 03:16:03] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.95 GiB is allocated by PyTorch, and 1.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:16:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 15        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78678 MiB |  78798 MiB |  79117 GiB |  79040 GiB |
|       from large pool |  78593 MiB |  78713 MiB |  78673 GiB |  78596 GiB |
|       from small pool |     85 MiB |     86 MiB |    443 GiB |    443 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78678 MiB |  78798 MiB |  79117 GiB |  79040 GiB |
|       from large pool |  78593 MiB |  78713 MiB |  78673 GiB |  78596 GiB |
|       from small pool |     85 MiB |     86 MiB |    443 GiB |    443 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78642 MiB |  78762 MiB |  78944 GiB |  78868 GiB |
|       from large pool |  78558 MiB |  78677 MiB |  78501 GiB |  78425 GiB |
|       from small pool |     84 MiB |     85 MiB |    443 GiB |    442 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB | 356340 MiB | 275836 MiB |
|       from large pool |  80416 MiB |  80416 MiB | 354934 MiB | 274518 MiB |
|       from small pool |     88 MiB |    436 MiB |   1406 MiB |   1318 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1705 MiB |   8357 MiB |  80009 GiB |  80008 GiB |
|       from large pool |   1702 MiB |   8349 MiB |  79504 GiB |  79502 GiB |
|       from small pool |      2 MiB |     29 MiB |    505 GiB |    505 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1873    |    1876    |    5242 K  |    5240 K  |
|       from large pool |     462    |     464    |    2546 K  |    2545 K  |
|       from small pool |    1411    |    1414    |    2696 K  |    2694 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1873    |    1876    |    5242 K  |    5240 K  |
|       from large pool |     462    |     464    |    2546 K  |    2545 K  |
|       from small pool |    1411    |    1414    |    2696 K  |    2694 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     221    |     689    |    2198    |    1977    |
|       from large pool |     177    |     471    |    1495    |    1318    |
|       from small pool |      44    |     218    |     703    |     659    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     167    |     168    |    2940 K  |    2940 K  |
|       from large pool |     120    |     125    |    1671 K  |    1671 K  |
|       from small pool |      47    |      59    |    1269 K  |    1269 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:16:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:16:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:16:03] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:16:03] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:16:05]    INFO >> epoch 002:   1018 / 1539 loss=3.751, wps=4048.1, ups=5.88, wpb=687.9, bsz=687.9, num_updates=2550, lr=0.001, gnorm=5.649, clip=0, train_wall=7, gb_free=72.9, wall=450 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:16:13]    INFO >> epoch 002:   1068 / 1539 loss=3.723, wps=5035.3, ups=6.59, wpb=763.7, bsz=763.7, num_updates=2600, lr=0.001, gnorm=4.314, clip=0, train_wall=7, gb_free=73.5, wall=458 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:16:22]    INFO >> epoch 002:   1118 / 1539 loss=3.61, wps=5076.9, ups=6.41, wpb=791.9, bsz=791.9, num_updates=2650, lr=0.001, gnorm=4.824, clip=0, train_wall=7, gb_free=69.2, wall=466 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:16:30]    INFO >> epoch 002:   1168 / 1539 loss=3.707, wps=5072.8, ups=6.59, wpb=770.1, bsz=770.1, num_updates=2700, lr=0.001, gnorm=4.69, clip=0, train_wall=7, gb_free=72.3, wall=473 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:16:37]    INFO >> epoch 002:   1218 / 1539 loss=3.885, wps=4742.9, ups=6.68, wpb=710, bsz=710, num_updates=2750, lr=0.001, gnorm=4.498, clip=0, train_wall=7, gb_free=70.9, wall=481 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:16:40] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 121.25 MiB is free. Including non-PyTorch memory, this process has 79.00 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 5.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:16:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72821 MiB |  75425 MiB |  86354 GiB |  86283 GiB |
|       from large pool |  72808 MiB |  75412 MiB |  85870 GiB |  85799 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72821 MiB |  75425 MiB |  86354 GiB |  86283 GiB |
|       from large pool |  72808 MiB |  75412 MiB |  85870 GiB |  85799 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB |  86167 GiB |  86096 GiB |
|       from large pool |  72788 MiB |  75391 MiB |  85684 GiB |  85613 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80384 MiB |  80384 MiB | 387082 MiB | 306698 MiB |
|       from large pool |  80360 MiB |  80360 MiB | 385538 MiB | 305178 MiB |
|       from small pool |     24 MiB |    226 MiB |   1544 MiB |   1520 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5660 MiB |  11732 MiB |  87446 GiB |  87441 GiB |
|       from large pool |   5649 MiB |  11720 MiB |  86894 GiB |  86889 GiB |
|       from small pool |     11 MiB |     25 MiB |    551 GiB |    551 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |    5719 K  |    5718 K  |
|       from large pool |     312    |     320    |    2781 K  |    2781 K  |
|       from small pool |     291    |     356    |    2937 K  |    2937 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |    5719 K  |    5718 K  |
|       from large pool |     312    |     320    |    2781 K  |    2781 K  |
|       from small pool |     291    |     356    |    2937 K  |    2937 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     120    |     229    |    2295    |    2175    |
|       from large pool |     108    |     116    |    1523    |    1415    |
|       from small pool |      12    |     113    |     772    |     760    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     118    |     118    |    3207 K  |    3207 K  |
|       from large pool |      91    |      91    |    1823 K  |    1823 K  |
|       from small pool |      27    |      54    |    1383 K  |    1383 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:16:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:16:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:16:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:16:40] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:16:45]    INFO >> epoch 002:   1269 / 1539 loss=3.647, wps=4756.8, ups=6.22, wpb=765.3, bsz=765.3, num_updates=2800, lr=0.001, gnorm=4.629, clip=0, train_wall=7, gb_free=70.5, wall=489 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:16:54]    INFO >> epoch 002:   1319 / 1539 loss=3.611, wps=4674.3, ups=7.07, wpb=661.5, bsz=661.5, num_updates=2850, lr=0.001, gnorm=5.123, clip=2, train_wall=7, gb_free=75, wall=496 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:17:01]    INFO >> epoch 002:   1369 / 1539 loss=3.811, wps=5001.9, ups=6.87, wpb=728.5, bsz=728.5, num_updates=2900, lr=0.001, gnorm=3.985, clip=0, train_wall=7, gb_free=70.5, wall=503 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:17:08]    INFO >> epoch 002:   1419 / 1539 loss=3.652, wps=4407.6, ups=6.74, wpb=653.6, bsz=653.6, num_updates=2950, lr=0.001, gnorm=4.684, clip=0, train_wall=7, gb_free=64.8, wall=511 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:17:18]    INFO >> epoch 002:   1469 / 1539 loss=3.706, wps=3736.5, ups=5.32, wpb=702.5, bsz=702.5, num_updates=3000, lr=0.001, gnorm=4.555, clip=0, train_wall=9, gb_free=70.3, wall=520 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:17:27]    INFO >> epoch 002:   1519 / 1539 loss=3.742, wps=4442.3, ups=6.6, wpb=673.3, bsz=673.3, num_updates=3050, lr=0.001, gnorm=4.148, clip=0, train_wall=7, gb_free=74.2, wall=528 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:17:30]    INFO >> epoch 002 | loss 3.785 | wps 4476.7 | ups 6.28 | wpb 712.7 | bsz 712.7 | num_updates 3070 | lr 0.001 | gnorm 5.114 | clip 0.1 | train_wall 215 | gb_free 72.4 | wall 531 (progress_bar.py:267, print())[0m
[33m[2025-11-19 03:17:30] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:17:43]    INFO >> epoch 002 | valid on 'valid' subset | loss 3.763 | wps 11644.6 | wpb 5412.5 | bsz 5412.5 | num_updates 3070 | best_loss 3.943 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 03:17:43]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:17:43]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/checkpoints/checkpoint_last.pt (epoch 2 @ 3070 updates, score 3.763) (writing took 0.012438 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 03:17:43] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:17:47]    INFO >> epoch 003:     30 / 1539 loss=3.709, wps=1643.6, ups=2.41, wpb=681.1, bsz=681.1, num_updates=3100, lr=0.00098, gnorm=4.613, clip=0, train_wall=7, gb_free=70.7, wall=548 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:17:55]    INFO >> epoch 003:     80 / 1539 loss=3.708, wps=5272.9, ups=6.82, wpb=772.8, bsz=772.8, num_updates=3150, lr=0.00098, gnorm=4.251, clip=0, train_wall=7, gb_free=73.4, wall=556 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:17:59] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 78.04 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 2.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:17:59] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76326 MiB |  78600 MiB | 102451 GiB | 102377 GiB |
|       from large pool |  76307 MiB |  78583 MiB | 101871 GiB | 101796 GiB |
|       from small pool |     18 MiB |     19 MiB |    580 GiB |    580 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76326 MiB |  78600 MiB | 102451 GiB | 102377 GiB |
|       from large pool |  76307 MiB |  78583 MiB | 101871 GiB | 101796 GiB |
|       from small pool |     18 MiB |     19 MiB |    580 GiB |    580 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 102235 GiB | 102160 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 101655 GiB | 101581 GiB |
|       from small pool |     18 MiB |     19 MiB |    579 GiB |    579 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79400 MiB |  79400 MiB | 483464 MiB | 404064 MiB |
|       from large pool |  79376 MiB |  79376 MiB | 481770 MiB | 402394 MiB |
|       from small pool |     24 MiB |     74 MiB |   1694 MiB |   1670 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3073 MiB |   4962 MiB | 103116 GiB | 103113 GiB |
|       from large pool |   3068 MiB |   4948 MiB | 102459 GiB | 102456 GiB |
|       from small pool |      5 MiB |     27 MiB |    657 GiB |    657 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |    6746 K  |    6745 K  |
|       from large pool |     340    |     346    |    3209 K  |    3208 K  |
|       from small pool |     300    |     356    |    3536 K  |    3536 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |    6746 K  |    6745 K  |
|       from large pool |     340    |     346    |    3209 K  |    3208 K  |
|       from small pool |     300    |     356    |    3536 K  |    3536 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     106    |     118    |    2420    |    2314    |
|       from large pool |      94    |      94    |    1573    |    1479    |
|       from small pool |      12    |      37    |     847    |     835    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     110    |     112    |    3792 K  |    3792 K  |
|       from large pool |      85    |      87    |    2095 K  |    2095 K  |
|       from small pool |      25    |      57    |    1696 K  |    1696 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:17:59] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:17:59] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:17:59] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:17:59] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:18:05]    INFO >> epoch 003:    131 / 1539 loss=3.565, wps=4668.2, ups=5.59, wpb=835, bsz=835, num_updates=3200, lr=0.00098, gnorm=4.743, clip=0, train_wall=7, gb_free=71.9, wall=565 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:18:12]    INFO >> epoch 003:    181 / 1539 loss=3.813, wps=4710.2, ups=7.17, wpb=656.8, bsz=656.8, num_updates=3250, lr=0.00098, gnorm=4.25, clip=0, train_wall=7, gb_free=73, wall=572 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:18:17] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 153.25 MiB is free. Including non-PyTorch memory, this process has 78.97 GiB memory in use. Of the allocated memory 75.67 GiB is allocated by PyTorch, and 2.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:18:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 23        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76958 MiB |  77481 MiB | 105988 GiB | 105912 GiB |
|       from large pool |  76945 MiB |  77468 MiB | 105386 GiB | 105311 GiB |
|       from small pool |     12 MiB |     21 MiB |    601 GiB |    601 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76958 MiB |  77481 MiB | 105988 GiB | 105912 GiB |
|       from large pool |  76945 MiB |  77468 MiB | 105386 GiB | 105311 GiB |
|       from small pool |     12 MiB |     21 MiB |    601 GiB |    601 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 105764 GiB | 105689 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 105164 GiB | 105089 GiB |
|       from small pool |     12 MiB |     21 MiB |    600 GiB |    600 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80352 MiB |  80352 MiB | 489864 MiB | 409512 MiB |
|       from large pool |  80326 MiB |  80326 MiB | 487984 MiB | 407658 MiB |
|       from small pool |     26 MiB |    210 MiB |   1880 MiB |   1854 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3393 MiB |   8163 MiB | 107069 GiB | 107066 GiB |
|       from large pool |   3380 MiB |   8148 MiB | 106388 GiB | 106385 GiB |
|       from small pool |     13 MiB |     27 MiB |    681 GiB |    681 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |    6985 K  |    6984 K  |
|       from large pool |     315    |     322    |    3323 K  |    3323 K  |
|       from small pool |     291    |     356    |    3662 K  |    3661 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |    6985 K  |    6984 K  |
|       from large pool |     315    |     322    |    3323 K  |    3323 K  |
|       from small pool |     291    |     356    |    3662 K  |    3661 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     100    |     199    |    2520    |    2420    |
|       from large pool |      87    |      94    |    1580    |    1493    |
|       from small pool |      13    |     105    |     940    |     927    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      95    |      95    |    3925 K  |    3925 K  |
|       from large pool |      69    |      69    |    2166 K  |    2166 K  |
|       from small pool |      26    |      58    |    1758 K  |    1758 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:18:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:18:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:18:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:18:17] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:18:20]    INFO >> epoch 003:    232 / 1539 loss=3.522, wps=4583.4, ups=5.97, wpb=767.2, bsz=767.2, num_updates=3300, lr=0.00098, gnorm=4.31, clip=0, train_wall=7, gb_free=74, wall=580 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:18:28]    INFO >> epoch 003:    282 / 1539 loss=3.529, wps=5127.7, ups=6.82, wpb=751.9, bsz=751.9, num_updates=3350, lr=0.00098, gnorm=4.183, clip=0, train_wall=7, gb_free=70.6, wall=587 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:18:37]    INFO >> epoch 003:    332 / 1539 loss=3.702, wps=4976.4, ups=6.3, wpb=790.4, bsz=790.4, num_updates=3400, lr=0.00098, gnorm=4.558, clip=0, train_wall=8, gb_free=73.8, wall=595 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:18:45]    INFO >> epoch 003:    382 / 1539 loss=3.675, wps=4406.8, ups=6.4, wpb=688.4, bsz=688.4, num_updates=3450, lr=0.00098, gnorm=4.157, clip=0, train_wall=7, gb_free=72.5, wall=603 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:18:52]    INFO >> epoch 003:    432 / 1539 loss=3.67, wps=4529.3, ups=6.89, wpb=657.8, bsz=657.8, num_updates=3500, lr=0.00098, gnorm=4.137, clip=0, train_wall=7, gb_free=66.2, wall=610 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:19:00]    INFO >> epoch 003:    482 / 1539 loss=3.576, wps=4925.3, ups=6.53, wpb=754, bsz=754, num_updates=3550, lr=0.00098, gnorm=4.614, clip=0, train_wall=7, gb_free=73.1, wall=618 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:19:08]    INFO >> epoch 003:    532 / 1539 loss=3.441, wps=5201.1, ups=6.75, wpb=770.5, bsz=770.5, num_updates=3600, lr=0.00098, gnorm=4.631, clip=0, train_wall=7, gb_free=73.8, wall=625 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:19:16]    INFO >> epoch 003:    582 / 1539 loss=3.745, wps=4806.8, ups=6.67, wpb=720.1, bsz=720.1, num_updates=3650, lr=0.00098, gnorm=3.839, clip=0, train_wall=7, gb_free=71.6, wall=633 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:19:23]    INFO >> epoch 003:    632 / 1539 loss=3.658, wps=4878.7, ups=7.23, wpb=674.5, bsz=674.5, num_updates=3700, lr=0.00098, gnorm=4.449, clip=0, train_wall=7, gb_free=66.5, wall=640 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:19:30]    INFO >> epoch 003:    682 / 1539 loss=3.676, wps=5024.8, ups=6.64, wpb=756.5, bsz=756.5, num_updates=3750, lr=0.00098, gnorm=4.168, clip=0, train_wall=7, gb_free=75.1, wall=647 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:19:40]    INFO >> epoch 003:    732 / 1539 loss=3.429, wps=4861, ups=6, wpb=810.3, bsz=810.3, num_updates=3800, lr=0.00098, gnorm=4.746, clip=0, train_wall=8, gb_free=73.9, wall=656 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:19:48]    INFO >> epoch 003:    782 / 1539 loss=3.479, wps=5181.5, ups=6.52, wpb=795, bsz=795, num_updates=3850, lr=0.00098, gnorm=3.68, clip=0, train_wall=7, gb_free=73.7, wall=663 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:19:49] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 29.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.96 GiB is allocated by PyTorch, and 1.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:19:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78744 MiB |  78804 MiB | 123508 GiB | 123431 GiB |
|       from large pool |  78658 MiB |  78718 MiB | 122809 GiB | 122733 GiB |
|       from small pool |     85 MiB |     86 MiB |    698 GiB |    697 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78744 MiB |  78804 MiB | 123508 GiB | 123431 GiB |
|       from large pool |  78658 MiB |  78718 MiB | 122809 GiB | 122733 GiB |
|       from small pool |     85 MiB |     86 MiB |    698 GiB |    697 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78703 MiB |  78762 MiB | 123247 GiB | 123170 GiB |
|       from large pool |  78617 MiB |  78677 MiB | 122550 GiB | 122473 GiB |
|       from small pool |     85 MiB |     86 MiB |    697 GiB |    696 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80476 MiB |  80476 MiB | 517426 MiB | 436950 MiB |
|       from large pool |  80386 MiB |  80386 MiB | 515324 MiB | 434938 MiB |
|       from small pool |     90 MiB |    246 MiB |   2102 MiB |   2012 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1671 MiB |   7867 MiB | 124793 GiB | 124791 GiB |
|       from large pool |   1667 MiB |   7857 MiB | 124000 GiB | 123998 GiB |
|       from small pool |      4 MiB |     27 MiB |    792 GiB |    792 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1884    |    1887    |    8145 K  |    8143 K  |
|       from large pool |     463    |     464    |    3900 K  |    3900 K  |
|       from small pool |    1421    |    1424    |    4244 K  |    4243 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1884    |    1887    |    8145 K  |    8143 K  |
|       from large pool |     463    |     464    |    3900 K  |    3900 K  |
|       from small pool |    1421    |    1424    |    4244 K  |    4243 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     225    |     300    |    2734    |    2509    |
|       from large pool |     180    |     180    |    1683    |    1503    |
|       from small pool |      45    |     123    |    1051    |    1006    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     169    |     169    |    4572 K  |    4572 K  |
|       from large pool |     121    |     124    |    2540 K  |    2540 K  |
|       from small pool |      48    |      51    |    2031 K  |    2031 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:19:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:19:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:19:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:19:49] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:19:56]    INFO >> epoch 003:    833 / 1539 loss=3.599, wps=4663.2, ups=6.34, wpb=736, bsz=736, num_updates=3900, lr=0.00098, gnorm=4.712, clip=0, train_wall=7, gb_free=73, wall=671 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:20:03]    INFO >> epoch 003:    883 / 1539 loss=3.676, wps=4772.9, ups=6.66, wpb=717, bsz=717, num_updates=3950, lr=0.00098, gnorm=4.002, clip=0, train_wall=7, gb_free=72.5, wall=679 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:20:12]    INFO >> epoch 003:    933 / 1539 loss=3.563, wps=4782.8, ups=6.56, wpb=729.5, bsz=729.5, num_updates=4000, lr=0.00098, gnorm=4.315, clip=0, train_wall=7, gb_free=67.3, wall=686 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:20:21]    INFO >> epoch 003:    983 / 1539 loss=3.677, wps=3948.5, ups=5.77, wpb=684.3, bsz=684.3, num_updates=4050, lr=0.00098, gnorm=4.648, clip=0, train_wall=8, gb_free=71.5, wall=695 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:20:28]    INFO >> epoch 003:   1033 / 1539 loss=3.683, wps=4414.1, ups=6.94, wpb=636.3, bsz=636.3, num_updates=4100, lr=0.00098, gnorm=4.008, clip=0, train_wall=7, gb_free=68.3, wall=702 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:20:35]    INFO >> epoch 003:   1083 / 1539 loss=3.647, wps=4959.5, ups=7.36, wpb=673.9, bsz=673.9, num_updates=4150, lr=0.00098, gnorm=4.031, clip=0, train_wall=6, gb_free=72.9, wall=709 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:20:42] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 39.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.52 GiB is allocated by PyTorch, and 2.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:20:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78301 MiB |  78361 MiB | 133746 GiB | 133670 GiB |
|       from large pool |  77916 MiB |  77976 MiB | 132993 GiB | 132917 GiB |
|       from small pool |    385 MiB |    386 MiB |    753 GiB |    752 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78301 MiB |  78361 MiB | 133746 GiB | 133670 GiB |
|       from large pool |  77916 MiB |  77976 MiB | 132993 GiB | 132917 GiB |
|       from small pool |    385 MiB |    386 MiB |    753 GiB |    752 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78077 MiB |  78137 MiB | 133463 GiB | 133387 GiB |
|       from large pool |  77694 MiB |  77753 MiB | 132711 GiB | 132635 GiB |
|       from small pool |    383 MiB |    384 MiB |    752 GiB |    751 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80466 MiB |  80504 MiB | 539302 MiB | 458836 MiB |
|       from large pool |  80042 MiB |  80326 MiB | 536864 MiB | 456822 MiB |
|       from small pool |    424 MiB |    426 MiB |   2438 MiB |   2014 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2104 MiB |   6644 MiB | 134070 GiB | 134068 GiB |
|       from large pool |   2065 MiB |   6637 MiB | 133214 GiB | 133212 GiB |
|       from small pool |     38 MiB |     40 MiB |    856 GiB |    856 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7109    |    7112    |    8817 K  |    8810 K  |
|       from large pool |     916    |     917    |    4239 K  |    4238 K  |
|       from small pool |    6193    |    6196    |    4578 K  |    4572 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7109    |    7112    |    8817 K  |    8810 K  |
|       from large pool |     916    |     917    |    4239 K  |    4238 K  |
|       from small pool |    6193    |    6196    |    4578 K  |    4572 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     742    |     743    |    3261    |    2519    |
|       from large pool |     530    |     530    |    2042    |    1512    |
|       from small pool |     212    |     213    |    1219    |    1007    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     510    |     511    |    4950 K  |    4949 K  |
|       from large pool |     132    |     132    |    2765 K  |    2765 K  |
|       from small pool |     378    |     379    |    2185 K  |    2184 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:20:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:20:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:20:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:20:42] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:20:43]    INFO >> epoch 003:   1134 / 1539 loss=3.478, wps=4379.3, ups=6.4, wpb=684.8, bsz=684.8, num_updates=4200, lr=0.00098, gnorm=4.479, clip=0, train_wall=6, gb_free=73, wall=717 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:20:51]    INFO >> epoch 003:   1184 / 1539 loss=3.585, wps=4520.7, ups=6.71, wpb=673.6, bsz=673.6, num_updates=4250, lr=0.00098, gnorm=4.501, clip=0, train_wall=7, gb_free=73.5, wall=724 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:20:59]    INFO >> epoch 003:   1234 / 1539 loss=3.581, wps=4657.2, ups=6.49, wpb=718.1, bsz=718.1, num_updates=4300, lr=0.00098, gnorm=4.231, clip=0, train_wall=7, gb_free=70.5, wall=732 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:21:06]    INFO >> epoch 003:   1284 / 1539 loss=3.523, wps=4535.6, ups=7.32, wpb=619.7, bsz=619.7, num_updates=4350, lr=0.00098, gnorm=3.959, clip=0, train_wall=6, gb_free=73.5, wall=739 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:21:13]    INFO >> epoch 003:   1334 / 1539 loss=3.545, wps=5017, ups=7.09, wpb=708, bsz=708, num_updates=4400, lr=0.00098, gnorm=3.759, clip=0, train_wall=7, gb_free=72.1, wall=746 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:21:21]    INFO >> epoch 003:   1384 / 1539 loss=3.476, wps=4942.3, ups=7.14, wpb=692.7, bsz=692.7, num_updates=4450, lr=0.00098, gnorm=4.512, clip=0, train_wall=7, gb_free=74.2, wall=753 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:21:28]    INFO >> epoch 003:   1434 / 1539 loss=3.615, wps=4769.5, ups=7.32, wpb=651.7, bsz=651.7, num_updates=4500, lr=0.00098, gnorm=4.344, clip=0, train_wall=6, gb_free=72.1, wall=760 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:21:36]    INFO >> epoch 003:   1484 / 1539 loss=3.651, wps=4437, ups=6.73, wpb=658.9, bsz=658.9, num_updates=4550, lr=0.00098, gnorm=4.338, clip=0, train_wall=7, gb_free=72.4, wall=767 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:21:43]    INFO >> epoch 003:   1534 / 1539 loss=3.663, wps=4665.2, ups=7.04, wpb=662.9, bsz=662.9, num_updates=4600, lr=0.00098, gnorm=4.102, clip=0, train_wall=7, gb_free=72, wall=774 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:21:43]    INFO >> epoch 003 | loss 3.603 | wps 4482 | ups 6.29 | wpb 712.7 | bsz 712.7 | num_updates 4605 | lr 0.00098 | gnorm 4.29 | clip 0 | train_wall 214 | gb_free 74.4 | wall 775 (progress_bar.py:267, print())[0m
[33m[2025-11-19 03:21:43] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:21:57]    INFO >> epoch 003 | valid on 'valid' subset | loss 3.756 | wps 11814.1 | wpb 5412.5 | bsz 5412.5 | num_updates 4605 | best_loss 3.943 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 03:21:58]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:21:58]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/checkpoints/checkpoint_last.pt (epoch 3 @ 4605 updates, score 3.756) (writing took 0.013294 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 03:21:58] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:22:05]    INFO >> epoch 004:     45 / 1539 loss=3.417, wps=1820.5, ups=2.42, wpb=751.7, bsz=751.7, num_updates=4650, lr=0.000941, gnorm=3.976, clip=0, train_wall=7, gb_free=72.7, wall=795 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:22:12]    INFO >> epoch 004:     95 / 1539 loss=3.484, wps=5028.4, ups=6.65, wpb=756, bsz=756, num_updates=4700, lr=0.000941, gnorm=4.48, clip=0, train_wall=7, gb_free=67.8, wall=803 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:22:22]    INFO >> epoch 004:    145 / 1539 loss=3.56, wps=5372.4, ups=5.12, wpb=1048.6, bsz=1048.6, num_updates=4750, lr=0.000941, gnorm=4.807, clip=0, train_wall=9, gb_free=75.1, wall=812 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:22:30]    INFO >> epoch 004:    195 / 1539 loss=3.531, wps=5044.7, ups=7.19, wpb=701.3, bsz=701.3, num_updates=4800, lr=0.000941, gnorm=3.613, clip=0, train_wall=7, gb_free=71.9, wall=819 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:22:38]    INFO >> epoch 004:    245 / 1539 loss=3.582, wps=5064.9, ups=6.81, wpb=743.6, bsz=743.6, num_updates=4850, lr=0.000941, gnorm=4.288, clip=0, train_wall=7, gb_free=73.5, wall=827 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:22:45]    INFO >> epoch 004:    295 / 1539 loss=3.536, wps=4597.4, ups=7.1, wpb=647.4, bsz=647.4, num_updates=4900, lr=0.000941, gnorm=3.768, clip=0, train_wall=7, gb_free=74.6, wall=834 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:22:52]    INFO >> epoch 004:    345 / 1539 loss=3.55, wps=4927.1, ups=7.17, wpb=687.4, bsz=687.4, num_updates=4950, lr=0.000941, gnorm=3.668, clip=0, train_wall=7, gb_free=69.9, wall=841 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:23:00]    INFO >> epoch 004:    395 / 1539 loss=3.365, wps=5152.5, ups=6.77, wpb=761.1, bsz=761.1, num_updates=5000, lr=0.000941, gnorm=4.542, clip=0, train_wall=7, gb_free=67.8, wall=848 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:23:08]    INFO >> epoch 004:    445 / 1539 loss=3.677, wps=3977.2, ups=6.31, wpb=630.5, bsz=630.5, num_updates=5050, lr=0.000941, gnorm=3.987, clip=0, train_wall=7, gb_free=71.4, wall=856 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:23:15]    INFO >> epoch 004:    495 / 1539 loss=3.583, wps=5003.3, ups=6.95, wpb=720.1, bsz=720.1, num_updates=5100, lr=0.000941, gnorm=3.297, clip=0, train_wall=7, gb_free=74, wall=863 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:23:23]    INFO >> epoch 004:    545 / 1539 loss=3.575, wps=4785.3, ups=6.82, wpb=701.6, bsz=701.6, num_updates=5150, lr=0.000941, gnorm=4.541, clip=0, train_wall=7, gb_free=73.5, wall=870 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:23:30]    INFO >> epoch 004:    595 / 1539 loss=3.531, wps=4448.9, ups=7.06, wpb=629.8, bsz=629.8, num_updates=5200, lr=0.000941, gnorm=4.021, clip=0, train_wall=7, gb_free=71, wall=878 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:23:31] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 13.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 1.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:23:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79173 MiB |  79233 MiB | 167559 GiB | 167482 GiB |
|       from large pool |  79083 MiB |  79143 MiB | 166611 GiB | 166534 GiB |
|       from small pool |     89 MiB |     91 MiB |    948 GiB |    948 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79173 MiB |  79233 MiB | 167559 GiB | 167482 GiB |
|       from large pool |  79083 MiB |  79143 MiB | 166611 GiB | 166534 GiB |
|       from small pool |     89 MiB |     91 MiB |    948 GiB |    948 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79123 MiB |  79183 MiB | 167207 GiB | 167130 GiB |
|       from large pool |  79034 MiB |  79093 MiB | 166260 GiB | 166183 GiB |
|       from small pool |     89 MiB |     90 MiB |    946 GiB |    946 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80492 MiB |  80492 MiB | 564646 MiB | 484154 MiB |
|       from large pool |  80398 MiB |  80398 MiB | 562140 MiB | 481742 MiB |
|       from small pool |     94 MiB |     94 MiB |   2506 MiB |   2412 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1258 MiB |   7162 MiB | 164244 GiB | 164243 GiB |
|       from large pool |   1254 MiB |   7152 MiB | 163170 GiB | 163169 GiB |
|       from small pool |      4 MiB |     27 MiB |   1074 GiB |   1074 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1961    |    1964    |   11044 K  |   11042 K  |
|       from large pool |     470    |     471    |    5271 K  |    5271 K  |
|       from small pool |    1491    |    1494    |    5772 K  |    5770 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1961    |    1964    |   11044 K  |   11042 K  |
|       from large pool |     470    |     471    |    5271 K  |    5271 K  |
|       from small pool |    1491    |    1494    |    5772 K  |    5770 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     223    |     223    |    3348    |    3125    |
|       from large pool |     176    |     176    |    2095    |    1919    |
|       from small pool |      47    |      47    |    1253    |    1206    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     140    |     140    |    6216 K  |    6216 K  |
|       from large pool |      92    |      96    |    3444 K  |    3444 K  |
|       from small pool |      48    |      58    |    2772 K  |    2772 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:23:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:23:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:23:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:23:31] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:23:39]    INFO >> epoch 004:    646 / 1539 loss=3.546, wps=4439.2, ups=6.61, wpb=671.1, bsz=671.1, num_updates=5250, lr=0.000941, gnorm=4.04, clip=0, train_wall=7, gb_free=68.7, wall=885 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:23:47] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 983.25 MiB is free. Including non-PyTorch memory, this process has 78.16 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:23:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76323 MiB |  78596 MiB | 170178 GiB | 170104 GiB |
|       from large pool |  76304 MiB |  78579 MiB | 169217 GiB | 169142 GiB |
|       from small pool |     18 MiB |     24 MiB |    961 GiB |    961 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76323 MiB |  78596 MiB | 170178 GiB | 170104 GiB |
|       from large pool |  76304 MiB |  78579 MiB | 169217 GiB | 169142 GiB |
|       from small pool |     18 MiB |     24 MiB |    961 GiB |    961 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 169821 GiB | 169746 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 168861 GiB | 168786 GiB |
|       from small pool |     18 MiB |     24 MiB |    960 GiB |    959 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79522 MiB |  80432 MiB | 637044 MiB | 557522 MiB |
|       from large pool |  79496 MiB |  80338 MiB | 634538 MiB | 555042 MiB |
|       from small pool |     26 MiB |     94 MiB |   2506 MiB |   2480 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3198 MiB |   4143 MiB | 166741 GiB | 166737 GiB |
|       from large pool |   3191 MiB |   4135 MiB | 165651 GiB | 165648 GiB |
|       from small pool |      7 MiB |     23 MiB |   1089 GiB |   1089 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   11213 K  |   11212 K  |
|       from large pool |     340    |     346    |    5361 K  |    5361 K  |
|       from small pool |     300    |     356    |    5852 K  |    5851 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   11213 K  |   11212 K  |
|       from large pool |     340    |     346    |    5361 K  |    5361 K  |
|       from small pool |     300    |     356    |    5852 K  |    5851 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     107    |     222    |    3403    |    3296    |
|       from large pool |      94    |     175    |    2150    |    2056    |
|       from small pool |      13    |      47    |    1253    |    1240    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     126    |     128    |    6309 K  |    6309 K  |
|       from large pool |      98    |     100    |    3503 K  |    3503 K  |
|       from small pool |      28    |      51    |    2805 K  |    2805 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:23:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:23:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:23:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:23:47] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:23:48]    INFO >> epoch 004:    697 / 1539 loss=3.525, wps=3460.9, ups=5.44, wpb=636.6, bsz=636.6, num_updates=5300, lr=0.000941, gnorm=3.946, clip=0, train_wall=7, gb_free=67.8, wall=894 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:23:56]    INFO >> epoch 004:    747 / 1539 loss=3.617, wps=4059.1, ups=6.03, wpb=673.1, bsz=673.1, num_updates=5350, lr=0.000941, gnorm=3.241, clip=0, train_wall=8, gb_free=72.9, wall=903 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:24:03]    INFO >> epoch 004:    797 / 1539 loss=3.44, wps=4910.3, ups=6.91, wpb=710.1, bsz=710.1, num_updates=5400, lr=0.000941, gnorm=3.768, clip=0, train_wall=7, gb_free=72.3, wall=910 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:24:12]    INFO >> epoch 004:    847 / 1539 loss=3.497, wps=5157.8, ups=6.9, wpb=747.5, bsz=747.5, num_updates=5450, lr=0.000941, gnorm=3.789, clip=0, train_wall=7, gb_free=63.6, wall=917 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:24:20]    INFO >> epoch 004:    897 / 1539 loss=3.565, wps=4893, ups=6.29, wpb=778.3, bsz=778.3, num_updates=5500, lr=0.000941, gnorm=4.232, clip=0, train_wall=8, gb_free=70.2, wall=925 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:24:27]    INFO >> epoch 004:    947 / 1539 loss=3.459, wps=4581, ups=7.08, wpb=646.7, bsz=646.7, num_updates=5550, lr=0.000941, gnorm=3.836, clip=0, train_wall=7, gb_free=67.5, wall=932 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:24:35]    INFO >> epoch 004:    997 / 1539 loss=3.678, wps=4826.2, ups=6.31, wpb=764.9, bsz=764.9, num_updates=5600, lr=0.000941, gnorm=3.806, clip=0, train_wall=7, gb_free=75.5, wall=940 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:24:43]    INFO >> epoch 004:   1047 / 1539 loss=3.553, wps=4396.1, ups=6.99, wpb=628.6, bsz=628.6, num_updates=5650, lr=0.000941, gnorm=3.432, clip=0, train_wall=7, gb_free=72.5, wall=947 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:24:51]    INFO >> epoch 004:   1097 / 1539 loss=3.53, wps=5066.4, ups=6.94, wpb=729.6, bsz=729.6, num_updates=5700, lr=0.000941, gnorm=3.908, clip=0, train_wall=7, gb_free=71.1, wall=954 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:24:58]    INFO >> epoch 004:   1147 / 1539 loss=3.519, wps=4671.8, ups=6.91, wpb=676.4, bsz=676.4, num_updates=5750, lr=0.000941, gnorm=3.961, clip=0, train_wall=7, gb_free=56.5, wall=962 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:25:05]    INFO >> epoch 004:   1197 / 1539 loss=3.585, wps=4991.4, ups=6.61, wpb=754.7, bsz=754.7, num_updates=5800, lr=0.000941, gnorm=4.398, clip=0, train_wall=7, gb_free=75.6, wall=969 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:25:14]    INFO >> epoch 004:   1247 / 1539 loss=3.467, wps=4610.2, ups=7.34, wpb=627.8, bsz=627.8, num_updates=5850, lr=0.000941, gnorm=3.165, clip=0, train_wall=6, gb_free=75.6, wall=976 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:25:21]    INFO >> epoch 004:   1297 / 1539 loss=3.565, wps=4224.1, ups=6.55, wpb=644.6, bsz=644.6, num_updates=5900, lr=0.000941, gnorm=3.89, clip=0, train_wall=7, gb_free=74.6, wall=984 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:25:29]    INFO >> epoch 004:   1347 / 1539 loss=3.538, wps=5335.5, ups=6.66, wpb=801.1, bsz=801.1, num_updates=5950, lr=0.000941, gnorm=4.047, clip=0, train_wall=7, gb_free=66.8, wall=991 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:25:36] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 2.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:25:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 34        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75053 MiB |  75996 MiB | 190873 GiB | 190800 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 189802 GiB | 189729 GiB |
|       from small pool |     12 MiB |     13 MiB |   1070 GiB |   1070 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75053 MiB |  75996 MiB | 190873 GiB | 190800 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 189802 GiB | 189729 GiB |
|       from small pool |     12 MiB |     13 MiB |   1070 GiB |   1070 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 190472 GiB | 190399 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 189403 GiB | 189330 GiB |
|       from small pool |     12 MiB |     13 MiB |   1069 GiB |   1069 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78634 MiB |  79742 MiB | 645400 MiB | 566766 MiB |
|       from large pool |  78608 MiB |  79496 MiB | 642674 MiB | 564066 MiB |
|       from small pool |     26 MiB |    246 MiB |   2726 MiB |   2700 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3580 MiB |   7454 MiB | 189478 GiB | 189475 GiB |
|       from large pool |   3567 MiB |   7440 MiB | 188264 GiB | 188260 GiB |
|       from small pool |     13 MiB |     21 MiB |   1214 GiB |   1214 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   12564 K  |   12563 K  |
|       from large pool |     314    |     322    |    6052 K  |    6052 K  |
|       from small pool |     291    |     336    |    6511 K  |    6510 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   12564 K  |   12563 K  |
|       from large pool |     314    |     322    |    6052 K  |    6052 K  |
|       from small pool |     291    |     336    |    6511 K  |    6510 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     217    |    3521    |    3430    |
|       from large pool |      78    |      94    |    2158    |    2080    |
|       from small pool |      13    |     123    |    1363    |    1350    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      90    |    7038 K  |    7038 K  |
|       from large pool |      60    |      61    |    3937 K  |    3937 K  |
|       from small pool |      29    |      51    |    3100 K  |    3100 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:25:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:25:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:25:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:25:36] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:25:37]    INFO >> epoch 004:   1398 / 1539 loss=3.501, wps=4658.7, ups=5.89, wpb=791.5, bsz=791.5, num_updates=6000, lr=0.000941, gnorm=4.345, clip=0, train_wall=7, gb_free=71.6, wall=1000 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:25:44]    INFO >> epoch 004:   1448 / 1539 loss=3.57, wps=5066.6, ups=7.12, wpb=711.7, bsz=711.7, num_updates=6050, lr=0.000941, gnorm=4.276, clip=0, train_wall=7, gb_free=73.5, wall=1007 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:25:53]    INFO >> epoch 004:   1498 / 1539 loss=3.567, wps=4666.8, ups=6.84, wpb=682, bsz=682, num_updates=6100, lr=0.000941, gnorm=3.947, clip=0, train_wall=7, gb_free=74.2, wall=1014 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:25:55] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 25.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.10 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:25:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 35        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78888 MiB |  78948 MiB | 194338 GiB | 194261 GiB |
|       from large pool |  78495 MiB |  78555 MiB | 193245 GiB | 193169 GiB |
|       from small pool |    393 MiB |    394 MiB |   1092 GiB |   1092 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78888 MiB |  78948 MiB | 194338 GiB | 194261 GiB |
|       from large pool |  78495 MiB |  78555 MiB | 193245 GiB | 193169 GiB |
|       from small pool |    393 MiB |    394 MiB |   1092 GiB |   1092 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78858 MiB |  78917 MiB | 193930 GiB | 193853 GiB |
|       from large pool |  78467 MiB |  78526 MiB | 192839 GiB | 192762 GiB |
|       from small pool |    390 MiB |    392 MiB |   1090 GiB |   1090 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80480 MiB |  80482 MiB | 647248 MiB | 566768 MiB |
|       from large pool |  80048 MiB |  80048 MiB | 644114 MiB | 564066 MiB |
|       from small pool |    432 MiB |    434 MiB |   3134 MiB |   2702 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1531 MiB |   4145 MiB | 193503 GiB | 193501 GiB |
|       from large pool |   1492 MiB |   4108 MiB | 192263 GiB | 192261 GiB |
|       from small pool |     38 MiB |     41 MiB |   1239 GiB |   1239 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7252    |    7255    |   12813 K  |   12806 K  |
|       from large pool |     929    |     930    |    6169 K  |    6168 K  |
|       from small pool |    6323    |    6326    |    6643 K  |    6637 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7252    |    7255    |   12813 K  |   12806 K  |
|       from large pool |     929    |     930    |    6169 K  |    6168 K  |
|       from small pool |    6323    |    6326    |    6643 K  |    6637 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     318    |     319    |    3749    |    3431    |
|       from large pool |     102    |     102    |    2182    |    2080    |
|       from small pool |     216    |     217    |    1567    |    1351    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     462    |     464    |    7175 K  |    7174 K  |
|       from large pool |      77    |      77    |    4009 K  |    4009 K  |
|       from small pool |     385    |     387    |    3165 K  |    3165 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:25:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:25:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:25:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:25:55] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:25:59]    INFO >> epoch 004 | loss 3.54 | wps 4468 | ups 6.27 | wpb 712.7 | bsz 712.7 | num_updates 6140 | lr 0.000941 | gnorm 3.956 | clip 0 | train_wall 215 | gb_free 70.1 | wall 1020 (progress_bar.py:267, print())[0m
[33m[2025-11-19 03:25:59] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:26:12]    INFO >> epoch 004 | valid on 'valid' subset | loss 3.733 | wps 11470.1 | wpb 5412.5 | bsz 5412.5 | num_updates 6140 | best_loss 3.943 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 03:26:12]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:26:12]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/checkpoints/checkpoint_last.pt (epoch 4 @ 6140 updates, score 3.733) (writing took 0.012324 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 03:26:12] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:26:14]    INFO >> epoch 005:     10 / 1539 loss=3.628, wps=1532, ups=2.38, wpb=643.2, bsz=643.2, num_updates=6150, lr=0.000886, gnorm=3.846, clip=0, train_wall=6, gb_free=72.1, wall=1035 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:26:21]    INFO >> epoch 005:     60 / 1539 loss=3.541, wps=4700.8, ups=6.67, wpb=704.5, bsz=704.5, num_updates=6200, lr=0.000886, gnorm=3.614, clip=0, train_wall=7, gb_free=73.4, wall=1042 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:26:28]    INFO >> epoch 005:    110 / 1539 loss=3.536, wps=5013, ups=7.11, wpb=705.4, bsz=705.4, num_updates=6250, lr=0.000886, gnorm=3.987, clip=0, train_wall=7, gb_free=71.5, wall=1049 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:26:35]    INFO >> epoch 005:    160 / 1539 loss=3.505, wps=4622.8, ups=7.23, wpb=639.6, bsz=639.6, num_updates=6300, lr=0.000886, gnorm=3.849, clip=0, train_wall=7, gb_free=74, wall=1056 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:26:43]    INFO >> epoch 005:    210 / 1539 loss=3.491, wps=4578.7, ups=6.82, wpb=671.6, bsz=671.6, num_updates=6350, lr=0.000886, gnorm=3.299, clip=0, train_wall=7, gb_free=71, wall=1064 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:26:51]    INFO >> epoch 005:    260 / 1539 loss=3.468, wps=5428.7, ups=6.06, wpb=895.2, bsz=895.2, num_updates=6400, lr=0.000886, gnorm=4.318, clip=0, train_wall=8, gb_free=67.8, wall=1072 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:27:01]    INFO >> epoch 005:    310 / 1539 loss=3.472, wps=4879.5, ups=6.92, wpb=704.8, bsz=704.8, num_updates=6450, lr=0.000886, gnorm=4.06, clip=0, train_wall=7, gb_free=71.8, wall=1079 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:27:08]    INFO >> epoch 005:    360 / 1539 loss=3.354, wps=5201.9, ups=6.73, wpb=772.8, bsz=772.8, num_updates=6500, lr=0.000886, gnorm=3.915, clip=0, train_wall=7, gb_free=69.5, wall=1087 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:27:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 27.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.85 GiB is allocated by PyTorch, and 1.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:27:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 36        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78634 MiB |  78694 MiB | 211161 GiB | 211084 GiB |
|       from large pool |  78243 MiB |  78303 MiB | 209958 GiB | 209881 GiB |
|       from small pool |    390 MiB |    391 MiB |   1202 GiB |   1202 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78634 MiB |  78694 MiB | 211161 GiB | 211084 GiB |
|       from large pool |  78243 MiB |  78303 MiB | 209958 GiB | 209881 GiB |
|       from small pool |    390 MiB |    391 MiB |   1202 GiB |   1202 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78618 MiB |  78677 MiB | 210720 GiB | 210643 GiB |
|       from large pool |  78229 MiB |  78288 MiB | 209518 GiB | 209442 GiB |
|       from small pool |    388 MiB |    389 MiB |   1201 GiB |   1200 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80478 MiB |  80480 MiB | 647308 MiB | 566830 MiB |
|       from large pool |  80048 MiB |  80048 MiB | 644174 MiB | 564126 MiB |
|       from small pool |    430 MiB |    432 MiB |   3134 MiB |   2704 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1783 MiB |   5192 MiB | 209170 GiB | 209168 GiB |
|       from large pool |   1744 MiB |   5187 MiB | 207809 GiB | 207807 GiB |
|       from small pool |     39 MiB |     41 MiB |   1361 GiB |   1361 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7208    |    7211    |   13967 K  |   13960 K  |
|       from large pool |     925    |     926    |    6636 K  |    6635 K  |
|       from small pool |    6283    |    6286    |    7330 K  |    7324 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7208    |    7211    |   13967 K  |   13960 K  |
|       from large pool |     925    |     926    |    6636 K  |    6635 K  |
|       from small pool |    6283    |    6286    |    7330 K  |    7324 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     317    |     318    |    3750    |    3433    |
|       from large pool |     102    |     102    |    2183    |    2081    |
|       from small pool |     215    |     216    |    1567    |    1352    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     476    |     477    |    7847 K  |    7846 K  |
|       from large pool |      90    |      90    |    4312 K  |    4312 K  |
|       from small pool |     386    |     387    |    3534 K  |    3534 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:27:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:27:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:27:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:27:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:27:16]    INFO >> epoch 005:    411 / 1539 loss=3.539, wps=4538.1, ups=6.66, wpb=681.6, bsz=681.6, num_updates=6550, lr=0.000886, gnorm=3.765, clip=0, train_wall=7, gb_free=73.9, wall=1094 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:27:23]    INFO >> epoch 005:    461 / 1539 loss=3.584, wps=4374.1, ups=7.19, wpb=608, bsz=608, num_updates=6600, lr=0.000886, gnorm=3.446, clip=0, train_wall=7, gb_free=74.7, wall=1101 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:27:36]    INFO >> epoch 005:    511 / 1539 loss=3.428, wps=3087.1, ups=4.23, wpb=730.5, bsz=730.5, num_updates=6650, lr=0.000886, gnorm=3.867, clip=0, train_wall=11, gb_free=69.8, wall=1113 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:27:43]    INFO >> epoch 005:    561 / 1539 loss=3.594, wps=4391.6, ups=7.08, wpb=620.7, bsz=620.7, num_updates=6700, lr=0.000886, gnorm=3.529, clip=0, train_wall=7, gb_free=72.5, wall=1120 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:27:47] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.48 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:27:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 38        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79276 MiB |  79336 MiB | 216573 GiB | 216496 GiB |
|       from large pool |  79185 MiB |  79245 MiB | 215342 GiB | 215265 GiB |
|       from small pool |     91 MiB |     92 MiB |   1230 GiB |   1230 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79276 MiB |  79336 MiB | 216573 GiB | 216496 GiB |
|       from large pool |  79185 MiB |  79245 MiB | 215342 GiB | 215265 GiB |
|       from small pool |     91 MiB |     92 MiB |   1230 GiB |   1230 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79244 MiB |  79303 MiB | 216121 GiB | 216043 GiB |
|       from large pool |  79153 MiB |  79212 MiB | 214892 GiB | 214815 GiB |
|       from small pool |     90 MiB |     92 MiB |   1228 GiB |   1228 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB | 647732 MiB | 567230 MiB |
|       from large pool |  80408 MiB |  80408 MiB | 644594 MiB | 564186 MiB |
|       from small pool |     94 MiB |    430 MiB |   3138 MiB |   3044 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1165 MiB |   6539 MiB | 214881 GiB | 214880 GiB |
|       from large pool |   1162 MiB |   6529 MiB | 213487 GiB | 213486 GiB |
|       from small pool |      2 MiB |     25 MiB |   1393 GiB |   1393 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1983    |    1986    |   14316 K  |   14314 K  |
|       from large pool |     472    |     473    |    6818 K  |    6818 K  |
|       from small pool |    1511    |    1514    |    7497 K  |    7496 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1983    |    1986    |   14316 K  |   14314 K  |
|       from large pool |     472    |     473    |    6818 K  |    6818 K  |
|       from small pool |    1511    |    1514    |    7497 K  |    7496 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     155    |     317    |    3759    |    3604    |
|       from large pool |     108    |     108    |    2190    |    2082    |
|       from small pool |      47    |     215    |    1569    |    1522    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     138    |     140    |    8036 K  |    8036 K  |
|       from large pool |      88    |      91    |    4428 K  |    4428 K  |
|       from small pool |      50    |      58    |    3608 K  |    3608 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:27:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:27:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:27:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:27:47] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:27:51]    INFO >> epoch 005:    612 / 1539 loss=3.519, wps=4719.4, ups=6.24, wpb=756, bsz=756, num_updates=6750, lr=0.000886, gnorm=3.786, clip=0, train_wall=7, gb_free=70.1, wall=1128 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:27:59]    INFO >> epoch 005:    662 / 1539 loss=3.444, wps=4872.3, ups=6.61, wpb=736.8, bsz=736.8, num_updates=6800, lr=0.000886, gnorm=3.738, clip=0, train_wall=7, gb_free=74.2, wall=1136 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:28:07]    INFO >> epoch 005:    712 / 1539 loss=3.559, wps=4957.3, ups=6.8, wpb=729, bsz=729, num_updates=6850, lr=0.000886, gnorm=4.129, clip=0, train_wall=7, gb_free=71.4, wall=1143 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:28:14]    INFO >> epoch 005:    762 / 1539 loss=3.435, wps=5094.8, ups=7.1, wpb=717.1, bsz=717.1, num_updates=6900, lr=0.000886, gnorm=3.869, clip=0, train_wall=7, gb_free=72.9, wall=1150 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:28:22]    INFO >> epoch 005:    812 / 1539 loss=3.566, wps=4173.5, ups=6.2, wpb=673.7, bsz=673.7, num_updates=6950, lr=0.000886, gnorm=3.631, clip=0, train_wall=8, gb_free=69.7, wall=1158 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:28:30]    INFO >> epoch 005:    862 / 1539 loss=3.413, wps=4602.3, ups=6.86, wpb=670.9, bsz=670.9, num_updates=7000, lr=0.000886, gnorm=3.563, clip=0, train_wall=7, gb_free=66.5, wall=1165 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:28:38]    INFO >> epoch 005:    912 / 1539 loss=3.415, wps=4493.3, ups=6.74, wpb=667, bsz=667, num_updates=7050, lr=0.000886, gnorm=3.898, clip=0, train_wall=7, gb_free=71.7, wall=1173 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:28:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 131.25 MiB is free. Including non-PyTorch memory, this process has 78.99 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 3.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:28:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 39        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76325 MiB |  78599 MiB | 227233 GiB | 227158 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 225947 GiB | 225873 GiB |
|       from small pool |     18 MiB |     24 MiB |   1285 GiB |   1285 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76325 MiB |  78599 MiB | 227233 GiB | 227158 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 225947 GiB | 225873 GiB |
|       from small pool |     18 MiB |     24 MiB |   1285 GiB |   1285 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 226757 GiB | 226683 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 225474 GiB | 225399 GiB |
|       from small pool |     18 MiB |     24 MiB |   1283 GiB |   1283 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80374 MiB |  80442 MiB | 647732 MiB | 567358 MiB |
|       from large pool |  80348 MiB |  80348 MiB | 644594 MiB | 564246 MiB |
|       from small pool |     26 MiB |     94 MiB |   3138 MiB |   3112 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4048 MiB |   5924 MiB | 225929 GiB | 225926 GiB |
|       from large pool |   4041 MiB |   5910 MiB | 224474 GiB | 224470 GiB |
|       from small pool |      7 MiB |     23 MiB |   1455 GiB |   1455 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   14999 K  |   14999 K  |
|       from large pool |     340    |     346    |    7173 K  |    7173 K  |
|       from small pool |     300    |     356    |    7826 K  |    7825 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   14999 K  |   14999 K  |
|       from large pool |     340    |     346    |    7173 K  |    7173 K  |
|       from small pool |     300    |     356    |    7826 K  |    7825 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     120    |     154    |    3759    |    3639    |
|       from large pool |     107    |     107    |    2190    |    2083    |
|       from small pool |      13    |      47    |    1569    |    1556    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     111    |     113    |    8408 K  |    8408 K  |
|       from large pool |      84    |      86    |    4655 K  |    4655 K  |
|       from small pool |      27    |      48    |    3752 K  |    3752 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:28:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:28:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:28:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:28:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:28:46]    INFO >> epoch 005:    963 / 1539 loss=3.531, wps=4333.2, ups=6.4, wpb=676.6, bsz=676.6, num_updates=7100, lr=0.000886, gnorm=3.669, clip=0, train_wall=7, gb_free=70.4, wall=1180 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:28:55]    INFO >> epoch 005:   1013 / 1539 loss=3.367, wps=5436.2, ups=5.7, wpb=954.2, bsz=954.2, num_updates=7150, lr=0.000886, gnorm=3.803, clip=0, train_wall=8, gb_free=70.3, wall=1189 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:29:03]    INFO >> epoch 005:   1063 / 1539 loss=3.36, wps=4904.1, ups=6.2, wpb=791.1, bsz=791.1, num_updates=7200, lr=0.000886, gnorm=3.334, clip=0, train_wall=8, gb_free=64.7, wall=1197 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:29:11]    INFO >> epoch 005:   1113 / 1539 loss=3.421, wps=4452.4, ups=7.11, wpb=626.1, bsz=626.1, num_updates=7250, lr=0.000886, gnorm=4.169, clip=0, train_wall=7, gb_free=74.2, wall=1204 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:29:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 199.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 4.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:29:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 43        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72823 MiB |  75427 MiB | 232756 GiB | 232684 GiB |
|       from large pool |  72810 MiB |  75414 MiB | 231439 GiB | 231368 GiB |
|       from small pool |     12 MiB |     21 MiB |   1316 GiB |   1316 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72823 MiB |  75427 MiB | 232756 GiB | 232684 GiB |
|       from large pool |  72810 MiB |  75414 MiB | 231439 GiB | 231368 GiB |
|       from small pool |     12 MiB |     21 MiB |   1316 GiB |   1316 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 232269 GiB | 232198 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 230954 GiB | 230883 GiB |
|       from small pool |     12 MiB |     21 MiB |   1314 GiB |   1314 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80306 MiB |  80306 MiB | 688436 MiB | 608130 MiB |
|       from large pool |  80280 MiB |  80280 MiB | 685098 MiB | 604818 MiB |
|       from small pool |     26 MiB |    226 MiB |   3338 MiB |   3312 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5580 MiB |   9572 MiB | 230913 GiB | 230908 GiB |
|       from large pool |   5567 MiB |   9557 MiB | 229421 GiB | 229416 GiB |
|       from small pool |     13 MiB |     31 MiB |   1491 GiB |   1491 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   15362 K  |   15361 K  |
|       from large pool |     312    |     320    |    7349 K  |    7348 K  |
|       from small pool |     291    |     356    |    8012 K  |    8012 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   15362 K  |   15361 K  |
|       from large pool |     312    |     320    |    7349 K  |    7348 K  |
|       from small pool |     291    |     356    |    8012 K  |    8012 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     170    |     326    |    3990    |    3820    |
|       from large pool |     157    |     213    |    2321    |    2164    |
|       from small pool |      13    |     113    |    1669    |    1656    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     159    |     161    |    8616 K  |    8616 K  |
|       from large pool |     133    |     135    |    4773 K  |    4773 K  |
|       from small pool |      26    |      58    |    3842 K  |    3842 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:29:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:29:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:29:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:29:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:29:20]    INFO >> epoch 005:   1164 / 1539 loss=3.478, wps=4605.7, ups=5.9, wpb=780.4, bsz=780.4, num_updates=7300, lr=0.000886, gnorm=4.216, clip=0, train_wall=7, gb_free=69.6, wall=1213 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:29:27]    INFO >> epoch 005:   1214 / 1539 loss=3.553, wps=4471.9, ups=6.8, wpb=657.7, bsz=657.7, num_updates=7350, lr=0.000886, gnorm=3.371, clip=0, train_wall=7, gb_free=68.7, wall=1220 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:29:34]    INFO >> epoch 005:   1264 / 1539 loss=3.478, wps=4809.3, ups=7.57, wpb=635.1, bsz=635.1, num_updates=7400, lr=0.000886, gnorm=3.652, clip=0, train_wall=6, gb_free=73.5, wall=1227 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:29:43]    INFO >> epoch 005:   1314 / 1539 loss=3.44, wps=4367.9, ups=6.7, wpb=651.5, bsz=651.5, num_updates=7450, lr=0.000886, gnorm=3.541, clip=0, train_wall=7, gb_free=70, wall=1234 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:29:50]    INFO >> epoch 005:   1364 / 1539 loss=3.489, wps=4878.2, ups=6.72, wpb=725.6, bsz=725.6, num_updates=7500, lr=0.000886, gnorm=3.597, clip=0, train_wall=7, gb_free=74.7, wall=1242 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:29:57]    INFO >> epoch 005:   1414 / 1539 loss=3.542, wps=4376.6, ups=6.95, wpb=630.1, bsz=630.1, num_updates=7550, lr=0.000886, gnorm=3.196, clip=0, train_wall=7, gb_free=74.2, wall=1249 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:30:05]    INFO >> epoch 005:   1464 / 1539 loss=3.353, wps=5049.8, ups=6.61, wpb=763.7, bsz=763.7, num_updates=7600, lr=0.000886, gnorm=4.682, clip=2, train_wall=7, gb_free=48.5, wall=1256 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:30:13]    INFO >> epoch 005:   1514 / 1539 loss=3.555, wps=5049.6, ups=6.29, wpb=802.7, bsz=802.7, num_updates=7650, lr=0.000886, gnorm=3.838, clip=0, train_wall=8, gb_free=54.7, wall=1264 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:30:18]    INFO >> epoch 005 | loss 3.481 | wps 4403.1 | ups 6.18 | wpb 712.7 | bsz 712.7 | num_updates 7675 | lr 0.000886 | gnorm 3.787 | clip 0.1 | train_wall 219 | gb_free 63.5 | wall 1268 (progress_bar.py:267, print())[0m
[33m[2025-11-19 03:30:18] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:30:31]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.7 | wps 11718.3 | wpb 5412.5 | bsz 5412.5 | num_updates 7675 | best_loss 3.943 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 03:30:31]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:30:31]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/checkpoints/checkpoint_last.pt (epoch 5 @ 7675 updates, score 3.7) (writing took 0.012311 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 03:30:31] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:30:35]    INFO >> epoch 006:     25 / 1539 loss=3.589, wps=1728.4, ups=2.44, wpb=708.8, bsz=708.8, num_updates=7700, lr=0.000817, gnorm=3.798, clip=0, train_wall=7, gb_free=74.9, wall=1285 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:30:43]    INFO >> epoch 006:     75 / 1539 loss=3.461, wps=5496.8, ups=6.36, wpb=863.7, bsz=863.7, num_updates=7750, lr=0.000817, gnorm=3.958, clip=0, train_wall=7, gb_free=75.2, wall=1293 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:30:51]    INFO >> epoch 006:    125 / 1539 loss=3.406, wps=4749.6, ups=7.03, wpb=675.3, bsz=675.3, num_updates=7800, lr=0.000817, gnorm=4.417, clip=0, train_wall=7, gb_free=70.3, wall=1300 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:30:58]    INFO >> epoch 006:    175 / 1539 loss=3.464, wps=4625.3, ups=6.71, wpb=689, bsz=689, num_updates=7850, lr=0.000817, gnorm=4.273, clip=0, train_wall=7, gb_free=74.3, wall=1307 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:31:01] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 535.25 MiB is free. Including non-PyTorch memory, this process has 78.59 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 4.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:31:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 44        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75055 MiB |  75998 MiB | 254753 GiB | 254679 GiB |
|       from large pool |  75042 MiB |  75985 MiB | 253307 GiB | 253233 GiB |
|       from small pool |     12 MiB |     15 MiB |   1446 GiB |   1446 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75055 MiB |  75998 MiB | 254753 GiB | 254679 GiB |
|       from large pool |  75042 MiB |  75985 MiB | 253307 GiB | 253233 GiB |
|       from small pool |     12 MiB |     15 MiB |   1446 GiB |   1446 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 254221 GiB | 254148 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 252777 GiB | 252704 GiB |
|       from small pool |     12 MiB |     15 MiB |   1443 GiB |   1443 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79970 MiB |  80406 MiB | 690438 MiB | 610468 MiB |
|       from large pool |  79944 MiB |  80306 MiB | 687026 MiB | 607082 MiB |
|       from small pool |     26 MiB |    100 MiB |   3412 MiB |   3386 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4914 MiB |   8989 MiB | 249564 GiB | 249559 GiB |
|       from large pool |   4901 MiB |   8976 MiB | 247930 GiB | 247925 GiB |
|       from small pool |     13 MiB |     25 MiB |   1633 GiB |   1633 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   16790 K  |   16790 K  |
|       from large pool |     314    |     322    |    7980 K  |    7979 K  |
|       from small pool |     291    |     356    |    8810 K  |    8810 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   16790 K  |   16790 K  |
|       from large pool |     314    |     322    |    7980 K  |    7979 K  |
|       from small pool |     291    |     356    |    8810 K  |    8810 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     210    |    4031    |    3859    |
|       from large pool |     159    |     160    |    2325    |    2166    |
|       from small pool |      13    |      50    |    1706    |    1693    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     164    |     164    |    9442 K  |    9442 K  |
|       from large pool |     136    |     136    |    5197 K  |    5196 K  |
|       from small pool |      28    |      57    |    4245 K  |    4245 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:31:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:31:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:31:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:31:01] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-19 03:31:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 535.25 MiB is free. Including non-PyTorch memory, this process has 78.59 GiB memory in use. Of the allocated memory 72.94 GiB is allocated by PyTorch, and 5.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:31:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 45        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69875 MiB |  74872 MiB | 255914 GiB | 255845 GiB |
|       from large pool |  69858 MiB |  74855 MiB | 254462 GiB | 254394 GiB |
|       from small pool |     17 MiB |     17 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69875 MiB |  74872 MiB | 255914 GiB | 255845 GiB |
|       from large pool |  69858 MiB |  74855 MiB | 254462 GiB | 254394 GiB |
|       from small pool |     17 MiB |     17 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69846 MiB |  74842 MiB | 255380 GiB | 255311 GiB |
|       from large pool |  69829 MiB |  74825 MiB | 253930 GiB | 253862 GiB |
|       from small pool |     17 MiB |     17 MiB |   1449 GiB |   1449 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79970 MiB |  80006 MiB | 690474 MiB | 610504 MiB |
|       from large pool |  79944 MiB |  79944 MiB | 687026 MiB | 607082 MiB |
|       from small pool |     26 MiB |     62 MiB |   3448 MiB |   3422 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  10094 MiB |  11472 MiB | 250620 GiB | 250610 GiB |
|       from large pool |  10085 MiB |  11463 MiB | 248981 GiB | 248971 GiB |
|       from small pool |      8 MiB |     27 MiB |   1639 GiB |   1639 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     643    |     661    |   16854 K  |   16853 K  |
|       from large pool |     344    |     362    |    8013 K  |    8013 K  |
|       from small pool |     299    |     356    |    8840 K  |    8840 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     643    |     661    |   16854 K  |   16853 K  |
|       from large pool |     344    |     362    |    8013 K  |    8013 K  |
|       from small pool |     299    |     356    |    8840 K  |    8840 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     190    |    4049    |    3877    |
|       from large pool |     159    |     159    |    2325    |    2166    |
|       from small pool |      13    |      31    |    1724    |    1711    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     151    |     153    |    9477 K  |    9477 K  |
|       from large pool |     121    |     123    |    5219 K  |    5219 K  |
|       from small pool |      30    |      62    |    4258 K  |    4258 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:31:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:31:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:31:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:31:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:31:07]    INFO >> epoch 006:    227 / 1539 loss=3.463, wps=4091.8, ups=5.69, wpb=719.2, bsz=719.2, num_updates=7900, lr=0.000817, gnorm=4.298, clip=0, train_wall=7, gb_free=71.4, wall=1316 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:31:14]    INFO >> epoch 006:    277 / 1539 loss=3.504, wps=4517.8, ups=7.22, wpb=625.7, bsz=625.7, num_updates=7950, lr=0.000817, gnorm=3.193, clip=0, train_wall=7, gb_free=74.5, wall=1323 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:31:23]    INFO >> epoch 006:    327 / 1539 loss=3.326, wps=5349.5, ups=6.44, wpb=830.5, bsz=830.5, num_updates=8000, lr=0.000817, gnorm=4.291, clip=0, train_wall=7, gb_free=70.3, wall=1331 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:31:31]    INFO >> epoch 006:    377 / 1539 loss=3.561, wps=4412.1, ups=6.71, wpb=657.6, bsz=657.6, num_updates=8050, lr=0.000817, gnorm=3.359, clip=0, train_wall=7, gb_free=71.1, wall=1338 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:31:38]    INFO >> epoch 006:    427 / 1539 loss=3.415, wps=5339.9, ups=6.92, wpb=772.1, bsz=772.1, num_updates=8100, lr=0.000817, gnorm=3.972, clip=0, train_wall=7, gb_free=75, wall=1345 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:31:40] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 2.39 GiB is free. Including non-PyTorch memory, this process has 76.72 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 5.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:31:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 47        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72059 MiB |  72460 MiB | 262338 GiB | 262268 GiB |
|       from large pool |  72042 MiB |  72442 MiB | 260852 GiB | 260781 GiB |
|       from small pool |     17 MiB |     21 MiB |   1486 GiB |   1486 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72059 MiB |  72460 MiB | 262338 GiB | 262268 GiB |
|       from large pool |  72042 MiB |  72442 MiB | 260852 GiB | 260781 GiB |
|       from small pool |     17 MiB |     21 MiB |   1486 GiB |   1486 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 261790 GiB | 261719 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 260305 GiB | 260235 GiB |
|       from small pool |     17 MiB |     21 MiB |   1484 GiB |   1484 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78056 MiB |  80154 MiB | 696490 MiB | 618434 MiB |
|       from large pool |  78026 MiB |  79944 MiB | 692858 MiB | 614832 MiB |
|       from small pool |     30 MiB |    210 MiB |   3632 MiB |   3602 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5996 MiB |   8377 MiB | 256686 GiB | 256680 GiB |
|       from large pool |   5983 MiB |   8364 MiB | 255006 GiB | 255000 GiB |
|       from small pool |     12 MiB |     31 MiB |   1680 GiB |   1680 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   17287 K  |   17286 K  |
|       from large pool |     308    |     315    |    8230 K  |    8230 K  |
|       from small pool |     298    |     356    |    9056 K  |    9056 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   17287 K  |   17286 K  |
|       from large pool |     308    |     315    |    8230 K  |    8230 K  |
|       from small pool |     298    |     356    |    9056 K  |    9056 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     264    |    4144    |    3972    |
|       from large pool |     157    |     159    |    2328    |    2171    |
|       from small pool |      15    |     105    |    1816    |    1801    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     169    |     169    |    9721 K  |    9721 K  |
|       from large pool |     137    |     137    |    5362 K  |    5362 K  |
|       from small pool |      32    |      55    |    4358 K  |    4358 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:31:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:31:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:31:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:31:40] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:31:46]    INFO >> epoch 006:    478 / 1539 loss=3.478, wps=4541.1, ups=6.43, wpb=705.8, bsz=705.8, num_updates=8150, lr=0.000817, gnorm=3.928, clip=0, train_wall=7, gb_free=65, wall=1353 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:31:54]    INFO >> epoch 006:    528 / 1539 loss=3.438, wps=4464.5, ups=6.31, wpb=707.6, bsz=707.6, num_updates=8200, lr=0.000817, gnorm=4.328, clip=0, train_wall=7, gb_free=72, wall=1361 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:32:03]    INFO >> epoch 006:    578 / 1539 loss=3.59, wps=5215, ups=6.54, wpb=797.6, bsz=797.6, num_updates=8250, lr=0.000817, gnorm=3.903, clip=0, train_wall=7, gb_free=74.1, wall=1369 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:32:09]    INFO >> epoch 006:    628 / 1539 loss=3.453, wps=4728.1, ups=7.33, wpb=644.9, bsz=644.9, num_updates=8300, lr=0.000817, gnorm=3.624, clip=0, train_wall=6, gb_free=70.5, wall=1376 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:32:17]    INFO >> epoch 006:    678 / 1539 loss=3.521, wps=4785.5, ups=6.55, wpb=730.2, bsz=730.2, num_updates=8350, lr=0.000817, gnorm=3.422, clip=0, train_wall=7, gb_free=73.6, wall=1383 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:32:24]    INFO >> epoch 006:    728 / 1539 loss=3.476, wps=4417.2, ups=7.17, wpb=616.1, bsz=616.1, num_updates=8400, lr=0.000817, gnorm=3.465, clip=0, train_wall=7, gb_free=62, wall=1390 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:32:25] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.16 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:32:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 48        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77932 MiB |  77992 MiB | 270810 GiB | 270734 GiB |
|       from large pool |  77549 MiB |  77609 MiB | 269273 GiB | 269197 GiB |
|       from small pool |    383 MiB |    384 MiB |   1536 GiB |   1536 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77932 MiB |  77992 MiB | 270810 GiB | 270734 GiB |
|       from large pool |  77549 MiB |  77609 MiB | 269273 GiB | 269197 GiB |
|       from small pool |    383 MiB |    384 MiB |   1536 GiB |   1536 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77897 MiB |  77956 MiB | 270242 GiB | 270166 GiB |
|       from large pool |  77516 MiB |  77575 MiB | 268708 GiB | 268632 GiB |
|       from small pool |    381 MiB |    382 MiB |   1534 GiB |   1533 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 698924 MiB | 618436 MiB |
|       from large pool |  80066 MiB |  80066 MiB | 694898 MiB | 614832 MiB |
|       from small pool |    422 MiB |    424 MiB |   4026 MiB |   3604 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2495 MiB |   6667 MiB | 264815 GiB | 264813 GiB |
|       from large pool |   2456 MiB |   6661 MiB | 263077 GiB | 263075 GiB |
|       from small pool |     38 MiB |     40 MiB |   1737 GiB |   1737 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7076    |    7079    |   17882 K  |   17874 K  |
|       from large pool |     913    |     914    |    8520 K  |    8519 K  |
|       from small pool |    6163    |    6166    |    9361 K  |    9355 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7076    |    7079    |   17882 K  |   17874 K  |
|       from large pool |     913    |     914    |    8520 K  |    8519 K  |
|       from small pool |    6163    |    6166    |    9361 K  |    9355 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     402    |     403    |    4375    |    3973    |
|       from large pool |     191    |     191    |    2362    |    2171    |
|       from small pool |     211    |     212    |    2013    |    1802    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     527    |     528    |   10056 K  |   10055 K  |
|       from large pool |     150    |     150    |    5552 K  |    5552 K  |
|       from small pool |     377    |     378    |    4503 K  |    4503 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:32:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:32:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:32:25] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:32:25] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:32:33]    INFO >> epoch 006:    779 / 1539 loss=3.504, wps=4311.6, ups=6.71, wpb=642.5, bsz=642.5, num_updates=8450, lr=0.000817, gnorm=2.86, clip=0, train_wall=7, gb_free=74.1, wall=1398 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:32:40]    INFO >> epoch 006:    829 / 1539 loss=3.441, wps=4497.6, ups=6.85, wpb=656.8, bsz=656.8, num_updates=8500, lr=0.000817, gnorm=3.423, clip=0, train_wall=7, gb_free=66.6, wall=1405 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:32:47]    INFO >> epoch 006:    879 / 1539 loss=3.425, wps=4957.9, ups=6.8, wpb=729.6, bsz=729.6, num_updates=8550, lr=0.000817, gnorm=3.576, clip=0, train_wall=7, gb_free=70.2, wall=1412 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:32:56]    INFO >> epoch 006:    929 / 1539 loss=3.554, wps=5003.2, ups=6.06, wpb=826.1, bsz=826.1, num_updates=8600, lr=0.000817, gnorm=3.839, clip=0, train_wall=8, gb_free=73.1, wall=1421 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:33:04]    INFO >> epoch 006:    979 / 1539 loss=3.521, wps=4726.2, ups=6.74, wpb=701.1, bsz=701.1, num_updates=8650, lr=0.000817, gnorm=3.262, clip=0, train_wall=7, gb_free=75.1, wall=1428 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:33:13] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.57 GiB. GPU 0 has a total capacity of 79.14 GiB of which 2.27 GiB is free. Including non-PyTorch memory, this process has 76.84 GiB memory in use. Of the allocated memory 66.03 GiB is allocated by PyTorch, and 10.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:33:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 49        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63499 MiB |  70130 MiB | 279629 GiB | 279567 GiB |
|       from large pool |  63479 MiB |  70110 MiB | 278045 GiB | 277983 GiB |
|       from small pool |     20 MiB |     60 MiB |   1584 GiB |   1584 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63499 MiB |  70130 MiB | 279629 GiB | 279567 GiB |
|       from large pool |  63479 MiB |  70110 MiB | 278045 GiB | 277983 GiB |
|       from small pool |     20 MiB |     60 MiB |   1584 GiB |   1584 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63481 MiB |  70111 MiB | 279042 GiB | 278980 GiB |
|       from large pool |  63461 MiB |  70091 MiB | 277460 GiB | 277398 GiB |
|       from small pool |     20 MiB |     60 MiB |   1582 GiB |   1582 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78178 MiB |  80428 MiB | 698924 MiB | 620746 MiB |
|       from large pool |  78146 MiB |  80006 MiB | 694898 MiB | 616752 MiB |
|       from small pool |     32 MiB |    422 MiB |   4026 MiB |   3994 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7300 MiB |  10790 MiB | 272798 GiB | 272791 GiB |
|       from large pool |   7288 MiB |  10778 MiB | 271004 GiB | 270997 GiB |
|       from small pool |     11 MiB |     27 MiB |   1793 GiB |   1793 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |    1326    |   18465 K  |   18465 K  |
|       from large pool |     330    |     403    |    8814 K  |    8814 K  |
|       from small pool |     316    |     924    |    9651 K  |    9650 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |    1326    |   18465 K  |   18465 K  |
|       from large pool |     330    |     403    |    8814 K  |    8814 K  |
|       from small pool |     316    |     924    |    9651 K  |    9650 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     176    |     401    |    4375    |    4199    |
|       from large pool |     160    |     190    |    2362    |    2202    |
|       from small pool |      16    |     211    |    2013    |    1997    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     164    |     213    |   10388 K  |   10388 K  |
|       from large pool |     129    |     160    |    5749 K  |    5749 K  |
|       from small pool |      35    |      54    |    4638 K  |    4638 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:33:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:33:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:33:13] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:33:13] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:33:13]    INFO >> epoch 006:   1030 / 1539 loss=3.478, wps=4408.4, ups=6.04, wpb=730.1, bsz=730.1, num_updates=8700, lr=0.000817, gnorm=3.311, clip=0, train_wall=7, gb_free=11.5, wall=1436 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:33:20]    INFO >> epoch 006:   1080 / 1539 loss=3.532, wps=4826.9, ups=6.67, wpb=723.5, bsz=723.5, num_updates=8750, lr=0.000817, gnorm=3.379, clip=0, train_wall=7, gb_free=72.4, wall=1444 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:33:28]    INFO >> epoch 006:   1130 / 1539 loss=3.374, wps=4680.6, ups=6.18, wpb=757.6, bsz=757.6, num_updates=8800, lr=0.000817, gnorm=3.961, clip=0, train_wall=8, gb_free=72.1, wall=1452 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:33:37]    INFO >> epoch 006:   1180 / 1539 loss=3.608, wps=4929, ups=6.66, wpb=740.5, bsz=740.5, num_updates=8850, lr=0.000817, gnorm=3.909, clip=0, train_wall=7, gb_free=69, wall=1459 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:33:45]    INFO >> epoch 006:   1230 / 1539 loss=3.47, wps=4250.6, ups=6.45, wpb=659.3, bsz=659.3, num_updates=8900, lr=0.000817, gnorm=3.633, clip=0, train_wall=7, gb_free=73.8, wall=1467 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:33:52]    INFO >> epoch 006:   1280 / 1539 loss=3.524, wps=4436.9, ups=6.82, wpb=650.6, bsz=650.6, num_updates=8950, lr=0.000817, gnorm=3.963, clip=0, train_wall=7, gb_free=75.3, wall=1475 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:34:00]    INFO >> epoch 006:   1330 / 1539 loss=3.428, wps=4958.2, ups=6.69, wpb=741, bsz=741, num_updates=9000, lr=0.000817, gnorm=3.828, clip=0, train_wall=7, gb_free=71.3, wall=1482 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:34:07]    INFO >> epoch 006:   1380 / 1539 loss=3.475, wps=4371.8, ups=7.18, wpb=608.9, bsz=608.9, num_updates=9050, lr=0.000817, gnorm=3.619, clip=0, train_wall=7, gb_free=68.9, wall=1489 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:34:16]    INFO >> epoch 006:   1430 / 1539 loss=3.47, wps=4955.8, ups=6.59, wpb=751.8, bsz=751.8, num_updates=9100, lr=0.000817, gnorm=3.739, clip=0, train_wall=7, gb_free=70.5, wall=1497 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:34:23]    INFO >> epoch 006:   1480 / 1539 loss=3.408, wps=4631.2, ups=6.46, wpb=717, bsz=717, num_updates=9150, lr=0.000817, gnorm=3.248, clip=0, train_wall=7, gb_free=69.4, wall=1504 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:34:30]    INFO >> epoch 006:   1530 / 1539 loss=3.41, wps=4846.9, ups=7.19, wpb=674.1, bsz=674.1, num_updates=9200, lr=0.000817, gnorm=3.343, clip=0, train_wall=7, gb_free=74.6, wall=1511 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:34:32]    INFO >> epoch 006 | loss 3.475 | wps 4465.3 | ups 6.28 | wpb 711.1 | bsz 711.1 | num_updates 9209 | lr 0.000817 | gnorm 3.707 | clip 0 | train_wall 215 | gb_free 72.3 | wall 1513 (progress_bar.py:267, print())[0m
[33m[2025-11-19 03:34:32] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:34:46]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.716 | wps 11915.2 | wpb 5412.5 | bsz 5412.5 | num_updates 9209 | best_loss 3.943 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 03:34:46]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:34:46]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/checkpoints/checkpoint_last.pt (epoch 6 @ 9209 updates, score 3.716) (writing took 0.012483 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 03:34:46] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:34:53]    INFO >> epoch 007:     41 / 1539 loss=3.598, wps=1668.9, ups=2.37, wpb=704.3, bsz=704.3, num_updates=9250, lr=0.000739, gnorm=3.679, clip=0, train_wall=7, gb_free=71.6, wall=1532 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:34:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.82 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 70.77 GiB is allocated by PyTorch, and 6.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:34:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 51        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72064 MiB |  72463 MiB | 300570 GiB | 300499 GiB |
|       from large pool |  72047 MiB |  72446 MiB | 298863 GiB | 298793 GiB |
|       from small pool |     17 MiB |     25 MiB |   1706 GiB |   1706 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72064 MiB |  72463 MiB | 300570 GiB | 300499 GiB |
|       from large pool |  72047 MiB |  72446 MiB | 298863 GiB | 298793 GiB |
|       from small pool |     17 MiB |     25 MiB |   1706 GiB |   1706 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 299940 GiB | 299870 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 298236 GiB | 298165 GiB |
|       from small pool |     17 MiB |     25 MiB |   1704 GiB |   1704 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78638 MiB |  78794 MiB | 712374 MiB | 633736 MiB |
|       from large pool |  78610 MiB |  78670 MiB | 708256 MiB | 629646 MiB |
|       from small pool |     28 MiB |    124 MiB |   4118 MiB |   4090 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6573 MiB |   8375 MiB | 290276 GiB | 290270 GiB |
|       from large pool |   6562 MiB |   8363 MiB | 288349 GiB | 288343 GiB |
|       from small pool |     10 MiB |     35 MiB |   1927 GiB |   1927 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   19804 K  |   19803 K  |
|       from large pool |     308    |     315    |    9401 K  |    9401 K  |
|       from small pool |     298    |     356    |   10402 K  |   10402 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   19804 K  |   19803 K  |
|       from large pool |     308    |     315    |    9401 K  |    9401 K  |
|       from small pool |     298    |     356    |   10402 K  |   10402 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     170    |     222    |    4426    |    4256    |
|       from large pool |     156    |     160    |    2367    |    2211    |
|       from small pool |      14    |      62    |    2059    |    2045    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     165    |     166    |   11169 K  |   11169 K  |
|       from large pool |     137    |     138    |    6142 K  |    6142 K  |
|       from small pool |      28    |      60    |    5026 K  |    5026 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:34:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:34:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:34:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:34:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:35:00]    INFO >> epoch 007:     92 / 1539 loss=3.479, wps=4426.3, ups=6.56, wpb=674.7, bsz=674.7, num_updates=9300, lr=0.000739, gnorm=3.406, clip=0, train_wall=6, gb_free=74.6, wall=1540 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:35:08]    INFO >> epoch 007:    142 / 1539 loss=3.484, wps=4451.3, ups=6.71, wpb=663.1, bsz=663.1, num_updates=9350, lr=0.000739, gnorm=3.67, clip=0, train_wall=7, gb_free=73.4, wall=1547 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:35:16]    INFO >> epoch 007:    192 / 1539 loss=3.363, wps=5760, ups=6.51, wpb=885.1, bsz=885.1, num_updates=9400, lr=0.000739, gnorm=3.35, clip=0, train_wall=7, gb_free=67.7, wall=1555 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:35:25]    INFO >> epoch 007:    242 / 1539 loss=3.568, wps=5042, ups=6.35, wpb=794.3, bsz=794.3, num_updates=9450, lr=0.000739, gnorm=3.466, clip=0, train_wall=7, gb_free=70.7, wall=1563 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:35:32]    INFO >> epoch 007:    292 / 1539 loss=3.38, wps=4754.3, ups=6.8, wpb=699.6, bsz=699.6, num_updates=9500, lr=0.000739, gnorm=3.728, clip=0, train_wall=7, gb_free=71.4, wall=1570 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:35:37] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 6.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:35:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 52        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69006 MiB |  71763 MiB | 308769 GiB | 308702 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 307018 GiB | 306950 GiB |
|       from small pool |     16 MiB |     18 MiB |   1751 GiB |   1751 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69006 MiB |  71763 MiB | 308769 GiB | 308702 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 307018 GiB | 306950 GiB |
|       from small pool |     16 MiB |     18 MiB |   1751 GiB |   1751 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 308120 GiB | 308053 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 306372 GiB | 306304 GiB |
|       from small pool |     16 MiB |     17 MiB |   1748 GiB |   1748 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78636 MiB |  78836 MiB | 712572 MiB | 633936 MiB |
|       from large pool |  78610 MiB |  78610 MiB | 708256 MiB | 629646 MiB |
|       from small pool |     26 MiB |    226 MiB |   4316 MiB |   4290 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6901 MiB |  10618 MiB | 298159 GiB | 298153 GiB |
|       from large pool |   6892 MiB |  10608 MiB | 296181 GiB | 296174 GiB |
|       from small pool |      9 MiB |     23 MiB |   1978 GiB |   1978 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   20344 K  |   20344 K  |
|       from large pool |     343    |     351    |    9674 K  |    9674 K  |
|       from small pool |     298    |     342    |   10670 K  |   10669 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   20344 K  |   20344 K  |
|       from large pool |     343    |     351    |    9674 K  |    9674 K  |
|       from small pool |     298    |     342    |   10670 K  |   10669 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     169    |     269    |    4525    |    4356    |
|       from large pool |     156    |     156    |    2367    |    2211    |
|       from small pool |      13    |     113    |    2158    |    2145    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     151    |     155    |   11473 K  |   11473 K  |
|       from large pool |     124    |     128    |    6323 K  |    6323 K  |
|       from small pool |      27    |      52    |    5150 K  |    5149 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:35:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:35:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:35:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:35:37] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:35:41]    INFO >> epoch 007:    343 / 1539 loss=3.374, wps=4000.6, ups=5.82, wpb=687.1, bsz=687.1, num_updates=9550, lr=0.000739, gnorm=4.213, clip=0, train_wall=8, gb_free=69.5, wall=1579 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:35:48]    INFO >> epoch 007:    393 / 1539 loss=3.66, wps=4312.1, ups=7.21, wpb=598.1, bsz=598.1, num_updates=9600, lr=0.000739, gnorm=3.212, clip=0, train_wall=7, gb_free=62, wall=1586 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:35:57]    INFO >> epoch 007:    443 / 1539 loss=3.375, wps=4466, ups=6.59, wpb=677.8, bsz=677.8, num_updates=9650, lr=0.000739, gnorm=3.684, clip=0, train_wall=7, gb_free=74.5, wall=1593 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:36:01] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 123.25 MiB is free. Including non-PyTorch memory, this process has 79.00 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:36:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 54        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76958 MiB |  77480 MiB | 313195 GiB | 313120 GiB |
|       from large pool |  76945 MiB |  77467 MiB | 311423 GiB | 311348 GiB |
|       from small pool |     12 MiB |     18 MiB |   1772 GiB |   1772 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76958 MiB |  77480 MiB | 313195 GiB | 313120 GiB |
|       from large pool |  76945 MiB |  77467 MiB | 311423 GiB | 311348 GiB |
|       from small pool |     12 MiB |     18 MiB |   1772 GiB |   1772 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 312537 GiB | 312462 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 310767 GiB | 310692 GiB |
|       from small pool |     12 MiB |     18 MiB |   1769 GiB |   1769 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80382 MiB |  80382 MiB | 719314 MiB | 638932 MiB |
|       from large pool |  80358 MiB |  80358 MiB | 714952 MiB | 634594 MiB |
|       from small pool |     24 MiB |     72 MiB |   4362 MiB |   4338 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3423 MiB |   7648 MiB | 302333 GiB | 302329 GiB |
|       from large pool |   3412 MiB |   7637 MiB | 300330 GiB | 300327 GiB |
|       from small pool |     11 MiB |     21 MiB |   2002 GiB |   2002 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   20614 K  |   20613 K  |
|       from large pool |     315    |     322    |    9816 K  |    9816 K  |
|       from small pool |     291    |     356    |   10797 K  |   10797 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   20614 K  |   20613 K  |
|       from large pool |     315    |     322    |    9816 K  |    9816 K  |
|       from small pool |     291    |     356    |   10797 K  |   10797 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     138    |     197    |    4556    |    4418    |
|       from large pool |     126    |     161    |    2375    |    2249    |
|       from small pool |      12    |      36    |    2181    |    2169    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     134    |     134    |   11622 K  |   11622 K  |
|       from large pool |     107    |     107    |    6417 K  |    6417 K  |
|       from small pool |      27    |      50    |    5204 K  |    5204 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:36:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:36:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:36:01] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:36:01] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:36:05]    INFO >> epoch 007:    494 / 1539 loss=3.482, wps=4450.3, ups=5.99, wpb=742.8, bsz=742.8, num_updates=9700, lr=0.000739, gnorm=3.343, clip=0, train_wall=7, gb_free=72.2, wall=1602 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:36:12]    INFO >> epoch 007:    544 / 1539 loss=3.495, wps=4573.4, ups=7.25, wpb=631.2, bsz=631.2, num_updates=9750, lr=0.000739, gnorm=3.396, clip=0, train_wall=6, gb_free=74.3, wall=1609 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:36:19]    INFO >> epoch 007:    594 / 1539 loss=3.445, wps=4805.1, ups=7.02, wpb=684.3, bsz=684.3, num_updates=9800, lr=0.000739, gnorm=3.164, clip=0, train_wall=7, gb_free=75.1, wall=1616 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:36:28]    INFO >> epoch 007:    644 / 1539 loss=3.427, wps=4633.3, ups=6.64, wpb=697.8, bsz=697.8, num_updates=9850, lr=0.000739, gnorm=4.018, clip=0, train_wall=7, gb_free=70.6, wall=1623 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:36:35]    INFO >> epoch 007:    694 / 1539 loss=3.421, wps=5468.1, ups=6.54, wpb=835.9, bsz=835.9, num_updates=9900, lr=0.000739, gnorm=3.762, clip=0, train_wall=7, gb_free=73.9, wall=1631 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:36:43]    INFO >> epoch 007:    744 / 1539 loss=3.512, wps=4839.5, ups=6.8, wpb=712, bsz=712, num_updates=9950, lr=0.000739, gnorm=3.17, clip=0, train_wall=7, gb_free=72.8, wall=1638 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:36:50]    INFO >> epoch 007:    794 / 1539 loss=3.485, wps=4490.9, ups=6.86, wpb=654.6, bsz=654.6, num_updates=10000, lr=0.000739, gnorm=3.743, clip=0, train_wall=7, gb_free=72.1, wall=1646 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:36:59]    INFO >> epoch 007:    844 / 1539 loss=3.525, wps=4906.1, ups=6.79, wpb=722.7, bsz=722.7, num_updates=10050, lr=0.000739, gnorm=3.295, clip=0, train_wall=7, gb_free=70.6, wall=1653 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:37:06]    INFO >> epoch 007:    894 / 1539 loss=3.499, wps=4603, ups=6.69, wpb=688.1, bsz=688.1, num_updates=10100, lr=0.000739, gnorm=3.534, clip=0, train_wall=7, gb_free=73.8, wall=1660 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:37:14]    INFO >> epoch 007:    944 / 1539 loss=3.501, wps=4801.1, ups=6.87, wpb=699, bsz=699, num_updates=10150, lr=0.000739, gnorm=3.897, clip=0, train_wall=7, gb_free=71.9, wall=1668 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:37:21]    INFO >> epoch 007:    994 / 1539 loss=3.525, wps=4826.5, ups=6.56, wpb=736.1, bsz=736.1, num_updates=10200, lr=0.000739, gnorm=3.305, clip=0, train_wall=7, gb_free=70.3, wall=1675 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:37:28]    INFO >> epoch 007:   1044 / 1539 loss=3.473, wps=4870.7, ups=6.9, wpb=705.7, bsz=705.7, num_updates=10250, lr=0.000739, gnorm=3.344, clip=0, train_wall=7, gb_free=73, wall=1683 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:37:37]    INFO >> epoch 007:   1094 / 1539 loss=3.377, wps=4620.4, ups=6.66, wpb=693.4, bsz=693.4, num_updates=10300, lr=0.000739, gnorm=3.606, clip=0, train_wall=7, gb_free=71.6, wall=1690 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:37:46]    INFO >> epoch 007:   1144 / 1539 loss=3.277, wps=4763.6, ups=6.07, wpb=784.9, bsz=784.9, num_updates=10350, lr=0.000739, gnorm=3.948, clip=0, train_wall=8, gb_free=74, wall=1698 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:37:54]    INFO >> epoch 007:   1194 / 1539 loss=3.397, wps=5080.4, ups=6.26, wpb=811.5, bsz=811.5, num_updates=10400, lr=0.000739, gnorm=4.057, clip=0, train_wall=8, gb_free=72.8, wall=1706 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:38:01]    INFO >> epoch 007:   1244 / 1539 loss=3.423, wps=4706.9, ups=6.86, wpb=686.2, bsz=686.2, num_updates=10450, lr=0.000739, gnorm=4.1, clip=2, train_wall=7, gb_free=68.3, wall=1714 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:38:09]    INFO >> epoch 007:   1294 / 1539 loss=3.505, wps=4479.7, ups=6.99, wpb=640.4, bsz=640.4, num_updates=10500, lr=0.000739, gnorm=3.385, clip=0, train_wall=7, gb_free=70.7, wall=1721 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:38:16]    INFO >> epoch 007:   1344 / 1539 loss=3.404, wps=4932.5, ups=7.07, wpb=697.4, bsz=697.4, num_updates=10550, lr=0.000739, gnorm=3.773, clip=0, train_wall=7, gb_free=70.4, wall=1728 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:38:24]    INFO >> epoch 007:   1394 / 1539 loss=3.323, wps=4678, ups=6.69, wpb=699.1, bsz=699.1, num_updates=10600, lr=0.000739, gnorm=3.653, clip=0, train_wall=7, gb_free=67.1, wall=1735 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:38:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 51.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.76 GiB is allocated by PyTorch, and 1.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:38:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 56        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78538 MiB |  78598 MiB | 341065 GiB | 340988 GiB |
|       from large pool |  78151 MiB |  78211 MiB | 339139 GiB | 339063 GiB |
|       from small pool |    386 MiB |    388 MiB |   1925 GiB |   1925 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78538 MiB |  78598 MiB | 341065 GiB | 340988 GiB |
|       from large pool |  78151 MiB |  78211 MiB | 339139 GiB | 339063 GiB |
|       from small pool |    386 MiB |    388 MiB |   1925 GiB |   1925 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78257 MiB |  78317 MiB | 340344 GiB | 340268 GiB |
|       from large pool |  77872 MiB |  77932 MiB | 338421 GiB | 338345 GiB |
|       from small pool |    384 MiB |    386 MiB |   1922 GiB |   1922 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80454 MiB |  80456 MiB | 750378 MiB | 669924 MiB |
|       from large pool |  80028 MiB |  80028 MiB | 745612 MiB | 665584 MiB |
|       from small pool |    426 MiB |    428 MiB |   4766 MiB |   4340 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1855 MiB |   6794 MiB | 329927 GiB | 329925 GiB |
|       from large pool |   1816 MiB |   6788 MiB | 327748 GiB | 327746 GiB |
|       from small pool |     39 MiB |     40 MiB |   2178 GiB |   2178 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7142    |    7145    |   22485 K  |   22478 K  |
|       from large pool |     919    |     920    |   10761 K  |   10760 K  |
|       from small pool |    6223    |    6226    |   11723 K  |   11717 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7142    |    7145    |   22485 K  |   22478 K  |
|       from large pool |     919    |     920    |   10761 K  |   10760 K  |
|       from small pool |    6223    |    6226    |   11723 K  |   11717 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     838    |     839    |    5269    |    4431    |
|       from large pool |     625    |     625    |    2886    |    2261    |
|       from small pool |     213    |     214    |    2383    |    2170    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     517    |     518    |   12660 K  |   12660 K  |
|       from large pool |     138    |     138    |    7032 K  |    7032 K  |
|       from small pool |     379    |     380    |    5627 K  |    5627 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:38:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:38:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:38:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:38:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:38:32]    INFO >> epoch 007:   1445 / 1539 loss=3.487, wps=4371.6, ups=6.11, wpb=716, bsz=716, num_updates=10650, lr=0.000739, gnorm=3.244, clip=0, train_wall=7, gb_free=68.1, wall=1744 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:38:40]    INFO >> epoch 007:   1495 / 1539 loss=3.469, wps=5084.1, ups=7.13, wpb=713.5, bsz=713.5, num_updates=10700, lr=0.000739, gnorm=3.219, clip=0, train_wall=7, gb_free=70.4, wall=1751 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:38:47]    INFO >> epoch 007 | loss 3.449 | wps 4469.9 | ups 6.27 | wpb 712.7 | bsz 712.7 | num_updates 10744 | lr 0.000739 | gnorm 3.561 | clip 0.1 | train_wall 216 | gb_free 70.2 | wall 1757 (progress_bar.py:267, print())[0m
[33m[2025-11-19 03:38:47] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:39:00]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.714 | wps 11409.4 | wpb 5412.5 | bsz 5412.5 | num_updates 10744 | best_loss 3.943 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 03:39:01]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:39:01]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/checkpoints/checkpoint_last.pt (epoch 7 @ 10744 updates, score 3.714) (writing took 0.014634 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 03:39:01] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:39:02]    INFO >> epoch 008:      6 / 1539 loss=3.308, wps=1718.9, ups=2.35, wpb=730.3, bsz=730.3, num_updates=10750, lr=0.000654, gnorm=2.952, clip=0, train_wall=7, gb_free=68.7, wall=1772 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:39:09]    INFO >> epoch 008:     56 / 1539 loss=3.317, wps=5046.7, ups=6.86, wpb=735.8, bsz=735.8, num_updates=10800, lr=0.000654, gnorm=3.767, clip=0, train_wall=7, gb_free=73.9, wall=1779 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:39:18]    INFO >> epoch 008:    106 / 1539 loss=3.408, wps=4962.3, ups=6.47, wpb=766.8, bsz=766.8, num_updates=10850, lr=0.000654, gnorm=3.498, clip=0, train_wall=7, gb_free=73.2, wall=1787 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:39:25]    INFO >> epoch 008:    156 / 1539 loss=3.398, wps=4547.6, ups=6.97, wpb=652.3, bsz=652.3, num_updates=10900, lr=0.000654, gnorm=3.37, clip=0, train_wall=7, gb_free=75.6, wall=1794 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:39:33]    INFO >> epoch 008:    206 / 1539 loss=3.433, wps=5273.1, ups=6.32, wpb=834.7, bsz=834.7, num_updates=10950, lr=0.000654, gnorm=3.641, clip=0, train_wall=7, gb_free=73.5, wall=1802 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:39:40]    INFO >> epoch 008:    256 / 1539 loss=3.469, wps=4852.3, ups=6.8, wpb=714.1, bsz=714.1, num_updates=11000, lr=0.000654, gnorm=3.19, clip=0, train_wall=7, gb_free=71.5, wall=1809 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:39:49]    INFO >> epoch 008:    306 / 1539 loss=3.601, wps=4883.9, ups=6.78, wpb=720.8, bsz=720.8, num_updates=11050, lr=0.000654, gnorm=3.631, clip=0, train_wall=7, gb_free=71.8, wall=1817 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:39:57]    INFO >> epoch 008:    356 / 1539 loss=3.436, wps=4171.6, ups=6.27, wpb=665.6, bsz=665.6, num_updates=11100, lr=0.000654, gnorm=3.59, clip=0, train_wall=8, gb_free=74.7, wall=1825 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:40:05]    INFO >> epoch 008:    406 / 1539 loss=3.454, wps=4751.5, ups=6.72, wpb=707, bsz=707, num_updates=11150, lr=0.000654, gnorm=3.993, clip=0, train_wall=7, gb_free=72.1, wall=1832 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:40:12]    INFO >> epoch 008:    456 / 1539 loss=3.359, wps=5653.6, ups=6.46, wpb=875.6, bsz=875.6, num_updates=11200, lr=0.000654, gnorm=3.834, clip=0, train_wall=7, gb_free=72.6, wall=1840 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:40:21]    INFO >> epoch 008:    506 / 1539 loss=3.463, wps=5115.7, ups=6.82, wpb=749.6, bsz=749.6, num_updates=11250, lr=0.000654, gnorm=3.358, clip=0, train_wall=7, gb_free=74.3, wall=1847 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:40:28]    INFO >> epoch 008:    556 / 1539 loss=3.363, wps=4439.3, ups=6.82, wpb=651.2, bsz=651.2, num_updates=11300, lr=0.000654, gnorm=3.642, clip=0, train_wall=7, gb_free=71.8, wall=1854 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:40:36]    INFO >> epoch 008:    606 / 1539 loss=3.502, wps=4585.6, ups=6.72, wpb=682.8, bsz=682.8, num_updates=11350, lr=0.000654, gnorm=3.407, clip=0, train_wall=7, gb_free=75.2, wall=1862 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:40:43]    INFO >> epoch 008:    656 / 1539 loss=3.459, wps=4624.9, ups=7.27, wpb=636.2, bsz=636.2, num_updates=11400, lr=0.000654, gnorm=3.279, clip=0, train_wall=6, gb_free=72.5, wall=1869 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:40:50]    INFO >> epoch 008:    706 / 1539 loss=3.38, wps=4708.1, ups=6.8, wpb=691.9, bsz=691.9, num_updates=11450, lr=0.000654, gnorm=3.921, clip=0, train_wall=7, gb_free=70, wall=1876 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:40:59]    INFO >> epoch 008:    756 / 1539 loss=3.493, wps=4857.1, ups=6.68, wpb=727, bsz=727, num_updates=11500, lr=0.000654, gnorm=3.311, clip=0, train_wall=7, gb_free=70.9, wall=1884 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:41:06]    INFO >> epoch 008:    806 / 1539 loss=3.408, wps=4648.8, ups=6.7, wpb=694.2, bsz=694.2, num_updates=11550, lr=0.000654, gnorm=3.21, clip=0, train_wall=7, gb_free=68.9, wall=1891 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:41:13]    INFO >> epoch 008:    856 / 1539 loss=3.415, wps=4349.1, ups=7.3, wpb=595.7, bsz=595.7, num_updates=11600, lr=0.000654, gnorm=2.864, clip=0, train_wall=6, gb_free=71.5, wall=1898 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:41:21]    INFO >> epoch 008:    906 / 1539 loss=3.235, wps=5091.2, ups=6.26, wpb=813.3, bsz=813.3, num_updates=11650, lr=0.000654, gnorm=3.689, clip=0, train_wall=8, gb_free=74, wall=1906 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:41:29]    INFO >> epoch 008:    956 / 1539 loss=3.439, wps=4832.8, ups=7.17, wpb=674.1, bsz=674.1, num_updates=11700, lr=0.000654, gnorm=3.47, clip=0, train_wall=7, gb_free=72.9, wall=1913 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:41:35] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 33.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:41:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 59        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78274 MiB |  78334 MiB | 377370 GiB | 377294 GiB |
|       from large pool |  78193 MiB |  78253 MiB | 375231 GiB | 375154 GiB |
|       from small pool |     80 MiB |     82 MiB |   2139 GiB |   2139 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78274 MiB |  78334 MiB | 377370 GiB | 377294 GiB |
|       from large pool |  78193 MiB |  78253 MiB | 375231 GiB | 375154 GiB |
|       from small pool |     80 MiB |     82 MiB |   2139 GiB |   2139 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78222 MiB |  78281 MiB | 376569 GiB | 376492 GiB |
|       from large pool |  78141 MiB |  78201 MiB | 374432 GiB | 374356 GiB |
|       from small pool |     80 MiB |     81 MiB |   2136 GiB |   2136 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80472 MiB |  80498 MiB |    830 GiB | 770050 MiB |
|       from large pool |  80388 MiB |  80388 MiB |    825 GiB | 765090 MiB |
|       from small pool |     84 MiB |    246 MiB |      4 GiB |   4960 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2137 MiB |   7144 MiB | 358004 GiB | 358002 GiB |
|       from large pool |   2134 MiB |   7133 MiB | 355584 GiB | 355582 GiB |
|       from small pool |      3 MiB |     31 MiB |   2420 GiB |   2420 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1796    |    1799    |   24927 K  |   24926 K  |
|       from large pool |     455    |     456    |   11890 K  |   11890 K  |
|       from small pool |    1341    |    1344    |   13037 K  |   13035 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1796    |    1799    |   24927 K  |   24926 K  |
|       from large pool |     455    |     456    |   11890 K  |   11890 K  |
|       from small pool |    1341    |    1344    |   13037 K  |   13035 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     241    |     562    |    5863    |    5622    |
|       from large pool |     199    |     439    |    3341    |    3142    |
|       from small pool |      42    |     123    |    2522    |    2480    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     157    |     158    |   14104 K  |   14104 K  |
|       from large pool |     113    |     113    |    7829 K  |    7829 K  |
|       from small pool |      44    |      58    |    6275 K  |    6275 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:41:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:41:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:41:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:41:35] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:41:38]    INFO >> epoch 008:   1007 / 1539 loss=3.447, wps=3747.8, ups=6.22, wpb=602.8, bsz=602.8, num_updates=11750, lr=0.000654, gnorm=3.363, clip=0, train_wall=6, gb_free=70.6, wall=1921 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:41:48]    INFO >> epoch 008:   1057 / 1539 loss=3.387, wps=4025.7, ups=4.86, wpb=828.4, bsz=828.4, num_updates=11800, lr=0.000654, gnorm=3.548, clip=0, train_wall=10, gb_free=71.7, wall=1931 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:41:55]    INFO >> epoch 008:   1107 / 1539 loss=3.463, wps=4648, ups=7.03, wpb=661.2, bsz=661.2, num_updates=11850, lr=0.000654, gnorm=3.435, clip=0, train_wall=7, gb_free=72.8, wall=1938 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:42:03]    INFO >> epoch 008:   1157 / 1539 loss=3.612, wps=3986.6, ups=7.15, wpb=557.5, bsz=557.5, num_updates=11900, lr=0.000654, gnorm=3.264, clip=0, train_wall=7, gb_free=73.2, wall=1945 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:42:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 523.25 MiB is free. Including non-PyTorch memory, this process has 78.61 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:42:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 61        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76954 MiB |  77477 MiB | 382821 GiB | 382746 GiB |
|       from large pool |  76941 MiB |  77464 MiB | 380654 GiB | 380579 GiB |
|       from small pool |     12 MiB |     19 MiB |   2167 GiB |   2167 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76954 MiB |  77477 MiB | 382821 GiB | 382746 GiB |
|       from large pool |  76941 MiB |  77464 MiB | 380654 GiB | 380579 GiB |
|       from small pool |     12 MiB |     19 MiB |   2167 GiB |   2167 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 382008 GiB | 381933 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 379844 GiB | 379769 GiB |
|       from small pool |     12 MiB |     19 MiB |   2164 GiB |   2164 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79982 MiB |  80024 MiB |    903 GiB |    825 GiB |
|       from large pool |  79958 MiB |  79958 MiB |    898 GiB |    820 GiB |
|       from small pool |     24 MiB |     66 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3027 MiB |   8674 MiB | 363935 GiB | 363932 GiB |
|       from large pool |   3016 MiB |   8662 MiB | 361483 GiB | 361480 GiB |
|       from small pool |     11 MiB |     23 MiB |   2451 GiB |   2451 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   25272 K  |   25272 K  |
|       from large pool |     315    |     322    |   12070 K  |   12070 K  |
|       from small pool |     291    |     348    |   13202 K  |   13201 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   25272 K  |   25272 K  |
|       from large pool |     315    |     322    |   12070 K  |   12070 K  |
|       from small pool |     291    |     348    |   13202 K  |   13201 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      90    |     111    |    5926    |    5836    |
|       from large pool |      78    |      78    |    3384    |    3306    |
|       from small pool |      12    |      33    |    2542    |    2530    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      95    |      95    |   14290 K  |   14290 K  |
|       from large pool |      67    |      67    |    7943 K  |    7943 K  |
|       from small pool |      28    |      52    |    6347 K  |    6347 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:42:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:42:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:42:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:42:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:42:12]    INFO >> epoch 008:   1208 / 1539 loss=3.494, wps=4324.2, ups=6.05, wpb=715, bsz=715, num_updates=11950, lr=0.000654, gnorm=3.678, clip=0, train_wall=7, gb_free=76, wall=1954 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:42:17] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 49.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.61 GiB is allocated by PyTorch, and 981.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:42:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 62        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79414 MiB |  79474 MiB | 385104 GiB | 385026 GiB |
|       from large pool |  79015 MiB |  79075 MiB | 382922 GiB | 382845 GiB |
|       from small pool |    398 MiB |    399 MiB |   2181 GiB |   2181 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79414 MiB |  79474 MiB | 385104 GiB | 385026 GiB |
|       from large pool |  79015 MiB |  79075 MiB | 382922 GiB | 382845 GiB |
|       from small pool |    398 MiB |    399 MiB |   2181 GiB |   2181 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79398 MiB |  79458 MiB | 384287 GiB | 384209 GiB |
|       from large pool |  79002 MiB |  79061 MiB | 382108 GiB | 382031 GiB |
|       from small pool |    396 MiB |    397 MiB |   2178 GiB |   2178 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80456 MiB |  80458 MiB |    903 GiB |    825 GiB |
|       from large pool |  80018 MiB |  80018 MiB |    898 GiB |    820 GiB |
|       from small pool |    438 MiB |    440 MiB |      5 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    981 MiB |   5675 MiB | 366570 GiB | 366569 GiB |
|       from large pool |    942 MiB |   5668 MiB | 364102 GiB | 364101 GiB |
|       from small pool |     39 MiB |     41 MiB |   2468 GiB |   2468 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7351    |    7354    |   25429 K  |   25421 K  |
|       from large pool |     938    |     939    |   12139 K  |   12138 K  |
|       from small pool |    6413    |    6416    |   13290 K  |   13283 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7351    |    7354    |   25429 K  |   25421 K  |
|       from large pool |     938    |     939    |   12139 K  |   12138 K  |
|       from small pool |    6413    |    6416    |   13290 K  |   13283 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     298    |     299    |    6135    |    5837    |
|       from large pool |      79    |      79    |    3385    |    3306    |
|       from small pool |     219    |     220    |    2750    |    2531    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     471    |     473    |   14378 K  |   14378 K  |
|       from large pool |      80    |      80    |    7986 K  |    7985 K  |
|       from small pool |     391    |     393    |    6392 K  |    6392 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:42:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:42:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:42:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:42:17] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:42:20]    INFO >> epoch 008:   1259 / 1539 loss=3.525, wps=5002.9, ups=5.88, wpb=850.7, bsz=850.7, num_updates=12000, lr=0.000654, gnorm=3.915, clip=0, train_wall=7, gb_free=73.2, wall=1962 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:42:27]    INFO >> epoch 008:   1309 / 1539 loss=3.46, wps=4800.9, ups=6.82, wpb=704.3, bsz=704.3, num_updates=12050, lr=0.000654, gnorm=3.537, clip=0, train_wall=7, gb_free=72.7, wall=1969 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:42:36]    INFO >> epoch 008:   1359 / 1539 loss=3.43, wps=4468.3, ups=6.72, wpb=664.7, bsz=664.7, num_updates=12100, lr=0.000654, gnorm=3.583, clip=0, train_wall=7, gb_free=69.7, wall=1977 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:42:38] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.20 GiB is free. Including non-PyTorch memory, this process has 77.92 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:42:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 33           |        cudaMalloc retries: 64        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76324 MiB |  78599 MiB | 388762 GiB | 388687 GiB |
|       from large pool |  76305 MiB |  78581 MiB | 386561 GiB | 386487 GiB |
|       from small pool |     18 MiB |     19 MiB |   2200 GiB |   2200 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76324 MiB |  78599 MiB | 388762 GiB | 388687 GiB |
|       from large pool |  76305 MiB |  78581 MiB | 386561 GiB | 386487 GiB |
|       from small pool |     18 MiB |     19 MiB |   2200 GiB |   2200 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 387938 GiB | 387863 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 385741 GiB | 385666 GiB |
|       from small pool |     18 MiB |     19 MiB |   2197 GiB |   2197 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79276 MiB |  80396 MiB |    974 GiB |    897 GiB |
|       from large pool |  79252 MiB |  79958 MiB |    969 GiB |    891 GiB |
|       from small pool |     24 MiB |    438 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2951 MiB |   4272 MiB | 370688 GiB | 370685 GiB |
|       from large pool |   2946 MiB |   4266 MiB | 368198 GiB | 368195 GiB |
|       from small pool |      5 MiB |     27 MiB |   2490 GiB |   2490 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     637    |     643    |   25655 K  |   25655 K  |
|       from large pool |     340    |     346    |   12256 K  |   12256 K  |
|       from small pool |     297    |     348    |   13399 K  |   13398 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     637    |     643    |   25655 K  |   25655 K  |
|       from large pool |     340    |     346    |   12256 K  |   12256 K  |
|       from small pool |     297    |     348    |   13399 K  |   13398 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     106    |     297    |    6193    |    6087    |
|       from large pool |      94    |      95    |    3443    |    3349    |
|       from small pool |      12    |     219    |    2750    |    2738    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     113    |     115    |   14501 K  |   14501 K  |
|       from large pool |      89    |      91    |    8059 K  |    8059 K  |
|       from small pool |      24    |      51    |    6442 K  |    6442 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:42:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:42:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:42:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:42:38] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:42:46]    INFO >> epoch 008:   1410 / 1539 loss=3.548, wps=4151, ups=5.04, wpb=824.4, bsz=824.4, num_updates=12150, lr=0.000654, gnorm=3.157, clip=0, train_wall=7, gb_free=73, wall=1987 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:42:54]    INFO >> epoch 008:   1460 / 1539 loss=3.441, wps=4248.2, ups=5.94, wpb=715.2, bsz=715.2, num_updates=12200, lr=0.000654, gnorm=4.038, clip=0, train_wall=8, gb_free=75, wall=1995 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:43:02]    INFO >> epoch 008:   1510 / 1539 loss=3.465, wps=4401.7, ups=6.61, wpb=666.4, bsz=666.4, num_updates=12250, lr=0.000654, gnorm=3.67, clip=0, train_wall=7, gb_free=67.3, wall=2003 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:43:06]    INFO >> epoch 008 | loss 3.442 | wps 4380.2 | ups 6.15 | wpb 712.7 | bsz 712.7 | num_updates 12279 | lr 0.000654 | gnorm 3.523 | clip 0 | train_wall 218 | gb_free 74.8 | wall 2007 (progress_bar.py:267, print())[0m
[33m[2025-11-19 03:43:06] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:43:21]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.74 | wps 11708.8 | wpb 5412.5 | bsz 5412.5 | num_updates 12279 | best_loss 3.943 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 03:43:21]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:43:21]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/checkpoints/checkpoint_last.pt (epoch 8 @ 12279 updates, score 3.74) (writing took 0.012514 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 03:43:21] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:43:24]    INFO >> epoch 009:     21 / 1539 loss=3.499, wps=1645.1, ups=2.46, wpb=668.9, bsz=668.9, num_updates=12300, lr=0.000568, gnorm=3.617, clip=0, train_wall=7, gb_free=73.1, wall=2023 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:43:31]    INFO >> epoch 009:     71 / 1539 loss=3.485, wps=4455.9, ups=6.72, wpb=663.6, bsz=663.6, num_updates=12350, lr=0.000568, gnorm=3.362, clip=0, train_wall=7, gb_free=73.5, wall=2031 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:43:38]    INFO >> epoch 009:    121 / 1539 loss=3.421, wps=4762.8, ups=6.92, wpb=688.4, bsz=688.4, num_updates=12400, lr=0.000568, gnorm=3.843, clip=0, train_wall=7, gb_free=73.7, wall=2038 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:43:48]    INFO >> epoch 009:    171 / 1539 loss=3.263, wps=5351.1, ups=6.35, wpb=842.4, bsz=842.4, num_updates=12450, lr=0.000568, gnorm=4.117, clip=0, train_wall=7, gb_free=71.8, wall=2046 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:43:55]    INFO >> epoch 009:    221 / 1539 loss=3.47, wps=4907.3, ups=7.1, wpb=691.5, bsz=691.5, num_updates=12500, lr=0.000568, gnorm=3.614, clip=0, train_wall=7, gb_free=71.5, wall=2053 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:44:02]    INFO >> epoch 009:    271 / 1539 loss=3.444, wps=4994.3, ups=7.1, wpb=703.6, bsz=703.6, num_updates=12550, lr=0.000568, gnorm=3.881, clip=0, train_wall=7, gb_free=70, wall=2060 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:44:09]    INFO >> epoch 009:    321 / 1539 loss=3.477, wps=5151.3, ups=6.57, wpb=784.5, bsz=784.5, num_updates=12600, lr=0.000568, gnorm=3.911, clip=0, train_wall=7, gb_free=66.3, wall=2067 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:44:19]    INFO >> epoch 009:    371 / 1539 loss=3.5, wps=4202.5, ups=6, wpb=701, bsz=701, num_updates=12650, lr=0.000568, gnorm=4.052, clip=0, train_wall=8, gb_free=65.8, wall=2076 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:44:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 213.25 MiB is free. Including non-PyTorch memory, this process has 78.91 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 4.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:44:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 34           |        cudaMalloc retries: 66        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75049 MiB |  75992 MiB | 409585 GiB | 409512 GiB |
|       from large pool |  75037 MiB |  75979 MiB | 407262 GiB | 407188 GiB |
|       from small pool |     12 MiB |     16 MiB |   2323 GiB |   2323 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75049 MiB |  75992 MiB | 409585 GiB | 409512 GiB |
|       from large pool |  75037 MiB |  75979 MiB | 407262 GiB | 407188 GiB |
|       from small pool |     12 MiB |     16 MiB |   2323 GiB |   2323 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 408722 GiB | 408649 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 406402 GiB | 406329 GiB |
|       from small pool |     12 MiB |     16 MiB |   2319 GiB |   2319 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80292 MiB |  80292 MiB |    979 GiB |    901 GiB |
|       from large pool |  80266 MiB |  80266 MiB |    974 GiB |    895 GiB |
|       from small pool |     26 MiB |    124 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5242 MiB |   8856 MiB | 391453 GiB | 391448 GiB |
|       from large pool |   5228 MiB |   8843 MiB | 388828 GiB | 388823 GiB |
|       from small pool |     13 MiB |     27 MiB |   2625 GiB |   2625 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   26995 K  |   26995 K  |
|       from large pool |     314    |     322    |   12839 K  |   12839 K  |
|       from small pool |     291    |     341    |   14155 K  |   14155 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   26995 K  |   26995 K  |
|       from large pool |     314    |     322    |   12839 K  |   12839 K  |
|       from small pool |     291    |     341    |   14155 K  |   14155 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     103    |     156    |    6250    |    6147    |
|       from large pool |      90    |      94    |    3450    |    3360    |
|       from small pool |      13    |      62    |    2800    |    2787    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      98    |      98    |   15249 K  |   15248 K  |
|       from large pool |      72    |      72    |    8431 K  |    8431 K  |
|       from small pool |      26    |      49    |    6817 K  |    6817 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:44:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:44:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:44:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:44:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:44:27]    INFO >> epoch 009:    422 / 1539 loss=3.492, wps=4354, ups=6.08, wpb=716.4, bsz=716.4, num_updates=12700, lr=0.000568, gnorm=3.532, clip=0, train_wall=7, gb_free=73.1, wall=2084 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:44:34]    INFO >> epoch 009:    472 / 1539 loss=3.369, wps=4822.4, ups=6.98, wpb=691.3, bsz=691.3, num_updates=12750, lr=0.000568, gnorm=3.397, clip=0, train_wall=7, gb_free=74, wall=2091 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:44:42]    INFO >> epoch 009:    522 / 1539 loss=3.335, wps=5029.9, ups=6.61, wpb=760.8, bsz=760.8, num_updates=12800, lr=0.000568, gnorm=3.494, clip=0, train_wall=7, gb_free=72.7, wall=2099 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:44:51]    INFO >> epoch 009:    572 / 1539 loss=3.429, wps=4787, ups=6.38, wpb=750.9, bsz=750.9, num_updates=12850, lr=0.000568, gnorm=3.911, clip=0, train_wall=7, gb_free=74.2, wall=2107 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:44:52] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 25.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.35 GiB is allocated by PyTorch, and 1.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:44:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 35           |        cudaMalloc retries: 67        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79147 MiB |  79207 MiB | 415679 GiB | 415601 GiB |
|       from large pool |  79057 MiB |  79117 MiB | 413324 GiB | 413246 GiB |
|       from small pool |     89 MiB |     91 MiB |   2355 GiB |   2354 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79147 MiB |  79207 MiB | 415679 GiB | 415601 GiB |
|       from large pool |  79057 MiB |  79117 MiB | 413324 GiB | 413246 GiB |
|       from small pool |     89 MiB |     91 MiB |   2355 GiB |   2354 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79123 MiB |  79183 MiB | 414803 GiB | 414726 GiB |
|       from large pool |  79034 MiB |  79093 MiB | 412452 GiB | 412375 GiB |
|       from small pool |     89 MiB |     90 MiB |   2351 GiB |   2351 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80480 MiB |  80482 MiB |    979 GiB |    901 GiB |
|       from large pool |  80386 MiB |  80386 MiB |    974 GiB |    895 GiB |
|       from small pool |     94 MiB |     96 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1272 MiB |   7349 MiB | 398191 GiB | 398190 GiB |
|       from large pool |   1268 MiB |   7339 MiB | 395529 GiB | 395528 GiB |
|       from small pool |      4 MiB |     27 MiB |   2662 GiB |   2662 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1961    |    1964    |   27385 K  |   27383 K  |
|       from large pool |     470    |     471    |   13039 K  |   13038 K  |
|       from small pool |    1491    |    1494    |   14346 K  |   14345 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1961    |    1964    |   27385 K  |   27383 K  |
|       from large pool |     470    |     471    |   13039 K  |   13038 K  |
|       from small pool |    1491    |    1494    |   14346 K  |   14345 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     139    |     140    |    6287    |    6148    |
|       from large pool |      92    |      92    |    3452    |    3360    |
|       from small pool |      47    |      48    |    2835    |    2788    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     129    |     130    |   15460 K  |   15459 K  |
|       from large pool |      78    |      79    |    8556 K  |    8556 K  |
|       from small pool |      51    |      57    |    6903 K  |    6903 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:44:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:44:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:44:52] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:44:52] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:44:59]    INFO >> epoch 009:    623 / 1539 loss=3.457, wps=4339.7, ups=6.12, wpb=709.4, bsz=709.4, num_updates=12900, lr=0.000568, gnorm=3.386, clip=0, train_wall=7, gb_free=65.9, wall=2115 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:45:06]    INFO >> epoch 009:    673 / 1539 loss=3.46, wps=4621.7, ups=7.02, wpb=658.7, bsz=658.7, num_updates=12950, lr=0.000568, gnorm=3.356, clip=0, train_wall=7, gb_free=71.5, wall=2122 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:45:14]    INFO >> epoch 009:    723 / 1539 loss=3.475, wps=4719.5, ups=7.03, wpb=671.1, bsz=671.1, num_updates=13000, lr=0.000568, gnorm=3.352, clip=0, train_wall=7, gb_free=71.2, wall=2129 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:45:23]    INFO >> epoch 009:    773 / 1539 loss=3.512, wps=4533.7, ups=6.53, wpb=693.9, bsz=693.9, num_updates=13050, lr=0.000568, gnorm=3.36, clip=0, train_wall=7, gb_free=57.9, wall=2137 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:45:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 155.25 MiB is free. Including non-PyTorch memory, this process has 78.96 GiB memory in use. Of the allocated memory 77.59 GiB is allocated by PyTorch, and 893.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:45:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 36           |        cudaMalloc retries: 68        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79058 MiB |  79456 MiB | 422841 GiB | 422763 GiB |
|       from large pool |  79039 MiB |  79437 MiB | 420449 GiB | 420372 GiB |
|       from small pool |     18 MiB |     19 MiB |   2391 GiB |   2391 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79058 MiB |  79456 MiB | 422841 GiB | 422763 GiB |
|       from large pool |  79039 MiB |  79437 MiB | 420449 GiB | 420372 GiB |
|       from small pool |     18 MiB |     19 MiB |   2391 GiB |   2391 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79031 MiB |  79429 MiB | 421950 GiB | 421873 GiB |
|       from large pool |  79012 MiB |  79410 MiB | 419562 GiB | 419485 GiB |
|       from small pool |     18 MiB |     19 MiB |   2388 GiB |   2388 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80350 MiB |  80420 MiB |    979 GiB |    901 GiB |
|       from large pool |  80326 MiB |  80326 MiB |    974 GiB |    895 GiB |
|       from small pool |     24 MiB |     94 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1291 MiB |   5527 MiB | 406064 GiB | 406063 GiB |
|       from large pool |   1286 MiB |   5521 MiB | 403360 GiB | 403359 GiB |
|       from small pool |      5 MiB |     21 MiB |   2704 GiB |   2704 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     647    |   27845 K  |   27844 K  |
|       from large pool |     341    |     347    |   13278 K  |   13278 K  |
|       from small pool |     300    |     348    |   14567 K  |   14566 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     647    |   27845 K  |   27844 K  |
|       from large pool |     341    |     347    |   13278 K  |   13278 K  |
|       from small pool |     300    |     348    |   14567 K  |   14566 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     103    |     138    |    6287    |    6184    |
|       from large pool |      91    |      91    |    3452    |    3361    |
|       from small pool |      12    |      47    |    2835    |    2823    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      94    |     100    |   15707 K  |   15707 K  |
|       from large pool |      70    |      76    |    8707 K  |    8707 K  |
|       from small pool |      24    |      45    |    7000 K  |    7000 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:45:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:45:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:45:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:45:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:45:31]    INFO >> epoch 009:    824 / 1539 loss=3.443, wps=4525.3, ups=6.29, wpb=719.6, bsz=719.6, num_updates=13100, lr=0.000568, gnorm=3.909, clip=0, train_wall=7, gb_free=70.5, wall=2145 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:45:37]    INFO >> epoch 009:    874 / 1539 loss=3.497, wps=4672.3, ups=7.25, wpb=644.4, bsz=644.4, num_updates=13150, lr=0.000568, gnorm=3.222, clip=0, train_wall=6, gb_free=74.8, wall=2151 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:45:44]    INFO >> epoch 009:    924 / 1539 loss=3.525, wps=4436.1, ups=7.17, wpb=618.7, bsz=618.7, num_updates=13200, lr=0.000568, gnorm=3.074, clip=0, train_wall=7, gb_free=73.5, wall=2158 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:45:54]    INFO >> epoch 009:    974 / 1539 loss=3.365, wps=4719, ups=5.49, wpb=859.7, bsz=859.7, num_updates=13250, lr=0.000568, gnorm=3.869, clip=0, train_wall=9, gb_free=70.8, wall=2167 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:46:02]    INFO >> epoch 009:   1024 / 1539 loss=3.453, wps=4879.5, ups=6.79, wpb=718.9, bsz=718.9, num_updates=13300, lr=0.000568, gnorm=3.416, clip=0, train_wall=7, gb_free=69.9, wall=2175 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:46:10]    INFO >> epoch 009:   1074 / 1539 loss=3.375, wps=4850, ups=6.68, wpb=725.8, bsz=725.8, num_updates=13350, lr=0.000568, gnorm=3.745, clip=0, train_wall=7, gb_free=65.3, wall=2182 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:46:17]    INFO >> epoch 009:   1124 / 1539 loss=3.484, wps=4964.8, ups=6.53, wpb=760.3, bsz=760.3, num_updates=13400, lr=0.000568, gnorm=4.107, clip=0, train_wall=7, gb_free=73.5, wall=2190 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:46:24]    INFO >> epoch 009:   1174 / 1539 loss=3.57, wps=4784.3, ups=7.14, wpb=669.6, bsz=669.6, num_updates=13450, lr=0.000568, gnorm=3.013, clip=0, train_wall=7, gb_free=72.7, wall=2197 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:46:34]    INFO >> epoch 009:   1224 / 1539 loss=3.248, wps=5237.3, ups=6.08, wpb=862, bsz=862, num_updates=13500, lr=0.000568, gnorm=3.323, clip=0, train_wall=8, gb_free=69, wall=2205 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:46:42]    INFO >> epoch 009:   1274 / 1539 loss=3.457, wps=5057.5, ups=6.51, wpb=776.3, bsz=776.3, num_updates=13550, lr=0.000568, gnorm=3.65, clip=0, train_wall=7, gb_free=72.6, wall=2213 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:46:49]    INFO >> epoch 009:   1324 / 1539 loss=3.475, wps=4561.2, ups=6.4, wpb=712.6, bsz=712.6, num_updates=13600, lr=0.000568, gnorm=3.401, clip=0, train_wall=7, gb_free=75.1, wall=2221 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:46:57]    INFO >> epoch 009:   1374 / 1539 loss=3.062, wps=4920.2, ups=6.56, wpb=749.6, bsz=749.6, num_updates=13650, lr=0.000568, gnorm=3.146, clip=0, train_wall=7, gb_free=69.9, wall=2228 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:47:04] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 37.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.24 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:47:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 37           |        cudaMalloc retries: 70        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78007 MiB |  78067 MiB | 440587 GiB | 440511 GiB |
|       from large pool |  77625 MiB |  77685 MiB | 438093 GiB | 438017 GiB |
|       from small pool |    382 MiB |    383 MiB |   2494 GiB |   2493 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78007 MiB |  78067 MiB | 440587 GiB | 440511 GiB |
|       from large pool |  77625 MiB |  77685 MiB | 438093 GiB | 438017 GiB |
|       from small pool |    382 MiB |    383 MiB |   2494 GiB |   2493 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77777 MiB |  77836 MiB | 439658 GiB | 439582 GiB |
|       from large pool |  77397 MiB |  77456 MiB | 437167 GiB | 437092 GiB |
|       from small pool |    380 MiB |    381 MiB |   2490 GiB |   2490 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80468 MiB |  80468 MiB |   1006 GiB |    927 GiB |
|       from large pool |  80046 MiB |  80046 MiB |   1000 GiB |    922 GiB |
|       from small pool |    422 MiB |    422 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2400 MiB |   7496 MiB | 423371 GiB | 423368 GiB |
|       from large pool |   2360 MiB |   7489 MiB | 420548 GiB | 420545 GiB |
|       from small pool |     39 MiB |     40 MiB |   2823 GiB |   2822 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7054    |    7057    |   29063 K  |   29056 K  |
|       from large pool |     911    |     912    |   13873 K  |   13872 K  |
|       from small pool |    6143    |    6146    |   15190 K  |   15184 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7054    |    7057    |   29063 K  |   29056 K  |
|       from large pool |     911    |     912    |   13873 K  |   13872 K  |
|       from small pool |    6143    |    6146    |   15190 K  |   15184 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     742    |     742    |    6936    |    6194    |
|       from large pool |     531    |     531    |    3902    |    3371    |
|       from small pool |     211    |     211    |    3034    |    2823    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     503    |     503    |   16390 K  |   16390 K  |
|       from large pool |     127    |     127    |    9094 K  |    9094 K  |
|       from small pool |     376    |     376    |    7296 K  |    7295 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:47:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:47:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:47:04] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:47:04] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:47:06]    INFO >> epoch 009:   1425 / 1539 loss=3.328, wps=4492.3, ups=6.64, wpb=676.7, bsz=676.7, num_updates=13700, lr=0.000568, gnorm=4.023, clip=0, train_wall=7, gb_free=72, wall=2236 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:47:13]    INFO >> epoch 009:   1475 / 1539 loss=3.553, wps=4403.6, ups=6.94, wpb=634.1, bsz=634.1, num_updates=13750, lr=0.000568, gnorm=3.238, clip=0, train_wall=7, gb_free=72.9, wall=2243 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:47:20]    INFO >> epoch 009:   1525 / 1539 loss=3.435, wps=4307.8, ups=6.91, wpb=623.3, bsz=623.3, num_updates=13800, lr=0.000568, gnorm=3.268, clip=0, train_wall=7, gb_free=67.9, wall=2250 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:47:22]    INFO >> epoch 009 | loss 3.427 | wps 4458.9 | ups 6.26 | wpb 712.7 | bsz 712.7 | num_updates 13814 | lr 0.000568 | gnorm 3.562 | clip 0 | train_wall 217 | gb_free 74.2 | wall 2252 (progress_bar.py:267, print())[0m
[33m[2025-11-19 03:47:22] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:47:35]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.584 | wps 11697.9 | wpb 5412.5 | bsz 5412.5 | num_updates 13814 | best_loss 3.943 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 03:47:37]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:47:37]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/checkpoints/checkpoint_last.pt (epoch 9 @ 13814 updates, score 3.584) (writing took 0.013269 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 03:47:37] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:47:44]    INFO >> epoch 010:     36 / 1539 loss=3.411, wps=1588.1, ups=2.26, wpb=701.4, bsz=701.4, num_updates=13850, lr=0.000483, gnorm=3.291, clip=0, train_wall=8, gb_free=72.8, wall=2272 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:47:51]    INFO >> epoch 010:     86 / 1539 loss=3.408, wps=5017.5, ups=7, wpb=716.9, bsz=716.9, num_updates=13900, lr=0.000483, gnorm=3.389, clip=0, train_wall=7, gb_free=69.6, wall=2280 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:47:58]    INFO >> epoch 010:    136 / 1539 loss=3.433, wps=4400.3, ups=7.02, wpb=626.5, bsz=626.5, num_updates=13950, lr=0.000483, gnorm=3.737, clip=0, train_wall=7, gb_free=74.1, wall=2287 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:48:06]    INFO >> epoch 010:    186 / 1539 loss=3.306, wps=4599.7, ups=6.18, wpb=744.2, bsz=744.2, num_updates=14000, lr=0.000483, gnorm=4.586, clip=0, train_wall=8, gb_free=74.2, wall=2295 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:48:16]    INFO >> epoch 010:    236 / 1539 loss=3.407, wps=5216.8, ups=6.04, wpb=863.9, bsz=863.9, num_updates=14050, lr=0.000483, gnorm=3.567, clip=0, train_wall=8, gb_free=74.4, wall=2303 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:48:23]    INFO >> epoch 010:    286 / 1539 loss=3.404, wps=4454, ups=6.59, wpb=676, bsz=676, num_updates=14100, lr=0.000483, gnorm=3.625, clip=0, train_wall=7, gb_free=73.9, wall=2311 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:48:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 951.25 MiB is free. Including non-PyTorch memory, this process has 78.19 GiB memory in use. Of the allocated memory 74.10 GiB is allocated by PyTorch, and 3.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:48:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 38           |        cudaMalloc retries: 72        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72064 MiB |  75873 MiB | 458286 GiB | 458216 GiB |
|       from large pool |  72046 MiB |  75855 MiB | 455686 GiB | 455615 GiB |
|       from small pool |     17 MiB |     18 MiB |   2600 GiB |   2600 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72064 MiB |  75873 MiB | 458286 GiB | 458216 GiB |
|       from large pool |  72046 MiB |  75855 MiB | 455686 GiB | 455615 GiB |
|       from small pool |     17 MiB |     18 MiB |   2600 GiB |   2600 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  75852 MiB | 457323 GiB | 457252 GiB |
|       from large pool |  72027 MiB |  75835 MiB | 454726 GiB | 454656 GiB |
|       from small pool |     17 MiB |     18 MiB |   2596 GiB |   2596 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79554 MiB |  79754 MiB |   1029 GiB |    952 GiB |
|       from large pool |  79528 MiB |  79528 MiB |   1023 GiB |    946 GiB |
|       from small pool |     26 MiB |    226 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4761 MiB |   7040 MiB | 438558 GiB | 438554 GiB |
|       from large pool |   4753 MiB |   7031 MiB | 435619 GiB | 435614 GiB |
|       from small pool |      8 MiB |     23 MiB |   2939 GiB |   2939 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     615    |   30202 K  |   30201 K  |
|       from large pool |     308    |     317    |   14352 K  |   14352 K  |
|       from small pool |     298    |     342    |   15849 K  |   15849 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     615    |   30202 K  |   30201 K  |
|       from large pool |     308    |     317    |   14352 K  |   14352 K  |
|       from small pool |     298    |     342    |   15849 K  |   15849 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     150    |     250    |    7046    |    6896    |
|       from large pool |     137    |     137    |    3911    |    3774    |
|       from small pool |      13    |     113    |    3135    |    3122    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     139    |     140    |   17056 K  |   17056 K  |
|       from large pool |     113    |     114    |    9412 K  |    9412 K  |
|       from small pool |      26    |      49    |    7644 K  |    7644 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:48:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:48:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:48:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:48:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:48:31]    INFO >> epoch 010:    337 / 1539 loss=3.389, wps=4564.9, ups=6.49, wpb=703.9, bsz=703.9, num_updates=14150, lr=0.000483, gnorm=3.265, clip=0, train_wall=7, gb_free=67.2, wall=2318 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:48:39]    INFO >> epoch 010:    387 / 1539 loss=3.443, wps=4566.8, ups=6.31, wpb=723.5, bsz=723.5, num_updates=14200, lr=0.000483, gnorm=3.665, clip=0, train_wall=7, gb_free=73.3, wall=2326 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:48:47]    INFO >> epoch 010:    437 / 1539 loss=3.426, wps=4415.1, ups=6.99, wpb=631.4, bsz=631.4, num_updates=14250, lr=0.000483, gnorm=3.255, clip=0, train_wall=7, gb_free=72.6, wall=2333 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:48:55]    INFO >> epoch 010:    487 / 1539 loss=3.417, wps=4377.6, ups=6.55, wpb=668.5, bsz=668.5, num_updates=14300, lr=0.000483, gnorm=3.079, clip=0, train_wall=7, gb_free=72.5, wall=2341 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:49:02]    INFO >> epoch 010:    537 / 1539 loss=3.262, wps=4705.7, ups=7.29, wpb=645.7, bsz=645.7, num_updates=14350, lr=0.000483, gnorm=3.639, clip=0, train_wall=6, gb_free=75, wall=2348 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:49:10]    INFO >> epoch 010:    587 / 1539 loss=3.52, wps=4351.6, ups=6.54, wpb=665.2, bsz=665.2, num_updates=14400, lr=0.000483, gnorm=3.086, clip=0, train_wall=7, gb_free=71.4, wall=2356 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:49:18]    INFO >> epoch 010:    637 / 1539 loss=3.494, wps=5021.9, ups=6.98, wpb=719.7, bsz=719.7, num_updates=14450, lr=0.000483, gnorm=3.033, clip=0, train_wall=7, gb_free=69.1, wall=2363 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:49:25]    INFO >> epoch 010:    687 / 1539 loss=3.299, wps=5077.1, ups=6.99, wpb=726.6, bsz=726.6, num_updates=14500, lr=0.000483, gnorm=3.427, clip=0, train_wall=7, gb_free=73.3, wall=2370 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:49:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 41.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.29 GiB is allocated by PyTorch, and 2.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:49:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 39           |        cudaMalloc retries: 73        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78063 MiB |  78123 MiB | 469353 GiB | 469277 GiB |
|       from large pool |  77678 MiB |  77738 MiB | 466692 GiB | 466616 GiB |
|       from small pool |    384 MiB |    385 MiB |   2661 GiB |   2660 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78063 MiB |  78123 MiB | 469353 GiB | 469277 GiB |
|       from large pool |  77678 MiB |  77738 MiB | 466692 GiB | 466616 GiB |
|       from small pool |    384 MiB |    385 MiB |   2661 GiB |   2660 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78017 MiB |  78077 MiB | 468366 GiB | 468290 GiB |
|       from large pool |  77635 MiB |  77694 MiB | 465709 GiB | 465633 GiB |
|       from small pool |    382 MiB |    383 MiB |   2657 GiB |   2656 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80464 MiB |  80464 MiB |   1033 GiB |    954 GiB |
|       from large pool |  80040 MiB |  80040 MiB |   1026 GiB |    948 GiB |
|       from small pool |    424 MiB |    424 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2340 MiB |   6606 MiB | 449463 GiB | 449460 GiB |
|       from large pool |   2301 MiB |   6599 MiB | 446453 GiB | 446451 GiB |
|       from small pool |     39 MiB |     40 MiB |   3009 GiB |   3009 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7098    |    7101    |   30947 K  |   30940 K  |
|       from large pool |     915    |     916    |   14731 K  |   14730 K  |
|       from small pool |    6183    |    6186    |   16216 K  |   16209 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7098    |    7101    |   30947 K  |   30940 K  |
|       from large pool |     915    |     916    |   14731 K  |   14730 K  |
|       from small pool |    6183    |    6186    |   16216 K  |   16209 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     402    |     402    |    7299    |    6897    |
|       from large pool |     190    |     190    |    3965    |    3775    |
|       from small pool |     212    |     212    |    3334    |    3122    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     495    |     496    |   17470 K  |   17469 K  |
|       from large pool |     118    |     118    |    9660 K  |    9660 K  |
|       from small pool |     377    |     378    |    7809 K  |    7809 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:49:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:49:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:49:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:49:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:49:33]    INFO >> epoch 010:    738 / 1539 loss=3.593, wps=4250.9, ups=6.61, wpb=642.7, bsz=642.7, num_updates=14550, lr=0.000483, gnorm=2.945, clip=0, train_wall=7, gb_free=68.7, wall=2377 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:49:40]    INFO >> epoch 010:    788 / 1539 loss=3.385, wps=4766.3, ups=6.59, wpb=723.3, bsz=723.3, num_updates=14600, lr=0.000483, gnorm=3.375, clip=0, train_wall=7, gb_free=67.8, wall=2385 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:49:48]    INFO >> epoch 010:    838 / 1539 loss=3.505, wps=4528.5, ups=6.72, wpb=673.9, bsz=673.9, num_updates=14650, lr=0.000483, gnorm=3.851, clip=0, train_wall=7, gb_free=66.8, wall=2392 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:49:57]    INFO >> epoch 010:    888 / 1539 loss=3.247, wps=5203.9, ups=6.13, wpb=848.7, bsz=848.7, num_updates=14700, lr=0.000483, gnorm=4.12, clip=0, train_wall=8, gb_free=76.2, wall=2401 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:50:05]    INFO >> epoch 010:    938 / 1539 loss=3.448, wps=4921.5, ups=6.9, wpb=713.5, bsz=713.5, num_updates=14750, lr=0.000483, gnorm=3.393, clip=0, train_wall=7, gb_free=74.7, wall=2408 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:50:12]    INFO >> epoch 010:    988 / 1539 loss=3.525, wps=4841.8, ups=7.01, wpb=690.7, bsz=690.7, num_updates=14800, lr=0.000483, gnorm=3.497, clip=0, train_wall=7, gb_free=74.4, wall=2415 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:50:19]    INFO >> epoch 010:   1038 / 1539 loss=3.516, wps=4664.7, ups=6.94, wpb=671.7, bsz=671.7, num_updates=14850, lr=0.000483, gnorm=3.379, clip=0, train_wall=7, gb_free=70, wall=2422 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:50:28]    INFO >> epoch 010:   1088 / 1539 loss=3.432, wps=5382.8, ups=6.7, wpb=803.8, bsz=803.8, num_updates=14900, lr=0.000483, gnorm=3.426, clip=0, train_wall=7, gb_free=72.8, wall=2430 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:50:35]    INFO >> epoch 010:   1138 / 1539 loss=3.401, wps=4202.1, ups=6.86, wpb=612.7, bsz=612.7, num_updates=14950, lr=0.000483, gnorm=3.337, clip=0, train_wall=7, gb_free=71.8, wall=2437 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:50:42] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.47 GiB is allocated by PyTorch, and 2.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:50:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 40           |        cudaMalloc retries: 75        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78250 MiB |  78310 MiB | 483183 GiB | 483106 GiB |
|       from large pool |  78169 MiB |  78229 MiB | 480446 GiB | 480370 GiB |
|       from small pool |     80 MiB |     82 MiB |   2736 GiB |   2736 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78250 MiB |  78310 MiB | 483183 GiB | 483106 GiB |
|       from large pool |  78169 MiB |  78229 MiB | 480446 GiB | 480370 GiB |
|       from small pool |     80 MiB |     82 MiB |   2736 GiB |   2736 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78222 MiB |  78281 MiB | 482164 GiB | 482088 GiB |
|       from large pool |  78141 MiB |  78201 MiB | 479431 GiB | 479355 GiB |
|       from small pool |     80 MiB |     81 MiB |   2732 GiB |   2732 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB |   1034 GiB |    955 GiB |
|       from large pool |  80404 MiB |  80404 MiB |   1027 GiB |    949 GiB |
|       from small pool |     84 MiB |    424 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2177 MiB |   9884 MiB | 462313 GiB | 462311 GiB |
|       from large pool |   2174 MiB |   9877 MiB | 459216 GiB | 459214 GiB |
|       from small pool |      3 MiB |     21 MiB |   3097 GiB |   3097 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1796    |    1799    |   31868 K  |   31866 K  |
|       from large pool |     455    |     456    |   15195 K  |   15195 K  |
|       from small pool |    1341    |    1344    |   16672 K  |   16671 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1796    |    1799    |   31868 K  |   31866 K  |
|       from large pool |     455    |     456    |   15195 K  |   15195 K  |
|       from small pool |    1341    |    1344    |   16672 K  |   16671 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     224    |     401    |    7333    |    7109    |
|       from large pool |     182    |     189    |    3968    |    3786    |
|       from small pool |      42    |     212    |    3365    |    3323    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     183    |     186    |   17990 K  |   17989 K  |
|       from large pool |     140    |     144    |    9969 K  |    9969 K  |
|       from small pool |      43    |      50    |    8020 K  |    8020 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:50:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:50:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:50:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:50:42] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:50:44]    INFO >> epoch 010:   1189 / 1539 loss=3.471, wps=4620.3, ups=5.86, wpb=787.9, bsz=787.9, num_updates=15000, lr=0.000483, gnorm=3.547, clip=0, train_wall=7, gb_free=70.1, wall=2445 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:50:51]    INFO >> epoch 010:   1239 / 1539 loss=3.4, wps=4996.4, ups=6.35, wpb=786.3, bsz=786.3, num_updates=15050, lr=0.000483, gnorm=3.385, clip=0, train_wall=7, gb_free=73.2, wall=2453 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:51:01]    INFO >> epoch 010:   1289 / 1539 loss=3.397, wps=4656.1, ups=6.07, wpb=766.5, bsz=766.5, num_updates=15100, lr=0.000483, gnorm=3.279, clip=0, train_wall=8, gb_free=72.7, wall=2462 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:51:08]    INFO >> epoch 010:   1339 / 1539 loss=3.53, wps=4554.7, ups=6.74, wpb=675.5, bsz=675.5, num_updates=15150, lr=0.000483, gnorm=3.149, clip=0, train_wall=7, gb_free=74.4, wall=2469 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:51:16]    INFO >> epoch 010:   1389 / 1539 loss=3.345, wps=5011, ups=6.72, wpb=746.1, bsz=746.1, num_updates=15200, lr=0.000483, gnorm=3.634, clip=0, train_wall=7, gb_free=73.6, wall=2476 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:51:23]    INFO >> epoch 010:   1439 / 1539 loss=3.444, wps=4707.1, ups=6.93, wpb=679.6, bsz=679.6, num_updates=15250, lr=0.000483, gnorm=3.566, clip=0, train_wall=7, gb_free=70.1, wall=2484 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:51:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 251.25 MiB is free. Including non-PyTorch memory, this process has 78.87 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 4.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:51:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 41           |        cudaMalloc retries: 78        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72822 MiB |  75426 MiB | 491921 GiB | 491850 GiB |
|       from large pool |  72809 MiB |  75413 MiB | 489137 GiB | 489066 GiB |
|       from small pool |     12 MiB |     24 MiB |   2784 GiB |   2784 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72822 MiB |  75426 MiB | 491921 GiB | 491850 GiB |
|       from large pool |  72809 MiB |  75413 MiB | 489137 GiB | 489066 GiB |
|       from small pool |     12 MiB |     24 MiB |   2784 GiB |   2784 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 490883 GiB | 490812 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 488103 GiB | 488032 GiB |
|       from small pool |     12 MiB |     24 MiB |   2780 GiB |   2780 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80254 MiB |  80254 MiB |   1073 GiB |    995 GiB |
|       from large pool |  80228 MiB |  80228 MiB |   1066 GiB |    988 GiB |
|       from small pool |     26 MiB |    210 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5153 MiB |   9503 MiB | 470017 GiB | 470012 GiB |
|       from large pool |   5140 MiB |   9488 MiB | 466866 GiB | 466861 GiB |
|       from small pool |     13 MiB |     31 MiB |   3151 GiB |   3151 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   32444 K  |   32444 K  |
|       from large pool |     312    |     320    |   15487 K  |   15486 K  |
|       from small pool |     291    |     356    |   16957 K  |   16957 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   32444 K  |   32444 K  |
|       from large pool |     312    |     320    |   15487 K  |   15486 K  |
|       from small pool |     291    |     356    |   16957 K  |   16957 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     185    |     325    |    7505    |    7320    |
|       from large pool |     172    |     220    |    4077    |    3905    |
|       from small pool |      13    |     105    |    3428    |    3415    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     168    |     169    |   18317 K  |   18316 K  |
|       from large pool |     142    |     143    |   10165 K  |   10165 K  |
|       from small pool |      26    |      59    |    8151 K  |    8151 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:51:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:51:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:51:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:51:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:51:32]    INFO >> epoch 010:   1490 / 1539 loss=3.473, wps=4356.9, ups=6.27, wpb=694.7, bsz=694.7, num_updates=15300, lr=0.000483, gnorm=3.62, clip=0, train_wall=7, gb_free=64.7, wall=2492 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:51:40]    INFO >> epoch 010 | loss 3.421 | wps 4436.5 | ups 6.23 | wpb 712.7 | bsz 712.7 | num_updates 15349 | lr 0.000483 | gnorm 3.479 | clip 0 | train_wall 218 | gb_free 70.8 | wall 2499 (progress_bar.py:267, print())[0m
[33m[2025-11-19 03:51:40] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:51:52]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.707 | wps 12080.3 | wpb 5412.5 | bsz 5412.5 | num_updates 15349 | best_loss 3.943 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 03:51:53]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:51:53]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/checkpoints/checkpoint_last.pt (epoch 10 @ 15349 updates, score 3.707) (writing took 0.013457 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 03:51:53] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:51:53]    INFO >> epoch 011:      1 / 1539 loss=3.4, wps=1783, ups=2.45, wpb=728.6, bsz=728.6, num_updates=15350, lr=0.000403, gnorm=3.519, clip=0, train_wall=7, gb_free=68.7, wall=2512 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:52:00]    INFO >> epoch 011:     51 / 1539 loss=3.545, wps=4593.1, ups=7.1, wpb=647.3, bsz=647.3, num_updates=15400, lr=0.000403, gnorm=3.562, clip=0, train_wall=7, gb_free=75, wall=2519 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:52:09]    INFO >> epoch 011:    101 / 1539 loss=3.354, wps=4885.6, ups=6.7, wpb=729.7, bsz=729.7, num_updates=15450, lr=0.000403, gnorm=3.623, clip=0, train_wall=7, gb_free=74.4, wall=2527 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:52:16]    INFO >> epoch 011:    151 / 1539 loss=3.555, wps=4783.4, ups=7.18, wpb=666, bsz=666, num_updates=15500, lr=0.000403, gnorm=3.324, clip=0, train_wall=7, gb_free=71.4, wall=2534 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:52:23]    INFO >> epoch 011:    201 / 1539 loss=3.267, wps=5124, ups=6.66, wpb=769.4, bsz=769.4, num_updates=15550, lr=0.000403, gnorm=3.466, clip=0, train_wall=7, gb_free=71.9, wall=2541 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:52:30]    INFO >> epoch 011:    251 / 1539 loss=3.45, wps=5063.2, ups=6.94, wpb=729.4, bsz=729.4, num_updates=15600, lr=0.000403, gnorm=3.342, clip=0, train_wall=7, gb_free=73.3, wall=2548 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:52:38]    INFO >> epoch 011:    301 / 1539 loss=3.357, wps=4208.7, ups=6.75, wpb=623.3, bsz=623.3, num_updates=15650, lr=0.000403, gnorm=3.27, clip=0, train_wall=7, gb_free=74.6, wall=2556 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:52:47]    INFO >> epoch 011:    351 / 1539 loss=3.383, wps=4876.3, ups=6.49, wpb=751.3, bsz=751.3, num_updates=15700, lr=0.000403, gnorm=3.773, clip=0, train_wall=7, gb_free=72, wall=2563 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:52:54]    INFO >> epoch 011:    401 / 1539 loss=3.375, wps=4810, ups=7.1, wpb=677.5, bsz=677.5, num_updates=15750, lr=0.000403, gnorm=3.638, clip=0, train_wall=7, gb_free=72.3, wall=2570 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:53:01]    INFO >> epoch 011:    451 / 1539 loss=3.503, wps=4436.1, ups=7.11, wpb=624.1, bsz=624.1, num_updates=15800, lr=0.000403, gnorm=3.611, clip=0, train_wall=7, gb_free=72.8, wall=2577 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:53:09]    INFO >> epoch 011:    501 / 1539 loss=3.556, wps=3714.6, ups=6.12, wpb=606.7, bsz=606.7, num_updates=15850, lr=0.000403, gnorm=3.282, clip=0, train_wall=8, gb_free=75, wall=2586 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:53:17]    INFO >> epoch 011:    551 / 1539 loss=3.414, wps=4622.8, ups=7.16, wpb=645.8, bsz=645.8, num_updates=15900, lr=0.000403, gnorm=3.582, clip=0, train_wall=7, gb_free=73.9, wall=2593 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:53:18] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 31.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.39 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:53:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 42           |        cudaMalloc retries: 79        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78168 MiB |  78228 MiB | 514019 GiB | 513943 GiB |
|       from large pool |  77782 MiB |  77842 MiB | 511102 GiB | 511026 GiB |
|       from small pool |    385 MiB |    386 MiB |   2916 GiB |   2916 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78168 MiB |  78228 MiB | 514019 GiB | 513943 GiB |
|       from large pool |  77782 MiB |  77842 MiB | 511102 GiB | 511026 GiB |
|       from small pool |    385 MiB |    386 MiB |   2916 GiB |   2916 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78137 MiB |  78197 MiB | 512935 GiB | 512858 GiB |
|       from large pool |  77753 MiB |  77813 MiB | 510022 GiB | 509946 GiB |
|       from small pool |    383 MiB |    384 MiB |   2912 GiB |   2912 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80474 MiB |  80476 MiB |   1076 GiB |    997 GiB |
|       from large pool |  80050 MiB |  80050 MiB |   1069 GiB |    990 GiB |
|       from small pool |    424 MiB |    426 MiB |      7 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2245 MiB |   6868 MiB | 489029 GiB | 489027 GiB |
|       from large pool |   2207 MiB |   6862 MiB | 485731 GiB | 485729 GiB |
|       from small pool |     38 MiB |     40 MiB |   3298 GiB |   3298 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7120    |    7123    |   33914 K  |   33907 K  |
|       from large pool |     917    |     918    |   16134 K  |   16134 K  |
|       from small pool |    6203    |    6206    |   17779 K  |   17773 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7120    |    7123    |   33914 K  |   33907 K  |
|       from large pool |     917    |     918    |   16134 K  |   16134 K  |
|       from small pool |    6203    |    6206    |   17779 K  |   17773 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     417    |     418    |    7740    |    7323    |
|       from large pool |     205    |     205    |    4112    |    3907    |
|       from small pool |     212    |     213    |    3628    |    3416    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     548    |     550    |   19162 K  |   19162 K  |
|       from large pool |     171    |     171    |   10604 K  |   10603 K  |
|       from small pool |     377    |     379    |    8558 K  |    8558 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:53:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:53:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:53:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:53:18] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:53:25]    INFO >> epoch 011:    602 / 1539 loss=3.48, wps=4291.5, ups=6.18, wpb=694.1, bsz=694.1, num_updates=15950, lr=0.000403, gnorm=3.434, clip=0, train_wall=7, gb_free=71.6, wall=2601 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:53:33]    INFO >> epoch 011:    652 / 1539 loss=3.378, wps=5198.6, ups=6.38, wpb=815.3, bsz=815.3, num_updates=16000, lr=0.000403, gnorm=3.153, clip=0, train_wall=7, gb_free=71.5, wall=2609 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:53:41]    INFO >> epoch 011:    702 / 1539 loss=3.495, wps=4567.7, ups=6.91, wpb=661.1, bsz=661.1, num_updates=16050, lr=0.000403, gnorm=3.245, clip=0, train_wall=7, gb_free=71, wall=2616 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:53:50]    INFO >> epoch 011:    752 / 1539 loss=3.326, wps=4395, ups=6.01, wpb=731.8, bsz=731.8, num_updates=16100, lr=0.000403, gnorm=3.775, clip=0, train_wall=8, gb_free=72.6, wall=2624 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:53:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 457.25 MiB is free. Including non-PyTorch memory, this process has 78.67 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 4.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:53:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 43           |        cudaMalloc retries: 81        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72821 MiB |  75424 MiB | 521026 GiB | 520955 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 518072 GiB | 518001 GiB |
|       from small pool |     12 MiB |     21 MiB |   2953 GiB |   2953 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72821 MiB |  75424 MiB | 521026 GiB | 520955 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 518072 GiB | 518001 GiB |
|       from small pool |     12 MiB |     21 MiB |   2953 GiB |   2953 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 519925 GiB | 519854 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 516976 GiB | 516905 GiB |
|       from small pool |     12 MiB |     21 MiB |   2949 GiB |   2949 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80048 MiB |  80424 MiB |   1078 GiB |   1000 GiB |
|       from large pool |  80024 MiB |  80400 MiB |   1071 GiB |    993 GiB |
|       from small pool |     24 MiB |    424 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7226 MiB |  10655 MiB | 495401 GiB | 495394 GiB |
|       from large pool |   7215 MiB |  10644 MiB | 492059 GiB | 492052 GiB |
|       from small pool |     11 MiB |     27 MiB |   3341 GiB |   3341 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   34373 K  |   34372 K  |
|       from large pool |     312    |     320    |   16366 K  |   16366 K  |
|       from small pool |     291    |     356    |   18006 K  |   18006 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   34373 K  |   34372 K  |
|       from large pool |     312    |     320    |   16366 K  |   16366 K  |
|       from small pool |     291    |     356    |   18006 K  |   18006 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     182    |     416    |    7743    |    7561    |
|       from large pool |     170    |     204    |    4115    |    3945    |
|       from small pool |      12    |     212    |    3628    |    3616    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     172    |     173    |   19426 K  |   19426 K  |
|       from large pool |     147    |     148    |   10761 K  |   10761 K  |
|       from small pool |      25    |      59    |    8665 K  |    8665 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:53:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:53:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:53:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:53:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:53:58]    INFO >> epoch 011:    803 / 1539 loss=3.559, wps=3907.4, ups=6.66, wpb=586.3, bsz=586.3, num_updates=16150, lr=0.000403, gnorm=3.078, clip=0, train_wall=6, gb_free=62.8, wall=2632 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:54:05]    INFO >> epoch 011:    853 / 1539 loss=3.444, wps=4545, ups=6.92, wpb=657, bsz=657, num_updates=16200, lr=0.000403, gnorm=3.017, clip=0, train_wall=7, gb_free=74.8, wall=2639 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:54:12]    INFO >> epoch 011:    903 / 1539 loss=3.396, wps=5355.1, ups=6.87, wpb=779.8, bsz=779.8, num_updates=16250, lr=0.000403, gnorm=3.618, clip=0, train_wall=7, gb_free=72.7, wall=2646 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:54:21]    INFO >> epoch 011:    953 / 1539 loss=3.483, wps=4958.4, ups=6.65, wpb=746, bsz=746, num_updates=16300, lr=0.000403, gnorm=3.068, clip=0, train_wall=7, gb_free=72, wall=2654 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:54:29]    INFO >> epoch 011:   1003 / 1539 loss=3.329, wps=4956.3, ups=6.72, wpb=737.8, bsz=737.8, num_updates=16350, lr=0.000403, gnorm=3.585, clip=0, train_wall=7, gb_free=71.3, wall=2661 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:54:36]    INFO >> epoch 011:   1053 / 1539 loss=3.323, wps=4702.5, ups=6.79, wpb=692.9, bsz=692.9, num_updates=16400, lr=0.000403, gnorm=3.029, clip=0, train_wall=7, gb_free=73.7, wall=2668 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:54:41] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 455.25 MiB is free. Including non-PyTorch memory, this process has 78.67 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 8.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:54:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 44           |        cudaMalloc retries: 82        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69006 MiB |  71763 MiB | 529585 GiB | 529518 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 526586 GiB | 526518 GiB |
|       from small pool |     16 MiB |     17 MiB |   2999 GiB |   2999 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69006 MiB |  71763 MiB | 529585 GiB | 529518 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 526586 GiB | 526518 GiB |
|       from small pool |     16 MiB |     17 MiB |   2999 GiB |   2999 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 528466 GiB | 528399 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 525471 GiB | 525403 GiB |
|       from small pool |     16 MiB |     17 MiB |   2995 GiB |   2995 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80050 MiB |  80148 MiB |   1079 GiB |   1000 GiB |
|       from large pool |  80024 MiB |  80024 MiB |   1071 GiB |    993 GiB |
|       from small pool |     26 MiB |    124 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8409 MiB |  12175 MiB | 503599 GiB | 503591 GiB |
|       from large pool |   8400 MiB |  12165 MiB | 500205 GiB | 500197 GiB |
|       from small pool |      9 MiB |     23 MiB |   3394 GiB |   3394 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   34930 K  |   34930 K  |
|       from large pool |     343    |     351    |   16651 K  |   16650 K  |
|       from small pool |     298    |     342    |   18279 K  |   18279 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   34930 K  |   34930 K  |
|       from large pool |     343    |     351    |   16651 K  |   16650 K  |
|       from small pool |     298    |     342    |   18279 K  |   18279 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     183    |     232    |    7793    |    7610    |
|       from large pool |     170    |     170    |    4115    |    3945    |
|       from small pool |      13    |      62    |    3678    |    3665    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     166    |     167    |   19738 K  |   19738 K  |
|       from large pool |     137    |     138    |   10950 K  |   10950 K  |
|       from small pool |      29    |      49    |    8787 K  |    8787 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:54:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:54:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:54:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:54:41] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:54:44]    INFO >> epoch 011:   1104 / 1539 loss=3.442, wps=4477.5, ups=5.86, wpb=764, bsz=764, num_updates=16450, lr=0.000403, gnorm=3.93, clip=0, train_wall=8, gb_free=75, wall=2677 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:54:54]    INFO >> epoch 011:   1154 / 1539 loss=3.287, wps=5654, ups=6.22, wpb=909, bsz=909, num_updates=16500, lr=0.000403, gnorm=3.691, clip=0, train_wall=8, gb_free=69.3, wall=2685 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:55:01]    INFO >> epoch 011:   1204 / 1539 loss=3.403, wps=4014, ups=6.6, wpb=608.2, bsz=608.2, num_updates=16550, lr=0.000403, gnorm=3.282, clip=0, train_wall=7, gb_free=68.8, wall=2693 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:55:10]    INFO >> epoch 011:   1254 / 1539 loss=3.48, wps=4686.3, ups=5.65, wpb=828.9, bsz=828.9, num_updates=16600, lr=0.000403, gnorm=3.482, clip=0, train_wall=8, gb_free=73.1, wall=2701 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:55:18]    INFO >> epoch 011:   1304 / 1539 loss=3.351, wps=4873, ups=6.59, wpb=739.1, bsz=739.1, num_updates=16650, lr=0.000403, gnorm=3.277, clip=0, train_wall=7, gb_free=71.9, wall=2709 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:55:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process has 78.06 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:55:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 45           |        cudaMalloc retries: 85        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76320 MiB |  78596 MiB | 536717 GiB | 536643 GiB |
|       from large pool |  76301 MiB |  78578 MiB | 533678 GiB | 533604 GiB |
|       from small pool |     18 MiB |     23 MiB |   3038 GiB |   3038 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76320 MiB |  78596 MiB | 536717 GiB | 536643 GiB |
|       from large pool |  76301 MiB |  78578 MiB | 533678 GiB | 533604 GiB |
|       from small pool |     18 MiB |     23 MiB |   3038 GiB |   3038 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 535583 GiB | 535508 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 532548 GiB | 532474 GiB |
|       from small pool |     18 MiB |     23 MiB |   3034 GiB |   3034 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79424 MiB |  79424 MiB |   1176 GiB |   1099 GiB |
|       from large pool |  79400 MiB |  79400 MiB |   1169 GiB |   1091 GiB |
|       from small pool |     24 MiB |     76 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3103 MiB |   4235 MiB | 510452 GiB | 510449 GiB |
|       from large pool |   3098 MiB |   4229 MiB | 507013 GiB | 507010 GiB |
|       from small pool |      5 MiB |     27 MiB |   3439 GiB |   3439 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   35395 K  |   35394 K  |
|       from large pool |     340    |     346    |   16881 K  |   16880 K  |
|       from small pool |     300    |     356    |   18513 K  |   18513 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   35395 K  |   35394 K  |
|       from large pool |     340    |     346    |   16881 K  |   16880 K  |
|       from small pool |     300    |     356    |   18513 K  |   18513 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     110    |     179    |    7980    |    7870    |
|       from large pool |      98    |     141    |    4185    |    4087    |
|       from small pool |      12    |      38    |    3795    |    3783    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     125    |     125    |   20000 K  |   20000 K  |
|       from large pool |     100    |     100    |   11102 K  |   11102 K  |
|       from small pool |      25    |      54    |    8898 K  |    8898 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:55:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:55:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:55:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:55:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:55:29]    INFO >> epoch 011:   1355 / 1539 loss=3.484, wps=3511.6, ups=5.1, wpb=689.1, bsz=689.1, num_updates=16700, lr=0.000403, gnorm=2.971, clip=0, train_wall=7, gb_free=56.8, wall=2719 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:55:37]    INFO >> epoch 011:   1405 / 1539 loss=3.483, wps=5098.4, ups=6.44, wpb=791.3, bsz=791.3, num_updates=16750, lr=0.000403, gnorm=3.612, clip=0, train_wall=7, gb_free=73.5, wall=2727 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:55:44]    INFO >> epoch 011:   1455 / 1539 loss=3.513, wps=4526.5, ups=6.8, wpb=665.3, bsz=665.3, num_updates=16800, lr=0.000403, gnorm=3.095, clip=0, train_wall=7, gb_free=65.3, wall=2734 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:55:52]    INFO >> epoch 011:   1505 / 1539 loss=3.133, wps=5021.1, ups=6.13, wpb=818.6, bsz=818.6, num_updates=16850, lr=0.000403, gnorm=4.013, clip=0, train_wall=8, gb_free=72.5, wall=2742 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:55:57]    INFO >> epoch 011 | loss 3.413 | wps 4407.1 | ups 6.18 | wpb 712.7 | bsz 712.7 | num_updates 16884 | lr 0.000403 | gnorm 3.432 | clip 0 | train_wall 218 | gb_free 73.9 | wall 2747 (progress_bar.py:267, print())[0m
[33m[2025-11-19 03:55:57] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 03:56:12]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.699 | wps 11735.9 | wpb 5412.5 | bsz 5412.5 | num_updates 16884 | best_loss 3.943 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 03:56:12]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:56:12]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/checkpoints/checkpoint_last.pt (epoch 11 @ 16884 updates, score 3.699) (writing took 0.013771 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-19 03:56:12]    INFO >> æ—©åœ: éªŒè¯æ€§èƒ½å·²10è½®æœªæå‡ (train_enhanced.py:345, single_main())[0m
[32m[2025-11-19 03:56:12]    INFO >> è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ 2703.6 ç§’ (train_enhanced.py:355, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 03:56:12]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 03:56:12]    INFO >> æ‰€æœ‰æ—¥å¿—å·²ä¿å­˜åˆ°: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-3/logs (train_enhanced.py:359, single_main())[0m

âœ“ exp_lr_1e-3 æˆåŠŸ

ç­‰å¾…3ç§’...

è¿›åº¦: 3/6

============================================================
å®éªŒ: exp_lr_1e-4 - å­¦ä¹ ç‡1e-4
æ—¶é—´: 2025-11-19 03:56:50
============================================================

[32m[2025-11-19 03:56:52]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/config.yml (train_enhanced.py:382, cli_main())[0m
[32m[2025-11-19 03:56:52]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:410, cli_main())[0m
[32m[2025-11-19 03:56:52]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/logs (train_enhanced.py:296, single_main())[0m
[32m[2025-11-19 03:56:52]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-19 03:56:52]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-19 03:56:52]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-19 03:57:00]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 64, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=64, out_features=64, bias=False)
      (2): Dropout(p=0.1, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(64, 64)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(128, 64)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=False)
      (1): Dropout(p=0.1, inplace=False)
      (2): Linear(in_features=64, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:303, single_main())[0m
[32m[2025-11-19 03:57:00]    INFO >> æ¨¡å‹: typilus, æŸå¤±å‡½æ•°: TypilusCriterion (train_enhanced.py:304, single_main())[0m
[32m[2025-11-19 03:57:00]    INFO >> æ¨¡å‹å‚æ•°: 847843 (å¯è®­ç»ƒ: 847843) (train_enhanced.py:305, single_main())[0m
[32m[2025-11-19 03:57:00]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-19 03:57:00]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80579 MB ; used memory = 1340 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-19 03:57:00]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-19 03:57:00]    INFO >> ä½¿ç”¨ 1 ä¸ªGPUè®­ç»ƒ (train_enhanced.py:311, single_main())[0m
[32m[2025-11-19 03:57:00]    INFO >> no existing checkpoint found /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-19 03:57:00]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-19 03:57:59]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-19 03:57:59] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-19 03:58:06]    INFO >> epoch 001:     50 / 1539 loss=5.57, wps=5307.8, ups=7.34, wpb=720, bsz=720, num_updates=50, lr=0.0001, gnorm=6.038, clip=0, train_wall=6, gb_free=74.2, wall=63 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:58:07] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 73.64 GiB is allocated by PyTorch, and 4.94 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:58:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75347 MiB |  75407 MiB |   1756 GiB |   1682 GiB |
|       from large pool |  74991 MiB |  75051 MiB |   1743 GiB |   1670 GiB |
|       from small pool |    356 MiB |    357 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75347 MiB |  75407 MiB |   1756 GiB |   1682 GiB |
|       from large pool |  74991 MiB |  75051 MiB |   1743 GiB |   1670 GiB |
|       from small pool |    356 MiB |    357 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75195 MiB |  75254 MiB |   1750 GiB |   1676 GiB |
|       from large pool |  74841 MiB |  74900 MiB |   1737 GiB |   1664 GiB |
|       from small pool |    354 MiB |    355 MiB |     12 GiB |     12 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80498 MiB |  91994 MiB |  11532 MiB |
|       from large pool |  80070 MiB |  80136 MiB |  91590 MiB |  11520 MiB |
|       from small pool |    392 MiB |    394 MiB |    404 MiB |     12 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5054 MiB |   5960 MiB |    875 GiB |    870 GiB |
|       from large pool |   5018 MiB |   5948 MiB |    859 GiB |    855 GiB |
|       from small pool |     35 MiB |     37 MiB |     15 GiB |     14 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    6579    |    6582    |  136626    |  130047    |
|       from large pool |     868    |     869    |   57719    |   56851    |
|       from small pool |    5711    |    5714    |   78907    |   73196    |
|---------------------------------------------------------------------------|
| Active allocs         |    6579    |    6582    |  136626    |  130047    |
|       from large pool |     868    |     869    |   57719    |   56851    |
|       from small pool |    5711    |    5714    |   78907    |   73196    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     758    |     793    |    1006    |     248    |
|       from large pool |     562    |     612    |     804    |     242    |
|       from small pool |     196    |     197    |     202    |       6    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     654    |     656    |   86039    |   85385    |
|       from large pool |     306    |     306    |   44256    |   43950    |
|       from small pool |     348    |     350    |   41783    |   41435    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:58:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:58:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:58:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:58:07] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:58:13]    INFO >> epoch 001:    101 / 1539 loss=5.692, wps=4677.1, ups=7.66, wpb=610.7, bsz=610.7, num_updates=100, lr=0.0001, gnorm=5.849, clip=0, train_wall=5, gb_free=75.6, wall=70 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:58:21]    INFO >> epoch 001:    151 / 1539 loss=5.62, wps=5759.5, ups=7.05, wpb=816.7, bsz=816.7, num_updates=150, lr=0.0001, gnorm=6.153, clip=0, train_wall=6, gb_free=74.2, wall=77 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:58:27]    INFO >> epoch 001:    201 / 1539 loss=5.567, wps=5242.6, ups=8.17, wpb=641.7, bsz=641.7, num_updates=200, lr=0.0001, gnorm=6.281, clip=0, train_wall=6, gb_free=74.8, wall=83 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:58:34]    INFO >> epoch 001:    251 / 1539 loss=5.663, wps=5067.7, ups=7.94, wpb=637.9, bsz=637.9, num_updates=250, lr=0.0001, gnorm=6.518, clip=0, train_wall=6, gb_free=71.5, wall=89 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:58:41]    INFO >> epoch 001:    301 / 1539 loss=5.57, wps=5458.4, ups=6.94, wpb=786.5, bsz=786.5, num_updates=300, lr=0.0001, gnorm=6.767, clip=0, train_wall=7, gb_free=73.8, wall=97 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:58:47]    INFO >> epoch 001:    351 / 1539 loss=5.703, wps=5350.4, ups=7.97, wpb=671.6, bsz=671.6, num_updates=350, lr=0.0001, gnorm=7.192, clip=0, train_wall=6, gb_free=72.4, wall=103 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:58:56]    INFO >> epoch 001:    401 / 1539 loss=5.694, wps=5887.4, ups=6.91, wpb=851.7, bsz=851.7, num_updates=400, lr=0.0001, gnorm=7.357, clip=0, train_wall=7, gb_free=73.2, wall=110 (progress_bar.py:258, log())[0m
[33m[2025-11-19 03:59:05] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.81 GiB is allocated by PyTorch, and 823.34 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 03:59:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79614 MiB |  79674 MiB |  12419 GiB |  12342 GiB |
|       from large pool |  79520 MiB |  79580 MiB |  12346 GiB |  12268 GiB |
|       from small pool |     94 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79614 MiB |  79674 MiB |  12419 GiB |  12342 GiB |
|       from large pool |  79520 MiB |  79580 MiB |  12346 GiB |  12268 GiB |
|       from small pool |     94 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79544 MiB |  79604 MiB |  12379 GiB |  12302 GiB |
|       from large pool |  79450 MiB |  79510 MiB |  12306 GiB |  12228 GiB |
|       from small pool |     93 MiB |     95 MiB |     73 GiB |     73 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 166722 MiB |  86224 MiB |
|       from large pool |  80400 MiB |  80400 MiB | 166242 MiB |  85842 MiB |
|       from small pool |     98 MiB |    392 MiB |    480 MiB |    382 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    823 MiB |   3119 MiB |   6143 GiB |   6143 GiB |
|       from large pool |    819 MiB |   3112 MiB |   6057 GiB |   6057 GiB |
|       from small pool |      3 MiB |     23 MiB |     85 GiB |     85 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    2036    |    2039    |     869 K  |     867 K  |
|       from large pool |     477    |     478    |     421 K  |     421 K  |
|       from small pool |    1559    |    1562    |     447 K  |     445 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    2036    |    2039    |     869 K  |     867 K  |
|       from large pool |     477    |     478    |     421 K  |     421 K  |
|       from small pool |    1559    |    1562    |     447 K  |     445 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     251    |     757    |    1201    |     950    |
|       from large pool |     202    |     561    |     961    |     759    |
|       from small pool |      49    |     196    |     240    |     191    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     132    |     133    |  540020    |  539888    |
|       from large pool |      79    |      81    |  324754    |  324675    |
|       from small pool |      53    |      55    |  215266    |  215213    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:59:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:59:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:59:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 03:59:05] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 03:59:08]    INFO >> epoch 001:    452 / 1539 loss=5.833, wps=2463.6, ups=3.9, wpb=631.6, bsz=631.6, num_updates=450, lr=0.0001, gnorm=6.638, clip=0, train_wall=6, gb_free=71.8, wall=123 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:59:18]    INFO >> epoch 001:    502 / 1539 loss=5.799, wps=3944.7, ups=5.31, wpb=743.1, bsz=743.1, num_updates=500, lr=0.0001, gnorm=6.936, clip=0, train_wall=9, gb_free=72.6, wall=132 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:59:26]    INFO >> epoch 001:    552 / 1539 loss=5.779, wps=4942.8, ups=7.52, wpb=657, bsz=657, num_updates=550, lr=0.0001, gnorm=6.985, clip=0, train_wall=6, gb_free=65.5, wall=139 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:59:32]    INFO >> epoch 001:    602 / 1539 loss=5.854, wps=5057.9, ups=7.56, wpb=668.6, bsz=668.6, num_updates=600, lr=0.0001, gnorm=7.03, clip=0, train_wall=6, gb_free=73.2, wall=146 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:59:40]    INFO >> epoch 001:    652 / 1539 loss=5.783, wps=4998.5, ups=7.02, wpb=712.2, bsz=712.2, num_updates=650, lr=0.0001, gnorm=7.833, clip=2, train_wall=7, gb_free=73.5, wall=153 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:59:47]    INFO >> epoch 001:    702 / 1539 loss=6.041, wps=4264.3, ups=6.34, wpb=672.5, bsz=672.5, num_updates=700, lr=0.0001, gnorm=6.057, clip=0, train_wall=7, gb_free=74.1, wall=161 (progress_bar.py:258, log())[0m
[32m[2025-11-19 03:59:55]    INFO >> epoch 001:    752 / 1539 loss=5.747, wps=5111.8, ups=6.75, wpb=757.4, bsz=757.4, num_updates=750, lr=0.0001, gnorm=7.735, clip=0, train_wall=7, gb_free=73.7, wall=168 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:00:03]    INFO >> epoch 001:    802 / 1539 loss=5.821, wps=5260, ups=7.25, wpb=725.2, bsz=725.2, num_updates=800, lr=0.0001, gnorm=7.046, clip=0, train_wall=6, gb_free=73.4, wall=175 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:00:10]    INFO >> epoch 001:    852 / 1539 loss=5.797, wps=4439.5, ups=6.92, wpb=641.1, bsz=641.1, num_updates=850, lr=0.0001, gnorm=7.55, clip=2, train_wall=7, gb_free=71.8, wall=182 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:00:17]    INFO >> epoch 001:    902 / 1539 loss=5.883, wps=4794.2, ups=7.26, wpb=660.1, bsz=660.1, num_updates=900, lr=0.0001, gnorm=7.339, clip=0, train_wall=6, gb_free=72.1, wall=189 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:00:24]    INFO >> epoch 001:    952 / 1539 loss=5.912, wps=4998.3, ups=7.01, wpb=713.2, bsz=713.2, num_updates=950, lr=0.0001, gnorm=6.774, clip=0, train_wall=7, gb_free=71.8, wall=196 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:00:28] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 901.25 MiB is free. Including non-PyTorch memory, this process has 78.24 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.82 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:00:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76322 MiB |  78597 MiB |  28415 GiB |  28340 GiB |
|       from large pool |  76303 MiB |  78579 MiB |  28261 GiB |  28187 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76322 MiB |  78597 MiB |  28415 GiB |  28340 GiB |
|       from large pool |  76303 MiB |  78579 MiB |  28261 GiB |  28187 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB |  28342 GiB |  28268 GiB |
|       from large pool |  76285 MiB |  78562 MiB |  28189 GiB |  28114 GiB |
|       from small pool |     18 MiB |     19 MiB |    153 GiB |    153 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79604 MiB |  79604 MiB | 305536 MiB | 225932 MiB |
|       from large pool |  79580 MiB |  79580 MiB | 304982 MiB | 225402 MiB |
|       from small pool |     24 MiB |     98 MiB |    554 MiB |    530 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3281 MiB |   4789 MiB |  24274 GiB |  24271 GiB |
|       from large pool |   3276 MiB |   4783 MiB |  24097 GiB |  24094 GiB |
|       from small pool |      5 MiB |     27 MiB |    177 GiB |    177 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     638    |     644    |    1881 K  |    1880 K  |
|       from large pool |     340    |     346    |     949 K  |     948 K  |
|       from small pool |     298    |     354    |     931 K  |     931 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     638    |     644    |    1881 K  |    1880 K  |
|       from large pool |     340    |     346    |     949 K  |     948 K  |
|       from small pool |     298    |     354    |     931 K  |     931 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      96    |     123    |    1327    |    1231    |
|       from large pool |      84    |      84    |    1050    |     966    |
|       from small pool |      12    |      49    |     277    |     265    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     115    |     117    |    1087 K  |    1087 K  |
|       from large pool |      93    |      95    |     661 K  |     661 K  |
|       from small pool |      22    |      56    |     426 K  |     426 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:00:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:00:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:00:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:00:28] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:00:35]    INFO >> epoch 001:   1003 / 1539 loss=5.856, wps=3511.7, ups=5.41, wpb=649.4, bsz=649.4, num_updates=1000, lr=0.0001, gnorm=7.773, clip=0, train_wall=7, gb_free=72.1, wall=205 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:00:43]    INFO >> epoch 001:   1053 / 1539 loss=5.844, wps=5379.9, ups=6.54, wpb=822.6, bsz=822.6, num_updates=1050, lr=0.0001, gnorm=6.994, clip=0, train_wall=7, gb_free=68, wall=213 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:00:50] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 197.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.76 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:00:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 9         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76955 MiB |  77477 MiB |  32758 GiB |  32683 GiB |
|       from large pool |  76942 MiB |  77464 MiB |  32579 GiB |  32504 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76955 MiB |  77477 MiB |  32758 GiB |  32683 GiB |
|       from large pool |  76942 MiB |  77464 MiB |  32579 GiB |  32504 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB |  32677 GiB |  32602 GiB |
|       from large pool |  76923 MiB |  77445 MiB |  32498 GiB |  32423 GiB |
|       from small pool |     12 MiB |     15 MiB |    178 GiB |    178 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80308 MiB |  80308 MiB | 310972 MiB | 230664 MiB |
|       from large pool |  80284 MiB |  80284 MiB | 310232 MiB | 229948 MiB |
|       from small pool |     24 MiB |    210 MiB |    740 MiB |    716 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3352 MiB |   9143 MiB |  29231 GiB |  29228 GiB |
|       from large pool |   3341 MiB |   9131 MiB |  29025 GiB |  29022 GiB |
|       from small pool |     11 MiB |     23 MiB |    206 GiB |    206 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     604    |     611    |    2166 K  |    2165 K  |
|       from large pool |     315    |     322    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1080 K  |    1079 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     604    |     611    |    2166 K  |    2165 K  |
|       from large pool |     315    |     322    |    1085 K  |    1085 K  |
|       from small pool |     289    |     354    |    1080 K  |    1079 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     189    |    1425    |    1337    |
|       from large pool |      76    |      84    |    1055    |     979    |
|       from small pool |      12    |     105    |     370    |     358    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      84    |      84    |    1245 K  |    1245 K  |
|       from large pool |      60    |      60    |     746 K  |     746 K  |
|       from small pool |      24    |      53    |     498 K  |     498 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:00:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:00:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:00:50] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:00:50] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:00:51]    INFO >> epoch 001:   1104 / 1539 loss=5.743, wps=5271.2, ups=5.67, wpb=929.6, bsz=929.6, num_updates=1100, lr=0.0001, gnorm=8.066, clip=2, train_wall=8, gb_free=72.8, wall=222 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:00:58]    INFO >> epoch 001:   1154 / 1539 loss=5.929, wps=4867.8, ups=7.03, wpb=692.6, bsz=692.6, num_updates=1150, lr=0.0001, gnorm=7.234, clip=0, train_wall=7, gb_free=73.1, wall=229 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:01:07]    INFO >> epoch 001:   1204 / 1539 loss=5.838, wps=4843, ups=7.14, wpb=678.4, bsz=678.4, num_updates=1200, lr=0.0001, gnorm=7.488, clip=0, train_wall=6, gb_free=71.2, wall=236 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:01:14]    INFO >> epoch 001:   1254 / 1539 loss=5.875, wps=5072.2, ups=6.94, wpb=730.9, bsz=730.9, num_updates=1250, lr=0.0001, gnorm=7.674, clip=0, train_wall=7, gb_free=71.3, wall=243 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:01:22]    INFO >> epoch 001:   1304 / 1539 loss=5.871, wps=4904.9, ups=6.67, wpb=735.4, bsz=735.4, num_updates=1300, lr=0.0001, gnorm=6.204, clip=0, train_wall=7, gb_free=74.3, wall=251 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:01:29]    INFO >> epoch 001:   1354 / 1539 loss=5.873, wps=4627.9, ups=7.04, wpb=657.2, bsz=657.2, num_updates=1350, lr=0.0001, gnorm=7.225, clip=2, train_wall=7, gb_free=73.4, wall=258 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:01:37]    INFO >> epoch 001:   1404 / 1539 loss=5.781, wps=4739.6, ups=6.65, wpb=712.5, bsz=712.5, num_updates=1400, lr=0.0001, gnorm=6.857, clip=2, train_wall=7, gb_free=73.3, wall=265 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:01:45]    INFO >> epoch 001:   1454 / 1539 loss=5.858, wps=4834.8, ups=6.89, wpb=702.2, bsz=702.2, num_updates=1450, lr=0.0001, gnorm=6.653, clip=0, train_wall=7, gb_free=71.6, wall=273 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:01:52]    INFO >> epoch 001:   1504 / 1539 loss=5.825, wps=4811.2, ups=6.91, wpb=695.9, bsz=695.9, num_updates=1500, lr=0.0001, gnorm=7.257, clip=0, train_wall=7, gb_free=70.8, wall=280 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:01:57]    INFO >> epoch 001 | loss 5.785 | wps 4790.4 | ups 6.72 | wpb 712.7 | bsz 712.7 | num_updates 1535 | lr 0.0001 | gnorm 6.975 | clip 0.3 | train_wall 201 | gb_free 76.6 | wall 285 (progress_bar.py:267, print())[0m
[33m[2025-11-19 04:01:57] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:02:12]    INFO >> epoch 001 | valid on 'valid' subset | loss 5.983 | wps 11101.9 | wpb 5412.5 | bsz 5412.5 | num_updates 1535 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-19 04:02:13]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 04:02:13]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/checkpoints/checkpoint_best.pt (epoch 1 @ 1535 updates, score 5.983) (writing took 0.014811 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 04:02:13] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-19 04:02:15]    INFO >> epoch 002:     15 / 1539 loss=5.818, wps=1722.6, ups=2.36, wpb=731.3, bsz=731.3, num_updates=1550, lr=0.0001, gnorm=6.524, clip=0, train_wall=7, gb_free=74.4, wall=301 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:02:21]    INFO >> epoch 002:     65 / 1539 loss=5.683, wps=4982.6, ups=7.56, wpb=658.8, bsz=658.8, num_updates=1600, lr=0.0001, gnorm=7.763, clip=0, train_wall=6, gb_free=73.4, wall=308 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:02:28]    INFO >> epoch 002:    115 / 1539 loss=5.71, wps=4960.1, ups=6.94, wpb=714.6, bsz=714.6, num_updates=1650, lr=0.0001, gnorm=7.442, clip=2, train_wall=7, gb_free=65.7, wall=315 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:02:36]    INFO >> epoch 002:    165 / 1539 loss=5.662, wps=5072.6, ups=6.87, wpb=738.8, bsz=738.8, num_updates=1700, lr=0.0001, gnorm=7.15, clip=2, train_wall=7, gb_free=73.6, wall=322 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:02:43]    INFO >> epoch 002:    215 / 1539 loss=5.834, wps=4975.1, ups=7.07, wpb=704.2, bsz=704.2, num_updates=1750, lr=0.0001, gnorm=7.26, clip=2, train_wall=7, gb_free=71.2, wall=329 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:02:52]    INFO >> epoch 002:    265 / 1539 loss=5.716, wps=5415.2, ups=6.2, wpb=873.5, bsz=873.5, num_updates=1800, lr=0.0001, gnorm=7.087, clip=0, train_wall=8, gb_free=74.7, wall=337 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:02:59]    INFO >> epoch 002:    315 / 1539 loss=5.711, wps=4639.5, ups=7.19, wpb=645.1, bsz=645.1, num_updates=1850, lr=0.0001, gnorm=6.99, clip=0, train_wall=7, gb_free=71.9, wall=344 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:03:06]    INFO >> epoch 002:    365 / 1539 loss=5.724, wps=4745.3, ups=7.24, wpb=655.8, bsz=655.8, num_updates=1900, lr=0.0001, gnorm=7.215, clip=0, train_wall=7, gb_free=74, wall=351 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:03:14]    INFO >> epoch 002:    415 / 1539 loss=5.746, wps=4719.8, ups=6.62, wpb=713.2, bsz=713.2, num_updates=1950, lr=0.0001, gnorm=6.438, clip=0, train_wall=7, gb_free=76.2, wall=359 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:03:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 197.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 3.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:03:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76327 MiB |  78599 MiB |  61960 GiB |  61886 GiB |
|       from large pool |  76308 MiB |  78581 MiB |  61612 GiB |  61537 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76327 MiB |  78599 MiB |  61960 GiB |  61886 GiB |
|       from large pool |  76308 MiB |  78581 MiB |  61612 GiB |  61537 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB |  61824 GiB |  61750 GiB |
|       from large pool |  76285 MiB |  78562 MiB |  61476 GiB |  61402 GiB |
|       from small pool |     18 MiB |     21 MiB |    348 GiB |    348 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80308 MiB |  80494 MiB | 311158 MiB | 230850 MiB |
|       from large pool |  80284 MiB |  80284 MiB | 310232 MiB | 229948 MiB |
|       from small pool |     24 MiB |    210 MiB |    926 MiB |    902 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3980 MiB |   5490 MiB |  61023 GiB |  61019 GiB |
|       from large pool |   3975 MiB |   5484 MiB |  60627 GiB |  60623 GiB |
|       from small pool |      5 MiB |     27 MiB |    395 GiB |    395 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |    4090 K  |    4090 K  |
|       from large pool |     340    |     346    |    1969 K  |    1969 K  |
|       from small pool |     300    |     355    |    2121 K  |    2120 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |    4090 K  |    4090 K  |
|       from large pool |     340    |     346    |    1969 K  |    1969 K  |
|       from small pool |     300    |     355    |    2121 K  |    2120 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      88    |     181    |    1518    |    1430    |
|       from large pool |      76    |      76    |    1055    |     979    |
|       from small pool |      12    |     105    |     463    |     451    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      93    |      99    |    2309 K  |    2309 K  |
|       from large pool |      70    |      76    |    1305 K  |    1305 K  |
|       from small pool |      23    |      51    |    1004 K  |    1004 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:03:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:03:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:03:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:03:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:03:23]    INFO >> epoch 002:    466 / 1539 loss=5.724, wps=4643.4, ups=6.37, wpb=729.4, bsz=729.4, num_updates=2000, lr=0.0001, gnorm=6.709, clip=0, train_wall=7, gb_free=71.6, wall=366 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:03:30]    INFO >> epoch 002:    516 / 1539 loss=5.672, wps=4793.4, ups=6.77, wpb=708.4, bsz=708.4, num_updates=2050, lr=0.0001, gnorm=7.624, clip=0, train_wall=7, gb_free=71.2, wall=374 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:03:37]    INFO >> epoch 002:    566 / 1539 loss=5.773, wps=4479.2, ups=7.2, wpb=622.3, bsz=622.3, num_updates=2100, lr=0.0001, gnorm=5.912, clip=0, train_wall=7, gb_free=74, wall=381 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:03:45]    INFO >> epoch 002:    616 / 1539 loss=5.559, wps=5645, ups=6.48, wpb=871.1, bsz=871.1, num_updates=2150, lr=0.0001, gnorm=6.733, clip=0, train_wall=7, gb_free=68.6, wall=389 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:03:54]    INFO >> epoch 002:    666 / 1539 loss=5.6, wps=5094.4, ups=6.59, wpb=772.9, bsz=772.9, num_updates=2200, lr=0.0001, gnorm=7.453, clip=0, train_wall=7, gb_free=68.4, wall=396 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:04:01]    INFO >> epoch 002:    716 / 1539 loss=5.738, wps=5092.8, ups=7.19, wpb=708.6, bsz=708.6, num_updates=2250, lr=0.0001, gnorm=6.249, clip=0, train_wall=7, gb_free=73.1, wall=403 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:04:08]    INFO >> epoch 002:    766 / 1539 loss=5.566, wps=4588.4, ups=6.9, wpb=665.2, bsz=665.2, num_updates=2300, lr=0.0001, gnorm=7.083, clip=0, train_wall=7, gb_free=75.7, wall=410 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:04:15]    INFO >> epoch 002:    816 / 1539 loss=5.707, wps=4597.3, ups=7.01, wpb=656, bsz=656, num_updates=2350, lr=0.0001, gnorm=5.829, clip=0, train_wall=7, gb_free=72.2, wall=417 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:04:23]    INFO >> epoch 002:    866 / 1539 loss=5.635, wps=4763.6, ups=6.62, wpb=719.8, bsz=719.8, num_updates=2400, lr=0.0001, gnorm=6.414, clip=0, train_wall=7, gb_free=75.7, wall=425 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:04:31]    INFO >> epoch 002:    916 / 1539 loss=5.598, wps=4541.6, ups=6.76, wpb=671.5, bsz=671.5, num_updates=2450, lr=0.0001, gnorm=6.756, clip=0, train_wall=7, gb_free=73.1, wall=432 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:04:39]    INFO >> epoch 002:    966 / 1539 loss=5.651, wps=4390, ups=6.88, wpb=637.9, bsz=637.9, num_updates=2500, lr=0.0001, gnorm=6.823, clip=0, train_wall=7, gb_free=69.2, wall=440 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:04:40] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 37.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.54 GiB is allocated by PyTorch, and 1.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:04:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79337 MiB |  79397 MiB |  78153 GiB |  78075 GiB |
|       from large pool |  78941 MiB |  79001 MiB |  77714 GiB |  77637 GiB |
|       from small pool |    395 MiB |    396 MiB |    438 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79337 MiB |  79397 MiB |  78153 GiB |  78075 GiB |
|       from large pool |  78941 MiB |  79001 MiB |  77714 GiB |  77637 GiB |
|       from small pool |    395 MiB |    396 MiB |    438 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79098 MiB |  79157 MiB |  77983 GiB |  77905 GiB |
|       from large pool |  78705 MiB |  78764 MiB |  77545 GiB |  77468 GiB |
|       from small pool |    393 MiB |    394 MiB |    437 GiB |    437 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80468 MiB |  80470 MiB | 335872 MiB | 255404 MiB |
|       from large pool |  80032 MiB |  80032 MiB | 334532 MiB | 254500 MiB |
|       from small pool |    436 MiB |    438 MiB |   1340 MiB |    904 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1070 MiB |   4610 MiB |  79115 GiB |  79114 GiB |
|       from large pool |   1030 MiB |   4603 MiB |  78616 GiB |  78615 GiB |
|       from small pool |     40 MiB |     41 MiB |    498 GiB |    498 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7296    |    7299    |    5179 K  |    5172 K  |
|       from large pool |     933    |     934    |    2516 K  |    2515 K  |
|       from small pool |    6363    |    6366    |    2663 K  |    2656 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7296    |    7299    |    5179 K  |    5172 K  |
|       from large pool |     933    |     934    |    2516 K  |    2515 K  |
|       from small pool |    6363    |    6366    |    2663 K  |    2656 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     690    |     691    |    2130    |    1440    |
|       from large pool |     472    |     472    |    1460    |     988    |
|       from small pool |     218    |     219    |     670    |     452    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     454    |     454    |    2903 K  |    2902 K  |
|       from large pool |      63    |      63    |    1651 K  |    1651 K  |
|       from small pool |     391    |     391    |    1251 K  |    1251 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:04:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:04:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:04:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:04:40] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-19 04:04:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 1.25 MiB is free. Including non-PyTorch memory, this process has 79.12 GiB memory in use. Of the allocated memory 76.95 GiB is allocated by PyTorch, and 1.67 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:04:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 15        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78678 MiB |  78798 MiB |  79117 GiB |  79040 GiB |
|       from large pool |  78593 MiB |  78713 MiB |  78673 GiB |  78596 GiB |
|       from small pool |     85 MiB |     86 MiB |    443 GiB |    443 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78678 MiB |  78798 MiB |  79117 GiB |  79040 GiB |
|       from large pool |  78593 MiB |  78713 MiB |  78673 GiB |  78596 GiB |
|       from small pool |     85 MiB |     86 MiB |    443 GiB |    443 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78642 MiB |  78762 MiB |  78944 GiB |  78868 GiB |
|       from large pool |  78558 MiB |  78677 MiB |  78501 GiB |  78425 GiB |
|       from small pool |     84 MiB |     85 MiB |    443 GiB |    442 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80504 MiB |  80504 MiB | 356340 MiB | 275836 MiB |
|       from large pool |  80416 MiB |  80416 MiB | 354934 MiB | 274518 MiB |
|       from small pool |     88 MiB |    436 MiB |   1406 MiB |   1318 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1705 MiB |   8357 MiB |  79995 GiB |  79994 GiB |
|       from large pool |   1702 MiB |   8349 MiB |  79489 GiB |  79488 GiB |
|       from small pool |      2 MiB |     29 MiB |    505 GiB |    505 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1873    |    1876    |    5242 K  |    5240 K  |
|       from large pool |     462    |     464    |    2546 K  |    2545 K  |
|       from small pool |    1411    |    1414    |    2696 K  |    2694 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1873    |    1876    |    5242 K  |    5240 K  |
|       from large pool |     462    |     464    |    2546 K  |    2545 K  |
|       from small pool |    1411    |    1414    |    2696 K  |    2694 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     221    |     689    |    2198    |    1977    |
|       from large pool |     177    |     471    |    1495    |    1318    |
|       from small pool |      44    |     218    |     703    |     659    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     167    |     168    |    2940 K  |    2940 K  |
|       from large pool |     120    |     125    |    1671 K  |    1671 K  |
|       from small pool |      47    |      59    |    1269 K  |    1269 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:04:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:04:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:04:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:04:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:04:47]    INFO >> epoch 002:   1018 / 1539 loss=5.598, wps=4054.1, ups=5.89, wpb=687.9, bsz=687.9, num_updates=2550, lr=0.0001, gnorm=6.339, clip=0, train_wall=6, gb_free=72.9, wall=448 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:04:55]    INFO >> epoch 002:   1068 / 1539 loss=5.499, wps=5045, ups=6.61, wpb=763.7, bsz=763.7, num_updates=2600, lr=0.0001, gnorm=7.544, clip=2, train_wall=7, gb_free=73.5, wall=456 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:05:04]    INFO >> epoch 002:   1118 / 1539 loss=5.408, wps=5158.7, ups=6.51, wpb=791.9, bsz=791.9, num_updates=2650, lr=0.0001, gnorm=7.019, clip=2, train_wall=7, gb_free=69.2, wall=463 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:05:11]    INFO >> epoch 002:   1168 / 1539 loss=5.564, wps=5090.4, ups=6.61, wpb=770.1, bsz=770.1, num_updates=2700, lr=0.0001, gnorm=7.015, clip=0, train_wall=7, gb_free=72.3, wall=471 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:05:19]    INFO >> epoch 002:   1218 / 1539 loss=5.595, wps=4713.5, ups=6.64, wpb=710, bsz=710, num_updates=2750, lr=0.0001, gnorm=6.385, clip=0, train_wall=7, gb_free=70.9, wall=479 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:05:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 121.25 MiB is free. Including non-PyTorch memory, this process has 79.00 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 5.04 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:05:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72821 MiB |  75425 MiB |  86354 GiB |  86283 GiB |
|       from large pool |  72808 MiB |  75412 MiB |  85870 GiB |  85799 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72821 MiB |  75425 MiB |  86354 GiB |  86283 GiB |
|       from large pool |  72808 MiB |  75412 MiB |  85870 GiB |  85799 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB |  86167 GiB |  86096 GiB |
|       from large pool |  72788 MiB |  75391 MiB |  85684 GiB |  85613 GiB |
|       from small pool |     12 MiB |     14 MiB |    483 GiB |    483 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80384 MiB |  80384 MiB | 387082 MiB | 306698 MiB |
|       from large pool |  80360 MiB |  80360 MiB | 385538 MiB | 305178 MiB |
|       from small pool |     24 MiB |    226 MiB |   1544 MiB |   1520 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5660 MiB |  11732 MiB |  87437 GiB |  87432 GiB |
|       from large pool |   5649 MiB |  11720 MiB |  86885 GiB |  86880 GiB |
|       from small pool |     11 MiB |     25 MiB |    551 GiB |    551 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |    5719 K  |    5718 K  |
|       from large pool |     312    |     320    |    2781 K  |    2781 K  |
|       from small pool |     291    |     356    |    2937 K  |    2937 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |    5719 K  |    5718 K  |
|       from large pool |     312    |     320    |    2781 K  |    2781 K  |
|       from small pool |     291    |     356    |    2937 K  |    2937 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     120    |     229    |    2295    |    2175    |
|       from large pool |     108    |     116    |    1523    |    1415    |
|       from small pool |      12    |     113    |     772    |     760    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     118    |     118    |    3207 K  |    3206 K  |
|       from large pool |      91    |      91    |    1823 K  |    1823 K  |
|       from small pool |      27    |      54    |    1383 K  |    1383 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:05:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:05:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:05:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:05:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:05:27]    INFO >> epoch 002:   1269 / 1539 loss=5.545, wps=4815.9, ups=6.29, wpb=765.3, bsz=765.3, num_updates=2800, lr=0.0001, gnorm=6.727, clip=0, train_wall=7, gb_free=70.5, wall=486 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:05:35]    INFO >> epoch 002:   1319 / 1539 loss=5.602, wps=4830.6, ups=7.3, wpb=661.5, bsz=661.5, num_updates=2850, lr=0.0001, gnorm=5.775, clip=0, train_wall=6, gb_free=75, wall=493 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:05:42]    INFO >> epoch 002:   1369 / 1539 loss=5.459, wps=5013.7, ups=6.88, wpb=728.5, bsz=728.5, num_updates=2900, lr=0.0001, gnorm=6.164, clip=2, train_wall=7, gb_free=70.5, wall=501 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:05:49]    INFO >> epoch 002:   1419 / 1539 loss=5.597, wps=4513.1, ups=6.91, wpb=653.6, bsz=653.6, num_updates=2950, lr=0.0001, gnorm=6.103, clip=0, train_wall=7, gb_free=64.8, wall=508 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:05:59]    INFO >> epoch 002:   1469 / 1539 loss=5.473, wps=3758.6, ups=5.35, wpb=702.5, bsz=702.5, num_updates=3000, lr=0.0001, gnorm=6.763, clip=0, train_wall=9, gb_free=70.3, wall=517 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:06:08]    INFO >> epoch 002:   1519 / 1539 loss=5.511, wps=4410.4, ups=6.55, wpb=673.3, bsz=673.3, num_updates=3050, lr=0.0001, gnorm=6.875, clip=0, train_wall=7, gb_free=74.2, wall=525 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:06:11]    INFO >> epoch 002 | loss 5.625 | wps 4500.3 | ups 6.31 | wpb 712.7 | bsz 712.7 | num_updates 3070 | lr 0.0001 | gnorm 6.793 | clip 0.4 | train_wall 214 | gb_free 72.4 | wall 528 (progress_bar.py:267, print())[0m
[33m[2025-11-19 04:06:11] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:06:24]    INFO >> epoch 002 | valid on 'valid' subset | loss 5.738 | wps 11775.2 | wpb 5412.5 | bsz 5412.5 | num_updates 3070 | best_loss 5.983 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 04:06:24]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 04:06:24]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/checkpoints/checkpoint_last.pt (epoch 2 @ 3070 updates, score 5.738) (writing took 0.012309 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 04:06:24] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:06:28]    INFO >> epoch 003:     30 / 1539 loss=5.5, wps=1658.8, ups=2.44, wpb=681.1, bsz=681.1, num_updates=3100, lr=9.8e-05, gnorm=6.36, clip=0, train_wall=7, gb_free=70.7, wall=545 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:06:36]    INFO >> epoch 003:     80 / 1539 loss=5.566, wps=5293.1, ups=6.85, wpb=772.8, bsz=772.8, num_updates=3150, lr=9.8e-05, gnorm=6.23, clip=0, train_wall=7, gb_free=73.4, wall=553 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:06:40] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.08 GiB is free. Including non-PyTorch memory, this process has 78.04 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 2.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:06:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76326 MiB |  78600 MiB | 102451 GiB | 102377 GiB |
|       from large pool |  76307 MiB |  78583 MiB | 101871 GiB | 101797 GiB |
|       from small pool |     18 MiB |     19 MiB |    580 GiB |    580 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76326 MiB |  78600 MiB | 102451 GiB | 102377 GiB |
|       from large pool |  76307 MiB |  78583 MiB | 101871 GiB | 101797 GiB |
|       from small pool |     18 MiB |     19 MiB |    580 GiB |    580 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 102235 GiB | 102160 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 101655 GiB | 101581 GiB |
|       from small pool |     18 MiB |     19 MiB |    579 GiB |    579 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79400 MiB |  79400 MiB | 483464 MiB | 404064 MiB |
|       from large pool |  79376 MiB |  79376 MiB | 481770 MiB | 402394 MiB |
|       from small pool |     24 MiB |     74 MiB |   1694 MiB |   1670 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3073 MiB |   4962 MiB | 103113 GiB | 103110 GiB |
|       from large pool |   3068 MiB |   4948 MiB | 102456 GiB | 102453 GiB |
|       from small pool |      5 MiB |     27 MiB |    657 GiB |    657 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |    6746 K  |    6745 K  |
|       from large pool |     340    |     346    |    3209 K  |    3208 K  |
|       from small pool |     300    |     356    |    3536 K  |    3536 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |    6746 K  |    6745 K  |
|       from large pool |     340    |     346    |    3209 K  |    3208 K  |
|       from small pool |     300    |     356    |    3536 K  |    3536 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     106    |     118    |    2420    |    2314    |
|       from large pool |      94    |      94    |    1573    |    1479    |
|       from small pool |      12    |      37    |     847    |     835    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     110    |     112    |    3792 K  |    3792 K  |
|       from large pool |      85    |      87    |    2095 K  |    2095 K  |
|       from small pool |      25    |      57    |    1696 K  |    1696 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:06:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:06:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:06:40] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:06:40] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:06:46]    INFO >> epoch 003:    131 / 1539 loss=5.495, wps=4736.4, ups=5.67, wpb=835, bsz=835, num_updates=3200, lr=9.8e-05, gnorm=6.467, clip=0, train_wall=7, gb_free=71.9, wall=561 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:06:53]    INFO >> epoch 003:    181 / 1539 loss=5.637, wps=4674.5, ups=7.12, wpb=656.8, bsz=656.8, num_updates=3250, lr=9.8e-05, gnorm=5.491, clip=0, train_wall=7, gb_free=73, wall=568 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:06:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 153.25 MiB is free. Including non-PyTorch memory, this process has 78.97 GiB memory in use. Of the allocated memory 75.67 GiB is allocated by PyTorch, and 2.80 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:06:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 23        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76958 MiB |  77481 MiB | 105988 GiB | 105913 GiB |
|       from large pool |  76945 MiB |  77468 MiB | 105386 GiB | 105311 GiB |
|       from small pool |     12 MiB |     21 MiB |    601 GiB |    601 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76958 MiB |  77481 MiB | 105988 GiB | 105913 GiB |
|       from large pool |  76945 MiB |  77468 MiB | 105386 GiB | 105311 GiB |
|       from small pool |     12 MiB |     21 MiB |    601 GiB |    601 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 105764 GiB | 105689 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 105164 GiB | 105089 GiB |
|       from small pool |     12 MiB |     21 MiB |    600 GiB |    600 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80352 MiB |  80352 MiB | 489864 MiB | 409512 MiB |
|       from large pool |  80326 MiB |  80326 MiB | 487984 MiB | 407658 MiB |
|       from small pool |     26 MiB |    210 MiB |   1880 MiB |   1854 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3393 MiB |   8163 MiB | 107067 GiB | 107064 GiB |
|       from large pool |   3380 MiB |   8148 MiB | 106386 GiB | 106383 GiB |
|       from small pool |     13 MiB |     27 MiB |    681 GiB |    681 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |    6985 K  |    6984 K  |
|       from large pool |     315    |     322    |    3323 K  |    3323 K  |
|       from small pool |     291    |     356    |    3662 K  |    3661 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |    6985 K  |    6984 K  |
|       from large pool |     315    |     322    |    3323 K  |    3323 K  |
|       from small pool |     291    |     356    |    3662 K  |    3661 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     100    |     199    |    2520    |    2420    |
|       from large pool |      87    |      94    |    1580    |    1493    |
|       from small pool |      13    |     105    |     940    |     927    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      95    |      95    |    3925 K  |    3925 K  |
|       from large pool |      69    |      69    |    2166 K  |    2166 K  |
|       from small pool |      26    |      58    |    1758 K  |    1758 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:06:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:06:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:06:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:06:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:07:01]    INFO >> epoch 003:    232 / 1539 loss=5.289, wps=4587.3, ups=5.98, wpb=767.2, bsz=767.2, num_updates=3300, lr=9.8e-05, gnorm=6.678, clip=2, train_wall=7, gb_free=74, wall=577 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:07:09]    INFO >> epoch 003:    282 / 1539 loss=5.522, wps=5084.5, ups=6.76, wpb=751.9, bsz=751.9, num_updates=3350, lr=9.8e-05, gnorm=5.455, clip=0, train_wall=7, gb_free=70.6, wall=584 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:07:18]    INFO >> epoch 003:    332 / 1539 loss=5.323, wps=5046.4, ups=6.38, wpb=790.4, bsz=790.4, num_updates=3400, lr=9.8e-05, gnorm=6.037, clip=0, train_wall=7, gb_free=73.8, wall=592 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:07:26]    INFO >> epoch 003:    382 / 1539 loss=5.473, wps=4341.1, ups=6.31, wpb=688.4, bsz=688.4, num_updates=3450, lr=9.8e-05, gnorm=6.243, clip=0, train_wall=8, gb_free=72.5, wall=600 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:07:33]    INFO >> epoch 003:    432 / 1539 loss=5.414, wps=4477.3, ups=6.81, wpb=657.8, bsz=657.8, num_updates=3500, lr=9.8e-05, gnorm=6.459, clip=0, train_wall=7, gb_free=66.2, wall=607 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:07:41]    INFO >> epoch 003:    482 / 1539 loss=5.43, wps=4906.8, ups=6.51, wpb=754, bsz=754, num_updates=3550, lr=9.8e-05, gnorm=7.178, clip=4, train_wall=7, gb_free=73.1, wall=615 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:07:50]    INFO >> epoch 003:    532 / 1539 loss=5.335, wps=5015.3, ups=6.51, wpb=770.5, bsz=770.5, num_updates=3600, lr=9.8e-05, gnorm=6.539, clip=0, train_wall=7, gb_free=73.8, wall=623 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:07:57]    INFO >> epoch 003:    582 / 1539 loss=5.49, wps=4667.7, ups=6.48, wpb=720.1, bsz=720.1, num_updates=3650, lr=9.8e-05, gnorm=5.669, clip=0, train_wall=7, gb_free=71.6, wall=630 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:08:04]    INFO >> epoch 003:    632 / 1539 loss=5.425, wps=4843.3, ups=7.18, wpb=674.5, bsz=674.5, num_updates=3700, lr=9.8e-05, gnorm=6.373, clip=0, train_wall=7, gb_free=66.5, wall=637 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:08:12]    INFO >> epoch 003:    682 / 1539 loss=5.413, wps=5031.3, ups=6.65, wpb=756.5, bsz=756.5, num_updates=3750, lr=9.8e-05, gnorm=6.474, clip=2, train_wall=7, gb_free=75.1, wall=645 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:08:22]    INFO >> epoch 003:    732 / 1539 loss=5.263, wps=4894.8, ups=6.04, wpb=810.3, bsz=810.3, num_updates=3800, lr=9.8e-05, gnorm=6.548, clip=0, train_wall=8, gb_free=73.9, wall=653 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:08:29]    INFO >> epoch 003:    782 / 1539 loss=5.261, wps=5200.2, ups=6.54, wpb=795, bsz=795, num_updates=3850, lr=9.8e-05, gnorm=7.035, clip=4, train_wall=7, gb_free=73.7, wall=661 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:08:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 29.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.96 GiB is allocated by PyTorch, and 1.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:08:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78744 MiB |  78804 MiB | 123508 GiB | 123431 GiB |
|       from large pool |  78658 MiB |  78718 MiB | 122810 GiB | 122733 GiB |
|       from small pool |     85 MiB |     86 MiB |    698 GiB |    697 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78744 MiB |  78804 MiB | 123508 GiB | 123431 GiB |
|       from large pool |  78658 MiB |  78718 MiB | 122810 GiB | 122733 GiB |
|       from small pool |     85 MiB |     86 MiB |    698 GiB |    697 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78703 MiB |  78762 MiB | 123247 GiB | 123170 GiB |
|       from large pool |  78617 MiB |  78677 MiB | 122550 GiB | 122473 GiB |
|       from small pool |     85 MiB |     86 MiB |    697 GiB |    696 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80476 MiB |  80476 MiB | 517426 MiB | 436950 MiB |
|       from large pool |  80386 MiB |  80386 MiB | 515324 MiB | 434938 MiB |
|       from small pool |     90 MiB |    246 MiB |   2102 MiB |   2012 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1671 MiB |   7867 MiB | 124794 GiB | 124792 GiB |
|       from large pool |   1667 MiB |   7857 MiB | 124001 GiB | 124000 GiB |
|       from small pool |      4 MiB |     27 MiB |    792 GiB |    792 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1884    |    1887    |    8145 K  |    8143 K  |
|       from large pool |     463    |     464    |    3900 K  |    3900 K  |
|       from small pool |    1421    |    1424    |    4244 K  |    4243 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1884    |    1887    |    8145 K  |    8143 K  |
|       from large pool |     463    |     464    |    3900 K  |    3900 K  |
|       from small pool |    1421    |    1424    |    4244 K  |    4243 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     225    |     300    |    2734    |    2509    |
|       from large pool |     180    |     180    |    1683    |    1503    |
|       from small pool |      45    |     123    |    1051    |    1006    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     169    |     169    |    4572 K  |    4572 K  |
|       from large pool |     121    |     124    |    2540 K  |    2540 K  |
|       from small pool |      48    |      51    |    2031 K  |    2031 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:08:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:08:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:08:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:08:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:08:37]    INFO >> epoch 003:    833 / 1539 loss=5.241, wps=4707.7, ups=6.4, wpb=736, bsz=736, num_updates=3900, lr=9.8e-05, gnorm=8.253, clip=2, train_wall=7, gb_free=73, wall=669 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:08:44]    INFO >> epoch 003:    883 / 1539 loss=5.466, wps=4796.8, ups=6.69, wpb=717, bsz=717, num_updates=3950, lr=9.8e-05, gnorm=5.975, clip=0, train_wall=7, gb_free=72.5, wall=676 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:08:52]    INFO >> epoch 003:    933 / 1539 loss=5.381, wps=4738.3, ups=6.5, wpb=729.5, bsz=729.5, num_updates=4000, lr=9.8e-05, gnorm=6.16, clip=0, train_wall=7, gb_free=67.3, wall=684 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:09:02]    INFO >> epoch 003:    983 / 1539 loss=5.354, wps=3904.2, ups=5.71, wpb=684.3, bsz=684.3, num_updates=4050, lr=9.8e-05, gnorm=6.04, clip=0, train_wall=8, gb_free=71.5, wall=693 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:09:09]    INFO >> epoch 003:   1033 / 1539 loss=5.291, wps=4441, ups=6.98, wpb=636.3, bsz=636.3, num_updates=4100, lr=9.8e-05, gnorm=5.995, clip=0, train_wall=7, gb_free=68.3, wall=700 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:09:16]    INFO >> epoch 003:   1083 / 1539 loss=5.317, wps=4958.2, ups=7.36, wpb=673.9, bsz=673.9, num_updates=4150, lr=9.8e-05, gnorm=5.754, clip=0, train_wall=6, gb_free=72.9, wall=707 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:09:23] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 39.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.52 GiB is allocated by PyTorch, and 2.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:09:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78301 MiB |  78361 MiB | 133746 GiB | 133670 GiB |
|       from large pool |  77916 MiB |  77976 MiB | 132993 GiB | 132917 GiB |
|       from small pool |    385 MiB |    386 MiB |    753 GiB |    752 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78301 MiB |  78361 MiB | 133746 GiB | 133670 GiB |
|       from large pool |  77916 MiB |  77976 MiB | 132993 GiB | 132917 GiB |
|       from small pool |    385 MiB |    386 MiB |    753 GiB |    752 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78077 MiB |  78137 MiB | 133463 GiB | 133387 GiB |
|       from large pool |  77694 MiB |  77753 MiB | 132711 GiB | 132635 GiB |
|       from small pool |    383 MiB |    384 MiB |    752 GiB |    751 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80466 MiB |  80504 MiB | 539302 MiB | 458836 MiB |
|       from large pool |  80042 MiB |  80326 MiB | 536864 MiB | 456822 MiB |
|       from small pool |    424 MiB |    426 MiB |   2438 MiB |   2014 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2104 MiB |   6644 MiB | 134072 GiB | 134070 GiB |
|       from large pool |   2065 MiB |   6637 MiB | 133215 GiB | 133213 GiB |
|       from small pool |     38 MiB |     40 MiB |    856 GiB |    856 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7109    |    7112    |    8817 K  |    8810 K  |
|       from large pool |     916    |     917    |    4239 K  |    4238 K  |
|       from small pool |    6193    |    6196    |    4578 K  |    4572 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7109    |    7112    |    8817 K  |    8810 K  |
|       from large pool |     916    |     917    |    4239 K  |    4238 K  |
|       from small pool |    6193    |    6196    |    4578 K  |    4572 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     742    |     743    |    3261    |    2519    |
|       from large pool |     530    |     530    |    2042    |    1512    |
|       from small pool |     212    |     213    |    1219    |    1007    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     510    |     511    |    4950 K  |    4949 K  |
|       from large pool |     132    |     132    |    2765 K  |    2765 K  |
|       from small pool |     378    |     379    |    2185 K  |    2184 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:09:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:09:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:09:23] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:09:23] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:09:24]    INFO >> epoch 003:   1134 / 1539 loss=5.304, wps=4381.2, ups=6.4, wpb=684.8, bsz=684.8, num_updates=4200, lr=9.8e-05, gnorm=5.941, clip=0, train_wall=6, gb_free=73, wall=714 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:09:33]    INFO >> epoch 003:   1184 / 1539 loss=5.322, wps=4542.5, ups=6.74, wpb=673.6, bsz=673.6, num_updates=4250, lr=9.8e-05, gnorm=5.773, clip=0, train_wall=7, gb_free=73.5, wall=722 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:09:40]    INFO >> epoch 003:   1234 / 1539 loss=5.201, wps=4775.2, ups=6.65, wpb=718.1, bsz=718.1, num_updates=4300, lr=9.8e-05, gnorm=6.14, clip=0, train_wall=7, gb_free=70.5, wall=729 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:09:47]    INFO >> epoch 003:   1284 / 1539 loss=5.305, wps=4512.9, ups=7.28, wpb=619.7, bsz=619.7, num_updates=4350, lr=9.8e-05, gnorm=5.814, clip=0, train_wall=6, gb_free=73.5, wall=736 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:09:54]    INFO >> epoch 003:   1334 / 1539 loss=5.28, wps=5118.5, ups=7.23, wpb=708, bsz=708, num_updates=4400, lr=9.8e-05, gnorm=5.574, clip=0, train_wall=7, gb_free=72.1, wall=743 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:10:02]    INFO >> epoch 003:   1384 / 1539 loss=5.137, wps=4971.6, ups=7.18, wpb=692.7, bsz=692.7, num_updates=4450, lr=9.8e-05, gnorm=6.186, clip=0, train_wall=7, gb_free=74.2, wall=750 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:10:09]    INFO >> epoch 003:   1434 / 1539 loss=5.173, wps=4693.3, ups=7.2, wpb=651.7, bsz=651.7, num_updates=4500, lr=9.8e-05, gnorm=5.689, clip=0, train_wall=7, gb_free=72.1, wall=757 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:10:17]    INFO >> epoch 003:   1484 / 1539 loss=5.213, wps=4404.4, ups=6.68, wpb=658.9, bsz=658.9, num_updates=4550, lr=9.8e-05, gnorm=5.558, clip=0, train_wall=7, gb_free=72.4, wall=764 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:10:24]    INFO >> epoch 003:   1534 / 1539 loss=5.276, wps=4652.5, ups=7.02, wpb=662.9, bsz=662.9, num_updates=4600, lr=9.8e-05, gnorm=5.814, clip=0, train_wall=7, gb_free=72, wall=772 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:10:25]    INFO >> epoch 003 | loss 5.358 | wps 4478.4 | ups 6.28 | wpb 712.7 | bsz 712.7 | num_updates 4605 | lr 9.8e-05 | gnorm 6.169 | clip 0.5 | train_wall 215 | gb_free 74.4 | wall 772 (progress_bar.py:267, print())[0m
[33m[2025-11-19 04:10:25] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:10:39]    INFO >> epoch 003 | valid on 'valid' subset | loss 5.209 | wps 11764.1 | wpb 5412.5 | bsz 5412.5 | num_updates 4605 | best_loss 5.983 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 04:10:39]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 04:10:39]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/checkpoints/checkpoint_last.pt (epoch 3 @ 4605 updates, score 5.209) (writing took 0.013601 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 04:10:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:10:46]    INFO >> epoch 004:     45 / 1539 loss=5.284, wps=1814.5, ups=2.41, wpb=751.7, bsz=751.7, num_updates=4650, lr=9.4e-05, gnorm=5.13, clip=0, train_wall=7, gb_free=72.7, wall=792 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:10:54]    INFO >> epoch 004:     95 / 1539 loss=5.042, wps=5012.7, ups=6.63, wpb=756, bsz=756, num_updates=4700, lr=9.4e-05, gnorm=6.338, clip=0, train_wall=7, gb_free=67.8, wall=800 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:11:03]    INFO >> epoch 004:    145 / 1539 loss=4.858, wps=5422.7, ups=5.17, wpb=1048.6, bsz=1048.6, num_updates=4750, lr=9.4e-05, gnorm=7.233, clip=0, train_wall=9, gb_free=75.1, wall=810 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:11:12]    INFO >> epoch 004:    195 / 1539 loss=5.122, wps=5032.9, ups=7.18, wpb=701.3, bsz=701.3, num_updates=4800, lr=9.4e-05, gnorm=6.282, clip=0, train_wall=7, gb_free=71.9, wall=816 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:11:19]    INFO >> epoch 004:    245 / 1539 loss=5.041, wps=4975.6, ups=6.69, wpb=743.6, bsz=743.6, num_updates=4850, lr=9.4e-05, gnorm=7.005, clip=0, train_wall=7, gb_free=73.5, wall=824 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:11:26]    INFO >> epoch 004:    295 / 1539 loss=5.043, wps=4683.9, ups=7.24, wpb=647.4, bsz=647.4, num_updates=4900, lr=9.4e-05, gnorm=5.943, clip=0, train_wall=7, gb_free=74.6, wall=831 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:11:33]    INFO >> epoch 004:    345 / 1539 loss=4.999, wps=4948.6, ups=7.2, wpb=687.4, bsz=687.4, num_updates=4950, lr=9.4e-05, gnorm=5.572, clip=0, train_wall=7, gb_free=69.9, wall=838 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:11:40]    INFO >> epoch 004:    395 / 1539 loss=4.838, wps=5182, ups=6.81, wpb=761.1, bsz=761.1, num_updates=5000, lr=9.4e-05, gnorm=6.747, clip=0, train_wall=7, gb_free=67.8, wall=845 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:11:50]    INFO >> epoch 004:    445 / 1539 loss=4.931, wps=3957.5, ups=6.28, wpb=630.5, bsz=630.5, num_updates=5050, lr=9.4e-05, gnorm=6.34, clip=0, train_wall=7, gb_free=71.4, wall=853 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:11:57]    INFO >> epoch 004:    495 / 1539 loss=5.009, wps=4955, ups=6.88, wpb=720.1, bsz=720.1, num_updates=5100, lr=9.4e-05, gnorm=5.648, clip=0, train_wall=7, gb_free=74, wall=860 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:12:04]    INFO >> epoch 004:    545 / 1539 loss=4.875, wps=4747.7, ups=6.77, wpb=701.6, bsz=701.6, num_updates=5150, lr=9.4e-05, gnorm=6.246, clip=0, train_wall=7, gb_free=73.5, wall=868 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:12:11]    INFO >> epoch 004:    595 / 1539 loss=4.829, wps=4408.1, ups=7, wpb=629.8, bsz=629.8, num_updates=5200, lr=9.4e-05, gnorm=5.128, clip=0, train_wall=7, gb_free=71, wall=875 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:12:12] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 13.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 77.38 GiB is allocated by PyTorch, and 1.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:12:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79173 MiB |  79233 MiB | 167559 GiB | 167482 GiB |
|       from large pool |  79083 MiB |  79143 MiB | 166611 GiB | 166534 GiB |
|       from small pool |     89 MiB |     91 MiB |    948 GiB |    948 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79173 MiB |  79233 MiB | 167559 GiB | 167482 GiB |
|       from large pool |  79083 MiB |  79143 MiB | 166611 GiB | 166534 GiB |
|       from small pool |     89 MiB |     91 MiB |    948 GiB |    948 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79123 MiB |  79183 MiB | 167207 GiB | 167130 GiB |
|       from large pool |  79034 MiB |  79093 MiB | 166260 GiB | 166183 GiB |
|       from small pool |     89 MiB |     90 MiB |    946 GiB |    946 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80492 MiB |  80492 MiB | 564646 MiB | 484154 MiB |
|       from large pool |  80398 MiB |  80398 MiB | 562140 MiB | 481742 MiB |
|       from small pool |     94 MiB |     94 MiB |   2506 MiB |   2412 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1258 MiB |   7162 MiB | 164246 GiB | 164244 GiB |
|       from large pool |   1254 MiB |   7152 MiB | 163171 GiB | 163170 GiB |
|       from small pool |      4 MiB |     27 MiB |   1074 GiB |   1074 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1961    |    1964    |   11044 K  |   11042 K  |
|       from large pool |     470    |     471    |    5271 K  |    5271 K  |
|       from small pool |    1491    |    1494    |    5772 K  |    5770 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1961    |    1964    |   11044 K  |   11042 K  |
|       from large pool |     470    |     471    |    5271 K  |    5271 K  |
|       from small pool |    1491    |    1494    |    5772 K  |    5770 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     223    |     223    |    3348    |    3125    |
|       from large pool |     176    |     176    |    2095    |    1919    |
|       from small pool |      47    |      47    |    1253    |    1206    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     140    |     140    |    6216 K  |    6216 K  |
|       from large pool |      92    |      96    |    3444 K  |    3444 K  |
|       from small pool |      48    |      58    |    2772 K  |    2772 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:12:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:12:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:12:12] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:12:12] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:12:20]    INFO >> epoch 004:    646 / 1539 loss=4.725, wps=4468.6, ups=6.66, wpb=671.1, bsz=671.1, num_updates=5250, lr=9.4e-05, gnorm=6.7, clip=0, train_wall=6, gb_free=68.7, wall=882 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:12:28] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 983.25 MiB is free. Including non-PyTorch memory, this process has 78.16 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:12:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76323 MiB |  78596 MiB | 170178 GiB | 170104 GiB |
|       from large pool |  76304 MiB |  78579 MiB | 169217 GiB | 169142 GiB |
|       from small pool |     18 MiB |     24 MiB |    961 GiB |    961 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76323 MiB |  78596 MiB | 170178 GiB | 170104 GiB |
|       from large pool |  76304 MiB |  78579 MiB | 169217 GiB | 169142 GiB |
|       from small pool |     18 MiB |     24 MiB |    961 GiB |    961 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 169821 GiB | 169746 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 168861 GiB | 168786 GiB |
|       from small pool |     18 MiB |     24 MiB |    960 GiB |    959 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79522 MiB |  80432 MiB | 637044 MiB | 557522 MiB |
|       from large pool |  79496 MiB |  80338 MiB | 634538 MiB | 555042 MiB |
|       from small pool |     26 MiB |     94 MiB |   2506 MiB |   2480 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3198 MiB |   4143 MiB | 166742 GiB | 166738 GiB |
|       from large pool |   3191 MiB |   4135 MiB | 165652 GiB | 165649 GiB |
|       from small pool |      7 MiB |     23 MiB |   1089 GiB |   1089 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   11213 K  |   11212 K  |
|       from large pool |     340    |     346    |    5361 K  |    5361 K  |
|       from small pool |     300    |     356    |    5852 K  |    5851 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   11213 K  |   11212 K  |
|       from large pool |     340    |     346    |    5361 K  |    5361 K  |
|       from small pool |     300    |     356    |    5852 K  |    5851 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     107    |     222    |    3403    |    3296    |
|       from large pool |      94    |     175    |    2150    |    2056    |
|       from small pool |      13    |      47    |    1253    |    1240    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     126    |     128    |    6309 K  |    6309 K  |
|       from large pool |      98    |     100    |    3503 K  |    3503 K  |
|       from small pool |      28    |      51    |    2805 K  |    2805 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:12:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:12:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:12:28] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:12:28] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:12:29]    INFO >> epoch 004:    697 / 1539 loss=4.75, wps=3441.4, ups=5.41, wpb=636.6, bsz=636.6, num_updates=5300, lr=9.4e-05, gnorm=5.899, clip=0, train_wall=7, gb_free=67.8, wall=892 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:12:38]    INFO >> epoch 004:    747 / 1539 loss=4.791, wps=4089.9, ups=6.08, wpb=673.1, bsz=673.1, num_updates=5350, lr=9.4e-05, gnorm=5.734, clip=0, train_wall=8, gb_free=72.9, wall=900 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:12:45]    INFO >> epoch 004:    797 / 1539 loss=4.776, wps=4926, ups=6.94, wpb=710.1, bsz=710.1, num_updates=5400, lr=9.4e-05, gnorm=5.357, clip=0, train_wall=7, gb_free=72.3, wall=907 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:12:54]    INFO >> epoch 004:    847 / 1539 loss=4.527, wps=5030.1, ups=6.73, wpb=747.5, bsz=747.5, num_updates=5450, lr=9.4e-05, gnorm=7.188, clip=0, train_wall=7, gb_free=63.6, wall=915 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:13:01]    INFO >> epoch 004:    897 / 1539 loss=4.656, wps=5038.3, ups=6.47, wpb=778.3, bsz=778.3, num_updates=5500, lr=9.4e-05, gnorm=6.844, clip=0, train_wall=7, gb_free=70.2, wall=922 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:13:08]    INFO >> epoch 004:    947 / 1539 loss=4.547, wps=4614.4, ups=7.14, wpb=646.7, bsz=646.7, num_updates=5550, lr=9.4e-05, gnorm=5.958, clip=0, train_wall=7, gb_free=67.5, wall=929 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:13:16]    INFO >> epoch 004:    997 / 1539 loss=4.631, wps=4900.7, ups=6.41, wpb=764.9, bsz=764.9, num_updates=5600, lr=9.4e-05, gnorm=5.794, clip=0, train_wall=7, gb_free=75.5, wall=937 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:13:25]    INFO >> epoch 004:   1047 / 1539 loss=4.557, wps=4458.1, ups=7.09, wpb=628.6, bsz=628.6, num_updates=5650, lr=9.4e-05, gnorm=5.971, clip=0, train_wall=7, gb_free=72.5, wall=944 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:13:32]    INFO >> epoch 004:   1097 / 1539 loss=4.731, wps=5014.7, ups=6.87, wpb=729.6, bsz=729.6, num_updates=5700, lr=9.4e-05, gnorm=5.166, clip=0, train_wall=7, gb_free=71.1, wall=951 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:13:39]    INFO >> epoch 004:   1147 / 1539 loss=4.519, wps=4663.2, ups=6.89, wpb=676.4, bsz=676.4, num_updates=5750, lr=9.4e-05, gnorm=6.842, clip=0, train_wall=7, gb_free=56.5, wall=959 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:13:47]    INFO >> epoch 004:   1197 / 1539 loss=4.534, wps=5048.8, ups=6.69, wpb=754.7, bsz=754.7, num_updates=5800, lr=9.4e-05, gnorm=6.595, clip=0, train_wall=7, gb_free=75.6, wall=966 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:13:53]    INFO >> epoch 004:   1247 / 1539 loss=4.483, wps=4573.8, ups=7.29, wpb=627.8, bsz=627.8, num_updates=5850, lr=9.4e-05, gnorm=5.804, clip=0, train_wall=6, gb_free=75.6, wall=973 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:14:02]    INFO >> epoch 004:   1297 / 1539 loss=4.498, wps=4231, ups=6.56, wpb=644.6, bsz=644.6, num_updates=5900, lr=9.4e-05, gnorm=6.692, clip=2, train_wall=7, gb_free=74.6, wall=981 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:14:10]    INFO >> epoch 004:   1347 / 1539 loss=4.538, wps=5355.1, ups=6.69, wpb=801.1, bsz=801.1, num_updates=5950, lr=9.4e-05, gnorm=6.692, clip=0, train_wall=7, gb_free=66.8, wall=988 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:14:17] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 2.99 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:14:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 34        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75053 MiB |  75996 MiB | 190873 GiB | 190800 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 189802 GiB | 189729 GiB |
|       from small pool |     12 MiB |     13 MiB |   1070 GiB |   1070 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75053 MiB |  75996 MiB | 190873 GiB | 190800 GiB |
|       from large pool |  75040 MiB |  75983 MiB | 189802 GiB | 189729 GiB |
|       from small pool |     12 MiB |     13 MiB |   1070 GiB |   1070 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 190472 GiB | 190399 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 189403 GiB | 189330 GiB |
|       from small pool |     12 MiB |     13 MiB |   1069 GiB |   1069 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78634 MiB |  79742 MiB | 645400 MiB | 566766 MiB |
|       from large pool |  78608 MiB |  79496 MiB | 642674 MiB | 564066 MiB |
|       from small pool |     26 MiB |    246 MiB |   2726 MiB |   2700 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3580 MiB |   7454 MiB | 189481 GiB | 189477 GiB |
|       from large pool |   3567 MiB |   7440 MiB | 188266 GiB | 188263 GiB |
|       from small pool |     13 MiB |     21 MiB |   1214 GiB |   1214 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   12564 K  |   12563 K  |
|       from large pool |     314    |     322    |    6052 K  |    6052 K  |
|       from small pool |     291    |     336    |    6511 K  |    6510 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   12564 K  |   12563 K  |
|       from large pool |     314    |     322    |    6052 K  |    6052 K  |
|       from small pool |     291    |     336    |    6511 K  |    6510 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     217    |    3521    |    3430    |
|       from large pool |      78    |      94    |    2158    |    2080    |
|       from small pool |      13    |     123    |    1363    |    1350    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      90    |    7038 K  |    7038 K  |
|       from large pool |      60    |      61    |    3937 K  |    3937 K  |
|       from small pool |      29    |      51    |    3100 K  |    3100 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:14:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:14:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:14:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:14:17] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:14:18]    INFO >> epoch 004:   1398 / 1539 loss=4.461, wps=4742.2, ups=5.99, wpb=791.5, bsz=791.5, num_updates=6000, lr=9.4e-05, gnorm=7.477, clip=2, train_wall=7, gb_free=71.6, wall=996 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:14:25]    INFO >> epoch 004:   1448 / 1539 loss=4.384, wps=4935.9, ups=6.93, wpb=711.7, bsz=711.7, num_updates=6050, lr=9.4e-05, gnorm=7.326, clip=0, train_wall=7, gb_free=73.5, wall=1004 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:14:34]    INFO >> epoch 004:   1498 / 1539 loss=4.397, wps=4738.7, ups=6.95, wpb=682, bsz=682, num_updates=6100, lr=9.4e-05, gnorm=7.215, clip=2, train_wall=7, gb_free=74.2, wall=1011 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:14:36] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 25.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.10 GiB is allocated by PyTorch, and 1.50 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:14:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 35        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78888 MiB |  78948 MiB | 194338 GiB | 194261 GiB |
|       from large pool |  78495 MiB |  78555 MiB | 193245 GiB | 193169 GiB |
|       from small pool |    393 MiB |    394 MiB |   1092 GiB |   1092 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78888 MiB |  78948 MiB | 194338 GiB | 194261 GiB |
|       from large pool |  78495 MiB |  78555 MiB | 193245 GiB | 193169 GiB |
|       from small pool |    393 MiB |    394 MiB |   1092 GiB |   1092 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78858 MiB |  78917 MiB | 193930 GiB | 193853 GiB |
|       from large pool |  78467 MiB |  78526 MiB | 192839 GiB | 192762 GiB |
|       from small pool |    390 MiB |    392 MiB |   1090 GiB |   1090 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80480 MiB |  80482 MiB | 647248 MiB | 566768 MiB |
|       from large pool |  80048 MiB |  80048 MiB | 644114 MiB | 564066 MiB |
|       from small pool |    432 MiB |    434 MiB |   3134 MiB |   2702 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1531 MiB |   4145 MiB | 193506 GiB | 193504 GiB |
|       from large pool |   1492 MiB |   4108 MiB | 192266 GiB | 192265 GiB |
|       from small pool |     38 MiB |     41 MiB |   1239 GiB |   1239 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7252    |    7255    |   12813 K  |   12806 K  |
|       from large pool |     929    |     930    |    6169 K  |    6168 K  |
|       from small pool |    6323    |    6326    |    6643 K  |    6637 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7252    |    7255    |   12813 K  |   12806 K  |
|       from large pool |     929    |     930    |    6169 K  |    6168 K  |
|       from small pool |    6323    |    6326    |    6643 K  |    6637 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     318    |     319    |    3749    |    3431    |
|       from large pool |     102    |     102    |    2182    |    2080    |
|       from small pool |     216    |     217    |    1567    |    1351    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     462    |     464    |    7175 K  |    7174 K  |
|       from large pool |      77    |      77    |    4009 K  |    4009 K  |
|       from small pool |     385    |     387    |    3165 K  |    3165 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:14:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:14:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:14:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:14:36] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:14:40]    INFO >> epoch 004 | loss 4.74 | wps 4469.5 | ups 6.27 | wpb 712.7 | bsz 712.7 | num_updates 6140 | lr 9.4e-05 | gnorm 6.298 | clip 0.2 | train_wall 214 | gb_free 70.1 | wall 1017 (progress_bar.py:267, print())[0m
[33m[2025-11-19 04:14:40] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:14:53]    INFO >> epoch 004 | valid on 'valid' subset | loss 4.376 | wps 11862.3 | wpb 5412.5 | bsz 5412.5 | num_updates 6140 | best_loss 5.983 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 04:14:53]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 04:14:53]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/checkpoints/checkpoint_last.pt (epoch 4 @ 6140 updates, score 4.376) (writing took 0.012599 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 04:14:53] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:14:55]    INFO >> epoch 005:     10 / 1539 loss=4.476, wps=1556.8, ups=2.42, wpb=643.2, bsz=643.2, num_updates=6150, lr=8.9e-05, gnorm=6.194, clip=0, train_wall=7, gb_free=72.1, wall=1031 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:15:02]    INFO >> epoch 005:     60 / 1539 loss=4.387, wps=4544.7, ups=6.45, wpb=704.5, bsz=704.5, num_updates=6200, lr=8.9e-05, gnorm=6.723, clip=0, train_wall=7, gb_free=73.4, wall=1039 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:15:11]    INFO >> epoch 005:    110 / 1539 loss=4.442, wps=4957.7, ups=7.03, wpb=705.4, bsz=705.4, num_updates=6250, lr=8.9e-05, gnorm=6.668, clip=0, train_wall=7, gb_free=71.5, wall=1046 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:15:18]    INFO >> epoch 005:    160 / 1539 loss=4.477, wps=4613.7, ups=7.21, wpb=639.6, bsz=639.6, num_updates=6300, lr=8.9e-05, gnorm=5.971, clip=0, train_wall=7, gb_free=74, wall=1053 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:15:25]    INFO >> epoch 005:    210 / 1539 loss=4.457, wps=4637.1, ups=6.9, wpb=671.6, bsz=671.6, num_updates=6350, lr=8.9e-05, gnorm=5.865, clip=0, train_wall=7, gb_free=71, wall=1061 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:15:33]    INFO >> epoch 005:    260 / 1539 loss=4.571, wps=5445.9, ups=6.08, wpb=895.2, bsz=895.2, num_updates=6400, lr=8.9e-05, gnorm=6.293, clip=0, train_wall=8, gb_free=67.8, wall=1069 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:15:42]    INFO >> epoch 005:    310 / 1539 loss=4.349, wps=4857, ups=6.89, wpb=704.8, bsz=704.8, num_updates=6450, lr=8.9e-05, gnorm=6.84, clip=0, train_wall=7, gb_free=71.8, wall=1076 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:15:49]    INFO >> epoch 005:    360 / 1539 loss=4.212, wps=5147.1, ups=6.66, wpb=772.8, bsz=772.8, num_updates=6500, lr=8.9e-05, gnorm=6.872, clip=2, train_wall=7, gb_free=69.5, wall=1084 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:15:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 27.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.85 GiB is allocated by PyTorch, and 1.74 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:15:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 36        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78634 MiB |  78694 MiB | 211161 GiB | 211084 GiB |
|       from large pool |  78243 MiB |  78303 MiB | 209958 GiB | 209881 GiB |
|       from small pool |    390 MiB |    391 MiB |   1202 GiB |   1202 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78634 MiB |  78694 MiB | 211161 GiB | 211084 GiB |
|       from large pool |  78243 MiB |  78303 MiB | 209958 GiB | 209881 GiB |
|       from small pool |    390 MiB |    391 MiB |   1202 GiB |   1202 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78618 MiB |  78677 MiB | 210720 GiB | 210643 GiB |
|       from large pool |  78229 MiB |  78288 MiB | 209518 GiB | 209442 GiB |
|       from small pool |    388 MiB |    389 MiB |   1201 GiB |   1200 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80478 MiB |  80480 MiB | 647308 MiB | 566830 MiB |
|       from large pool |  80048 MiB |  80048 MiB | 644174 MiB | 564126 MiB |
|       from small pool |    430 MiB |    432 MiB |   3134 MiB |   2704 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1783 MiB |   5192 MiB | 209175 GiB | 209173 GiB |
|       from large pool |   1744 MiB |   5187 MiB | 207813 GiB | 207811 GiB |
|       from small pool |     39 MiB |     41 MiB |   1361 GiB |   1361 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7208    |    7211    |   13967 K  |   13960 K  |
|       from large pool |     925    |     926    |    6636 K  |    6635 K  |
|       from small pool |    6283    |    6286    |    7330 K  |    7324 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7208    |    7211    |   13967 K  |   13960 K  |
|       from large pool |     925    |     926    |    6636 K  |    6635 K  |
|       from small pool |    6283    |    6286    |    7330 K  |    7324 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     317    |     318    |    3750    |    3433    |
|       from large pool |     102    |     102    |    2183    |    2081    |
|       from small pool |     215    |     216    |    1567    |    1352    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     476    |     477    |    7847 K  |    7846 K  |
|       from large pool |      90    |      90    |    4312 K  |    4312 K  |
|       from small pool |     386    |     387    |    3534 K  |    3534 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:15:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:15:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:15:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:15:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:15:57]    INFO >> epoch 005:    411 / 1539 loss=4.44, wps=4502.5, ups=6.61, wpb=681.6, bsz=681.6, num_updates=6550, lr=8.9e-05, gnorm=6.263, clip=0, train_wall=7, gb_free=73.9, wall=1091 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:16:04]    INFO >> epoch 005:    461 / 1539 loss=4.279, wps=4370.1, ups=7.19, wpb=608, bsz=608, num_updates=6600, lr=8.9e-05, gnorm=6.116, clip=0, train_wall=7, gb_free=74.7, wall=1098 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:16:17]    INFO >> epoch 005:    511 / 1539 loss=4.383, wps=3094.6, ups=4.24, wpb=730.5, bsz=730.5, num_updates=6650, lr=8.9e-05, gnorm=5.736, clip=0, train_wall=11, gb_free=69.8, wall=1110 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:16:24]    INFO >> epoch 005:    561 / 1539 loss=4.421, wps=4363.3, ups=7.03, wpb=620.7, bsz=620.7, num_updates=6700, lr=8.9e-05, gnorm=6.423, clip=0, train_wall=7, gb_free=72.5, wall=1117 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:16:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.48 GiB is allocated by PyTorch, and 1.14 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:16:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 38        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79276 MiB |  79336 MiB | 216573 GiB | 216496 GiB |
|       from large pool |  79185 MiB |  79245 MiB | 215342 GiB | 215265 GiB |
|       from small pool |     91 MiB |     92 MiB |   1230 GiB |   1230 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79276 MiB |  79336 MiB | 216573 GiB | 216496 GiB |
|       from large pool |  79185 MiB |  79245 MiB | 215342 GiB | 215265 GiB |
|       from small pool |     91 MiB |     92 MiB |   1230 GiB |   1230 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79244 MiB |  79303 MiB | 216121 GiB | 216043 GiB |
|       from large pool |  79153 MiB |  79212 MiB | 214892 GiB | 214815 GiB |
|       from small pool |     90 MiB |     92 MiB |   1228 GiB |   1228 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80504 MiB | 647732 MiB | 567230 MiB |
|       from large pool |  80408 MiB |  80408 MiB | 644594 MiB | 564186 MiB |
|       from small pool |     94 MiB |    430 MiB |   3138 MiB |   3044 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1165 MiB |   6539 MiB | 214885 GiB | 214884 GiB |
|       from large pool |   1162 MiB |   6529 MiB | 213491 GiB | 213490 GiB |
|       from small pool |      2 MiB |     25 MiB |   1393 GiB |   1393 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1983    |    1986    |   14316 K  |   14314 K  |
|       from large pool |     472    |     473    |    6818 K  |    6818 K  |
|       from small pool |    1511    |    1514    |    7497 K  |    7496 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1983    |    1986    |   14316 K  |   14314 K  |
|       from large pool |     472    |     473    |    6818 K  |    6818 K  |
|       from small pool |    1511    |    1514    |    7497 K  |    7496 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     155    |     317    |    3759    |    3604    |
|       from large pool |     108    |     108    |    2190    |    2082    |
|       from small pool |      47    |     215    |    1569    |    1522    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     138    |     140    |    8036 K  |    8036 K  |
|       from large pool |      88    |      91    |    4428 K  |    4428 K  |
|       from small pool |      50    |      58    |    3608 K  |    3608 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:16:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:16:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:16:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:16:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:16:32]    INFO >> epoch 005:    612 / 1539 loss=4.217, wps=4671, ups=6.18, wpb=756, bsz=756, num_updates=6750, lr=8.9e-05, gnorm=7.545, clip=0, train_wall=7, gb_free=70.1, wall=1125 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:16:40]    INFO >> epoch 005:    662 / 1539 loss=4.38, wps=4878.1, ups=6.62, wpb=736.8, bsz=736.8, num_updates=6800, lr=8.9e-05, gnorm=6.741, clip=0, train_wall=7, gb_free=74.2, wall=1133 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:16:49]    INFO >> epoch 005:    712 / 1539 loss=4.463, wps=4899.9, ups=6.72, wpb=729, bsz=729, num_updates=6850, lr=8.9e-05, gnorm=6.839, clip=0, train_wall=7, gb_free=71.4, wall=1140 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:16:56]    INFO >> epoch 005:    762 / 1539 loss=4.39, wps=4998.4, ups=6.97, wpb=717.1, bsz=717.1, num_updates=6900, lr=8.9e-05, gnorm=6.425, clip=2, train_wall=7, gb_free=72.9, wall=1147 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:17:04]    INFO >> epoch 005:    812 / 1539 loss=4.441, wps=4183.7, ups=6.21, wpb=673.7, bsz=673.7, num_updates=6950, lr=8.9e-05, gnorm=6.058, clip=0, train_wall=8, gb_free=69.7, wall=1155 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:17:11]    INFO >> epoch 005:    862 / 1539 loss=4.275, wps=4755.5, ups=7.09, wpb=670.9, bsz=670.9, num_updates=7000, lr=8.9e-05, gnorm=6.399, clip=0, train_wall=7, gb_free=66.5, wall=1162 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:17:20]    INFO >> epoch 005:    912 / 1539 loss=4.362, wps=4549.6, ups=6.82, wpb=667, bsz=667, num_updates=7050, lr=8.9e-05, gnorm=6.221, clip=0, train_wall=7, gb_free=71.7, wall=1170 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:17:26] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 131.25 MiB is free. Including non-PyTorch memory, this process has 78.99 GiB memory in use. Of the allocated memory 74.93 GiB is allocated by PyTorch, and 3.56 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:17:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 39        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76325 MiB |  78599 MiB | 227233 GiB | 227158 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 225947 GiB | 225873 GiB |
|       from small pool |     18 MiB |     24 MiB |   1285 GiB |   1285 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76325 MiB |  78599 MiB | 227233 GiB | 227158 GiB |
|       from large pool |  76306 MiB |  78582 MiB | 225947 GiB | 225873 GiB |
|       from small pool |     18 MiB |     24 MiB |   1285 GiB |   1285 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 226757 GiB | 226683 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 225474 GiB | 225399 GiB |
|       from small pool |     18 MiB |     24 MiB |   1283 GiB |   1283 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80374 MiB |  80442 MiB | 647732 MiB | 567358 MiB |
|       from large pool |  80348 MiB |  80348 MiB | 644594 MiB | 564246 MiB |
|       from small pool |     26 MiB |     94 MiB |   3138 MiB |   3112 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4048 MiB |   5924 MiB | 225934 GiB | 225930 GiB |
|       from large pool |   4041 MiB |   5910 MiB | 224478 GiB | 224474 GiB |
|       from small pool |      7 MiB |     23 MiB |   1455 GiB |   1455 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   14999 K  |   14999 K  |
|       from large pool |     340    |     346    |    7173 K  |    7173 K  |
|       from small pool |     300    |     356    |    7826 K  |    7825 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   14999 K  |   14999 K  |
|       from large pool |     340    |     346    |    7173 K  |    7173 K  |
|       from small pool |     300    |     356    |    7826 K  |    7825 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     120    |     154    |    3759    |    3639    |
|       from large pool |     107    |     107    |    2190    |    2083    |
|       from small pool |      13    |      47    |    1569    |    1556    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     111    |     113    |    8408 K  |    8408 K  |
|       from large pool |      84    |      86    |    4655 K  |    4655 K  |
|       from small pool |      27    |      48    |    3752 K  |    3752 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:17:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:17:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:17:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:17:26] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:17:27]    INFO >> epoch 005:    963 / 1539 loss=4.369, wps=4318.2, ups=6.38, wpb=676.6, bsz=676.6, num_updates=7100, lr=8.9e-05, gnorm=6.176, clip=0, train_wall=7, gb_free=70.4, wall=1177 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:17:36]    INFO >> epoch 005:   1013 / 1539 loss=4.181, wps=5495.7, ups=5.76, wpb=954.2, bsz=954.2, num_updates=7150, lr=8.9e-05, gnorm=6.997, clip=2, train_wall=8, gb_free=70.3, wall=1186 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:17:44]    INFO >> epoch 005:   1063 / 1539 loss=4.137, wps=4908.8, ups=6.21, wpb=791.1, bsz=791.1, num_updates=7200, lr=8.9e-05, gnorm=7.356, clip=2, train_wall=8, gb_free=64.7, wall=1194 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:17:53]    INFO >> epoch 005:   1113 / 1539 loss=4.233, wps=4416.2, ups=7.05, wpb=626.1, bsz=626.1, num_updates=7250, lr=8.9e-05, gnorm=6.294, clip=0, train_wall=7, gb_free=74.2, wall=1201 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:17:55] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 199.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 4.97 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:17:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 43        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72823 MiB |  75427 MiB | 232756 GiB | 232684 GiB |
|       from large pool |  72810 MiB |  75414 MiB | 231439 GiB | 231368 GiB |
|       from small pool |     12 MiB |     21 MiB |   1316 GiB |   1316 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72823 MiB |  75427 MiB | 232756 GiB | 232684 GiB |
|       from large pool |  72810 MiB |  75414 MiB | 231439 GiB | 231368 GiB |
|       from small pool |     12 MiB |     21 MiB |   1316 GiB |   1316 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 232269 GiB | 232198 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 230954 GiB | 230883 GiB |
|       from small pool |     12 MiB |     21 MiB |   1314 GiB |   1314 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80306 MiB |  80306 MiB | 688498 MiB | 608192 MiB |
|       from large pool |  80280 MiB |  80280 MiB | 685160 MiB | 604880 MiB |
|       from small pool |     26 MiB |    226 MiB |   3338 MiB |   3312 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5580 MiB |   9572 MiB | 230920 GiB | 230915 GiB |
|       from large pool |   5567 MiB |   9557 MiB | 229429 GiB | 229423 GiB |
|       from small pool |     13 MiB |     31 MiB |   1491 GiB |   1491 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   15362 K  |   15361 K  |
|       from large pool |     312    |     320    |    7349 K  |    7348 K  |
|       from small pool |     291    |     356    |    8012 K  |    8012 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   15362 K  |   15361 K  |
|       from large pool |     312    |     320    |    7349 K  |    7348 K  |
|       from small pool |     291    |     356    |    8012 K  |    8012 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     170    |     327    |    3991    |    3821    |
|       from large pool |     157    |     214    |    2322    |    2165    |
|       from small pool |      13    |     113    |    1669    |    1656    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     159    |     161    |    8616 K  |    8616 K  |
|       from large pool |     133    |     135    |    4773 K  |    4773 K  |
|       from small pool |      26    |      58    |    3842 K  |    3842 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:17:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:17:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:17:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:17:55] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:18:01]    INFO >> epoch 005:   1164 / 1539 loss=4.156, wps=4631.9, ups=5.94, wpb=780.4, bsz=780.4, num_updates=7300, lr=8.9e-05, gnorm=7.014, clip=2, train_wall=7, gb_free=69.6, wall=1210 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:18:08]    INFO >> epoch 005:   1214 / 1539 loss=4.504, wps=4455.6, ups=6.77, wpb=657.7, bsz=657.7, num_updates=7350, lr=8.9e-05, gnorm=5.703, clip=2, train_wall=7, gb_free=68.7, wall=1217 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:18:15]    INFO >> epoch 005:   1264 / 1539 loss=4.199, wps=4875.7, ups=7.68, wpb=635.1, bsz=635.1, num_updates=7400, lr=8.9e-05, gnorm=6.041, clip=0, train_wall=6, gb_free=73.5, wall=1224 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:18:22]    INFO >> epoch 005:   1314 / 1539 loss=4.448, wps=4423.2, ups=6.79, wpb=651.5, bsz=651.5, num_updates=7450, lr=8.9e-05, gnorm=5.383, clip=0, train_wall=7, gb_free=70, wall=1231 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:18:31]    INFO >> epoch 005:   1364 / 1539 loss=4.185, wps=4843.6, ups=6.68, wpb=725.6, bsz=725.6, num_updates=7500, lr=8.9e-05, gnorm=6.524, clip=0, train_wall=7, gb_free=74.7, wall=1239 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:18:38]    INFO >> epoch 005:   1414 / 1539 loss=4.393, wps=4412.8, ups=7, wpb=630.1, bsz=630.1, num_updates=7550, lr=8.9e-05, gnorm=5.893, clip=0, train_wall=7, gb_free=74.2, wall=1246 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:18:46]    INFO >> epoch 005:   1464 / 1539 loss=4.3, wps=5029.1, ups=6.59, wpb=763.7, bsz=763.7, num_updates=7600, lr=8.9e-05, gnorm=7.03, clip=2, train_wall=7, gb_free=48.5, wall=1253 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:18:54]    INFO >> epoch 005:   1514 / 1539 loss=4.167, wps=5124.6, ups=6.38, wpb=802.7, bsz=802.7, num_updates=7650, lr=8.9e-05, gnorm=7.4, clip=2, train_wall=7, gb_free=54.7, wall=1261 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:18:57]    INFO >> epoch 005 | loss 4.336 | wps 4411.9 | ups 6.19 | wpb 712.7 | bsz 712.7 | num_updates 7675 | lr 8.9e-05 | gnorm 6.475 | clip 0.5 | train_wall 219 | gb_free 63.5 | wall 1265 (progress_bar.py:267, print())[0m
[33m[2025-11-19 04:18:57] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:19:12]    INFO >> epoch 005 | valid on 'valid' subset | loss 4.218 | wps 11683.2 | wpb 5412.5 | bsz 5412.5 | num_updates 7675 | best_loss 5.983 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 04:19:12]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 04:19:12]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/checkpoints/checkpoint_last.pt (epoch 5 @ 7675 updates, score 4.218) (writing took 0.012379 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 04:19:12] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:19:15]    INFO >> epoch 006:     25 / 1539 loss=4.241, wps=1732.2, ups=2.44, wpb=708.8, bsz=708.8, num_updates=7700, lr=8.2e-05, gnorm=6.633, clip=0, train_wall=7, gb_free=74.9, wall=1282 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:19:23]    INFO >> epoch 006:     75 / 1539 loss=4.309, wps=5587, ups=6.47, wpb=863.7, bsz=863.7, num_updates=7750, lr=8.2e-05, gnorm=6.549, clip=0, train_wall=7, gb_free=75.2, wall=1289 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:19:30]    INFO >> epoch 006:    125 / 1539 loss=4.268, wps=4838.3, ups=7.17, wpb=675.3, bsz=675.3, num_updates=7800, lr=8.2e-05, gnorm=6.592, clip=0, train_wall=7, gb_free=70.3, wall=1296 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:19:39]    INFO >> epoch 006:    175 / 1539 loss=4.15, wps=4705.3, ups=6.83, wpb=689, bsz=689, num_updates=7850, lr=8.2e-05, gnorm=7.21, clip=2, train_wall=7, gb_free=74.3, wall=1304 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:19:41] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 535.25 MiB is free. Including non-PyTorch memory, this process has 78.59 GiB memory in use. Of the allocated memory 73.81 GiB is allocated by PyTorch, and 4.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:19:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 44        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75055 MiB |  75998 MiB | 254753 GiB | 254679 GiB |
|       from large pool |  75042 MiB |  75985 MiB | 253307 GiB | 253233 GiB |
|       from small pool |     12 MiB |     15 MiB |   1446 GiB |   1446 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75055 MiB |  75998 MiB | 254753 GiB | 254679 GiB |
|       from large pool |  75042 MiB |  75985 MiB | 253307 GiB | 253233 GiB |
|       from small pool |     12 MiB |     15 MiB |   1446 GiB |   1446 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 254221 GiB | 254148 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 252777 GiB | 252704 GiB |
|       from small pool |     12 MiB |     15 MiB |   1443 GiB |   1443 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79970 MiB |  80406 MiB | 690500 MiB | 610530 MiB |
|       from large pool |  79944 MiB |  80306 MiB | 687088 MiB | 607144 MiB |
|       from small pool |     26 MiB |    100 MiB |   3412 MiB |   3386 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4914 MiB |   8989 MiB | 249574 GiB | 249569 GiB |
|       from large pool |   4901 MiB |   8976 MiB | 247940 GiB | 247935 GiB |
|       from small pool |     13 MiB |     25 MiB |   1633 GiB |   1633 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   16790 K  |   16790 K  |
|       from large pool |     314    |     322    |    7980 K  |    7979 K  |
|       from small pool |     291    |     356    |    8810 K  |    8810 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   16790 K  |   16790 K  |
|       from large pool |     314    |     322    |    7980 K  |    7979 K  |
|       from small pool |     291    |     356    |    8810 K  |    8810 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     210    |    4032    |    3860    |
|       from large pool |     159    |     160    |    2326    |    2167    |
|       from small pool |      13    |      50    |    1706    |    1693    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     164    |     164    |    9442 K  |    9442 K  |
|       from large pool |     136    |     136    |    5197 K  |    5196 K  |
|       from small pool |      28    |      57    |    4245 K  |    4245 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:19:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:19:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:19:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:19:41] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-19 04:19:47] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 904.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 535.25 MiB is free. Including non-PyTorch memory, this process has 78.59 GiB memory in use. Of the allocated memory 72.94 GiB is allocated by PyTorch, and 5.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:19:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 45        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69875 MiB |  74872 MiB | 255914 GiB | 255845 GiB |
|       from large pool |  69858 MiB |  74855 MiB | 254462 GiB | 254394 GiB |
|       from small pool |     17 MiB |     17 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69875 MiB |  74872 MiB | 255914 GiB | 255845 GiB |
|       from large pool |  69858 MiB |  74855 MiB | 254462 GiB | 254394 GiB |
|       from small pool |     17 MiB |     17 MiB |   1451 GiB |   1451 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69846 MiB |  74842 MiB | 255380 GiB | 255311 GiB |
|       from large pool |  69829 MiB |  74825 MiB | 253930 GiB | 253862 GiB |
|       from small pool |     17 MiB |     17 MiB |   1449 GiB |   1449 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79970 MiB |  80006 MiB | 690536 MiB | 610566 MiB |
|       from large pool |  79944 MiB |  79944 MiB | 687088 MiB | 607144 MiB |
|       from small pool |     26 MiB |     62 MiB |   3448 MiB |   3422 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |  10094 MiB |  11472 MiB | 250631 GiB | 250621 GiB |
|       from large pool |  10085 MiB |  11463 MiB | 248991 GiB | 248982 GiB |
|       from small pool |      8 MiB |     27 MiB |   1639 GiB |   1639 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     643    |     661    |   16854 K  |   16853 K  |
|       from large pool |     344    |     362    |    8013 K  |    8013 K  |
|       from small pool |     299    |     356    |    8840 K  |    8840 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     643    |     661    |   16854 K  |   16853 K  |
|       from large pool |     344    |     362    |    8013 K  |    8013 K  |
|       from small pool |     299    |     356    |    8840 K  |    8840 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     190    |    4050    |    3878    |
|       from large pool |     159    |     159    |    2326    |    2167    |
|       from small pool |      13    |      31    |    1724    |    1711    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     151    |     153    |    9477 K  |    9477 K  |
|       from large pool |     121    |     123    |    5219 K  |    5219 K  |
|       from small pool |      30    |      62    |    4258 K  |    4258 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:19:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:19:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:19:47] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:19:47] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:19:48]    INFO >> epoch 006:    227 / 1539 loss=4.334, wps=4040.4, ups=5.62, wpb=719.2, bsz=719.2, num_updates=7900, lr=8.2e-05, gnorm=6.012, clip=0, train_wall=7, gb_free=71.4, wall=1312 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:19:55]    INFO >> epoch 006:    277 / 1539 loss=4.319, wps=4538.1, ups=7.25, wpb=625.7, bsz=625.7, num_updates=7950, lr=8.2e-05, gnorm=5.289, clip=0, train_wall=6, gb_free=74.5, wall=1319 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:20:02]    INFO >> epoch 006:    327 / 1539 loss=4.427, wps=5438.8, ups=6.55, wpb=830.5, bsz=830.5, num_updates=8000, lr=8.2e-05, gnorm=6.566, clip=0, train_wall=7, gb_free=70.3, wall=1327 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:20:11]    INFO >> epoch 006:    377 / 1539 loss=4.222, wps=4470.2, ups=6.8, wpb=657.6, bsz=657.6, num_updates=8050, lr=8.2e-05, gnorm=6.431, clip=0, train_wall=7, gb_free=71.1, wall=1334 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:20:18]    INFO >> epoch 006:    427 / 1539 loss=4.282, wps=5332.8, ups=6.91, wpb=772.1, bsz=772.1, num_updates=8100, lr=8.2e-05, gnorm=5.86, clip=0, train_wall=7, gb_free=75, wall=1342 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:20:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 2.39 GiB is free. Including non-PyTorch memory, this process has 76.72 GiB memory in use. Of the allocated memory 70.76 GiB is allocated by PyTorch, and 5.46 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:20:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 47        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72059 MiB |  72460 MiB | 262338 GiB | 262268 GiB |
|       from large pool |  72042 MiB |  72442 MiB | 260852 GiB | 260781 GiB |
|       from small pool |     17 MiB |     21 MiB |   1486 GiB |   1486 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72059 MiB |  72460 MiB | 262338 GiB | 262268 GiB |
|       from large pool |  72042 MiB |  72442 MiB | 260852 GiB | 260781 GiB |
|       from small pool |     17 MiB |     21 MiB |   1486 GiB |   1486 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 261790 GiB | 261719 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 260305 GiB | 260235 GiB |
|       from small pool |     17 MiB |     21 MiB |   1484 GiB |   1484 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78056 MiB |  80154 MiB | 696552 MiB | 618496 MiB |
|       from large pool |  78026 MiB |  79944 MiB | 692920 MiB | 614894 MiB |
|       from small pool |     30 MiB |    210 MiB |   3632 MiB |   3602 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5996 MiB |   8377 MiB | 256697 GiB | 256691 GiB |
|       from large pool |   5983 MiB |   8364 MiB | 255017 GiB | 255011 GiB |
|       from small pool |     12 MiB |     31 MiB |   1680 GiB |   1680 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   17287 K  |   17286 K  |
|       from large pool |     308    |     315    |    8230 K  |    8230 K  |
|       from small pool |     298    |     356    |    9056 K  |    9056 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   17287 K  |   17286 K  |
|       from large pool |     308    |     315    |    8230 K  |    8230 K  |
|       from small pool |     298    |     356    |    9056 K  |    9056 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     264    |    4145    |    3973    |
|       from large pool |     157    |     159    |    2329    |    2172    |
|       from small pool |      15    |     105    |    1816    |    1801    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     169    |     169    |    9721 K  |    9721 K  |
|       from large pool |     137    |     137    |    5362 K  |    5362 K  |
|       from small pool |      32    |      55    |    4358 K  |    4358 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:20:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:20:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:20:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:20:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:20:26]    INFO >> epoch 006:    478 / 1539 loss=4.331, wps=4540, ups=6.43, wpb=705.8, bsz=705.8, num_updates=8150, lr=8.2e-05, gnorm=5.793, clip=0, train_wall=7, gb_free=65, wall=1349 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:20:34]    INFO >> epoch 006:    528 / 1539 loss=4.15, wps=4509.4, ups=6.37, wpb=707.6, bsz=707.6, num_updates=8200, lr=8.2e-05, gnorm=6.394, clip=0, train_wall=7, gb_free=72, wall=1357 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:20:43]    INFO >> epoch 006:    578 / 1539 loss=4.108, wps=5264, ups=6.6, wpb=797.6, bsz=797.6, num_updates=8250, lr=8.2e-05, gnorm=6.267, clip=0, train_wall=7, gb_free=74.1, wall=1365 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:20:50]    INFO >> epoch 006:    628 / 1539 loss=4.103, wps=4708.5, ups=7.3, wpb=644.9, bsz=644.9, num_updates=8300, lr=8.2e-05, gnorm=6.439, clip=2, train_wall=6, gb_free=70.5, wall=1372 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:20:57]    INFO >> epoch 006:    678 / 1539 loss=4.134, wps=4844.1, ups=6.63, wpb=730.2, bsz=730.2, num_updates=8350, lr=8.2e-05, gnorm=6.453, clip=0, train_wall=7, gb_free=73.6, wall=1379 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:21:04]    INFO >> epoch 006:    728 / 1539 loss=4.237, wps=4451.3, ups=7.22, wpb=616.1, bsz=616.1, num_updates=8400, lr=8.2e-05, gnorm=5.729, clip=0, train_wall=7, gb_free=62, wall=1386 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:21:05] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.16 GiB is allocated by PyTorch, and 2.44 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:21:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 48        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77932 MiB |  77992 MiB | 270810 GiB | 270734 GiB |
|       from large pool |  77549 MiB |  77609 MiB | 269273 GiB | 269197 GiB |
|       from small pool |    383 MiB |    384 MiB |   1536 GiB |   1536 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77932 MiB |  77992 MiB | 270810 GiB | 270734 GiB |
|       from large pool |  77549 MiB |  77609 MiB | 269273 GiB | 269197 GiB |
|       from small pool |    383 MiB |    384 MiB |   1536 GiB |   1536 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77897 MiB |  77956 MiB | 270242 GiB | 270166 GiB |
|       from large pool |  77516 MiB |  77575 MiB | 268708 GiB | 268632 GiB |
|       from small pool |    381 MiB |    382 MiB |   1534 GiB |   1533 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB | 698986 MiB | 618498 MiB |
|       from large pool |  80066 MiB |  80066 MiB | 694960 MiB | 614894 MiB |
|       from small pool |    422 MiB |    424 MiB |   4026 MiB |   3604 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2495 MiB |   6667 MiB | 264824 GiB | 264822 GiB |
|       from large pool |   2456 MiB |   6661 MiB | 263086 GiB | 263084 GiB |
|       from small pool |     38 MiB |     40 MiB |   1738 GiB |   1738 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7076    |    7079    |   17882 K  |   17874 K  |
|       from large pool |     913    |     914    |    8520 K  |    8519 K  |
|       from small pool |    6163    |    6166    |    9361 K  |    9355 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7076    |    7079    |   17882 K  |   17874 K  |
|       from large pool |     913    |     914    |    8520 K  |    8519 K  |
|       from small pool |    6163    |    6166    |    9361 K  |    9355 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     402    |     403    |    4376    |    3974    |
|       from large pool |     191    |     191    |    2363    |    2172    |
|       from small pool |     211    |     212    |    2013    |    1802    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     526    |     527    |   10055 K  |   10055 K  |
|       from large pool |     150    |     150    |    5552 K  |    5552 K  |
|       from small pool |     376    |     377    |    4503 K  |    4502 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:21:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:21:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:21:05] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:21:05] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:21:11]    INFO >> epoch 006:    779 / 1539 loss=4.187, wps=4400.6, ups=6.85, wpb=642.5, bsz=642.5, num_updates=8450, lr=8.2e-05, gnorm=6.13, clip=0, train_wall=6, gb_free=74.1, wall=1393 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:21:20]    INFO >> epoch 006:    829 / 1539 loss=4.131, wps=4446.3, ups=6.77, wpb=656.8, bsz=656.8, num_updates=8500, lr=8.2e-05, gnorm=6.239, clip=0, train_wall=7, gb_free=66.6, wall=1401 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:21:27]    INFO >> epoch 006:    879 / 1539 loss=4.099, wps=4906, ups=6.72, wpb=729.6, bsz=729.6, num_updates=8550, lr=8.2e-05, gnorm=6.043, clip=0, train_wall=7, gb_free=70.2, wall=1408 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:21:36]    INFO >> epoch 006:    929 / 1539 loss=4.355, wps=5101.3, ups=6.17, wpb=826.1, bsz=826.1, num_updates=8600, lr=8.2e-05, gnorm=6.552, clip=0, train_wall=8, gb_free=73.1, wall=1416 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:21:43]    INFO >> epoch 006:    979 / 1539 loss=4.06, wps=4651.2, ups=6.63, wpb=701.1, bsz=701.1, num_updates=8650, lr=8.2e-05, gnorm=5.553, clip=0, train_wall=7, gb_free=75.1, wall=1424 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:21:53] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.57 GiB. GPU 0 has a total capacity of 79.14 GiB of which 2.27 GiB is free. Including non-PyTorch memory, this process has 76.84 GiB memory in use. Of the allocated memory 66.03 GiB is allocated by PyTorch, and 10.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:21:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 49        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  63499 MiB |  70130 MiB | 279629 GiB | 279567 GiB |
|       from large pool |  63479 MiB |  70110 MiB | 278045 GiB | 277983 GiB |
|       from small pool |     20 MiB |     60 MiB |   1584 GiB |   1584 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  63499 MiB |  70130 MiB | 279629 GiB | 279567 GiB |
|       from large pool |  63479 MiB |  70110 MiB | 278045 GiB | 277983 GiB |
|       from small pool |     20 MiB |     60 MiB |   1584 GiB |   1584 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  63481 MiB |  70111 MiB | 279042 GiB | 278980 GiB |
|       from large pool |  63461 MiB |  70091 MiB | 277460 GiB | 277398 GiB |
|       from small pool |     20 MiB |     60 MiB |   1582 GiB |   1582 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78178 MiB |  80428 MiB | 698986 MiB | 620808 MiB |
|       from large pool |  78146 MiB |  80006 MiB | 694960 MiB | 616814 MiB |
|       from small pool |     32 MiB |    422 MiB |   4026 MiB |   3994 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7300 MiB |  10790 MiB | 272808 GiB | 272800 GiB |
|       from large pool |   7288 MiB |  10778 MiB | 271014 GiB | 271007 GiB |
|       from small pool |     11 MiB |     29 MiB |   1793 GiB |   1793 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     646    |    1326    |   18465 K  |   18465 K  |
|       from large pool |     330    |     403    |    8814 K  |    8814 K  |
|       from small pool |     316    |     924    |    9651 K  |    9650 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     646    |    1326    |   18465 K  |   18465 K  |
|       from large pool |     330    |     403    |    8814 K  |    8814 K  |
|       from small pool |     316    |     924    |    9651 K  |    9650 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     176    |     401    |    4376    |    4200    |
|       from large pool |     160    |     190    |    2363    |    2203    |
|       from small pool |      16    |     211    |    2013    |    1997    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     165    |     213    |   10388 K  |   10388 K  |
|       from large pool |     129    |     160    |    5749 K  |    5749 K  |
|       from small pool |      36    |      56    |    4638 K  |    4638 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:21:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:21:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:21:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:21:53] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:21:53]    INFO >> epoch 006:   1030 / 1539 loss=4.181, wps=4310.5, ups=5.9, wpb=730.1, bsz=730.1, num_updates=8700, lr=8.2e-05, gnorm=6.332, clip=0, train_wall=7, gb_free=11.5, wall=1432 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:22:00]    INFO >> epoch 006:   1080 / 1539 loss=4.304, wps=4851.6, ups=6.71, wpb=723.5, bsz=723.5, num_updates=8750, lr=8.2e-05, gnorm=5.89, clip=0, train_wall=7, gb_free=72.4, wall=1440 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:22:09]    INFO >> epoch 006:   1130 / 1539 loss=4.084, wps=4662.7, ups=6.15, wpb=757.6, bsz=757.6, num_updates=8800, lr=8.2e-05, gnorm=7.04, clip=2, train_wall=8, gb_free=72.1, wall=1448 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:22:16]    INFO >> epoch 006:   1180 / 1539 loss=4.343, wps=5009.3, ups=6.76, wpb=740.5, bsz=740.5, num_updates=8850, lr=8.2e-05, gnorm=6.175, clip=0, train_wall=7, gb_free=69, wall=1455 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:22:25]    INFO >> epoch 006:   1230 / 1539 loss=4.166, wps=4222.3, ups=6.4, wpb=659.3, bsz=659.3, num_updates=8900, lr=8.2e-05, gnorm=6.075, clip=0, train_wall=7, gb_free=73.8, wall=1463 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:22:32]    INFO >> epoch 006:   1280 / 1539 loss=4.258, wps=4526.8, ups=6.96, wpb=650.6, bsz=650.6, num_updates=8950, lr=8.2e-05, gnorm=6.026, clip=0, train_wall=7, gb_free=75.3, wall=1470 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:22:40]    INFO >> epoch 006:   1330 / 1539 loss=4.202, wps=4960.5, ups=6.69, wpb=741, bsz=741, num_updates=9000, lr=8.2e-05, gnorm=6.324, clip=0, train_wall=7, gb_free=71.3, wall=1478 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:22:47]    INFO >> epoch 006:   1380 / 1539 loss=4.218, wps=4447.1, ups=7.3, wpb=608.9, bsz=608.9, num_updates=9050, lr=8.2e-05, gnorm=5.959, clip=0, train_wall=6, gb_free=68.9, wall=1485 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:22:55]    INFO >> epoch 006:   1430 / 1539 loss=4.225, wps=5023.6, ups=6.68, wpb=751.8, bsz=751.8, num_updates=9100, lr=8.2e-05, gnorm=6.459, clip=0, train_wall=7, gb_free=70.5, wall=1492 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:23:03]    INFO >> epoch 006:   1480 / 1539 loss=4.043, wps=4638.9, ups=6.47, wpb=717, bsz=717, num_updates=9150, lr=8.2e-05, gnorm=6.277, clip=2, train_wall=7, gb_free=69.4, wall=1500 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:23:10]    INFO >> epoch 006:   1530 / 1539 loss=4.08, wps=4905.9, ups=7.28, wpb=674.1, bsz=674.1, num_updates=9200, lr=8.2e-05, gnorm=5.689, clip=0, train_wall=6, gb_free=74.6, wall=1507 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:23:11]    INFO >> epoch 006 | loss 4.214 | wps 4488 | ups 6.31 | wpb 711.1 | bsz 711.1 | num_updates 9209 | lr 8.2e-05 | gnorm 6.199 | clip 0.3 | train_wall 214 | gb_free 72.3 | wall 1508 (progress_bar.py:267, print())[0m
[33m[2025-11-19 04:23:11] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:23:24]    INFO >> epoch 006 | valid on 'valid' subset | loss 4.101 | wps 11948.8 | wpb 5412.5 | bsz 5412.5 | num_updates 9209 | best_loss 5.983 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 04:23:24]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 04:23:24]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/checkpoints/checkpoint_last.pt (epoch 6 @ 9209 updates, score 4.101) (writing took 0.012756 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 04:23:24] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:23:32]    INFO >> epoch 007:     41 / 1539 loss=4.269, wps=1674.7, ups=2.38, wpb=704.3, bsz=704.3, num_updates=9250, lr=7.4e-05, gnorm=5.493, clip=0, train_wall=7, gb_free=71.6, wall=1528 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:23:34] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 70.77 GiB is allocated by PyTorch, and 6.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:23:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 51        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72064 MiB |  72463 MiB | 300570 GiB | 300499 GiB |
|       from large pool |  72047 MiB |  72446 MiB | 298863 GiB | 298793 GiB |
|       from small pool |     17 MiB |     25 MiB |   1706 GiB |   1706 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72064 MiB |  72463 MiB | 300570 GiB | 300499 GiB |
|       from large pool |  72047 MiB |  72446 MiB | 298863 GiB | 298793 GiB |
|       from small pool |     17 MiB |     25 MiB |   1706 GiB |   1706 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  72444 MiB | 299940 GiB | 299870 GiB |
|       from large pool |  72027 MiB |  72426 MiB | 298236 GiB | 298165 GiB |
|       from small pool |     17 MiB |     25 MiB |   1704 GiB |   1704 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78634 MiB |  78794 MiB | 712436 MiB | 633802 MiB |
|       from large pool |  78610 MiB |  78670 MiB | 708318 MiB | 629708 MiB |
|       from small pool |     24 MiB |    124 MiB |   4118 MiB |   4094 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6569 MiB |   8371 MiB | 290286 GiB | 290279 GiB |
|       from large pool |   6562 MiB |   8363 MiB | 288359 GiB | 288352 GiB |
|       from small pool |      6 MiB |     33 MiB |   1927 GiB |   1927 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   19804 K  |   19803 K  |
|       from large pool |     308    |     315    |    9401 K  |    9401 K  |
|       from small pool |     298    |     356    |   10402 K  |   10402 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   19804 K  |   19803 K  |
|       from large pool |     308    |     315    |    9401 K  |    9401 K  |
|       from small pool |     298    |     356    |   10402 K  |   10402 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     168    |     222    |    4427    |    4259    |
|       from large pool |     156    |     160    |    2368    |    2212    |
|       from small pool |      12    |      62    |    2059    |    2047    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     162    |     163    |   11162 K  |   11162 K  |
|       from large pool |     137    |     138    |    6142 K  |    6142 K  |
|       from small pool |      25    |      62    |    5020 K  |    5020 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:23:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:23:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:23:34] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:23:34] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:23:40]    INFO >> epoch 007:     92 / 1539 loss=4.067, wps=4554.2, ups=6.75, wpb=674.7, bsz=674.7, num_updates=9300, lr=7.4e-05, gnorm=6.782, clip=2, train_wall=6, gb_free=74.6, wall=1535 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:23:47]    INFO >> epoch 007:    142 / 1539 loss=4.119, wps=4503.2, ups=6.79, wpb=663.1, bsz=663.1, num_updates=9350, lr=7.4e-05, gnorm=6.737, clip=2, train_wall=7, gb_free=73.4, wall=1542 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:23:55]    INFO >> epoch 007:    192 / 1539 loss=3.91, wps=5598.2, ups=6.32, wpb=885.1, bsz=885.1, num_updates=9400, lr=7.4e-05, gnorm=7.162, clip=0, train_wall=7, gb_free=67.7, wall=1550 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:24:04]    INFO >> epoch 007:    242 / 1539 loss=4.191, wps=5097.2, ups=6.42, wpb=794.3, bsz=794.3, num_updates=9450, lr=7.4e-05, gnorm=6.74, clip=0, train_wall=7, gb_free=70.7, wall=1558 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:24:12]    INFO >> epoch 007:    292 / 1539 loss=4.038, wps=4747.4, ups=6.79, wpb=699.6, bsz=699.6, num_updates=9500, lr=7.4e-05, gnorm=6.826, clip=2, train_wall=7, gb_free=71.4, wall=1566 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:24:17] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 6.71 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:24:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 52        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69006 MiB |  71763 MiB | 308769 GiB | 308702 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 307018 GiB | 306950 GiB |
|       from small pool |     16 MiB |     18 MiB |   1751 GiB |   1751 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69006 MiB |  71763 MiB | 308769 GiB | 308702 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 307018 GiB | 306950 GiB |
|       from small pool |     16 MiB |     18 MiB |   1751 GiB |   1751 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 308120 GiB | 308053 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 306372 GiB | 306304 GiB |
|       from small pool |     16 MiB |     17 MiB |   1748 GiB |   1748 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78636 MiB |  78836 MiB | 712638 MiB | 634002 MiB |
|       from large pool |  78610 MiB |  78610 MiB | 708318 MiB | 629708 MiB |
|       from small pool |     26 MiB |    226 MiB |   4320 MiB |   4294 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6901 MiB |  10618 MiB | 298170 GiB | 298163 GiB |
|       from large pool |   6892 MiB |  10608 MiB | 296191 GiB | 296185 GiB |
|       from small pool |      9 MiB |     25 MiB |   1978 GiB |   1978 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   20344 K  |   20344 K  |
|       from large pool |     343    |     351    |    9674 K  |    9674 K  |
|       from small pool |     298    |     342    |   10670 K  |   10669 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   20344 K  |   20344 K  |
|       from large pool |     343    |     351    |    9674 K  |    9674 K  |
|       from small pool |     298    |     342    |   10670 K  |   10669 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     169    |     269    |    4528    |    4359    |
|       from large pool |     156    |     156    |    2368    |    2212    |
|       from small pool |      13    |     113    |    2160    |    2147    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     153    |     157    |   11466 K  |   11466 K  |
|       from large pool |     124    |     128    |    6323 K  |    6323 K  |
|       from small pool |      29    |      53    |    5142 K  |    5142 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:24:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:24:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:24:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:24:17] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:24:20]    INFO >> epoch 007:    343 / 1539 loss=4.079, wps=3897.8, ups=5.67, wpb=687.1, bsz=687.1, num_updates=9550, lr=7.4e-05, gnorm=6.846, clip=2, train_wall=8, gb_free=69.5, wall=1574 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:24:27]    INFO >> epoch 007:    393 / 1539 loss=4.264, wps=4327.6, ups=7.24, wpb=598.1, bsz=598.1, num_updates=9600, lr=7.4e-05, gnorm=5.616, clip=0, train_wall=7, gb_free=62, wall=1581 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:24:36]    INFO >> epoch 007:    443 / 1539 loss=4.121, wps=4463.9, ups=6.59, wpb=677.8, bsz=677.8, num_updates=9650, lr=7.4e-05, gnorm=5.756, clip=0, train_wall=7, gb_free=74.5, wall=1589 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:24:41] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 123.25 MiB is free. Including non-PyTorch memory, this process has 79.00 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.83 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:24:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 54        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76958 MiB |  77480 MiB | 313195 GiB | 313120 GiB |
|       from large pool |  76945 MiB |  77467 MiB | 311423 GiB | 311348 GiB |
|       from small pool |     12 MiB |     18 MiB |   1772 GiB |   1772 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76958 MiB |  77480 MiB | 313195 GiB | 313120 GiB |
|       from large pool |  76945 MiB |  77467 MiB | 311423 GiB | 311348 GiB |
|       from small pool |     12 MiB |     18 MiB |   1772 GiB |   1772 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 312537 GiB | 312462 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 310767 GiB | 310692 GiB |
|       from small pool |     12 MiB |     18 MiB |   1769 GiB |   1769 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80382 MiB |  80382 MiB | 719380 MiB | 638998 MiB |
|       from large pool |  80358 MiB |  80358 MiB | 715014 MiB | 634656 MiB |
|       from small pool |     24 MiB |     72 MiB |   4366 MiB |   4342 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3423 MiB |   7648 MiB | 302343 GiB | 302340 GiB |
|       from large pool |   3412 MiB |   7637 MiB | 300341 GiB | 300338 GiB |
|       from small pool |     11 MiB |     19 MiB |   2002 GiB |   2002 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   20614 K  |   20613 K  |
|       from large pool |     315    |     322    |    9816 K  |    9816 K  |
|       from small pool |     291    |     356    |   10797 K  |   10797 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   20614 K  |   20613 K  |
|       from large pool |     315    |     322    |    9816 K  |    9816 K  |
|       from small pool |     291    |     356    |   10797 K  |   10797 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     138    |     197    |    4559    |    4421    |
|       from large pool |     126    |     161    |    2376    |    2250    |
|       from small pool |      12    |      36    |    2183    |    2171    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     134    |     134    |   11614 K  |   11614 K  |
|       from large pool |     107    |     107    |    6417 K  |    6417 K  |
|       from small pool |      27    |      47    |    5197 K  |    5197 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:24:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:24:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:24:41] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:24:41] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:24:45]    INFO >> epoch 007:    494 / 1539 loss=4.146, wps=4422.3, ups=5.95, wpb=742.8, bsz=742.8, num_updates=9700, lr=7.4e-05, gnorm=6.343, clip=2, train_wall=7, gb_free=72.2, wall=1597 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:24:52]    INFO >> epoch 007:    544 / 1539 loss=4.135, wps=4548.9, ups=7.21, wpb=631.2, bsz=631.2, num_updates=9750, lr=7.4e-05, gnorm=5.567, clip=0, train_wall=7, gb_free=74.3, wall=1604 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:24:59]    INFO >> epoch 007:    594 / 1539 loss=4.247, wps=4795.5, ups=7.01, wpb=684.3, bsz=684.3, num_updates=9800, lr=7.4e-05, gnorm=6.01, clip=0, train_wall=7, gb_free=75.1, wall=1611 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:25:06]    INFO >> epoch 007:    644 / 1539 loss=4.094, wps=4658.4, ups=6.68, wpb=697.8, bsz=697.8, num_updates=9850, lr=7.4e-05, gnorm=6.515, clip=0, train_wall=7, gb_free=70.6, wall=1619 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:25:15]    INFO >> epoch 007:    694 / 1539 loss=4.03, wps=5520.8, ups=6.6, wpb=835.9, bsz=835.9, num_updates=9900, lr=7.4e-05, gnorm=7.652, clip=4, train_wall=7, gb_free=73.9, wall=1626 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:25:22]    INFO >> epoch 007:    744 / 1539 loss=4.095, wps=4902.7, ups=6.89, wpb=712, bsz=712, num_updates=9950, lr=7.4e-05, gnorm=5.601, clip=0, train_wall=7, gb_free=72.8, wall=1634 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:25:30]    INFO >> epoch 007:    794 / 1539 loss=4.101, wps=4432.4, ups=6.77, wpb=654.6, bsz=654.6, num_updates=10000, lr=7.4e-05, gnorm=6.157, clip=0, train_wall=7, gb_free=72.1, wall=1641 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:25:37]    INFO >> epoch 007:    844 / 1539 loss=4.105, wps=4848.7, ups=6.71, wpb=722.7, bsz=722.7, num_updates=10050, lr=7.4e-05, gnorm=6.783, clip=2, train_wall=7, gb_free=70.6, wall=1649 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:25:46]    INFO >> epoch 007:    894 / 1539 loss=4.153, wps=4612.7, ups=6.7, wpb=688.1, bsz=688.1, num_updates=10100, lr=7.4e-05, gnorm=5.942, clip=0, train_wall=7, gb_free=73.8, wall=1656 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:25:53]    INFO >> epoch 007:    944 / 1539 loss=3.996, wps=4834.9, ups=6.92, wpb=699, bsz=699, num_updates=10150, lr=7.4e-05, gnorm=6.365, clip=0, train_wall=7, gb_free=71.9, wall=1663 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:26:01]    INFO >> epoch 007:    994 / 1539 loss=4.049, wps=4840.4, ups=6.58, wpb=736.1, bsz=736.1, num_updates=10200, lr=7.4e-05, gnorm=6.661, clip=0, train_wall=7, gb_free=70.3, wall=1671 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:26:08]    INFO >> epoch 007:   1044 / 1539 loss=4.102, wps=4920, ups=6.97, wpb=705.7, bsz=705.7, num_updates=10250, lr=7.4e-05, gnorm=6.058, clip=0, train_wall=7, gb_free=73, wall=1678 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:26:17]    INFO >> epoch 007:   1094 / 1539 loss=4.101, wps=4615.9, ups=6.66, wpb=693.4, bsz=693.4, num_updates=10300, lr=7.4e-05, gnorm=6.385, clip=2, train_wall=7, gb_free=71.6, wall=1685 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:26:25]    INFO >> epoch 007:   1144 / 1539 loss=4.125, wps=4720.6, ups=6.01, wpb=784.9, bsz=784.9, num_updates=10350, lr=7.4e-05, gnorm=6.394, clip=0, train_wall=8, gb_free=74, wall=1694 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:26:33]    INFO >> epoch 007:   1194 / 1539 loss=4.115, wps=5205.5, ups=6.41, wpb=811.5, bsz=811.5, num_updates=10400, lr=7.4e-05, gnorm=6.305, clip=0, train_wall=7, gb_free=72.8, wall=1702 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:26:40]    INFO >> epoch 007:   1244 / 1539 loss=4.043, wps=4734.8, ups=6.9, wpb=686.2, bsz=686.2, num_updates=10450, lr=7.4e-05, gnorm=6.237, clip=0, train_wall=7, gb_free=68.3, wall=1709 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:26:47]    INFO >> epoch 007:   1294 / 1539 loss=4.061, wps=4454.1, ups=6.95, wpb=640.4, bsz=640.4, num_updates=10500, lr=7.4e-05, gnorm=5.79, clip=0, train_wall=7, gb_free=70.7, wall=1716 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:26:56]    INFO >> epoch 007:   1344 / 1539 loss=4.002, wps=4789.8, ups=6.87, wpb=697.4, bsz=697.4, num_updates=10550, lr=7.4e-05, gnorm=6.627, clip=2, train_wall=7, gb_free=70.4, wall=1723 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:27:04]    INFO >> epoch 007:   1394 / 1539 loss=3.985, wps=4641.8, ups=6.64, wpb=699.1, bsz=699.1, num_updates=10600, lr=7.4e-05, gnorm=6.64, clip=2, train_wall=7, gb_free=67.1, wall=1731 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:27:09] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 51.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 76.76 GiB is allocated by PyTorch, and 1.81 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:27:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 56        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78538 MiB |  78598 MiB | 341065 GiB | 340988 GiB |
|       from large pool |  78151 MiB |  78211 MiB | 339139 GiB | 339063 GiB |
|       from small pool |    386 MiB |    388 MiB |   1925 GiB |   1925 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78538 MiB |  78598 MiB | 341065 GiB | 340988 GiB |
|       from large pool |  78151 MiB |  78211 MiB | 339139 GiB | 339063 GiB |
|       from small pool |    386 MiB |    388 MiB |   1925 GiB |   1925 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78257 MiB |  78317 MiB | 340344 GiB | 340268 GiB |
|       from large pool |  77872 MiB |  77932 MiB | 338421 GiB | 338345 GiB |
|       from small pool |    384 MiB |    386 MiB |   1922 GiB |   1922 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80454 MiB |  80456 MiB | 750444 MiB | 669990 MiB |
|       from large pool |  80028 MiB |  80028 MiB | 745674 MiB | 665646 MiB |
|       from small pool |    426 MiB |    428 MiB |   4770 MiB |   4344 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1855 MiB |   6794 MiB | 329939 GiB | 329938 GiB |
|       from large pool |   1816 MiB |   6788 MiB | 327761 GiB | 327759 GiB |
|       from small pool |     39 MiB |     41 MiB |   2178 GiB |   2178 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7142    |    7145    |   22485 K  |   22478 K  |
|       from large pool |     919    |     920    |   10761 K  |   10760 K  |
|       from small pool |    6223    |    6226    |   11723 K  |   11717 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7142    |    7145    |   22485 K  |   22478 K  |
|       from large pool |     919    |     920    |   10761 K  |   10760 K  |
|       from small pool |    6223    |    6226    |   11723 K  |   11717 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     838    |     839    |    5272    |    4434    |
|       from large pool |     625    |     625    |    2887    |    2262    |
|       from small pool |     213    |     214    |    2385    |    2172    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     519    |     520    |   12654 K  |   12653 K  |
|       from large pool |     138    |     138    |    7032 K  |    7032 K  |
|       from small pool |     381    |     382    |    5621 K  |    5621 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:27:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:27:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:27:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:27:09] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:27:12]    INFO >> epoch 007:   1445 / 1539 loss=3.998, wps=4315.4, ups=6.03, wpb=716, bsz=716, num_updates=10650, lr=7.4e-05, gnorm=6.478, clip=0, train_wall=7, gb_free=68.1, wall=1739 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:27:19]    INFO >> epoch 007:   1495 / 1539 loss=4.052, wps=5159, ups=7.23, wpb=713.5, bsz=713.5, num_updates=10700, lr=7.4e-05, gnorm=6.803, clip=0, train_wall=7, gb_free=70.4, wall=1746 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:27:27]    INFO >> epoch 007 | loss 4.09 | wps 4463.8 | ups 6.26 | wpb 712.7 | bsz 712.7 | num_updates 10744 | lr 7.4e-05 | gnorm 6.365 | clip 0.7 | train_wall 217 | gb_free 70.2 | wall 1753 (progress_bar.py:267, print())[0m
[33m[2025-11-19 04:27:27] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:27:40]    INFO >> epoch 007 | valid on 'valid' subset | loss 4.05 | wps 11534.4 | wpb 5412.5 | bsz 5412.5 | num_updates 10744 | best_loss 5.983 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 04:27:41]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 04:27:41]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/checkpoints/checkpoint_last.pt (epoch 7 @ 10744 updates, score 4.05) (writing took 0.013320 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 04:27:41] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:27:41]    INFO >> epoch 008:      6 / 1539 loss=4.083, wps=1711.6, ups=2.34, wpb=730.3, bsz=730.3, num_updates=10750, lr=6.5e-05, gnorm=5.944, clip=0, train_wall=7, gb_free=68.7, wall=1767 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:27:49]    INFO >> epoch 008:     56 / 1539 loss=3.731, wps=5154.1, ups=7.01, wpb=735.8, bsz=735.8, num_updates=10800, lr=6.5e-05, gnorm=6.513, clip=2, train_wall=7, gb_free=73.9, wall=1775 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:27:58]    INFO >> epoch 008:    106 / 1539 loss=4.073, wps=4966.6, ups=6.48, wpb=766.8, bsz=766.8, num_updates=10850, lr=6.5e-05, gnorm=5.473, clip=0, train_wall=7, gb_free=73.2, wall=1782 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:28:05]    INFO >> epoch 008:    156 / 1539 loss=4.026, wps=4603, ups=7.06, wpb=652.3, bsz=652.3, num_updates=10900, lr=6.5e-05, gnorm=5.517, clip=0, train_wall=7, gb_free=75.6, wall=1789 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:28:13]    INFO >> epoch 008:    206 / 1539 loss=4.038, wps=5199.2, ups=6.23, wpb=834.7, bsz=834.7, num_updates=10950, lr=6.5e-05, gnorm=6.649, clip=2, train_wall=8, gb_free=73.5, wall=1797 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:28:20]    INFO >> epoch 008:    256 / 1539 loss=4.153, wps=4975.3, ups=6.97, wpb=714.1, bsz=714.1, num_updates=11000, lr=6.5e-05, gnorm=6.194, clip=0, train_wall=7, gb_free=71.5, wall=1805 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:28:27]    INFO >> epoch 008:    306 / 1539 loss=4.185, wps=4783, ups=6.64, wpb=720.8, bsz=720.8, num_updates=11050, lr=6.5e-05, gnorm=6.649, clip=0, train_wall=7, gb_free=71.8, wall=1812 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:28:37]    INFO >> epoch 008:    356 / 1539 loss=4.091, wps=4160.9, ups=6.25, wpb=665.6, bsz=665.6, num_updates=11100, lr=6.5e-05, gnorm=5.504, clip=0, train_wall=8, gb_free=74.7, wall=1820 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:28:44]    INFO >> epoch 008:    406 / 1539 loss=4.064, wps=4851.5, ups=6.86, wpb=707, bsz=707, num_updates=11150, lr=6.5e-05, gnorm=6.45, clip=0, train_wall=7, gb_free=72.1, wall=1827 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:28:52]    INFO >> epoch 008:    456 / 1539 loss=3.861, wps=5647.1, ups=6.45, wpb=875.6, bsz=875.6, num_updates=11200, lr=6.5e-05, gnorm=7.081, clip=0, train_wall=7, gb_free=72.6, wall=1835 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:28:59]    INFO >> epoch 008:    506 / 1539 loss=3.965, wps=5036.4, ups=6.72, wpb=749.6, bsz=749.6, num_updates=11250, lr=6.5e-05, gnorm=7.255, clip=0, train_wall=7, gb_free=74.3, wall=1843 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:29:08]    INFO >> epoch 008:    556 / 1539 loss=4.14, wps=4485, ups=6.89, wpb=651.2, bsz=651.2, num_updates=11300, lr=6.5e-05, gnorm=5.023, clip=0, train_wall=7, gb_free=71.8, wall=1850 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:29:15]    INFO >> epoch 008:    606 / 1539 loss=4.173, wps=4564.8, ups=6.69, wpb=682.8, bsz=682.8, num_updates=11350, lr=6.5e-05, gnorm=5.923, clip=0, train_wall=7, gb_free=75.2, wall=1857 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:29:22]    INFO >> epoch 008:    656 / 1539 loss=4.057, wps=4602.3, ups=7.23, wpb=636.2, bsz=636.2, num_updates=11400, lr=6.5e-05, gnorm=5.667, clip=0, train_wall=7, gb_free=72.5, wall=1864 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:29:30]    INFO >> epoch 008:    706 / 1539 loss=4.129, wps=4769.2, ups=6.89, wpb=691.9, bsz=691.9, num_updates=11450, lr=6.5e-05, gnorm=5.816, clip=0, train_wall=7, gb_free=70, wall=1871 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:29:38]    INFO >> epoch 008:    756 / 1539 loss=3.917, wps=4981.9, ups=6.85, wpb=727, bsz=727, num_updates=11500, lr=6.5e-05, gnorm=5.893, clip=0, train_wall=7, gb_free=70.9, wall=1879 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:29:46]    INFO >> epoch 008:    806 / 1539 loss=3.889, wps=4712.5, ups=6.79, wpb=694.2, bsz=694.2, num_updates=11550, lr=6.5e-05, gnorm=6.92, clip=2, train_wall=7, gb_free=68.9, wall=1886 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:29:52]    INFO >> epoch 008:    856 / 1539 loss=4.101, wps=4393.2, ups=7.37, wpb=595.7, bsz=595.7, num_updates=11600, lr=6.5e-05, gnorm=5.013, clip=0, train_wall=6, gb_free=71.5, wall=1893 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:30:00]    INFO >> epoch 008:    906 / 1539 loss=4.041, wps=5175.6, ups=6.36, wpb=813.3, bsz=813.3, num_updates=11650, lr=6.5e-05, gnorm=6.484, clip=0, train_wall=7, gb_free=74, wall=1901 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:30:07]    INFO >> epoch 008:    956 / 1539 loss=4.032, wps=4693.6, ups=6.96, wpb=674.1, bsz=674.1, num_updates=11700, lr=6.5e-05, gnorm=5.992, clip=0, train_wall=7, gb_free=72.9, wall=1908 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:30:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 33.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.50 GiB is allocated by PyTorch, and 2.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:30:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 59        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78274 MiB |  78334 MiB | 377371 GiB | 377294 GiB |
|       from large pool |  78193 MiB |  78253 MiB | 375231 GiB | 375154 GiB |
|       from small pool |     80 MiB |     82 MiB |   2139 GiB |   2139 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78274 MiB |  78334 MiB | 377371 GiB | 377294 GiB |
|       from large pool |  78193 MiB |  78253 MiB | 375231 GiB | 375154 GiB |
|       from small pool |     80 MiB |     82 MiB |   2139 GiB |   2139 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78222 MiB |  78281 MiB | 376569 GiB | 376492 GiB |
|       from large pool |  78141 MiB |  78201 MiB | 374432 GiB | 374356 GiB |
|       from small pool |     80 MiB |     81 MiB |   2136 GiB |   2136 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80472 MiB |  80498 MiB |    830 GiB | 770116 MiB |
|       from large pool |  80388 MiB |  80388 MiB |    825 GiB | 765152 MiB |
|       from small pool |     84 MiB |    246 MiB |      4 GiB |   4964 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2137 MiB |   7144 MiB | 358016 GiB | 358014 GiB |
|       from large pool |   2134 MiB |   7133 MiB | 355596 GiB | 355594 GiB |
|       from small pool |      3 MiB |     33 MiB |   2419 GiB |   2419 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1796    |    1799    |   24927 K  |   24926 K  |
|       from large pool |     455    |     456    |   11890 K  |   11890 K  |
|       from small pool |    1341    |    1344    |   13037 K  |   13035 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1796    |    1799    |   24927 K  |   24926 K  |
|       from large pool |     455    |     456    |   11890 K  |   11890 K  |
|       from small pool |    1341    |    1344    |   13037 K  |   13035 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     241    |     562    |    5866    |    5625    |
|       from large pool |     199    |     439    |    3342    |    3143    |
|       from small pool |      42    |     123    |    2524    |    2482    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     160    |     161    |   14099 K  |   14099 K  |
|       from large pool |     113    |     113    |    7829 K  |    7829 K  |
|       from small pool |      47    |      65    |    6269 K  |    6269 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:30:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:30:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:30:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:30:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:30:17]    INFO >> epoch 008:   1007 / 1539 loss=4.095, wps=3724.3, ups=6.18, wpb=602.8, bsz=602.8, num_updates=11750, lr=6.5e-05, gnorm=5.527, clip=0, train_wall=6, gb_free=70.6, wall=1916 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:30:27]    INFO >> epoch 008:   1057 / 1539 loss=3.862, wps=4055.2, ups=4.9, wpb=828.4, bsz=828.4, num_updates=11800, lr=6.5e-05, gnorm=5.849, clip=2, train_wall=10, gb_free=71.7, wall=1926 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:30:34]    INFO >> epoch 008:   1107 / 1539 loss=4.026, wps=4668.8, ups=7.06, wpb=661.2, bsz=661.2, num_updates=11850, lr=6.5e-05, gnorm=6.027, clip=0, train_wall=7, gb_free=72.8, wall=1933 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:30:41]    INFO >> epoch 008:   1157 / 1539 loss=4.134, wps=4025.3, ups=7.22, wpb=557.5, bsz=557.5, num_updates=11900, lr=6.5e-05, gnorm=5.269, clip=0, train_wall=7, gb_free=73.2, wall=1940 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:30:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 521.25 MiB is free. Including non-PyTorch memory, this process has 78.61 GiB memory in use. Of the allocated memory 75.66 GiB is allocated by PyTorch, and 2.45 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:30:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 61        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76954 MiB |  77477 MiB | 382821 GiB | 382746 GiB |
|       from large pool |  76941 MiB |  77464 MiB | 380654 GiB | 380579 GiB |
|       from small pool |     12 MiB |     19 MiB |   2167 GiB |   2167 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76954 MiB |  77477 MiB | 382821 GiB | 382746 GiB |
|       from large pool |  76941 MiB |  77464 MiB | 380654 GiB | 380579 GiB |
|       from small pool |     12 MiB |     19 MiB |   2167 GiB |   2167 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76936 MiB |  77458 MiB | 382008 GiB | 381933 GiB |
|       from large pool |  76923 MiB |  77445 MiB | 379844 GiB | 379769 GiB |
|       from small pool |     12 MiB |     19 MiB |   2164 GiB |   2164 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79984 MiB |  80024 MiB |    903 GiB |    825 GiB |
|       from large pool |  79958 MiB |  79958 MiB |    898 GiB |    820 GiB |
|       from small pool |     26 MiB |     66 MiB |      4 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3029 MiB |   8676 MiB | 363956 GiB | 363953 GiB |
|       from large pool |   3016 MiB |   8662 MiB | 361504 GiB | 361502 GiB |
|       from small pool |     13 MiB |     27 MiB |   2451 GiB |   2451 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     613    |   25272 K  |   25272 K  |
|       from large pool |     315    |     322    |   12070 K  |   12070 K  |
|       from small pool |     291    |     348    |   13202 K  |   13201 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     613    |   25272 K  |   25272 K  |
|       from large pool |     315    |     322    |   12070 K  |   12070 K  |
|       from small pool |     291    |     348    |   13202 K  |   13201 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      91    |     111    |    5928    |    5837    |
|       from large pool |      78    |      78    |    3385    |    3307    |
|       from small pool |      13    |      33    |    2543    |    2530    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      92    |      92    |   14285 K  |   14285 K  |
|       from large pool |      67    |      67    |    7943 K  |    7943 K  |
|       from small pool |      25    |      56    |    6342 K  |    6342 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:30:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:30:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:30:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:30:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:30:51]    INFO >> epoch 008:   1208 / 1539 loss=4.04, wps=4329.9, ups=6.06, wpb=715, bsz=715, num_updates=11950, lr=6.5e-05, gnorm=5.56, clip=0, train_wall=7, gb_free=76, wall=1949 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:30:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 47.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.61 GiB is allocated by PyTorch, and 983.67 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:30:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 62        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79414 MiB |  79474 MiB | 385104 GiB | 385026 GiB |
|       from large pool |  79015 MiB |  79075 MiB | 382922 GiB | 382845 GiB |
|       from small pool |    398 MiB |    399 MiB |   2181 GiB |   2181 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79414 MiB |  79474 MiB | 385104 GiB | 385026 GiB |
|       from large pool |  79015 MiB |  79075 MiB | 382922 GiB | 382845 GiB |
|       from small pool |    398 MiB |    399 MiB |   2181 GiB |   2181 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79398 MiB |  79458 MiB | 384287 GiB | 384209 GiB |
|       from large pool |  79002 MiB |  79061 MiB | 382108 GiB | 382031 GiB |
|       from small pool |    396 MiB |    397 MiB |   2178 GiB |   2178 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80458 MiB |  80458 MiB |    903 GiB |    825 GiB |
|       from large pool |  80018 MiB |  80018 MiB |    898 GiB |    820 GiB |
|       from small pool |    440 MiB |    440 MiB |      5 GiB |      4 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    983 MiB |   5675 MiB | 366593 GiB | 366592 GiB |
|       from large pool |    942 MiB |   5668 MiB | 364125 GiB | 364124 GiB |
|       from small pool |     41 MiB |     42 MiB |   2467 GiB |   2467 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7351    |    7354    |   25429 K  |   25421 K  |
|       from large pool |     938    |     939    |   12139 K  |   12138 K  |
|       from small pool |    6413    |    6416    |   13290 K  |   13283 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7351    |    7354    |   25429 K  |   25421 K  |
|       from large pool |     938    |     939    |   12139 K  |   12138 K  |
|       from small pool |    6413    |    6416    |   13290 K  |   13283 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     299    |     299    |    6136    |    5837    |
|       from large pool |      79    |      79    |    3386    |    3307    |
|       from small pool |     220    |     220    |    2750    |    2530    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     473    |     473    |   14372 K  |   14372 K  |
|       from large pool |      80    |      80    |    7985 K  |    7985 K  |
|       from small pool |     393    |     393    |    6387 K  |    6386 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:30:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:30:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:30:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:30:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:30:59]    INFO >> epoch 008:   1259 / 1539 loss=4.088, wps=5049.3, ups=5.94, wpb=850.7, bsz=850.7, num_updates=12000, lr=6.5e-05, gnorm=6.191, clip=0, train_wall=8, gb_free=73.2, wall=1957 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:31:06]    INFO >> epoch 008:   1309 / 1539 loss=4.035, wps=4744.5, ups=6.74, wpb=704.3, bsz=704.3, num_updates=12050, lr=6.5e-05, gnorm=5.893, clip=0, train_wall=7, gb_free=72.7, wall=1964 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:31:14]    INFO >> epoch 008:   1359 / 1539 loss=4.029, wps=4492.8, ups=6.76, wpb=664.7, bsz=664.7, num_updates=12100, lr=6.5e-05, gnorm=5.455, clip=0, train_wall=7, gb_free=69.7, wall=1972 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:31:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.20 GiB is free. Including non-PyTorch memory, this process has 77.92 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.49 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:31:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 33           |        cudaMalloc retries: 64        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76324 MiB |  78599 MiB | 388761 GiB | 388687 GiB |
|       from large pool |  76305 MiB |  78581 MiB | 386561 GiB | 386487 GiB |
|       from small pool |     18 MiB |     19 MiB |   2200 GiB |   2200 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76324 MiB |  78599 MiB | 388761 GiB | 388687 GiB |
|       from large pool |  76305 MiB |  78581 MiB | 386561 GiB | 386487 GiB |
|       from small pool |     18 MiB |     19 MiB |   2200 GiB |   2200 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 387938 GiB | 387863 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 385741 GiB | 385666 GiB |
|       from small pool |     18 MiB |     19 MiB |   2197 GiB |   2197 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79276 MiB |  80398 MiB |    974 GiB |    897 GiB |
|       from large pool |  79252 MiB |  79958 MiB |    969 GiB |    892 GiB |
|       from small pool |     24 MiB |    440 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2951 MiB |   4272 MiB | 370710 GiB | 370707 GiB |
|       from large pool |   2946 MiB |   4266 MiB | 368220 GiB | 368217 GiB |
|       from small pool |      5 MiB |     27 MiB |   2489 GiB |   2489 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     637    |     643    |   25655 K  |   25655 K  |
|       from large pool |     340    |     346    |   12256 K  |   12256 K  |
|       from small pool |     297    |     348    |   13399 K  |   13398 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     637    |     643    |   25655 K  |   25655 K  |
|       from large pool |     340    |     346    |   12256 K  |   12256 K  |
|       from small pool |     297    |     348    |   13399 K  |   13398 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     106    |     298    |    6194    |    6088    |
|       from large pool |      94    |      95    |    3444    |    3350    |
|       from small pool |      12    |     220    |    2750    |    2738    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     112    |     114    |   14496 K  |   14496 K  |
|       from large pool |      89    |      91    |    8059 K  |    8059 K  |
|       from small pool |      23    |      59    |    6437 K  |    6437 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:31:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:31:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:31:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:31:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:31:25]    INFO >> epoch 008:   1410 / 1539 loss=4.022, wps=4125, ups=5, wpb=824.4, bsz=824.4, num_updates=12150, lr=6.5e-05, gnorm=6.467, clip=2, train_wall=7, gb_free=73, wall=1982 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:31:33]    INFO >> epoch 008:   1460 / 1539 loss=4.178, wps=4323.7, ups=6.05, wpb=715.2, bsz=715.2, num_updates=12200, lr=6.5e-05, gnorm=5.515, clip=0, train_wall=8, gb_free=75, wall=1990 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:31:41]    INFO >> epoch 008:   1510 / 1539 loss=4.053, wps=4483.3, ups=6.73, wpb=666.4, bsz=666.4, num_updates=12250, lr=6.5e-05, gnorm=5.823, clip=0, train_wall=7, gb_free=67.3, wall=1997 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:31:45]    INFO >> epoch 008 | loss 4.033 | wps 4398.1 | ups 6.17 | wpb 712.7 | bsz 712.7 | num_updates 12279 | lr 6.5e-05 | gnorm 6.002 | clip 0.3 | train_wall 218 | gb_free 74.8 | wall 2002 (progress_bar.py:267, print())[0m
[33m[2025-11-19 04:31:45] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:32:00]    INFO >> epoch 008 | valid on 'valid' subset | loss 4.037 | wps 11640.1 | wpb 5412.5 | bsz 5412.5 | num_updates 12279 | best_loss 5.983 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 04:32:00]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 04:32:00]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/checkpoints/checkpoint_last.pt (epoch 8 @ 12279 updates, score 4.037) (writing took 0.012771 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 04:32:00] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:32:03]    INFO >> epoch 009:     21 / 1539 loss=3.968, wps=1634.7, ups=2.44, wpb=668.9, bsz=668.9, num_updates=12300, lr=5.7e-05, gnorm=6.329, clip=0, train_wall=7, gb_free=73.1, wall=2018 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:32:10]    INFO >> epoch 009:     71 / 1539 loss=4.058, wps=4513.4, ups=6.8, wpb=663.6, bsz=663.6, num_updates=12350, lr=5.7e-05, gnorm=5.448, clip=0, train_wall=7, gb_free=73.5, wall=2025 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:32:17]    INFO >> epoch 009:    121 / 1539 loss=3.906, wps=4790.3, ups=6.96, wpb=688.4, bsz=688.4, num_updates=12400, lr=5.7e-05, gnorm=6.672, clip=0, train_wall=7, gb_free=73.7, wall=2032 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:32:26]    INFO >> epoch 009:    171 / 1539 loss=3.914, wps=5348, ups=6.35, wpb=842.4, bsz=842.4, num_updates=12450, lr=5.7e-05, gnorm=7.28, clip=2, train_wall=7, gb_free=71.8, wall=2040 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:32:33]    INFO >> epoch 009:    221 / 1539 loss=4.099, wps=4945.6, ups=7.15, wpb=691.5, bsz=691.5, num_updates=12500, lr=5.7e-05, gnorm=5.667, clip=0, train_wall=7, gb_free=71.5, wall=2047 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:32:40]    INFO >> epoch 009:    271 / 1539 loss=4.068, wps=5000.9, ups=7.11, wpb=703.6, bsz=703.6, num_updates=12550, lr=5.7e-05, gnorm=5.702, clip=0, train_wall=7, gb_free=70, wall=2054 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:32:48]    INFO >> epoch 009:    321 / 1539 loss=4.097, wps=5168, ups=6.59, wpb=784.5, bsz=784.5, num_updates=12600, lr=5.7e-05, gnorm=6.056, clip=0, train_wall=7, gb_free=66.3, wall=2062 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:32:56]    INFO >> epoch 009:    371 / 1539 loss=4.028, wps=4191.8, ups=5.98, wpb=701, bsz=701, num_updates=12650, lr=5.7e-05, gnorm=6.269, clip=0, train_wall=8, gb_free=65.8, wall=2070 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:32:57] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.86 GiB. GPU 0 has a total capacity of 79.14 GiB of which 213.25 MiB is free. Including non-PyTorch memory, this process has 78.91 GiB memory in use. Of the allocated memory 73.80 GiB is allocated by PyTorch, and 4.61 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:32:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 34           |        cudaMalloc retries: 66        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75049 MiB |  75992 MiB | 409585 GiB | 409512 GiB |
|       from large pool |  75037 MiB |  75979 MiB | 407262 GiB | 407188 GiB |
|       from small pool |     12 MiB |     16 MiB |   2323 GiB |   2323 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75049 MiB |  75992 MiB | 409585 GiB | 409512 GiB |
|       from large pool |  75037 MiB |  75979 MiB | 407262 GiB | 407188 GiB |
|       from small pool |     12 MiB |     16 MiB |   2323 GiB |   2323 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75035 MiB |  75977 MiB | 408722 GiB | 408649 GiB |
|       from large pool |  75022 MiB |  75964 MiB | 406402 GiB | 406329 GiB |
|       from small pool |     12 MiB |     16 MiB |   2319 GiB |   2319 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80292 MiB |  80292 MiB |    979 GiB |    901 GiB |
|       from large pool |  80266 MiB |  80266 MiB |    974 GiB |    895 GiB |
|       from small pool |     26 MiB |    124 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5242 MiB |   8856 MiB | 391468 GiB | 391463 GiB |
|       from large pool |   5228 MiB |   8843 MiB | 388844 GiB | 388838 GiB |
|       from small pool |     13 MiB |     21 MiB |   2624 GiB |   2624 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   26995 K  |   26995 K  |
|       from large pool |     314    |     322    |   12839 K  |   12839 K  |
|       from small pool |     291    |     341    |   14155 K  |   14155 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   26995 K  |   26995 K  |
|       from large pool |     314    |     322    |   12839 K  |   12839 K  |
|       from small pool |     291    |     341    |   14155 K  |   14155 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     103    |     156    |    6251    |    6148    |
|       from large pool |      90    |      94    |    3451    |    3361    |
|       from small pool |      13    |      62    |    2800    |    2787    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      98    |      98    |   15252 K  |   15252 K  |
|       from large pool |      72    |      72    |    8430 K  |    8430 K  |
|       from small pool |      26    |      43    |    6822 K  |    6821 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:32:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:32:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:32:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:32:57] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:33:06]    INFO >> epoch 009:    422 / 1539 loss=4.011, wps=4362.6, ups=6.09, wpb=716.4, bsz=716.4, num_updates=12700, lr=5.7e-05, gnorm=5.61, clip=0, train_wall=7, gb_free=73.1, wall=2079 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:33:13]    INFO >> epoch 009:    472 / 1539 loss=3.97, wps=4657.3, ups=6.74, wpb=691.3, bsz=691.3, num_updates=12750, lr=5.7e-05, gnorm=5.294, clip=0, train_wall=7, gb_free=74, wall=2086 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:33:21]    INFO >> epoch 009:    522 / 1539 loss=3.995, wps=5003.8, ups=6.58, wpb=760.8, bsz=760.8, num_updates=12800, lr=5.7e-05, gnorm=6, clip=0, train_wall=7, gb_free=72.7, wall=2094 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:33:29]    INFO >> epoch 009:    572 / 1539 loss=3.989, wps=4793.7, ups=6.38, wpb=750.9, bsz=750.9, num_updates=12850, lr=5.7e-05, gnorm=6.76, clip=2, train_wall=7, gb_free=74.2, wall=2101 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:33:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 25.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.35 GiB is allocated by PyTorch, and 1.24 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:33:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 35           |        cudaMalloc retries: 67        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79147 MiB |  79207 MiB | 415679 GiB | 415602 GiB |
|       from large pool |  79057 MiB |  79117 MiB | 413324 GiB | 413247 GiB |
|       from small pool |     89 MiB |     91 MiB |   2355 GiB |   2354 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79147 MiB |  79207 MiB | 415679 GiB | 415602 GiB |
|       from large pool |  79057 MiB |  79117 MiB | 413324 GiB | 413247 GiB |
|       from small pool |     89 MiB |     91 MiB |   2355 GiB |   2354 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79123 MiB |  79183 MiB | 414803 GiB | 414726 GiB |
|       from large pool |  79034 MiB |  79093 MiB | 412452 GiB | 412375 GiB |
|       from small pool |     89 MiB |     90 MiB |   2351 GiB |   2351 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80480 MiB |  80482 MiB |    979 GiB |    901 GiB |
|       from large pool |  80386 MiB |  80386 MiB |    974 GiB |    895 GiB |
|       from small pool |     94 MiB |     96 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1272 MiB |   7349 MiB | 398206 GiB | 398205 GiB |
|       from large pool |   1268 MiB |   7339 MiB | 395545 GiB | 395544 GiB |
|       from small pool |      4 MiB |     23 MiB |   2660 GiB |   2660 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1961    |    1964    |   27385 K  |   27383 K  |
|       from large pool |     470    |     471    |   13039 K  |   13038 K  |
|       from small pool |    1491    |    1494    |   14346 K  |   14345 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1961    |    1964    |   27385 K  |   27383 K  |
|       from large pool |     470    |     471    |   13039 K  |   13038 K  |
|       from small pool |    1491    |    1494    |   14346 K  |   14345 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     139    |     140    |    6288    |    6149    |
|       from large pool |      92    |      92    |    3453    |    3361    |
|       from small pool |      47    |      48    |    2835    |    2788    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     130    |     131    |   15463 K  |   15463 K  |
|       from large pool |      78    |      79    |    8556 K  |    8556 K  |
|       from small pool |      52    |      54    |    6906 K  |    6906 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:33:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:33:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:33:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:33:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:33:38]    INFO >> epoch 009:    623 / 1539 loss=4.014, wps=4465.4, ups=6.29, wpb=709.4, bsz=709.4, num_updates=12900, lr=5.7e-05, gnorm=5.618, clip=0, train_wall=7, gb_free=65.9, wall=2109 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:33:45]    INFO >> epoch 009:    673 / 1539 loss=3.949, wps=4513.2, ups=6.85, wpb=658.7, bsz=658.7, num_updates=12950, lr=5.7e-05, gnorm=6.748, clip=0, train_wall=7, gb_free=71.5, wall=2117 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:33:53]    INFO >> epoch 009:    723 / 1539 loss=4.013, wps=4568.2, ups=6.81, wpb=671.1, bsz=671.1, num_updates=13000, lr=5.7e-05, gnorm=6.061, clip=0, train_wall=7, gb_free=71.2, wall=2124 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:34:00]    INFO >> epoch 009:    773 / 1539 loss=4.117, wps=4558.1, ups=6.57, wpb=693.9, bsz=693.9, num_updates=13050, lr=5.7e-05, gnorm=5.927, clip=2, train_wall=7, gb_free=57.9, wall=2132 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:34:09] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 151.25 MiB is free. Including non-PyTorch memory, this process has 78.97 GiB memory in use. Of the allocated memory 77.59 GiB is allocated by PyTorch, and 897.78 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:34:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 36           |        cudaMalloc retries: 68        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79058 MiB |  79456 MiB | 422841 GiB | 422763 GiB |
|       from large pool |  79039 MiB |  79437 MiB | 420449 GiB | 420372 GiB |
|       from small pool |     18 MiB |     19 MiB |   2391 GiB |   2391 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79058 MiB |  79456 MiB | 422841 GiB | 422763 GiB |
|       from large pool |  79039 MiB |  79437 MiB | 420449 GiB | 420372 GiB |
|       from small pool |     18 MiB |     19 MiB |   2391 GiB |   2391 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79031 MiB |  79429 MiB | 421950 GiB | 421873 GiB |
|       from large pool |  79012 MiB |  79410 MiB | 419562 GiB | 419485 GiB |
|       from small pool |     18 MiB |     19 MiB |   2388 GiB |   2388 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80354 MiB |  80420 MiB |    979 GiB |    901 GiB |
|       from large pool |  80326 MiB |  80326 MiB |    974 GiB |    895 GiB |
|       from small pool |     28 MiB |     94 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1295 MiB |   5531 MiB | 406084 GiB | 406083 GiB |
|       from large pool |   1286 MiB |   5521 MiB | 403382 GiB | 403380 GiB |
|       from small pool |      9 MiB |     25 MiB |   2702 GiB |   2702 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     647    |   27845 K  |   27844 K  |
|       from large pool |     341    |     347    |   13278 K  |   13278 K  |
|       from small pool |     300    |     348    |   14567 K  |   14566 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     647    |   27845 K  |   27844 K  |
|       from large pool |     341    |     347    |   13278 K  |   13278 K  |
|       from small pool |     300    |     348    |   14567 K  |   14566 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     105    |     138    |    6288    |    6183    |
|       from large pool |      91    |      91    |    3453    |    3362    |
|       from small pool |      14    |      47    |    2835    |    2821    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      97    |     103    |   15710 K  |   15710 K  |
|       from large pool |      70    |      76    |    8707 K  |    8707 K  |
|       from small pool |      27    |      49    |    7003 K  |    7003 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:34:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:34:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:34:09] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:34:09] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:34:10]    INFO >> epoch 009:    824 / 1539 loss=3.937, wps=4504.6, ups=6.26, wpb=719.6, bsz=719.6, num_updates=13100, lr=5.7e-05, gnorm=6.291, clip=0, train_wall=7, gb_free=70.5, wall=2140 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:34:17]    INFO >> epoch 009:    874 / 1539 loss=4.031, wps=4715.6, ups=7.32, wpb=644.4, bsz=644.4, num_updates=13150, lr=5.7e-05, gnorm=5.388, clip=0, train_wall=6, gb_free=74.8, wall=2146 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:34:23]    INFO >> epoch 009:    924 / 1539 loss=4.1, wps=4474, ups=7.23, wpb=618.7, bsz=618.7, num_updates=13200, lr=5.7e-05, gnorm=4.904, clip=0, train_wall=7, gb_free=73.5, wall=2153 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:34:33]    INFO >> epoch 009:    974 / 1539 loss=3.801, wps=4702.5, ups=5.47, wpb=859.7, bsz=859.7, num_updates=13250, lr=5.7e-05, gnorm=6.509, clip=0, train_wall=9, gb_free=70.8, wall=2162 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:34:41]    INFO >> epoch 009:   1024 / 1539 loss=3.961, wps=4819, ups=6.7, wpb=718.9, bsz=718.9, num_updates=13300, lr=5.7e-05, gnorm=5.652, clip=0, train_wall=7, gb_free=69.9, wall=2170 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:34:49]    INFO >> epoch 009:   1074 / 1539 loss=3.815, wps=4808.7, ups=6.63, wpb=725.8, bsz=725.8, num_updates=13350, lr=5.7e-05, gnorm=6.575, clip=2, train_wall=7, gb_free=65.3, wall=2177 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:34:57]    INFO >> epoch 009:   1124 / 1539 loss=3.937, wps=5001.5, ups=6.58, wpb=760.3, bsz=760.3, num_updates=13400, lr=5.7e-05, gnorm=7.336, clip=4, train_wall=7, gb_free=73.5, wall=2185 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:35:04]    INFO >> epoch 009:   1174 / 1539 loss=4.059, wps=4803, ups=7.17, wpb=669.6, bsz=669.6, num_updates=13450, lr=5.7e-05, gnorm=5.344, clip=0, train_wall=7, gb_free=72.7, wall=2192 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:35:12]    INFO >> epoch 009:   1224 / 1539 loss=3.588, wps=5214.3, ups=6.05, wpb=862, bsz=862, num_updates=13500, lr=5.7e-05, gnorm=6.06, clip=2, train_wall=8, gb_free=69, wall=2200 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:35:21]    INFO >> epoch 009:   1274 / 1539 loss=3.963, wps=5064.8, ups=6.52, wpb=776.3, bsz=776.3, num_updates=13550, lr=5.7e-05, gnorm=6.067, clip=0, train_wall=7, gb_free=72.6, wall=2208 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:35:29]    INFO >> epoch 009:   1324 / 1539 loss=4.046, wps=4627.7, ups=6.49, wpb=712.6, bsz=712.6, num_updates=13600, lr=5.7e-05, gnorm=5.864, clip=0, train_wall=7, gb_free=75.1, wall=2216 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:35:36]    INFO >> epoch 009:   1374 / 1539 loss=3.848, wps=4836.4, ups=6.45, wpb=749.6, bsz=749.6, num_updates=13650, lr=5.7e-05, gnorm=5.588, clip=0, train_wall=7, gb_free=69.9, wall=2223 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:35:42] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 37.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.24 GiB is allocated by PyTorch, and 2.34 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:35:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 37           |        cudaMalloc retries: 70        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78007 MiB |  78067 MiB | 440587 GiB | 440511 GiB |
|       from large pool |  77625 MiB |  77685 MiB | 438093 GiB | 438017 GiB |
|       from small pool |    382 MiB |    383 MiB |   2494 GiB |   2493 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78007 MiB |  78067 MiB | 440587 GiB | 440511 GiB |
|       from large pool |  77625 MiB |  77685 MiB | 438093 GiB | 438017 GiB |
|       from small pool |    382 MiB |    383 MiB |   2494 GiB |   2493 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77777 MiB |  77836 MiB | 439658 GiB | 439582 GiB |
|       from large pool |  77397 MiB |  77456 MiB | 437167 GiB | 437092 GiB |
|       from small pool |    380 MiB |    381 MiB |   2490 GiB |   2490 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80468 MiB |  80468 MiB |   1006 GiB |    927 GiB |
|       from large pool |  80046 MiB |  80046 MiB |   1000 GiB |    922 GiB |
|       from small pool |    422 MiB |    422 MiB |      5 GiB |      5 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2400 MiB |   7496 MiB | 423378 GiB | 423376 GiB |
|       from large pool |   2360 MiB |   7489 MiB | 420558 GiB | 420555 GiB |
|       from small pool |     39 MiB |     40 MiB |   2820 GiB |   2820 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7054    |    7057    |   29063 K  |   29056 K  |
|       from large pool |     911    |     912    |   13873 K  |   13872 K  |
|       from small pool |    6143    |    6146    |   15190 K  |   15184 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7054    |    7057    |   29063 K  |   29056 K  |
|       from large pool |     911    |     912    |   13873 K  |   13872 K  |
|       from small pool |    6143    |    6146    |   15190 K  |   15184 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     742    |     742    |    6935    |    6193    |
|       from large pool |     531    |     531    |    3903    |    3372    |
|       from small pool |     211    |     211    |    3032    |    2821    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     503    |     503    |   16394 K  |   16393 K  |
|       from large pool |     127    |     127    |    9094 K  |    9094 K  |
|       from small pool |     376    |     376    |    7300 K  |    7299 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:35:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:35:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:35:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:35:42] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:35:44]    INFO >> epoch 009:   1425 / 1539 loss=3.903, wps=4449.8, ups=6.58, wpb=676.7, bsz=676.7, num_updates=13700, lr=5.7e-05, gnorm=5.836, clip=0, train_wall=7, gb_free=72, wall=2231 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:35:52]    INFO >> epoch 009:   1475 / 1539 loss=4.012, wps=4361.6, ups=6.88, wpb=634.1, bsz=634.1, num_updates=13750, lr=5.7e-05, gnorm=6.322, clip=0, train_wall=7, gb_free=72.9, wall=2238 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:36:00]    INFO >> epoch 009:   1525 / 1539 loss=3.99, wps=4350.5, ups=6.98, wpb=623.3, bsz=623.3, num_updates=13800, lr=5.7e-05, gnorm=5.913, clip=0, train_wall=7, gb_free=67.9, wall=2245 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:36:02]    INFO >> epoch 009 | loss 3.972 | wps 4450.7 | ups 6.25 | wpb 712.7 | bsz 712.7 | num_updates 13814 | lr 5.7e-05 | gnorm 6.021 | clip 0.5 | train_wall 217 | gb_free 74.2 | wall 2248 (progress_bar.py:267, print())[0m
[33m[2025-11-19 04:36:02] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:36:14]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.982 | wps 11884.7 | wpb 5412.5 | bsz 5412.5 | num_updates 13814 | best_loss 5.983 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 04:36:15]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 04:36:15]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/checkpoints/checkpoint_last.pt (epoch 9 @ 13814 updates, score 3.982) (writing took 0.012619 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 04:36:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:36:23]    INFO >> epoch 010:     36 / 1539 loss=3.965, wps=1603.9, ups=2.29, wpb=701.4, bsz=701.4, num_updates=13850, lr=4.8e-05, gnorm=5.561, clip=0, train_wall=8, gb_free=72.8, wall=2267 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:36:30]    INFO >> epoch 010:     86 / 1539 loss=3.992, wps=4938.8, ups=6.89, wpb=716.9, bsz=716.9, num_updates=13900, lr=4.8e-05, gnorm=6.439, clip=0, train_wall=7, gb_free=69.6, wall=2275 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:36:37]    INFO >> epoch 010:    136 / 1539 loss=3.914, wps=4354.5, ups=6.95, wpb=626.5, bsz=626.5, num_updates=13950, lr=4.8e-05, gnorm=5.506, clip=0, train_wall=7, gb_free=74.1, wall=2282 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:36:45]    INFO >> epoch 010:    186 / 1539 loss=3.922, wps=4614.5, ups=6.2, wpb=744.2, bsz=744.2, num_updates=14000, lr=4.8e-05, gnorm=6.214, clip=0, train_wall=8, gb_free=74.2, wall=2290 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:36:55]    INFO >> epoch 010:    236 / 1539 loss=3.818, wps=5189.5, ups=6.01, wpb=863.9, bsz=863.9, num_updates=14050, lr=4.8e-05, gnorm=6.548, clip=2, train_wall=8, gb_free=74.4, wall=2298 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:37:02]    INFO >> epoch 010:    286 / 1539 loss=3.959, wps=4551.7, ups=6.73, wpb=676, bsz=676, num_updates=14100, lr=4.8e-05, gnorm=6.191, clip=0, train_wall=7, gb_free=73.9, wall=2306 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:37:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 953.25 MiB is free. Including non-PyTorch memory, this process has 78.19 GiB memory in use. Of the allocated memory 74.10 GiB is allocated by PyTorch, and 3.59 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:37:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 38           |        cudaMalloc retries: 72        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72064 MiB |  75873 MiB | 458286 GiB | 458216 GiB |
|       from large pool |  72046 MiB |  75855 MiB | 455686 GiB | 455615 GiB |
|       from small pool |     17 MiB |     18 MiB |   2600 GiB |   2600 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72064 MiB |  75873 MiB | 458286 GiB | 458216 GiB |
|       from large pool |  72046 MiB |  75855 MiB | 455686 GiB | 455615 GiB |
|       from small pool |     17 MiB |     18 MiB |   2600 GiB |   2600 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72044 MiB |  75852 MiB | 457323 GiB | 457252 GiB |
|       from large pool |  72027 MiB |  75835 MiB | 454726 GiB | 454656 GiB |
|       from small pool |     17 MiB |     18 MiB |   2596 GiB |   2596 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79552 MiB |  79754 MiB |   1029 GiB |    952 GiB |
|       from large pool |  79528 MiB |  79528 MiB |   1023 GiB |    946 GiB |
|       from small pool |     24 MiB |    226 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4759 MiB |   7038 MiB | 438558 GiB | 438553 GiB |
|       from large pool |   4753 MiB |   7031 MiB | 435622 GiB | 435617 GiB |
|       from small pool |      6 MiB |     25 MiB |   2936 GiB |   2936 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     606    |     615    |   30202 K  |   30201 K  |
|       from large pool |     308    |     317    |   14352 K  |   14352 K  |
|       from small pool |     298    |     342    |   15849 K  |   15849 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     606    |     615    |   30202 K  |   30201 K  |
|       from large pool |     308    |     317    |   14352 K  |   14352 K  |
|       from small pool |     298    |     342    |   15849 K  |   15849 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     149    |     250    |    7045    |    6896    |
|       from large pool |     137    |     137    |    3912    |    3775    |
|       from small pool |      12    |     113    |    3133    |    3121    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     141    |     142    |   17052 K  |   17052 K  |
|       from large pool |     113    |     114    |    9412 K  |    9412 K  |
|       from small pool |      28    |      52    |    7640 K  |    7640 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:37:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:37:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:37:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:37:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:37:10]    INFO >> epoch 010:    337 / 1539 loss=3.977, wps=4580.9, ups=6.51, wpb=703.9, bsz=703.9, num_updates=14150, lr=4.8e-05, gnorm=6.003, clip=0, train_wall=7, gb_free=67.2, wall=2313 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:37:18]    INFO >> epoch 010:    387 / 1539 loss=3.989, wps=4688.3, ups=6.48, wpb=723.5, bsz=723.5, num_updates=14200, lr=4.8e-05, gnorm=5.43, clip=0, train_wall=7, gb_free=73.3, wall=2321 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:37:25]    INFO >> epoch 010:    437 / 1539 loss=3.853, wps=4313.8, ups=6.83, wpb=631.4, bsz=631.4, num_updates=14250, lr=4.8e-05, gnorm=5.844, clip=0, train_wall=7, gb_free=72.6, wall=2328 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:37:34]    INFO >> epoch 010:    487 / 1539 loss=4, wps=4294, ups=6.42, wpb=668.5, bsz=668.5, num_updates=14300, lr=4.8e-05, gnorm=5.61, clip=0, train_wall=7, gb_free=72.5, wall=2336 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:37:41]    INFO >> epoch 010:    537 / 1539 loss=3.811, wps=4824.9, ups=7.47, wpb=645.7, bsz=645.7, num_updates=14350, lr=4.8e-05, gnorm=5.465, clip=0, train_wall=6, gb_free=75, wall=2343 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:37:49]    INFO >> epoch 010:    587 / 1539 loss=3.976, wps=4397.7, ups=6.61, wpb=665.2, bsz=665.2, num_updates=14400, lr=4.8e-05, gnorm=6.119, clip=0, train_wall=7, gb_free=71.4, wall=2350 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:37:56]    INFO >> epoch 010:    637 / 1539 loss=3.945, wps=5021.9, ups=6.98, wpb=719.7, bsz=719.7, num_updates=14450, lr=4.8e-05, gnorm=6.643, clip=0, train_wall=7, gb_free=69.1, wall=2358 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:38:04]    INFO >> epoch 010:    687 / 1539 loss=3.908, wps=5022.5, ups=6.91, wpb=726.6, bsz=726.6, num_updates=14500, lr=4.8e-05, gnorm=5.641, clip=0, train_wall=7, gb_free=73.3, wall=2365 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:38:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 41.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.29 GiB is allocated by PyTorch, and 2.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:38:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 39           |        cudaMalloc retries: 73        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78063 MiB |  78123 MiB | 469353 GiB | 469277 GiB |
|       from large pool |  77678 MiB |  77738 MiB | 466692 GiB | 466616 GiB |
|       from small pool |    384 MiB |    385 MiB |   2661 GiB |   2660 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78063 MiB |  78123 MiB | 469353 GiB | 469277 GiB |
|       from large pool |  77678 MiB |  77738 MiB | 466692 GiB | 466616 GiB |
|       from small pool |    384 MiB |    385 MiB |   2661 GiB |   2660 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78017 MiB |  78077 MiB | 468366 GiB | 468290 GiB |
|       from large pool |  77635 MiB |  77694 MiB | 465709 GiB | 465633 GiB |
|       from small pool |    382 MiB |    383 MiB |   2657 GiB |   2656 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80464 MiB |  80464 MiB |   1033 GiB |    954 GiB |
|       from large pool |  80040 MiB |  80040 MiB |   1027 GiB |    948 GiB |
|       from small pool |    424 MiB |    424 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2340 MiB |   6606 MiB | 449455 GiB | 449453 GiB |
|       from large pool |   2301 MiB |   6599 MiB | 446449 GiB | 446446 GiB |
|       from small pool |     39 MiB |     40 MiB |   3006 GiB |   3006 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7098    |    7101    |   30947 K  |   30940 K  |
|       from large pool |     915    |     916    |   14731 K  |   14730 K  |
|       from small pool |    6183    |    6186    |   16216 K  |   16209 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7098    |    7101    |   30947 K  |   30940 K  |
|       from large pool |     915    |     916    |   14731 K  |   14730 K  |
|       from small pool |    6183    |    6186    |   16216 K  |   16209 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     402    |     402    |    7299    |    6897    |
|       from large pool |     190    |     190    |    3966    |    3776    |
|       from small pool |     212    |     212    |    3333    |    3121    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     495    |     496    |   17467 K  |   17466 K  |
|       from large pool |     118    |     118    |    9660 K  |    9660 K  |
|       from small pool |     377    |     378    |    7806 K  |    7805 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:38:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:38:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:38:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:38:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:38:12]    INFO >> epoch 010:    738 / 1539 loss=4.006, wps=4249.3, ups=6.61, wpb=642.7, bsz=642.7, num_updates=14550, lr=4.8e-05, gnorm=5.659, clip=0, train_wall=7, gb_free=68.7, wall=2372 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:38:20]    INFO >> epoch 010:    788 / 1539 loss=3.956, wps=4705.4, ups=6.51, wpb=723.3, bsz=723.3, num_updates=14600, lr=4.8e-05, gnorm=5.886, clip=0, train_wall=7, gb_free=67.8, wall=2380 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:38:27]    INFO >> epoch 010:    838 / 1539 loss=3.923, wps=4552.3, ups=6.75, wpb=673.9, bsz=673.9, num_updates=14650, lr=4.8e-05, gnorm=6.016, clip=2, train_wall=7, gb_free=66.8, wall=2387 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:38:36]    INFO >> epoch 010:    888 / 1539 loss=3.854, wps=5234.2, ups=6.17, wpb=848.7, bsz=848.7, num_updates=14700, lr=4.8e-05, gnorm=6.294, clip=0, train_wall=8, gb_free=76.2, wall=2396 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:38:44]    INFO >> epoch 010:    938 / 1539 loss=3.952, wps=4901, ups=6.87, wpb=713.5, bsz=713.5, num_updates=14750, lr=4.8e-05, gnorm=6.034, clip=2, train_wall=7, gb_free=74.7, wall=2403 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:38:51]    INFO >> epoch 010:    988 / 1539 loss=3.949, wps=4904.7, ups=7.1, wpb=690.7, bsz=690.7, num_updates=14800, lr=4.8e-05, gnorm=6.637, clip=0, train_wall=7, gb_free=74.4, wall=2410 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:38:58]    INFO >> epoch 010:   1038 / 1539 loss=4.006, wps=4678.8, ups=6.97, wpb=671.7, bsz=671.7, num_updates=14850, lr=4.8e-05, gnorm=6.073, clip=0, train_wall=7, gb_free=70, wall=2417 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:39:06]    INFO >> epoch 010:   1088 / 1539 loss=3.92, wps=5263.5, ups=6.55, wpb=803.8, bsz=803.8, num_updates=14900, lr=4.8e-05, gnorm=6.249, clip=0, train_wall=7, gb_free=72.8, wall=2425 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:39:14]    INFO >> epoch 010:   1138 / 1539 loss=3.949, wps=4245.2, ups=6.93, wpb=612.7, bsz=612.7, num_updates=14950, lr=4.8e-05, gnorm=5.264, clip=0, train_wall=7, gb_free=71.8, wall=2432 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:39:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 17.25 MiB is free. Including non-PyTorch memory, this process has 79.10 GiB memory in use. Of the allocated memory 76.47 GiB is allocated by PyTorch, and 2.13 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:39:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 40           |        cudaMalloc retries: 75        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78250 MiB |  78310 MiB | 483183 GiB | 483107 GiB |
|       from large pool |  78169 MiB |  78229 MiB | 480446 GiB | 480370 GiB |
|       from small pool |     80 MiB |     82 MiB |   2736 GiB |   2736 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78250 MiB |  78310 MiB | 483183 GiB | 483107 GiB |
|       from large pool |  78169 MiB |  78229 MiB | 480446 GiB | 480370 GiB |
|       from small pool |     80 MiB |     82 MiB |   2736 GiB |   2736 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78222 MiB |  78281 MiB | 482164 GiB | 482088 GiB |
|       from large pool |  78141 MiB |  78201 MiB | 479431 GiB | 479355 GiB |
|       from small pool |     80 MiB |     81 MiB |   2732 GiB |   2732 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80488 MiB |  80490 MiB |   1034 GiB |    955 GiB |
|       from large pool |  80404 MiB |  80404 MiB |   1028 GiB |    949 GiB |
|       from small pool |     84 MiB |    424 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2177 MiB |   9886 MiB | 462307 GiB | 462304 GiB |
|       from large pool |   2174 MiB |   9877 MiB | 459213 GiB | 459211 GiB |
|       from small pool |      3 MiB |     21 MiB |   3093 GiB |   3093 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1796    |    1799    |   31868 K  |   31866 K  |
|       from large pool |     455    |     456    |   15195 K  |   15195 K  |
|       from small pool |    1341    |    1344    |   16672 K  |   16671 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1796    |    1799    |   31868 K  |   31866 K  |
|       from large pool |     455    |     456    |   15195 K  |   15195 K  |
|       from small pool |    1341    |    1344    |   16672 K  |   16671 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     224    |     401    |    7332    |    7108    |
|       from large pool |     182    |     189    |    3969    |    3787    |
|       from small pool |      42    |     212    |    3363    |    3321    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     187    |     188    |   17986 K  |   17986 K  |
|       from large pool |     140    |     144    |    9970 K  |    9969 K  |
|       from small pool |      47    |      52    |    8016 K  |    8016 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:39:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:39:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:39:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:39:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:39:23]    INFO >> epoch 010:   1189 / 1539 loss=4.02, wps=4577.1, ups=5.81, wpb=787.9, bsz=787.9, num_updates=15000, lr=4.8e-05, gnorm=5.946, clip=0, train_wall=8, gb_free=70.1, wall=2440 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:39:31]    INFO >> epoch 010:   1239 / 1539 loss=3.827, wps=4984.8, ups=6.34, wpb=786.3, bsz=786.3, num_updates=15050, lr=4.8e-05, gnorm=6.365, clip=0, train_wall=7, gb_free=73.2, wall=2448 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:39:39]    INFO >> epoch 010:   1289 / 1539 loss=3.693, wps=4640.1, ups=6.05, wpb=766.5, bsz=766.5, num_updates=15100, lr=4.8e-05, gnorm=6.248, clip=0, train_wall=8, gb_free=72.7, wall=2457 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:39:47]    INFO >> epoch 010:   1339 / 1539 loss=4.03, wps=4638.3, ups=6.87, wpb=675.5, bsz=675.5, num_updates=15150, lr=4.8e-05, gnorm=5.475, clip=0, train_wall=7, gb_free=74.4, wall=2464 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:39:55]    INFO >> epoch 010:   1389 / 1539 loss=3.844, wps=5051.8, ups=6.77, wpb=746.1, bsz=746.1, num_updates=15200, lr=4.8e-05, gnorm=6.122, clip=2, train_wall=7, gb_free=73.6, wall=2471 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:40:02]    INFO >> epoch 010:   1439 / 1539 loss=3.83, wps=4591.7, ups=6.76, wpb=679.6, bsz=679.6, num_updates=15250, lr=4.8e-05, gnorm=5.86, clip=0, train_wall=7, gb_free=70.1, wall=2479 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:40:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 251.25 MiB is free. Including non-PyTorch memory, this process has 78.87 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 4.92 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:40:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 41           |        cudaMalloc retries: 78        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72822 MiB |  75426 MiB | 491921 GiB | 491850 GiB |
|       from large pool |  72809 MiB |  75413 MiB | 489137 GiB | 489066 GiB |
|       from small pool |     12 MiB |     24 MiB |   2784 GiB |   2784 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72822 MiB |  75426 MiB | 491921 GiB | 491850 GiB |
|       from large pool |  72809 MiB |  75413 MiB | 489137 GiB | 489066 GiB |
|       from small pool |     12 MiB |     24 MiB |   2784 GiB |   2784 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 490883 GiB | 490812 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 488103 GiB | 488032 GiB |
|       from small pool |     12 MiB |     24 MiB |   2780 GiB |   2780 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80254 MiB |  80254 MiB |   1073 GiB |    995 GiB |
|       from large pool |  80228 MiB |  80228 MiB |   1067 GiB |    988 GiB |
|       from small pool |     26 MiB |    210 MiB |      6 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5153 MiB |   9503 MiB | 470014 GiB | 470009 GiB |
|       from large pool |   5140 MiB |   9488 MiB | 466866 GiB | 466861 GiB |
|       from small pool |     13 MiB |     33 MiB |   3148 GiB |   3148 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   32444 K  |   32444 K  |
|       from large pool |     312    |     320    |   15487 K  |   15486 K  |
|       from small pool |     291    |     356    |   16957 K  |   16957 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   32444 K  |   32444 K  |
|       from large pool |     312    |     320    |   15487 K  |   15486 K  |
|       from small pool |     291    |     356    |   16957 K  |   16957 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     185    |     325    |    7504    |    7319    |
|       from large pool |     172    |     220    |    4078    |    3906    |
|       from small pool |      13    |     105    |    3426    |    3413    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     168    |     169    |   18312 K  |   18312 K  |
|       from large pool |     142    |     143    |   10166 K  |   10166 K  |
|       from small pool |      26    |      63    |    8146 K  |    8146 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:40:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:40:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:40:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:40:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:40:10]    INFO >> epoch 010:   1490 / 1539 loss=4.061, wps=4444.3, ups=6.4, wpb=694.7, bsz=694.7, num_updates=15300, lr=4.8e-05, gnorm=5.961, clip=0, train_wall=7, gb_free=64.7, wall=2487 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:40:19]    INFO >> epoch 010 | loss 3.926 | wps 4442.5 | ups 6.23 | wpb 712.7 | bsz 712.7 | num_updates 15349 | lr 4.8e-05 | gnorm 5.967 | clip 0.3 | train_wall 218 | gb_free 70.8 | wall 2494 (progress_bar.py:267, print())[0m
[33m[2025-11-19 04:40:19] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:40:32]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.943 | wps 11662.9 | wpb 5412.5 | bsz 5412.5 | num_updates 15349 | best_loss 5.983 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 04:40:32]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 04:40:32]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/checkpoints/checkpoint_last.pt (epoch 10 @ 15349 updates, score 3.943) (writing took 0.013508 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 04:40:32] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:40:32]    INFO >> epoch 011:      1 / 1539 loss=3.966, wps=1747.7, ups=2.4, wpb=728.6, bsz=728.6, num_updates=15350, lr=4e-05, gnorm=5.593, clip=0, train_wall=7, gb_free=68.7, wall=2507 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:40:39]    INFO >> epoch 011:     51 / 1539 loss=3.924, wps=4594, ups=7.1, wpb=647.3, bsz=647.3, num_updates=15400, lr=4e-05, gnorm=5.62, clip=0, train_wall=7, gb_free=75, wall=2514 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:40:47]    INFO >> epoch 011:    101 / 1539 loss=3.78, wps=4912.3, ups=6.73, wpb=729.7, bsz=729.7, num_updates=15450, lr=4e-05, gnorm=6.595, clip=0, train_wall=7, gb_free=74.4, wall=2522 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:40:55]    INFO >> epoch 011:    151 / 1539 loss=4.059, wps=4848, ups=7.28, wpb=666, bsz=666, num_updates=15500, lr=4e-05, gnorm=5.19, clip=0, train_wall=6, gb_free=71.4, wall=2529 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:41:03]    INFO >> epoch 011:    201 / 1539 loss=3.709, wps=5098, ups=6.63, wpb=769.4, bsz=769.4, num_updates=15550, lr=4e-05, gnorm=6.521, clip=4, train_wall=7, gb_free=71.9, wall=2536 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:41:10]    INFO >> epoch 011:    251 / 1539 loss=3.85, wps=5112.1, ups=7.01, wpb=729.4, bsz=729.4, num_updates=15600, lr=4e-05, gnorm=6.427, clip=0, train_wall=7, gb_free=73.3, wall=2543 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:41:17]    INFO >> epoch 011:    301 / 1539 loss=3.905, wps=4257.9, ups=6.83, wpb=623.3, bsz=623.3, num_updates=15650, lr=4e-05, gnorm=5.523, clip=0, train_wall=7, gb_free=74.6, wall=2551 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:41:26]    INFO >> epoch 011:    351 / 1539 loss=3.875, wps=4971.6, ups=6.62, wpb=751.3, bsz=751.3, num_updates=15700, lr=4e-05, gnorm=6.18, clip=0, train_wall=7, gb_free=72, wall=2558 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:41:33]    INFO >> epoch 011:    401 / 1539 loss=3.947, wps=4881.9, ups=7.21, wpb=677.5, bsz=677.5, num_updates=15750, lr=4e-05, gnorm=5.682, clip=0, train_wall=7, gb_free=72.3, wall=2565 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:41:40]    INFO >> epoch 011:    451 / 1539 loss=3.948, wps=4463.5, ups=7.15, wpb=624.1, bsz=624.1, num_updates=15800, lr=4e-05, gnorm=6.572, clip=0, train_wall=7, gb_free=72.8, wall=2572 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:41:48]    INFO >> epoch 011:    501 / 1539 loss=4.041, wps=3687, ups=6.08, wpb=606.7, bsz=606.7, num_updates=15850, lr=4e-05, gnorm=5.21, clip=0, train_wall=8, gb_free=75, wall=2580 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:41:55]    INFO >> epoch 011:    551 / 1539 loss=3.9, wps=4536.7, ups=7.03, wpb=645.8, bsz=645.8, num_updates=15900, lr=4e-05, gnorm=6.647, clip=2, train_wall=7, gb_free=73.9, wall=2588 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:41:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 31.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 76.39 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:41:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 42           |        cudaMalloc retries: 79        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78168 MiB |  78228 MiB | 514019 GiB | 513943 GiB |
|       from large pool |  77782 MiB |  77842 MiB | 511102 GiB | 511026 GiB |
|       from small pool |    385 MiB |    386 MiB |   2916 GiB |   2916 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78168 MiB |  78228 MiB | 514019 GiB | 513943 GiB |
|       from large pool |  77782 MiB |  77842 MiB | 511102 GiB | 511026 GiB |
|       from small pool |    385 MiB |    386 MiB |   2916 GiB |   2916 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78137 MiB |  78197 MiB | 512935 GiB | 512858 GiB |
|       from large pool |  77753 MiB |  77813 MiB | 510022 GiB | 509946 GiB |
|       from small pool |    383 MiB |    384 MiB |   2912 GiB |   2912 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80474 MiB |  80476 MiB |   1076 GiB |    997 GiB |
|       from large pool |  80050 MiB |  80050 MiB |   1069 GiB |    990 GiB |
|       from small pool |    424 MiB |    426 MiB |      7 GiB |      6 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2245 MiB |   6868 MiB | 489062 GiB | 489060 GiB |
|       from large pool |   2207 MiB |   6862 MiB | 485767 GiB | 485765 GiB |
|       from small pool |     38 MiB |     40 MiB |   3295 GiB |   3295 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    7120    |    7123    |   33914 K  |   33907 K  |
|       from large pool |     917    |     918    |   16134 K  |   16134 K  |
|       from small pool |    6203    |    6206    |   17779 K  |   17773 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    7120    |    7123    |   33914 K  |   33907 K  |
|       from large pool |     917    |     918    |   16134 K  |   16134 K  |
|       from small pool |    6203    |    6206    |   17779 K  |   17773 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     417    |     418    |    7739    |    7322    |
|       from large pool |     205    |     205    |    4113    |    3908    |
|       from small pool |     212    |     213    |    3626    |    3414    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     548    |     550    |   19159 K  |   19159 K  |
|       from large pool |     171    |     171    |   10604 K  |   10604 K  |
|       from small pool |     377    |     379    |    8555 K  |    8554 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:41:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:41:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:41:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:41:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:42:05]    INFO >> epoch 011:    602 / 1539 loss=3.94, wps=4318.5, ups=6.22, wpb=694.1, bsz=694.1, num_updates=15950, lr=4e-05, gnorm=5.832, clip=0, train_wall=7, gb_free=71.6, wall=2596 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:42:12]    INFO >> epoch 011:    652 / 1539 loss=3.814, wps=5226.7, ups=6.41, wpb=815.3, bsz=815.3, num_updates=16000, lr=4e-05, gnorm=6.426, clip=0, train_wall=7, gb_free=71.5, wall=2603 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:42:20]    INFO >> epoch 011:    702 / 1539 loss=4.007, wps=4607.9, ups=6.97, wpb=661.1, bsz=661.1, num_updates=16050, lr=4e-05, gnorm=5.287, clip=0, train_wall=7, gb_free=71, wall=2611 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:42:28]    INFO >> epoch 011:    752 / 1539 loss=3.783, wps=4411.7, ups=6.03, wpb=731.8, bsz=731.8, num_updates=16100, lr=4e-05, gnorm=6.49, clip=2, train_wall=8, gb_free=72.6, wall=2619 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:42:35] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 482.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 457.25 MiB is free. Including non-PyTorch memory, this process has 78.67 GiB memory in use. Of the allocated memory 73.46 GiB is allocated by PyTorch, and 4.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:42:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 43           |        cudaMalloc retries: 81        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  72821 MiB |  75424 MiB | 521026 GiB | 520955 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 518072 GiB | 518001 GiB |
|       from small pool |     12 MiB |     21 MiB |   2953 GiB |   2953 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  72821 MiB |  75424 MiB | 521026 GiB | 520955 GiB |
|       from large pool |  72808 MiB |  75411 MiB | 518072 GiB | 518001 GiB |
|       from small pool |     12 MiB |     21 MiB |   2953 GiB |   2953 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  72801 MiB |  75403 MiB | 519925 GiB | 519854 GiB |
|       from large pool |  72788 MiB |  75391 MiB | 516976 GiB | 516905 GiB |
|       from small pool |     12 MiB |     21 MiB |   2949 GiB |   2949 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80048 MiB |  80424 MiB |   1078 GiB |   1000 GiB |
|       from large pool |  80024 MiB |  80400 MiB |   1071 GiB |    993 GiB |
|       from small pool |     24 MiB |    424 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   7226 MiB |  10655 MiB | 495432 GiB | 495425 GiB |
|       from large pool |   7215 MiB |  10644 MiB | 492094 GiB | 492087 GiB |
|       from small pool |     11 MiB |     27 MiB |   3338 GiB |   3338 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   34373 K  |   34372 K  |
|       from large pool |     312    |     320    |   16366 K  |   16366 K  |
|       from small pool |     291    |     356    |   18006 K  |   18006 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   34373 K  |   34372 K  |
|       from large pool |     312    |     320    |   16366 K  |   16366 K  |
|       from small pool |     291    |     356    |   18006 K  |   18006 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     182    |     416    |    7742    |    7560    |
|       from large pool |     170    |     204    |    4116    |    3946    |
|       from small pool |      12    |     212    |    3626    |    3614    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     175    |     176    |   19423 K  |   19423 K  |
|       from large pool |     147    |     148    |   10761 K  |   10761 K  |
|       from small pool |      28    |      63    |    8662 K  |    8662 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:42:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:42:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:42:35] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:42:35] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:42:37]    INFO >> epoch 011:    803 / 1539 loss=3.959, wps=3889.9, ups=6.64, wpb=586.3, bsz=586.3, num_updates=16150, lr=4e-05, gnorm=5.759, clip=0, train_wall=6, gb_free=62.8, wall=2626 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:42:44]    INFO >> epoch 011:    853 / 1539 loss=3.881, wps=4628.2, ups=7.04, wpb=657, bsz=657, num_updates=16200, lr=4e-05, gnorm=6.217, clip=0, train_wall=7, gb_free=74.8, wall=2633 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:42:51]    INFO >> epoch 011:    903 / 1539 loss=3.918, wps=5391.1, ups=6.91, wpb=779.8, bsz=779.8, num_updates=16250, lr=4e-05, gnorm=6.165, clip=0, train_wall=7, gb_free=72.7, wall=2641 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:42:59]    INFO >> epoch 011:    953 / 1539 loss=3.775, wps=4988.5, ups=6.69, wpb=746, bsz=746, num_updates=16300, lr=4e-05, gnorm=5.992, clip=0, train_wall=7, gb_free=72, wall=2648 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:43:07]    INFO >> epoch 011:   1003 / 1539 loss=3.932, wps=4841.5, ups=6.56, wpb=737.8, bsz=737.8, num_updates=16350, lr=4e-05, gnorm=5.834, clip=0, train_wall=7, gb_free=71.3, wall=2656 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:43:15]    INFO >> epoch 011:   1053 / 1539 loss=3.783, wps=4697.6, ups=6.78, wpb=692.9, bsz=692.9, num_updates=16400, lr=4e-05, gnorm=5.411, clip=0, train_wall=7, gb_free=73.7, wall=2663 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:43:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 457.25 MiB is free. Including non-PyTorch memory, this process has 78.67 GiB memory in use. Of the allocated memory 70.08 GiB is allocated by PyTorch, and 8.09 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:43:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 44           |        cudaMalloc retries: 82        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69006 MiB |  71763 MiB | 529586 GiB | 529518 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 526586 GiB | 526518 GiB |
|       from small pool |     16 MiB |     17 MiB |   2999 GiB |   2999 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69006 MiB |  71763 MiB | 529586 GiB | 529518 GiB |
|       from large pool |  68989 MiB |  71746 MiB | 526586 GiB | 526518 GiB |
|       from small pool |     16 MiB |     17 MiB |   2999 GiB |   2999 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  68980 MiB |  71736 MiB | 528466 GiB | 528399 GiB |
|       from large pool |  68963 MiB |  71719 MiB | 525471 GiB | 525403 GiB |
|       from small pool |     16 MiB |     17 MiB |   2995 GiB |   2995 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80048 MiB |  80148 MiB |   1079 GiB |   1000 GiB |
|       from large pool |  80024 MiB |  80024 MiB |   1071 GiB |    993 GiB |
|       from small pool |     24 MiB |    124 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8407 MiB |  12173 MiB | 503623 GiB | 503614 GiB |
|       from large pool |   8400 MiB |  12165 MiB | 500231 GiB | 500223 GiB |
|       from small pool |      7 MiB |     23 MiB |   3391 GiB |   3391 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   34930 K  |   34930 K  |
|       from large pool |     343    |     351    |   16651 K  |   16650 K  |
|       from small pool |     298    |     342    |   18279 K  |   18279 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   34930 K  |   34930 K  |
|       from large pool |     343    |     351    |   16651 K  |   16650 K  |
|       from small pool |     298    |     342    |   18279 K  |   18279 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     182    |     232    |    7792    |    7610    |
|       from large pool |     170    |     170    |    4116    |    3946    |
|       from small pool |      12    |      62    |    3676    |    3664    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     164    |     165    |   19736 K  |   19736 K  |
|       from large pool |     137    |     138    |   10951 K  |   10951 K  |
|       from small pool |      27    |      55    |    8785 K  |    8785 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:43:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:43:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:43:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:43:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:43:23]    INFO >> epoch 011:   1104 / 1539 loss=3.983, wps=4455.3, ups=5.83, wpb=764, bsz=764, num_updates=16450, lr=4e-05, gnorm=5.555, clip=0, train_wall=8, gb_free=75, wall=2672 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:43:31]    INFO >> epoch 011:   1154 / 1539 loss=3.668, wps=5685.4, ups=6.25, wpb=909, bsz=909, num_updates=16500, lr=4e-05, gnorm=6.984, clip=2, train_wall=8, gb_free=69.3, wall=2680 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:43:40]    INFO >> epoch 011:   1204 / 1539 loss=3.886, wps=4047.8, ups=6.66, wpb=608.2, bsz=608.2, num_updates=16550, lr=4e-05, gnorm=5.913, clip=0, train_wall=7, gb_free=68.8, wall=2687 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:43:49]    INFO >> epoch 011:   1254 / 1539 loss=3.948, wps=4771.8, ups=5.76, wpb=828.9, bsz=828.9, num_updates=16600, lr=4e-05, gnorm=5.906, clip=0, train_wall=8, gb_free=73.1, wall=2696 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:43:56]    INFO >> epoch 011:   1304 / 1539 loss=3.896, wps=4957.9, ups=6.71, wpb=739.1, bsz=739.1, num_updates=16650, lr=4e-05, gnorm=6.56, clip=0, train_wall=7, gb_free=71.9, wall=2703 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:43:59] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.66 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.06 GiB is free. Including non-PyTorch memory, this process has 78.06 GiB memory in use. Of the allocated memory 74.92 GiB is allocated by PyTorch, and 2.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:43:59] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 45           |        cudaMalloc retries: 85        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  76320 MiB |  78596 MiB | 536717 GiB | 536643 GiB |
|       from large pool |  76301 MiB |  78578 MiB | 533678 GiB | 533604 GiB |
|       from small pool |     18 MiB |     23 MiB |   3038 GiB |   3038 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  76320 MiB |  78596 MiB | 536717 GiB | 536643 GiB |
|       from large pool |  76301 MiB |  78578 MiB | 533678 GiB | 533604 GiB |
|       from small pool |     18 MiB |     23 MiB |   3038 GiB |   3038 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  76304 MiB |  78579 MiB | 535583 GiB | 535508 GiB |
|       from large pool |  76285 MiB |  78562 MiB | 532548 GiB | 532474 GiB |
|       from small pool |     18 MiB |     23 MiB |   3034 GiB |   3034 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79424 MiB |  79424 MiB |   1176 GiB |   1099 GiB |
|       from large pool |  79400 MiB |  79400 MiB |   1169 GiB |   1091 GiB |
|       from small pool |     24 MiB |     76 MiB |      7 GiB |      7 GiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3103 MiB |   4235 MiB | 510470 GiB | 510467 GiB |
|       from large pool |   3098 MiB |   4229 MiB | 507033 GiB | 507030 GiB |
|       from small pool |      5 MiB |     27 MiB |   3436 GiB |   3436 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   35395 K  |   35394 K  |
|       from large pool |     340    |     346    |   16881 K  |   16880 K  |
|       from small pool |     300    |     356    |   18513 K  |   18513 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   35395 K  |   35394 K  |
|       from large pool |     340    |     346    |   16881 K  |   16880 K  |
|       from small pool |     300    |     356    |   18513 K  |   18513 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     110    |     179    |    7980    |    7870    |
|       from large pool |      98    |     141    |    4186    |    4088    |
|       from small pool |      12    |      38    |    3794    |    3782    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     125    |     125    |   20000 K  |   20000 K  |
|       from large pool |     100    |     100    |   11103 K  |   11103 K  |
|       from small pool |      25    |      61    |    8897 K  |    8897 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:43:59] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:43:59] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:43:59] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:43:59] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:44:06]    INFO >> epoch 011:   1355 / 1539 loss=3.892, wps=3501.7, ups=5.08, wpb=689.1, bsz=689.1, num_updates=16700, lr=4e-05, gnorm=6.277, clip=0, train_wall=7, gb_free=56.8, wall=2713 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:44:15]    INFO >> epoch 011:   1405 / 1539 loss=4.01, wps=5046.9, ups=6.38, wpb=791.3, bsz=791.3, num_updates=16750, lr=4e-05, gnorm=6.6, clip=2, train_wall=7, gb_free=73.5, wall=2721 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:44:23]    INFO >> epoch 011:   1455 / 1539 loss=3.873, wps=4516.8, ups=6.79, wpb=665.3, bsz=665.3, num_updates=16800, lr=4e-05, gnorm=6.007, clip=0, train_wall=7, gb_free=65.3, wall=2728 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:44:31]    INFO >> epoch 011:   1505 / 1539 loss=3.881, wps=5090.6, ups=6.22, wpb=818.6, bsz=818.6, num_updates=16850, lr=4e-05, gnorm=5.604, clip=0, train_wall=8, gb_free=72.5, wall=2736 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:44:36]    INFO >> epoch 011 | loss 3.887 | wps 4415.7 | ups 6.2 | wpb 712.7 | bsz 712.7 | num_updates 16884 | lr 4e-05 | gnorm 6.028 | clip 0.4 | train_wall 218 | gb_free 73.9 | wall 2742 (progress_bar.py:267, print())[0m
[33m[2025-11-19 04:44:36] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:44:50]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.942 | wps 11980.7 | wpb 5412.5 | bsz 5412.5 | num_updates 16884 | best_loss 5.983 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 04:44:50]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 04:44:50]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/checkpoints/checkpoint_last.pt (epoch 11 @ 16884 updates, score 3.942) (writing took 0.012529 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-19 04:44:50]    INFO >> æ—©åœ: éªŒè¯æ€§èƒ½å·²10è½®æœªæå‡ (train_enhanced.py:345, single_main())[0m
[32m[2025-11-19 04:44:50]    INFO >> è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ 2698.4 ç§’ (train_enhanced.py:355, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 04:44:51]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 04:44:51]    INFO >> æ‰€æœ‰æ—¥å¿—å·²ä¿å­˜åˆ°: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_lr_1e-4/logs (train_enhanced.py:359, single_main())[0m

âœ“ exp_lr_1e-4 æˆåŠŸ

ç­‰å¾…3ç§’...

è¿›åº¦: 4/6

============================================================
å®éªŒ: exp_batch_64 - æ‰¹é‡å¤§å°64
æ—¶é—´: 2025-11-19 04:45:30
============================================================

[32m[2025-11-19 04:45:32]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/config.yml (train_enhanced.py:382, cli_main())[0m
[32m[2025-11-19 04:45:32]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:410, cli_main())[0m
[32m[2025-11-19 04:45:32]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/logs (train_enhanced.py:296, single_main())[0m
[32m[2025-11-19 04:45:32]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-19 04:45:32]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-19 04:45:32]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-19 04:45:39]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 64, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=64, out_features=64, bias=False)
      (2): Dropout(p=0.1, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(64, 64)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(128, 64)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=False)
      (1): Dropout(p=0.1, inplace=False)
      (2): Linear(in_features=64, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:303, single_main())[0m
[32m[2025-11-19 04:45:39]    INFO >> æ¨¡å‹: typilus, æŸå¤±å‡½æ•°: TypilusCriterion (train_enhanced.py:304, single_main())[0m
[32m[2025-11-19 04:45:39]    INFO >> æ¨¡å‹å‚æ•°: 847843 (å¯è®­ç»ƒ: 847843) (train_enhanced.py:305, single_main())[0m
[32m[2025-11-19 04:45:40]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-19 04:45:40]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80579 MB ; used memory = 1340 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-19 04:45:40]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-19 04:45:40]    INFO >> ä½¿ç”¨ 1 ä¸ªGPUè®­ç»ƒ (train_enhanced.py:311, single_main())[0m
[32m[2025-11-19 04:45:40]    INFO >> no existing checkpoint found /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-19 04:45:40]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-19 04:46:39]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-19 04:46:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[33m[2025-11-19 04:46:53] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 79.14 GiB of which 591.25 MiB is free. Including non-PyTorch memory, this process has 78.54 GiB memory in use. Of the allocated memory 76.83 GiB is allocated by PyTorch, and 1.21 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:46:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 1            |        cudaMalloc retries: 2         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78200 MiB |  79037 MiB |   3188 GiB |   3111 GiB |
|       from large pool |  78187 MiB |  79024 MiB |   3173 GiB |   3097 GiB |
|       from small pool |     12 MiB |     15 MiB |     14 GiB |     14 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78200 MiB |  79037 MiB |   3188 GiB |   3111 GiB |
|       from large pool |  78187 MiB |  79024 MiB |   3173 GiB |   3097 GiB |
|       from small pool |     12 MiB |     15 MiB |     14 GiB |     14 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78178 MiB |  79013 MiB |   3183 GiB |   3107 GiB |
|       from large pool |  78165 MiB |  79000 MiB |   3169 GiB |   3092 GiB |
|       from small pool |     12 MiB |     15 MiB |     14 GiB |     14 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79914 MiB |  80162 MiB | 133518 MiB |  53604 MiB |
|       from large pool |  79898 MiB |  80146 MiB | 133218 MiB |  53320 MiB |
|       from small pool |     16 MiB |    288 MiB |    300 MiB |    284 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1713 MiB |   7413 MiB |   1613 GiB |   1611 GiB |
|       from large pool |   1710 MiB |   7408 MiB |   1596 GiB |   1594 GiB |
|       from small pool |      3 MiB |     17 MiB |     17 GiB |     17 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     611    |  146462    |  145859    |
|       from large pool |     314    |     322    |   58203    |   57889    |
|       from small pool |     289    |     334    |   88259    |   87970    |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     611    |  146462    |  145859    |
|       from large pool |     314    |     322    |   58203    |   57889    |
|       from small pool |     289    |     334    |   88259    |   87970    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     144    |     519    |     712    |     568    |
|       from large pool |     136    |     375    |     562    |     426    |
|       from small pool |       8    |     144    |     150    |     142    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     119    |     121    |   97011    |   96892    |
|       from large pool |     102    |     104    |   43493    |   43391    |
|       from small pool |      17    |      38    |   53518    |   53501    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:46:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:46:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:46:53] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:46:53] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:46:54]    INFO >> epoch 001:     51 / 770 loss=5.749, wps=4962.5, ups=3.34, wpb=1487.5, bsz=1487.5, num_updates=50, lr=0.0004, gnorm=6.247, clip=0, train_wall=13, gb_free=61, wall=72 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:47:07]    INFO >> epoch 001:    101 / 770 loss=5.597, wps=5764.9, ups=4.27, wpb=1348.8, bsz=1348.8, num_updates=100, lr=0.0004, gnorm=7.499, clip=0, train_wall=11, gb_free=67.6, wall=83 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:47:19]    INFO >> epoch 001:    151 / 770 loss=5.825, wps=5970.9, ups=4.05, wpb=1472.9, bsz=1472.9, num_updates=150, lr=0.0004, gnorm=9.182, clip=0, train_wall=11, gb_free=67.4, wall=96 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:47:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.61 GiB is free. Including non-PyTorch memory, this process has 77.51 GiB memory in use. Of the allocated memory 72.83 GiB is allocated by PyTorch, and 4.18 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:47:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 2            |        cudaMalloc retries: 4         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73953 MiB |  75089 MiB |   9523 GiB |   9451 GiB |
|       from large pool |  73939 MiB |  75075 MiB |   9481 GiB |   9409 GiB |
|       from small pool |     14 MiB |     15 MiB |     42 GiB |     42 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73953 MiB |  75089 MiB |   9523 GiB |   9451 GiB |
|       from large pool |  73939 MiB |  75075 MiB |   9481 GiB |   9409 GiB |
|       from small pool |     14 MiB |     15 MiB |     42 GiB |     42 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73942 MiB |  75078 MiB |   9510 GiB |   9437 GiB |
|       from large pool |  73928 MiB |  75064 MiB |   9468 GiB |   9395 GiB |
|       from small pool |     14 MiB |     15 MiB |     42 GiB |     42 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78860 MiB |  80038 MiB | 139596 MiB |  60736 MiB |
|       from large pool |  78840 MiB |  79898 MiB | 139172 MiB |  60332 MiB |
|       from small pool |     20 MiB |    140 MiB |    424 MiB |    404 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4906 MiB |  10481 MiB |   7298 GiB |   7293 GiB |
|       from large pool |   4900 MiB |  10474 MiB |   7248 GiB |   7243 GiB |
|       from small pool |      5 MiB |     15 MiB |     50 GiB |     50 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     564    |     572    |  436463    |  435899    |
|       from large pool |     276    |     284    |  182859    |  182583    |
|       from small pool |     288    |     334    |  253604    |  253316    |
|---------------------------------------------------------------------------|
| Active allocs         |     564    |     572    |  436463    |  435899    |
|       from large pool |     276    |     284    |  182859    |  182583    |
|       from small pool |     288    |     334    |  253604    |  253316    |
|---------------------------------------------------------------------------|
| GPU reserved segments |     107    |     206    |     781    |     674    |
|       from large pool |      97    |     136    |     569    |     472    |
|       from small pool |      10    |      70    |     212    |     202    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     105    |     105    |  273083    |  272978    |
|       from large pool |      85    |      85    |  124767    |  124682    |
|       from small pool |      20    |      34    |  148316    |  148296    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:47:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:47:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:47:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:47:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:47:34]    INFO >> epoch 001:    202 / 770 loss=6.068, wps=5676.2, ups=3.73, wpb=1523, bsz=1523, num_updates=200, lr=0.0004, gnorm=7.942, clip=0, train_wall=12, gb_free=60.3, wall=109 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:47:51]    INFO >> epoch 001:    252 / 770 loss=6.016, wps=4337.6, ups=3.02, wpb=1435.9, bsz=1435.9, num_updates=250, lr=0.0004, gnorm=8.361, clip=2, train_wall=11, gb_free=61.1, wall=126 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:48:04]    INFO >> epoch 001:    302 / 770 loss=5.844, wps=5399.8, ups=3.84, wpb=1407.9, bsz=1407.9, num_updates=300, lr=0.0004, gnorm=7.933, clip=0, train_wall=12, gb_free=54.7, wall=139 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:48:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.61 GiB is free. Including non-PyTorch memory, this process has 77.51 GiB memory in use. Of the allocated memory 73.73 GiB is allocated by PyTorch, and 3.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:48:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 3            |        cudaMalloc retries: 5         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75056 MiB |  75494 MiB |  18858 GiB |  18785 GiB |
|       from large pool |  75044 MiB |  75483 MiB |  18776 GiB |  18703 GiB |
|       from small pool |     11 MiB |     12 MiB |     81 GiB |     81 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75056 MiB |  75494 MiB |  18858 GiB |  18785 GiB |
|       from large pool |  75044 MiB |  75483 MiB |  18776 GiB |  18703 GiB |
|       from small pool |     11 MiB |     12 MiB |     81 GiB |     81 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75041 MiB |  75480 MiB |  18835 GiB |  18761 GiB |
|       from large pool |  75030 MiB |  75469 MiB |  18753 GiB |  18680 GiB |
|       from small pool |     11 MiB |     12 MiB |     81 GiB |     81 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78856 MiB |  78940 MiB | 139676 MiB |  60820 MiB |
|       from large pool |  78840 MiB |  78840 MiB | 139172 MiB |  60332 MiB |
|       from small pool |     16 MiB |    100 MiB |    504 MiB |    488 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3799 MiB |   8857 MiB |  17118 GiB |  17114 GiB |
|       from large pool |   3795 MiB |   8852 MiB |  17022 GiB |  17018 GiB |
|       from small pool |      4 MiB |     11 MiB |     96 GiB |     96 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     639    |     645    |     848 K  |     847 K  |
|       from large pool |     349    |     355    |     361 K  |     360 K  |
|       from small pool |     290    |     334    |     487 K  |     487 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     639    |     645    |     848 K  |     847 K  |
|       from large pool |     349    |     355    |     361 K  |     360 K  |
|       from small pool |     290    |     334    |     487 K  |     487 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     105    |     147    |     821    |     716    |
|       from large pool |      97    |      97    |     569    |     472    |
|       from small pool |       8    |      50    |     252    |     244    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     106    |     109    |  516330    |  516224    |
|       from large pool |      88    |      91    |  234400    |  234312    |
|       from small pool |      18    |      34    |  281930    |  281912    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:48:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:48:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:48:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:48:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:48:18]    INFO >> epoch 001:    353 / 770 loss=5.928, wps=5001.8, ups=3.91, wpb=1280, bsz=1280, num_updates=350, lr=0.0004, gnorm=7.653, clip=0, train_wall=11, gb_free=75.2, wall=152 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:48:19] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.58 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.62 GiB is free. Including non-PyTorch memory, this process has 77.50 GiB memory in use. Of the allocated memory 72.89 GiB is allocated by PyTorch, and 4.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:48:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 4            |        cudaMalloc retries: 6         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71008 MiB |  74643 MiB |  21460 GiB |  21391 GiB |
|       from large pool |  70985 MiB |  74621 MiB |  21369 GiB |  21299 GiB |
|       from small pool |     22 MiB |     22 MiB |     91 GiB |     91 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71008 MiB |  74643 MiB |  21460 GiB |  21391 GiB |
|       from large pool |  70985 MiB |  74621 MiB |  21369 GiB |  21299 GiB |
|       from small pool |     22 MiB |     22 MiB |     91 GiB |     91 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  70995 MiB |  74629 MiB |  21434 GiB |  21364 GiB |
|       from large pool |  70972 MiB |  74606 MiB |  21342 GiB |  21273 GiB |
|       from small pool |     22 MiB |     22 MiB |     91 GiB |     91 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78850 MiB |  78898 MiB | 139718 MiB |  60868 MiB |
|       from large pool |  78822 MiB |  78840 MiB | 139172 MiB |  60350 MiB |
|       from small pool |     28 MiB |     58 MiB |    546 MiB |    518 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5109 MiB |   8497 MiB |  19850 GiB |  19845 GiB |
|       from large pool |   5104 MiB |   8493 MiB |  19742 GiB |  19737 GiB |
|       from small pool |      5 MiB |     13 MiB |    108 GiB |    108 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     639    |     647    |     959 K  |     958 K  |
|       from large pool |     335    |     343    |     413 K  |     412 K  |
|       from small pool |     304    |     334    |     546 K  |     545 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     639    |     647    |     959 K  |     958 K  |
|       from large pool |     335    |     343    |     413 K  |     412 K  |
|       from small pool |     304    |     334    |     546 K  |     545 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     110    |     126    |     842    |     732    |
|       from large pool |      96    |      97    |     569    |     473    |
|       from small pool |      14    |      29    |     273    |     259    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     109    |     114    |  580728    |  580619    |
|       from large pool |      85    |      91    |  266591    |  266506    |
|       from small pool |      24    |      36    |  314137    |  314113    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:48:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:48:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:48:19] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:48:19] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:48:31]    INFO >> epoch 001:    404 / 770 loss=5.91, wps=4589.1, ups=3.8, wpb=1206.8, bsz=1206.8, num_updates=400, lr=0.0004, gnorm=6.772, clip=0, train_wall=11, gb_free=68.5, wall=165 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:48:46]    INFO >> epoch 001:    454 / 770 loss=5.64, wps=5544.2, ups=3.64, wpb=1523.5, bsz=1523.5, num_updates=450, lr=0.0004, gnorm=8.562, clip=0, train_wall=13, gb_free=65.6, wall=178 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:48:59]    INFO >> epoch 001:    504 / 770 loss=5.881, wps=5018.1, ups=3.81, wpb=1315.8, bsz=1315.8, num_updates=500, lr=0.0004, gnorm=7.239, clip=0, train_wall=12, gb_free=67.4, wall=192 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:49:13]    INFO >> epoch 001:    554 / 770 loss=5.623, wps=5449.3, ups=3.68, wpb=1480.1, bsz=1480.1, num_updates=550, lr=0.0004, gnorm=7.736, clip=2, train_wall=12, gb_free=66.8, wall=205 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:49:27]    INFO >> epoch 001:    604 / 770 loss=5.744, wps=5285.2, ups=3.87, wpb=1366.3, bsz=1366.3, num_updates=600, lr=0.0004, gnorm=7.595, clip=0, train_wall=12, gb_free=64, wall=218 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:49:40]    INFO >> epoch 001:    654 / 770 loss=5.723, wps=4958, ups=3.76, wpb=1320, bsz=1320, num_updates=650, lr=0.0004, gnorm=7.174, clip=0, train_wall=12, gb_free=67, wall=231 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:49:57]    INFO >> epoch 001:    704 / 770 loss=5.559, wps=5345.8, ups=3.37, wpb=1584.9, bsz=1584.9, num_updates=700, lr=0.0004, gnorm=7.872, clip=0, train_wall=14, gb_free=63, wall=246 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:50:11]    INFO >> epoch 001:    754 / 770 loss=5.649, wps=5443.2, ups=3.55, wpb=1533.7, bsz=1533.7, num_updates=750, lr=0.0004, gnorm=7.995, clip=0, train_wall=13, gb_free=52.2, wall=260 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:50:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 29.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.63 GiB is allocated by PyTorch, and 978.54 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:50:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 5            |        cudaMalloc retries: 7         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79439 MiB |  79497 MiB |  45487 GiB |  45409 GiB |
|       from large pool |  79167 MiB |  79225 MiB |  45285 GiB |  45208 GiB |
|       from small pool |    272 MiB |    273 MiB |    201 GiB |    201 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79439 MiB |  79497 MiB |  45487 GiB |  45409 GiB |
|       from large pool |  79167 MiB |  79225 MiB |  45285 GiB |  45208 GiB |
|       from small pool |    272 MiB |    273 MiB |    201 GiB |    201 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79415 MiB |  79473 MiB |  45433 GiB |  45355 GiB |
|       from large pool |  79144 MiB |  79202 MiB |  45231 GiB |  45154 GiB |
|       from small pool |    270 MiB |    271 MiB |    201 GiB |    201 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80476 MiB |  80476 MiB | 144076 MiB |  63600 MiB |
|       from large pool |  80170 MiB |  80170 MiB | 143252 MiB |  63082 MiB |
|       from small pool |    306 MiB |    306 MiB |    824 MiB |    518 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    976 MiB |   7198 MiB |  45557 GiB |  45556 GiB |
|       from large pool |    942 MiB |   7189 MiB |  45318 GiB |  45317 GiB |
|       from small pool |     33 MiB |     34 MiB |    238 GiB |    238 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    5248    |    5251    |    2099 K  |    2094 K  |
|       from large pool |     755    |     756    |     892 K  |     891 K  |
|       from small pool |    4493    |    4496    |    1207 K  |    1202 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    5248    |    5251    |    2099 K  |    2094 K  |
|       from large pool |     755    |     756    |     892 K  |     891 K  |
|       from small pool |    4493    |    4496    |    1207 K  |    1202 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     316    |     316    |    1049    |     733    |
|       from large pool |     163    |     163    |     637    |     474    |
|       from small pool |     153    |     153    |     412    |     259    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     415    |     416    |    1257 K  |    1257 K  |
|       from large pool |     140    |     141    |     561 K  |     561 K  |
|       from small pool |     275    |     275    |     695 K  |     695 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:50:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:50:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:50:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:50:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:50:15]    INFO >> epoch 001 | loss 5.777 | wps 5219 | ups 3.68 | wpb 1419.5 | bsz 1419.5 | num_updates 765 | lr 0.0004 | gnorm 7.693 | clip 0.3 | train_wall 183 | gb_free 59.8 | wall 265 (progress_bar.py:267, print())[0m
[33m[2025-11-19 04:50:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:50:30]    INFO >> epoch 001 | valid on 'valid' subset | loss 5.742 | wps 10760.5 | wpb 5412.5 | bsz 5412.5 | num_updates 765 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:114: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 35757 (\N{CJK UNIFIED IDEOGRAPH-8BAD}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 32451 (\N{CJK UNIFIED IDEOGRAPH-7EC3}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 32479 (\N{CJK UNIFIED IDEOGRAPH-7EDF}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 35745 (\N{CJK UNIFIED IDEOGRAPH-8BA1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 24635 (\N{CJK UNIFIED IDEOGRAPH-603B}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 36718 (\N{CJK UNIFIED IDEOGRAPH-8F6E}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 25968 (\N{CJK UNIFIED IDEOGRAPH-6570}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py:115: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(plots_dir / 'training.png', dpi=120, bbox_inches='tight')
[32m[2025-11-19 04:50:31]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 04:50:31]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/checkpoints/checkpoint_best.pt (epoch 1 @ 765 updates, score 5.742) (writing took 0.015019 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 04:50:31] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/utils/gradient_clip/fairseq_clip.py:56: UserWarning: amp_C fused kernels unavailable, disabling multi_tensor_l2norm; you may get better performance by installing NVIDIA's apex library
  warnings.warn(
[32m[2025-11-19 04:50:40]    INFO >> epoch 002:     35 / 770 loss=5.585, wps=2524.6, ups=1.77, wpb=1424.5, bsz=1424.5, num_updates=800, lr=0.0004, gnorm=6.768, clip=0, train_wall=12, gb_free=61.9, wall=288 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:50:53]    INFO >> epoch 002:     85 / 770 loss=5.569, wps=5382, ups=3.87, wpb=1389.2, bsz=1389.2, num_updates=850, lr=0.0004, gnorm=6.448, clip=0, train_wall=12, gb_free=63.5, wall=301 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:51:09]    INFO >> epoch 002:    135 / 770 loss=5.551, wps=5372.4, ups=3.43, wpb=1565.3, bsz=1565.3, num_updates=900, lr=0.0004, gnorm=7.604, clip=0, train_wall=14, gb_free=63.5, wall=316 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:51:23]    INFO >> epoch 002:    185 / 770 loss=5.368, wps=5352.9, ups=3.58, wpb=1495.2, bsz=1495.2, num_updates=950, lr=0.0004, gnorm=8.095, clip=2, train_wall=13, gb_free=60.5, wall=330 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:51:37]    INFO >> epoch 002:    235 / 770 loss=5.39, wps=5306.4, ups=3.81, wpb=1393.2, bsz=1393.2, num_updates=1000, lr=0.0004, gnorm=7.426, clip=2, train_wall=12, gb_free=20.6, wall=343 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:51:49] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.58 GiB. GPU 0 has a total capacity of 79.14 GiB of which 479.25 MiB is free. Including non-PyTorch memory, this process has 78.65 GiB memory in use. Of the allocated memory 69.67 GiB is allocated by PyTorch, and 8.48 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:51:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 6            |        cudaMalloc retries: 8         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71005 MiB |  71344 MiB |  66643 GiB |  66574 GiB |
|       from large pool |  70983 MiB |  71321 MiB |  66330 GiB |  66260 GiB |
|       from small pool |     22 MiB |     22 MiB |    313 GiB |    313 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71005 MiB |  71344 MiB |  66643 GiB |  66574 GiB |
|       from large pool |  70983 MiB |  71321 MiB |  66330 GiB |  66260 GiB |
|       from small pool |     22 MiB |     22 MiB |    313 GiB |    313 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  70995 MiB |  71333 MiB |  66562 GiB |  66492 GiB |
|       from large pool |  70972 MiB |  71310 MiB |  66249 GiB |  66179 GiB |
|       from small pool |     22 MiB |     22 MiB |    312 GiB |    312 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80026 MiB |  80416 MiB | 144076 MiB |  64050 MiB |
|       from large pool |  79998 MiB |  80110 MiB | 143252 MiB |  63254 MiB |
|       from small pool |     28 MiB |    306 MiB |    824 MiB |    796 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   9002 MiB |  10232 MiB |  63156 GiB |  63147 GiB |
|       from large pool |   8996 MiB |  10227 MiB |  62792 GiB |  62783 GiB |
|       from small pool |      5 MiB |     15 MiB |    363 GiB |    363 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     647    |    3155 K  |    3154 K  |
|       from large pool |     335    |     341    |    1261 K  |    1261 K  |
|       from small pool |     306    |     336    |    1893 K  |    1892 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     647    |    3155 K  |    3154 K  |
|       from large pool |     335    |     341    |    1261 K  |    1261 K  |
|       from small pool |     306    |     336    |    1893 K  |    1892 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     174    |     315    |    1049    |     875    |
|       from large pool |     160    |     162    |     637    |     477    |
|       from small pool |      14    |     153    |     412    |     398    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     174    |     174    |    1912 K  |    1911 K  |
|       from large pool |     150    |     150    |     810 K  |     810 K  |
|       from small pool |      24    |      38    |    1101 K  |    1101 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:51:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:51:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:51:49] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:51:49] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:51:50]    INFO >> epoch 002:    286 / 770 loss=5.462, wps=4755.3, ups=3.88, wpb=1226, bsz=1226, num_updates=1050, lr=0.0004, gnorm=6.592, clip=0, train_wall=12, gb_free=66.1, wall=356 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:51:57] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.83 GiB is free. Including non-PyTorch memory, this process has 77.29 GiB memory in use. Of the allocated memory 72.83 GiB is allocated by PyTorch, and 3.96 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:51:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 7            |        cudaMalloc retries: 10        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73958 MiB |  75093 MiB |  68316 GiB |  68244 GiB |
|       from large pool |  73944 MiB |  75079 MiB |  67996 GiB |  67924 GiB |
|       from small pool |     14 MiB |     15 MiB |    319 GiB |    319 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73958 MiB |  75093 MiB |  68316 GiB |  68244 GiB |
|       from large pool |  73944 MiB |  75079 MiB |  67996 GiB |  67924 GiB |
|       from small pool |     14 MiB |     15 MiB |    319 GiB |    319 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73942 MiB |  75078 MiB |  68232 GiB |  68160 GiB |
|       from large pool |  73928 MiB |  75064 MiB |  67913 GiB |  67841 GiB |
|       from small pool |     14 MiB |     15 MiB |    319 GiB |    319 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78632 MiB |  80028 MiB | 146688 MiB |  68056 MiB |
|       from large pool |  78612 MiB |  79980 MiB | 145844 MiB |  67232 MiB |
|       from small pool |     20 MiB |     48 MiB |    844 MiB |    824 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4133 MiB |  10023 MiB |  64651 GiB |  64647 GiB |
|       from large pool |   4127 MiB |  10016 MiB |  64280 GiB |  64276 GiB |
|       from small pool |      5 MiB |     19 MiB |    371 GiB |    371 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |    3225 K  |    3224 K  |
|       from large pool |     276    |     284    |    1294 K  |    1293 K  |
|       from small pool |     290    |     336    |    1931 K  |    1930 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |    3225 K  |    3224 K  |
|       from large pool |     276    |     284    |    1294 K  |    1293 K  |
|       from small pool |     290    |     336    |    1931 K  |    1930 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     108    |     183    |    1064    |     956    |
|       from large pool |      98    |     159    |     642    |     544    |
|       from small pool |      10    |      24    |     422    |     412    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      97    |      99    |    1954 K  |    1954 K  |
|       from large pool |      75    |      77    |     832 K  |     831 K  |
|       from small pool |      22    |      43    |    1122 K  |    1122 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:51:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:51:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:51:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:51:57] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:52:05]    INFO >> epoch 002:    337 / 770 loss=5.404, wps=5288.1, ups=3.64, wpb=1453, bsz=1453, num_updates=1100, lr=0.0004, gnorm=6.202, clip=0, train_wall=12, gb_free=61.6, wall=370 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:52:20]    INFO >> epoch 002:    387 / 770 loss=5.343, wps=5134.6, ups=3.55, wpb=1447.3, bsz=1447.3, num_updates=1150, lr=0.0004, gnorm=7.322, clip=0, train_wall=13, gb_free=58.3, wall=384 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:52:32]    INFO >> epoch 002:    437 / 770 loss=5.326, wps=4993.9, ups=3.89, wpb=1282.9, bsz=1282.9, num_updates=1200, lr=0.0004, gnorm=6.173, clip=0, train_wall=12, gb_free=62, wall=397 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:52:39] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 519.25 MiB is free. Including non-PyTorch memory, this process has 78.61 GiB memory in use. Of the allocated memory 76.04 GiB is allocated by PyTorch, and 2.07 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:52:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 8            |        cudaMalloc retries: 11        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75059 MiB |  77863 MiB |  76990 GiB |  76917 GiB |
|       from large pool |  75047 MiB |  77851 MiB |  76635 GiB |  76562 GiB |
|       from small pool |     11 MiB |     13 MiB |    354 GiB |    354 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75059 MiB |  77863 MiB |  76990 GiB |  76917 GiB |
|       from large pool |  75047 MiB |  77851 MiB |  76635 GiB |  76562 GiB |
|       from small pool |     11 MiB |     13 MiB |    354 GiB |    354 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75041 MiB |  77845 MiB |  76897 GiB |  76823 GiB |
|       from large pool |  75030 MiB |  77834 MiB |  76542 GiB |  76469 GiB |
|       from small pool |     11 MiB |     13 MiB |    354 GiB |    354 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79986 MiB |  80064 MiB | 148660 MiB |  68674 MiB |
|       from large pool |  79966 MiB |  79966 MiB | 147738 MiB |  67772 MiB |
|       from small pool |     20 MiB |     98 MiB |    922 MiB |    902 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3032 MiB |  10751 MiB |  73722 GiB |  73719 GiB |
|       from large pool |   3024 MiB |  10742 MiB |  73310 GiB |  73307 GiB |
|       from small pool |      8 MiB |     17 MiB |    412 GiB |    412 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |    3600 K  |    3599 K  |
|       from large pool |     349    |     357    |    1460 K  |    1460 K  |
|       from small pool |     292    |     336    |    2139 K  |    2138 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |    3600 K  |    3599 K  |
|       from large pool |     349    |     357    |    1460 K  |    1460 K  |
|       from small pool |     292    |     336    |    2139 K  |    2138 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     108    |     147    |    1104    |     996    |
|       from large pool |      98    |      98    |     643    |     545    |
|       from small pool |      10    |      49    |     461    |     451    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     100    |     108    |    2173 K  |    2173 K  |
|       from large pool |      79    |      87    |     934 K  |     934 K  |
|       from small pool |      21    |      37    |    1239 K  |    1239 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:52:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:52:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:52:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:52:39] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:52:46]    INFO >> epoch 002:    488 / 770 loss=5.255, wps=5007.5, ups=3.96, wpb=1264.4, bsz=1264.4, num_updates=1250, lr=0.0004, gnorm=6.498, clip=0, train_wall=11, gb_free=64.4, wall=409 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:52:59]    INFO >> epoch 002:    538 / 770 loss=5.066, wps=5316.7, ups=3.8, wpb=1398.1, bsz=1398.1, num_updates=1300, lr=0.0004, gnorm=7.324, clip=2, train_wall=12, gb_free=62.7, wall=422 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:53:11] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 27.25 MiB is free. Including non-PyTorch memory, this process has 79.09 GiB memory in use. Of the allocated memory 77.69 GiB is allocated by PyTorch, and 924.18 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:53:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 9            |        cudaMalloc retries: 12        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79495 MiB |  79553 MiB |  83741 GiB |  83663 GiB |
|       from large pool |  79222 MiB |  79281 MiB |  83355 GiB |  83277 GiB |
|       from small pool |    272 MiB |    273 MiB |    386 GiB |    385 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79495 MiB |  79553 MiB |  83741 GiB |  83663 GiB |
|       from large pool |  79222 MiB |  79281 MiB |  83355 GiB |  83277 GiB |
|       from small pool |    272 MiB |    273 MiB |    386 GiB |    385 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79473 MiB |  79531 MiB |  83639 GiB |  83562 GiB |
|       from large pool |  79202 MiB |  79260 MiB |  83254 GiB |  83176 GiB |
|       from small pool |    271 MiB |    272 MiB |    385 GiB |    385 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80478 MiB |  80480 MiB | 151048 MiB |  70570 MiB |
|       from large pool |  80172 MiB |  80172 MiB | 149838 MiB |  69666 MiB |
|       from small pool |    306 MiB |    308 MiB |   1210 MiB |    904 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    922 MiB |   8436 MiB |  80931 GiB |  80930 GiB |
|       from large pool |    889 MiB |   8428 MiB |  80482 GiB |  80481 GiB |
|       from small pool |     33 MiB |     35 MiB |    449 GiB |    449 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    5261    |    5264    |    3930 K  |    3925 K  |
|       from large pool |     756    |     757    |    1598 K  |    1597 K  |
|       from small pool |    4505    |    4508    |    2331 K  |    2327 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    5261    |    5264    |    3930 K  |    3925 K  |
|       from large pool |     756    |     757    |    1598 K  |    1597 K  |
|       from small pool |    4505    |    4508    |    2331 K  |    2327 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     285    |     286    |    1283    |     998    |
|       from large pool |     132    |     132    |     678    |     546    |
|       from small pool |     153    |     154    |     605    |     452    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     394    |     395    |    2369 K  |    2369 K  |
|       from large pool |     115    |     116    |    1019 K  |    1019 K  |
|       from small pool |     279    |     280    |    1350 K  |    1350 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:53:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:53:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:53:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:53:11] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:53:15]    INFO >> epoch 002:    589 / 770 loss=5.152, wps=4956.6, ups=3.52, wpb=1407, bsz=1407, num_updates=1350, lr=0.0004, gnorm=5.75, clip=0, train_wall=13, gb_free=52.9, wall=437 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:53:28]    INFO >> epoch 002:    639 / 770 loss=4.906, wps=4995.4, ups=3.73, wpb=1340.3, bsz=1340.3, num_updates=1400, lr=0.0004, gnorm=6.359, clip=0, train_wall=13, gb_free=65.2, wall=450 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:53:41]    INFO >> epoch 002:    689 / 770 loss=4.789, wps=5924.9, ups=3.87, wpb=1529.8, bsz=1529.8, num_updates=1450, lr=0.0004, gnorm=6.853, clip=0, train_wall=12, gb_free=66.6, wall=463 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:53:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 79.14 GiB of which 375.25 MiB is free. Including non-PyTorch memory, this process has 78.75 GiB memory in use. Of the allocated memory 76.83 GiB is allocated by PyTorch, and 1.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:53:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 10           |        cudaMalloc retries: 13        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78197 MiB |  79033 MiB |  93360 GiB |  93284 GiB |
|       from large pool |  78184 MiB |  79020 MiB |  92932 GiB |  92856 GiB |
|       from small pool |     12 MiB |     16 MiB |    427 GiB |    427 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78197 MiB |  79033 MiB |  93360 GiB |  93284 GiB |
|       from large pool |  78184 MiB |  79020 MiB |  92932 GiB |  92856 GiB |
|       from small pool |     12 MiB |     16 MiB |    427 GiB |    427 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78178 MiB |  79013 MiB |  93247 GiB |  93171 GiB |
|       from large pool |  78165 MiB |  79000 MiB |  92820 GiB |  92743 GiB |
|       from small pool |     12 MiB |     16 MiB |    427 GiB |    427 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80130 MiB |  80418 MiB | 151048 MiB |  70918 MiB |
|       from large pool |  80112 MiB |  80112 MiB | 149838 MiB |  69726 MiB |
|       from small pool |     18 MiB |    306 MiB |   1210 MiB |   1192 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1932 MiB |   8935 MiB |  89964 GiB |  89962 GiB |
|       from large pool |   1927 MiB |   8928 MiB |  89465 GiB |  89463 GiB |
|       from small pool |      5 MiB |     15 MiB |    498 GiB |    498 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |    4365 K  |    4364 K  |
|       from large pool |     314    |     322    |    1785 K  |    1785 K  |
|       from small pool |     291    |     342    |    2579 K  |    2578 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |    4365 K  |    4364 K  |
|       from large pool |     314    |     322    |    1785 K  |    1785 K  |
|       from small pool |     291    |     342    |    2579 K  |    2578 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     140    |     284    |    1283    |    1143    |
|       from large pool |     131    |     131    |     678    |     547    |
|       from small pool |       9    |     153    |     605    |     596    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     120    |     126    |    2634 K  |    2634 K  |
|       from large pool |     101    |     107    |    1141 K  |    1141 K  |
|       from small pool |      19    |      39    |    1493 K  |    1493 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:53:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:53:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:53:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:53:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:53:57]    INFO >> epoch 002:    740 / 770 loss=4.718, wps=5362.1, ups=3.46, wpb=1550.6, bsz=1550.6, num_updates=1500, lr=0.0004, gnorm=6.452, clip=0, train_wall=13, gb_free=62.4, wall=477 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:54:06]    INFO >> epoch 002 | loss 5.217 | wps 4908.2 | ups 3.46 | wpb 1419.5 | bsz 1419.5 | num_updates 1530 | lr 0.0004 | gnorm 6.773 | clip 0.4 | train_wall 193 | gb_free 11.6 | wall 486 (progress_bar.py:267, print())[0m
[33m[2025-11-19 04:54:06] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:54:20]    INFO >> epoch 002 | valid on 'valid' subset | loss 4.535 | wps 11640 | wpb 5412.5 | bsz 5412.5 | num_updates 1530 | best_loss 5.742 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 04:54:21]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 04:54:21]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/checkpoints/checkpoint_last.pt (epoch 2 @ 1530 updates, score 4.535) (writing took 0.014064 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 04:54:21] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:54:25]    INFO >> epoch 003:     20 / 770 loss=4.557, wps=2873.5, ups=1.88, wpb=1525.7, bsz=1525.7, num_updates=1550, lr=0.000392, gnorm=6.42, clip=0, train_wall=12, gb_free=68.4, wall=504 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:54:38]    INFO >> epoch 003:     70 / 770 loss=4.535, wps=5227.6, ups=3.95, wpb=1323.9, bsz=1323.9, num_updates=1600, lr=0.000392, gnorm=6.949, clip=0, train_wall=12, gb_free=65.2, wall=517 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:54:52]    INFO >> epoch 003:    120 / 770 loss=4.476, wps=5063.2, ups=3.81, wpb=1327.2, bsz=1327.2, num_updates=1650, lr=0.000392, gnorm=7.404, clip=0, train_wall=12, gb_free=66.9, wall=530 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:55:06]    INFO >> epoch 003:    170 / 770 loss=4.48, wps=5315.9, ups=3.73, wpb=1425.5, bsz=1425.5, num_updates=1700, lr=0.000392, gnorm=6.381, clip=0, train_wall=13, gb_free=63.3, wall=543 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:55:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 35.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 76.94 GiB is allocated by PyTorch, and 1.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:55:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 11           |        cudaMalloc retries: 14        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78729 MiB |  78787 MiB | 110266 GiB | 110189 GiB |
|       from large pool |  78464 MiB |  78522 MiB | 109743 GiB | 109666 GiB |
|       from small pool |    265 MiB |    266 MiB |    522 GiB |    522 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78729 MiB |  78787 MiB | 110266 GiB | 110189 GiB |
|       from large pool |  78464 MiB |  78522 MiB | 109743 GiB | 109666 GiB |
|       from small pool |    265 MiB |    266 MiB |    522 GiB |    522 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78711 MiB |  78769 MiB | 110132 GiB | 110055 GiB |
|       from large pool |  78447 MiB |  78505 MiB | 109610 GiB | 109533 GiB |
|       from small pool |    263 MiB |    264 MiB |    521 GiB |    521 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80470 MiB |  80476 MiB | 151394 MiB |  70924 MiB |
|       from large pool |  80172 MiB |  80172 MiB | 149898 MiB |  69726 MiB |
|       from small pool |    298 MiB |    304 MiB |   1496 MiB |   1198 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1680 MiB |   9815 MiB | 104237 GiB | 104235 GiB |
|       from large pool |   1647 MiB |   9807 MiB | 103633 GiB | 103631 GiB |
|       from small pool |     32 MiB |     33 MiB |    603 GiB |    603 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    5118    |    5121    |    5243 K  |    5238 K  |
|       from large pool |     743    |     744    |    2075 K  |    2074 K  |
|       from small pool |    4375    |    4378    |    3168 K  |    3164 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    5118    |    5121    |    5243 K  |    5238 K  |
|       from large pool |     743    |     744    |    2075 K  |    2074 K  |
|       from small pool |    4375    |    4378    |    3168 K  |    3164 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     281    |     284    |    1427    |    1146    |
|       from large pool |     132    |     132    |     679    |     547    |
|       from small pool |     149    |     152    |     748    |     599    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     382    |     383    |    3182 K  |    3182 K  |
|       from large pool |     114    |     115    |    1334 K  |    1334 K  |
|       from small pool |     268    |     268    |    1848 K  |    1847 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:55:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:55:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:55:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:55:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-19 04:55:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.74 GiB is free. Including non-PyTorch memory, this process has 77.38 GiB memory in use. Of the allocated memory 72.83 GiB is allocated by PyTorch, and 4.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:55:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 12           |        cudaMalloc retries: 16        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73958 MiB |  75094 MiB | 111621 GiB | 111549 GiB |
|       from large pool |  73944 MiB |  75080 MiB | 111093 GiB | 111020 GiB |
|       from small pool |     14 MiB |     15 MiB |    528 GiB |    528 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73958 MiB |  75094 MiB | 111621 GiB | 111549 GiB |
|       from large pool |  73944 MiB |  75080 MiB | 111093 GiB | 111020 GiB |
|       from small pool |     14 MiB |     15 MiB |    528 GiB |    528 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73942 MiB |  75078 MiB | 111485 GiB | 111413 GiB |
|       from large pool |  73928 MiB |  75064 MiB | 110958 GiB | 110886 GiB |
|       from small pool |     14 MiB |     15 MiB |    527 GiB |    527 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78728 MiB |  80410 MiB | 151970 MiB |  73242 MiB |
|       from large pool |  78708 MiB |  80112 MiB | 150474 MiB |  71766 MiB |
|       from small pool |     20 MiB |    298 MiB |   1496 MiB |   1476 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4709 MiB |  10274 MiB | 105519 GiB | 105515 GiB |
|       from large pool |   4703 MiB |  10267 MiB | 104908 GiB | 104904 GiB |
|       from small pool |      5 MiB |     17 MiB |    610 GiB |    610 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |    5303 K  |    5303 K  |
|       from large pool |     276    |     284    |    2100 K  |    2099 K  |
|       from small pool |     290    |     336    |    3203 K  |    3203 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |    5303 K  |    5303 K  |
|       from large pool |     276    |     284    |    2100 K  |    2099 K  |
|       from small pool |     290    |     336    |    3203 K  |    3203 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     109    |     280    |    1428    |    1319    |
|       from large pool |      99    |     131    |     680    |     581    |
|       from small pool |      10    |     149    |     748    |     738    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     103    |     103    |    3220 K  |    3220 K  |
|       from large pool |      82    |      82    |    1350 K  |    1350 K  |
|       from small pool |      21    |      44    |    1870 K  |    1870 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:55:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:55:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:55:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:55:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:55:20]    INFO >> epoch 003:    222 / 770 loss=4.414, wps=4982.4, ups=3.48, wpb=1429.9, bsz=1429.9, num_updates=1750, lr=0.000392, gnorm=7.899, clip=2, train_wall=13, gb_free=65.1, wall=557 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:55:30] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 79.14 GiB of which 497.25 MiB is free. Including non-PyTorch memory, this process has 78.63 GiB memory in use. Of the allocated memory 76.83 GiB is allocated by PyTorch, and 1.30 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:55:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 13           |        cudaMalloc retries: 17        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78198 MiB |  79035 MiB | 114723 GiB | 114646 GiB |
|       from large pool |  78186 MiB |  79022 MiB | 114181 GiB | 114104 GiB |
|       from small pool |     12 MiB |     14 MiB |    541 GiB |    541 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78198 MiB |  79035 MiB | 114723 GiB | 114646 GiB |
|       from large pool |  78186 MiB |  79022 MiB | 114181 GiB | 114104 GiB |
|       from small pool |     12 MiB |     14 MiB |    541 GiB |    541 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78178 MiB |  79013 MiB | 114584 GiB | 114507 GiB |
|       from large pool |  78165 MiB |  79000 MiB | 114043 GiB | 113966 GiB |
|       from small pool |     12 MiB |     14 MiB |    540 GiB |    540 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80008 MiB |  80132 MiB | 153434 MiB |  73426 MiB |
|       from large pool |  79992 MiB |  79992 MiB | 151818 MiB |  71826 MiB |
|       from small pool |     16 MiB |    140 MiB |   1616 MiB |   1600 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1809 MiB |   7468 MiB | 108749 GiB | 108748 GiB |
|       from large pool |   1805 MiB |   7463 MiB | 108123 GiB | 108121 GiB |
|       from small pool |      3 MiB |     25 MiB |    626 GiB |    626 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |    5441 K  |    5440 K  |
|       from large pool |     314    |     322    |    2158 K  |    2158 K  |
|       from small pool |     291    |     342    |    3283 K  |    3282 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |    5441 K  |    5440 K  |
|       from large pool |     314    |     322    |    2158 K  |    2158 K  |
|       from small pool |     291    |     342    |    3283 K  |    3282 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     109    |     171    |    1491    |    1382    |
|       from large pool |     101    |     101    |     683    |     582    |
|       from small pool |       8    |      70    |     808    |     800    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      91    |      93    |    3303 K  |    3303 K  |
|       from large pool |      70    |      72    |    1386 K  |    1386 K  |
|       from small pool |      21    |      51    |    1917 K  |    1916 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:55:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:55:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:55:30] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:55:30] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-19 04:55:31] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 515.25 MiB is free. Including non-PyTorch memory, this process has 78.61 GiB memory in use. Of the allocated memory 73.73 GiB is allocated by PyTorch, and 4.39 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:55:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 14           |        cudaMalloc retries: 18        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75059 MiB |  79035 MiB | 114880 GiB | 114806 GiB |
|       from large pool |  75047 MiB |  79022 MiB | 114338 GiB | 114265 GiB |
|       from small pool |     11 MiB |     14 MiB |    541 GiB |    541 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75059 MiB |  79035 MiB | 114880 GiB | 114806 GiB |
|       from large pool |  75047 MiB |  79022 MiB | 114338 GiB | 114265 GiB |
|       from small pool |     11 MiB |     14 MiB |    541 GiB |    541 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75041 MiB |  79013 MiB | 114741 GiB | 114667 GiB |
|       from large pool |  75030 MiB |  79000 MiB | 114200 GiB | 114126 GiB |
|       from small pool |     11 MiB |     14 MiB |    540 GiB |    540 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79990 MiB |  80132 MiB | 153434 MiB |  73444 MiB |
|       from large pool |  79974 MiB |  79992 MiB | 151818 MiB |  71844 MiB |
|       from small pool |     16 MiB |    140 MiB |   1616 MiB |   1600 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4930 MiB |  29790 MiB | 108901 GiB | 108896 GiB |
|       from large pool |   4926 MiB |  29785 MiB | 108274 GiB | 108269 GiB |
|       from small pool |      4 MiB |     25 MiB |    626 GiB |    626 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     647    |    5442 K  |    5441 K  |
|       from large pool |     349    |     355    |    2159 K  |    2158 K  |
|       from small pool |     292    |     342    |    3283 K  |    3282 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     647    |    5442 K  |    5441 K  |
|       from large pool |     349    |     355    |    2159 K  |    2158 K  |
|       from small pool |     292    |     342    |    3283 K  |    3282 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     108    |     171    |    1491    |    1383    |
|       from large pool |     100    |     101    |     683    |     583    |
|       from small pool |       8    |      70    |     808    |     800    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     105    |     109    |    3303 K  |    3303 K  |
|       from large pool |      85    |      88    |    1386 K  |    1386 K  |
|       from small pool |      20    |      51    |    1917 K  |    1917 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:55:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:55:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:55:31] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:55:31] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:55:37]    INFO >> epoch 003:    274 / 770 loss=4.555, wps=4771.9, ups=3.26, wpb=1463.7, bsz=1463.7, num_updates=1800, lr=0.000392, gnorm=6.307, clip=0, train_wall=13, gb_free=63.1, wall=573 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:55:50]    INFO >> epoch 003:    324 / 770 loss=4.434, wps=5201.6, ups=3.85, wpb=1350.3, bsz=1350.3, num_updates=1850, lr=0.000392, gnorm=6.751, clip=0, train_wall=12, gb_free=67.1, wall=586 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:56:04]    INFO >> epoch 003:    374 / 770 loss=4.255, wps=5354.3, ups=3.7, wpb=1448.8, bsz=1448.8, num_updates=1900, lr=0.000392, gnorm=7.69, clip=2, train_wall=13, gb_free=66.4, wall=599 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:56:17]    INFO >> epoch 003:    424 / 770 loss=4.382, wps=4966.5, ups=3.85, wpb=1290.7, bsz=1290.7, num_updates=1950, lr=0.000392, gnorm=6.051, clip=0, train_wall=12, gb_free=56.8, wall=612 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:56:33]    INFO >> epoch 003:    474 / 770 loss=4.306, wps=5485.9, ups=3.59, wpb=1526.7, bsz=1526.7, num_updates=2000, lr=0.000392, gnorm=6.193, clip=0, train_wall=13, gb_free=66.6, wall=626 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:56:47]    INFO >> epoch 003:    524 / 770 loss=4.129, wps=5432.7, ups=3.61, wpb=1505.5, bsz=1505.5, num_updates=2050, lr=0.000392, gnorm=7.061, clip=0, train_wall=13, gb_free=54.5, wall=640 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:56:57] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 810.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 503.25 MiB is free. Including non-PyTorch memory, this process has 78.62 GiB memory in use. Of the allocated memory 75.43 GiB is allocated by PyTorch, and 2.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:56:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 15           |        cudaMalloc retries: 19        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71995 MiB |  77398 MiB | 133184 GiB | 133114 GiB |
|       from large pool |  71972 MiB |  77375 MiB | 132560 GiB | 132490 GiB |
|       from small pool |     22 MiB |     22 MiB |    623 GiB |    623 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71995 MiB |  77398 MiB | 133184 GiB | 133114 GiB |
|       from large pool |  71972 MiB |  77375 MiB | 132560 GiB | 132490 GiB |
|       from small pool |     22 MiB |     22 MiB |    623 GiB |    623 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  71978 MiB |  77379 MiB | 133025 GiB | 132954 GiB |
|       from large pool |  71955 MiB |  77356 MiB | 132402 GiB | 132332 GiB |
|       from small pool |     22 MiB |     22 MiB |    622 GiB |    622 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80002 MiB |  80278 MiB | 153722 MiB |  73720 MiB |
|       from large pool |  79974 MiB |  79974 MiB | 151818 MiB |  71844 MiB |
|       from small pool |     28 MiB |    304 MiB |   1904 MiB |   1876 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5274 MiB |   8320 MiB | 128078 GiB | 128072 GiB |
|       from large pool |   5269 MiB |   8315 MiB | 127354 GiB | 127349 GiB |
|       from small pool |      5 MiB |     13 MiB |    723 GiB |    723 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     643    |     661    |    6291 K  |    6290 K  |
|       from large pool |     336    |     354    |    2521 K  |    2520 K  |
|       from small pool |     307    |     336    |    3770 K  |    3769 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     643    |     661    |    6291 K  |    6290 K  |
|       from large pool |     336    |     354    |    2521 K  |    2520 K  |
|       from small pool |     307    |     336    |    3770 K  |    3769 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     114    |     252    |    1635    |    1521    |
|       from large pool |     100    |     100    |     683    |     583    |
|       from small pool |      14    |     152    |     952    |     938    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     106    |     106    |    3807 K  |    3807 K  |
|       from large pool |      83    |      84    |    1608 K  |    1608 K  |
|       from small pool |      23    |      40    |    2198 K  |    2198 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:56:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:56:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:56:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:56:57] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:57:01]    INFO >> epoch 003:    575 / 770 loss=4.296, wps=5048.6, ups=3.58, wpb=1410.5, bsz=1410.5, num_updates=2100, lr=0.000392, gnorm=5.727, clip=0, train_wall=13, gb_free=62.2, wall=654 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:57:15]    INFO >> epoch 003:    625 / 770 loss=4.151, wps=5049.5, ups=3.69, wpb=1369.9, bsz=1369.9, num_updates=2150, lr=0.000392, gnorm=6.343, clip=0, train_wall=13, gb_free=59, wall=668 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:57:28]    INFO >> epoch 003:    675 / 770 loss=4.19, wps=5086.4, ups=3.84, wpb=1324.8, bsz=1324.8, num_updates=2200, lr=0.000392, gnorm=6.087, clip=0, train_wall=12, gb_free=57.9, wall=681 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:57:44]    INFO >> epoch 003:    725 / 770 loss=4.235, wps=5753.7, ups=3.46, wpb=1662.7, bsz=1662.7, num_updates=2250, lr=0.000392, gnorm=7.434, clip=2, train_wall=14, gb_free=67.2, wall=695 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:57:56]    INFO >> epoch 003 | loss 4.334 | wps 4909.7 | ups 3.46 | wpb 1419.5 | bsz 1419.5 | num_updates 2295 | lr 0.000392 | gnorm 6.687 | clip 0.4 | train_wall 194 | gb_free 67.2 | wall 707 (progress_bar.py:267, print())[0m
[33m[2025-11-19 04:57:56] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:58:10]    INFO >> epoch 003 | valid on 'valid' subset | loss 4.095 | wps 11317.9 | wpb 5412.5 | bsz 5412.5 | num_updates 2295 | best_loss 5.742 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 04:58:10]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 04:58:10]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/checkpoints/checkpoint_last.pt (epoch 3 @ 2295 updates, score 4.095) (writing took 0.012658 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 04:58:10] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 04:58:13]    INFO >> epoch 004:      5 / 770 loss=4.133, wps=2659.5, ups=1.85, wpb=1434.5, bsz=1434.5, num_updates=2300, lr=0.000376, gnorm=6.044, clip=0, train_wall=13, gb_free=62.7, wall=722 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:58:26]    INFO >> epoch 004:     55 / 770 loss=4.166, wps=5605.6, ups=3.78, wpb=1481.9, bsz=1481.9, num_updates=2350, lr=0.000376, gnorm=6.361, clip=0, train_wall=13, gb_free=56.4, wall=735 (progress_bar.py:258, log())[0m
[33m[2025-11-19 04:58:38] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 79.14 GiB of which 515.25 MiB is free. Including non-PyTorch memory, this process has 78.61 GiB memory in use. Of the allocated memory 76.83 GiB is allocated by PyTorch, and 1.29 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 04:58:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 16           |        cudaMalloc retries: 20        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78196 MiB |  79032 MiB | 156106 GiB | 156030 GiB |
|       from large pool |  78183 MiB |  79019 MiB | 155360 GiB | 155284 GiB |
|       from small pool |     12 MiB |     15 MiB |    746 GiB |    746 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78196 MiB |  79032 MiB | 156106 GiB | 156030 GiB |
|       from large pool |  78183 MiB |  79019 MiB | 155360 GiB | 155284 GiB |
|       from small pool |     12 MiB |     15 MiB |    746 GiB |    746 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78178 MiB |  79013 MiB | 155921 GiB | 155845 GiB |
|       from large pool |  78165 MiB |  79000 MiB | 155176 GiB | 155100 GiB |
|       from small pool |     12 MiB |     15 MiB |    745 GiB |    745 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79990 MiB |  80278 MiB | 156730 MiB |  76740 MiB |
|       from large pool |  79974 MiB |  79974 MiB | 154550 MiB |  74576 MiB |
|       from small pool |     16 MiB |    304 MiB |   2180 MiB |   2164 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1793 MiB |   5542 MiB | 149839 GiB | 149837 GiB |
|       from large pool |   1790 MiB |   5538 MiB | 148977 GiB | 148975 GiB |
|       from small pool |      3 MiB |     17 MiB |    862 GiB |    862 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |    7450 K  |    7449 K  |
|       from large pool |     314    |     322    |    2924 K  |    2924 K  |
|       from small pool |     291    |     336    |    4525 K  |    4525 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |    7450 K  |    7449 K  |
|       from large pool |     314    |     322    |    2924 K  |    2924 K  |
|       from small pool |     291    |     336    |    4525 K  |    4525 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     108    |     252    |    1774    |    1666    |
|       from large pool |     100    |     100    |     684    |     584    |
|       from small pool |       8    |     152    |    1090    |    1082    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      92    |      92    |    4507 K  |    4506 K  |
|       from large pool |      72    |      72    |    1861 K  |    1861 K  |
|       from small pool |      20    |      44    |    2645 K  |    2645 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:58:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:58:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:58:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 04:58:38] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 04:58:40]    INFO >> epoch 004:    106 / 770 loss=4.124, wps=5148.6, ups=3.5, wpb=1469, bsz=1469, num_updates=2400, lr=0.000376, gnorm=6.898, clip=0, train_wall=13, gb_free=61.1, wall=750 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:58:55]    INFO >> epoch 004:    156 / 770 loss=4.183, wps=5553.1, ups=3.66, wpb=1516.1, bsz=1516.1, num_updates=2450, lr=0.000376, gnorm=6.871, clip=0, train_wall=13, gb_free=67, wall=763 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:59:08]    INFO >> epoch 004:    206 / 770 loss=4.189, wps=5391.6, ups=3.86, wpb=1396, bsz=1396, num_updates=2500, lr=0.000376, gnorm=6.201, clip=0, train_wall=12, gb_free=59.1, wall=776 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:59:23]    INFO >> epoch 004:    256 / 770 loss=4.086, wps=4738.8, ups=3.59, wpb=1321.3, bsz=1321.3, num_updates=2550, lr=0.000376, gnorm=6.41, clip=2, train_wall=13, gb_free=59.3, wall=790 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:59:37]    INFO >> epoch 004:    306 / 770 loss=3.956, wps=5647.3, ups=3.57, wpb=1581.2, bsz=1581.2, num_updates=2600, lr=0.000376, gnorm=7.337, clip=2, train_wall=13, gb_free=61, wall=804 (progress_bar.py:258, log())[0m
[32m[2025-11-19 04:59:51]    INFO >> epoch 004:    356 / 770 loss=4.158, wps=5739.5, ups=3.62, wpb=1587.2, bsz=1587.2, num_updates=2650, lr=0.000376, gnorm=7.094, clip=0, train_wall=13, gb_free=63.1, wall=818 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:00:05]    INFO >> epoch 004:    406 / 770 loss=4.045, wps=5607.4, ups=4.1, wpb=1369, bsz=1369, num_updates=2700, lr=0.000376, gnorm=6.507, clip=0, train_wall=12, gb_free=38, wall=830 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:00:19]    INFO >> epoch 004:    456 / 770 loss=4.162, wps=4789.8, ups=3.51, wpb=1365, bsz=1365, num_updates=2750, lr=0.000376, gnorm=6.256, clip=0, train_wall=13, gb_free=63.1, wall=844 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:00:21] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 0 has a total capacity of 79.14 GiB of which 511.25 MiB is free. Including non-PyTorch memory, this process has 78.62 GiB memory in use. Of the allocated memory 72.84 GiB is allocated by PyTorch, and 5.28 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:00:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 17           |        cudaMalloc retries: 21        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73962 MiB |  75097 MiB | 177795 GiB | 177723 GiB |
|       from large pool |  73948 MiB |  75083 MiB | 176953 GiB | 176881 GiB |
|       from small pool |     14 MiB |     17 MiB |    841 GiB |    841 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73962 MiB |  75097 MiB | 177795 GiB | 177723 GiB |
|       from large pool |  73948 MiB |  75083 MiB | 176953 GiB | 176881 GiB |
|       from small pool |     14 MiB |     17 MiB |    841 GiB |    841 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73942 MiB |  75078 MiB | 177585 GiB | 177513 GiB |
|       from large pool |  73928 MiB |  75064 MiB | 176744 GiB | 176672 GiB |
|       from small pool |     14 MiB |     17 MiB |    840 GiB |    840 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79994 MiB |  80262 MiB | 157002 MiB |  77008 MiB |
|       from large pool |  79974 MiB |  79974 MiB | 154550 MiB |  74576 MiB |
|       from small pool |     20 MiB |    288 MiB |   2452 MiB |   2432 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5491 MiB |  10051 MiB | 172519 GiB | 172513 GiB |
|       from large pool |   5485 MiB |  10044 MiB | 171544 GiB | 171538 GiB |
|       from small pool |      5 MiB |     19 MiB |    975 GiB |    975 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |    8445 K  |    8445 K  |
|       from large pool |     276    |     284    |    3349 K  |    3348 K  |
|       from small pool |     290    |     342    |    5096 K  |    5096 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |    8445 K  |    8445 K  |
|       from large pool |     276    |     284    |    3349 K  |    3348 K  |
|       from small pool |     290    |     342    |    5096 K  |    5096 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     110    |     244    |    1910    |    1800    |
|       from large pool |     100    |     100    |     684    |     584    |
|       from small pool |      10    |     144    |    1226    |    1216    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     104    |     106    |    5099 K  |    5099 K  |
|       from large pool |      82    |      84    |    2122 K  |    2122 K  |
|       from small pool |      22    |      44    |    2976 K  |    2976 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:00:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:00:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:00:21] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:00:21] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-19 05:00:33] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.03 GiB is free. Including non-PyTorch memory, this process has 78.09 GiB memory in use. Of the allocated memory 73.73 GiB is allocated by PyTorch, and 3.86 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:00:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 18           |        cudaMalloc retries: 22        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75060 MiB |  75499 MiB | 180183 GiB | 180109 GiB |
|       from large pool |  75048 MiB |  75487 MiB | 179332 GiB | 179259 GiB |
|       from small pool |     11 MiB |     14 MiB |    850 GiB |    850 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75060 MiB |  75499 MiB | 180183 GiB | 180109 GiB |
|       from large pool |  75048 MiB |  75487 MiB | 179332 GiB | 179259 GiB |
|       from small pool |     11 MiB |     14 MiB |    850 GiB |    850 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75041 MiB |  75480 MiB | 179969 GiB | 179896 GiB |
|       from large pool |  75030 MiB |  75469 MiB | 179120 GiB | 179047 GiB |
|       from small pool |     11 MiB |     14 MiB |    849 GiB |    849 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79450 MiB |  79490 MiB | 157038 MiB |  77588 MiB |
|       from large pool |  79434 MiB |  79434 MiB | 154550 MiB |  75116 MiB |
|       from small pool |     16 MiB |     56 MiB |   2488 MiB |   2472 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4389 MiB |  11378 MiB | 175036 GiB | 175032 GiB |
|       from large pool |   4385 MiB |  11373 MiB | 174050 GiB | 174046 GiB |
|       from small pool |      4 MiB |     23 MiB |    985 GiB |    985 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     647    |    8545 K  |    8544 K  |
|       from large pool |     349    |     355    |    3397 K  |    3396 K  |
|       from small pool |     292    |     342    |    5148 K  |    5148 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     647    |    8545 K  |    8544 K  |
|       from large pool |     349    |     355    |    3397 K  |    3396 K  |
|       from small pool |     292    |     342    |    5148 K  |    5148 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     107    |     127    |    1928    |    1821    |
|       from large pool |      99    |      99    |     684    |     585    |
|       from small pool |       8    |      28    |    1244    |    1236    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      99    |      99    |    5155 K  |    5155 K  |
|       from large pool |      81    |      81    |    2151 K  |    2151 K  |
|       from small pool |      18    |      49    |    3003 K  |    3003 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:00:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:00:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:00:33] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:00:33] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:00:33]    INFO >> epoch 004:    508 / 770 loss=4.158, wps=4556.9, ups=3.77, wpb=1207.1, bsz=1207.1, num_updates=2800, lr=0.000376, gnorm=5.819, clip=0, train_wall=11, gb_free=6.3, wall=858 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:00:46]    INFO >> epoch 004:    558 / 770 loss=3.969, wps=5068.3, ups=3.92, wpb=1294.4, bsz=1294.4, num_updates=2850, lr=0.000376, gnorm=5.825, clip=2, train_wall=12, gb_free=59, wall=870 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:01:02]    INFO >> epoch 004:    608 / 770 loss=3.984, wps=5140.4, ups=3.52, wpb=1460.5, bsz=1460.5, num_updates=2900, lr=0.000376, gnorm=5.793, clip=0, train_wall=13, gb_free=64.6, wall=885 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:01:15]    INFO >> epoch 004:    658 / 770 loss=4.011, wps=5379.4, ups=3.92, wpb=1373.3, bsz=1373.3, num_updates=2950, lr=0.000376, gnorm=6.629, clip=0, train_wall=12, gb_free=71.5, wall=897 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:01:17] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 47.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.47 GiB is allocated by PyTorch, and 1.11 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:01:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 19           |        cudaMalloc retries: 23        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79266 MiB |  79324 MiB | 189563 GiB | 189485 GiB |
|       from large pool |  78995 MiB |  79054 MiB | 188671 GiB | 188594 GiB |
|       from small pool |    270 MiB |    271 MiB |    891 GiB |    891 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79266 MiB |  79324 MiB | 189563 GiB | 189485 GiB |
|       from large pool |  78995 MiB |  79054 MiB | 188671 GiB | 188594 GiB |
|       from small pool |    270 MiB |    271 MiB |    891 GiB |    891 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79239 MiB |  79297 MiB | 189339 GiB | 189262 GiB |
|       from large pool |  78970 MiB |  79028 MiB | 188448 GiB | 188371 GiB |
|       from small pool |    268 MiB |    270 MiB |    890 GiB |    890 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80458 MiB |  80458 MiB | 158046 MiB |  77588 MiB |
|       from large pool |  80154 MiB |  80154 MiB | 155270 MiB |  75116 MiB |
|       from small pool |    304 MiB |    304 MiB |   2776 MiB |   2472 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1131 MiB |   6826 MiB | 184890 GiB | 184889 GiB |
|       from large pool |   1098 MiB |   6820 MiB | 183856 GiB | 183855 GiB |
|       from small pool |     33 MiB |     34 MiB |   1034 GiB |   1034 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    5217    |    5220    |    8978 K  |    8973 K  |
|       from large pool |     752    |     753    |    3582 K  |    3581 K  |
|       from small pool |    4465    |    4468    |    5396 K  |    5391 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    5217    |    5220    |    8978 K  |    8973 K  |
|       from large pool |     752    |     753    |    3582 K  |    3581 K  |
|       from small pool |    4465    |    4468    |    5396 K  |    5391 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     263    |     263    |    2084    |    1821    |
|       from large pool |     111    |     111    |     696    |     585    |
|       from small pool |     152    |     152    |    1388    |    1236    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     365    |     367    |    5410 K  |    5409 K  |
|       from large pool |      94    |      95    |    2265 K  |    2265 K  |
|       from small pool |     271    |     272    |    3144 K  |    3144 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:01:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:01:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:01:17] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:01:17] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:01:29]    INFO >> epoch 004:    709 / 770 loss=4.094, wps=4978.8, ups=3.58, wpb=1391.9, bsz=1391.9, num_updates=3000, lr=0.000376, gnorm=6.48, clip=0, train_wall=13, gb_free=64.5, wall=911 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:01:43]    INFO >> epoch 004:    759 / 770 loss=3.846, wps=5603.1, ups=3.75, wpb=1495.5, bsz=1495.5, num_updates=3050, lr=0.000376, gnorm=5.857, clip=0, train_wall=13, gb_free=60.9, wall=925 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:01:45] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.58 GiB. GPU 0 has a total capacity of 79.14 GiB of which 383.25 MiB is free. Including non-PyTorch memory, this process has 78.74 GiB memory in use. Of the allocated memory 72.89 GiB is allocated by PyTorch, and 5.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:01:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 20           |        cudaMalloc retries: 24        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71007 MiB |  74642 MiB | 195382 GiB | 195313 GiB |
|       from large pool |  70985 MiB |  74619 MiB | 194464 GiB | 194394 GiB |
|       from small pool |     22 MiB |     22 MiB |    918 GiB |    918 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71007 MiB |  74642 MiB | 195382 GiB | 195313 GiB |
|       from large pool |  70985 MiB |  74619 MiB | 194464 GiB | 194394 GiB |
|       from small pool |     22 MiB |     22 MiB |    918 GiB |    918 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  70995 MiB |  74629 MiB | 195152 GiB | 195083 GiB |
|       from large pool |  70972 MiB |  74606 MiB | 194235 GiB | 194165 GiB |
|       from small pool |     22 MiB |     22 MiB |    917 GiB |    917 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80122 MiB |  80398 MiB | 158046 MiB |  77924 MiB |
|       from large pool |  80094 MiB |  80094 MiB | 155270 MiB |  75176 MiB |
|       from small pool |     28 MiB |    304 MiB |   2776 MiB |   2748 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3632 MiB |   9150 MiB | 190804 GiB | 190800 GiB |
|       from large pool |   3626 MiB |   9146 MiB | 189737 GiB | 189734 GiB |
|       from small pool |      5 MiB |     17 MiB |   1066 GiB |   1066 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |    9253 K  |    9252 K  |
|       from large pool |     335    |     343    |    3695 K  |    3695 K  |
|       from small pool |     306    |     336    |    5557 K  |    5557 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |    9253 K  |    9252 K  |
|       from large pool |     335    |     343    |    3695 K  |    3695 K  |
|       from small pool |     306    |     336    |    5557 K  |    5557 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     124    |     262    |    2084    |    1960    |
|       from large pool |     110    |     110    |     696    |     586    |
|       from small pool |      14    |     152    |    1388    |    1374    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     120    |     124    |    5577 K  |    5577 K  |
|       from large pool |      91    |      96    |    2336 K  |    2336 K  |
|       from small pool |      29    |      39    |    3240 K  |    3240 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:01:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:01:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:01:45] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:01:45] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:01:47]    INFO >> epoch 004 | loss 4.071 | wps 4914.5 | ups 3.46 | wpb 1419.5 | bsz 1419.5 | num_updates 3060 | lr 0.000376 | gnorm 6.431 | clip 0.4 | train_wall 193 | gb_free 60.8 | wall 928 (progress_bar.py:267, print())[0m
[33m[2025-11-19 05:01:47] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 05:02:00]    INFO >> epoch 004 | valid on 'valid' subset | loss 4.023 | wps 11382.6 | wpb 5412.5 | bsz 5412.5 | num_updates 3060 | best_loss 5.742 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 05:02:00]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 05:02:00]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/checkpoints/checkpoint_last.pt (epoch 4 @ 3060 updates, score 4.023) (writing took 0.012764 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 05:02:00] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 05:02:12]    INFO >> epoch 005:     40 / 770 loss=3.982, wps=2423.2, ups=1.85, wpb=1312.4, bsz=1312.4, num_updates=3100, lr=0.000354, gnorm=5.816, clip=0, train_wall=12, gb_free=63.9, wall=952 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:02:25]    INFO >> epoch 005:     90 / 770 loss=3.946, wps=4942.7, ups=3.82, wpb=1293.8, bsz=1293.8, num_updates=3150, lr=0.000354, gnorm=5.703, clip=0, train_wall=12, gb_free=63.2, wall=965 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:02:38]    INFO >> epoch 005:    140 / 770 loss=4.023, wps=5398.4, ups=3.78, wpb=1426.4, bsz=1426.4, num_updates=3200, lr=0.000354, gnorm=6.058, clip=0, train_wall=12, gb_free=54.5, wall=978 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:02:54]    INFO >> epoch 005:    190 / 770 loss=3.982, wps=5427.9, ups=3.41, wpb=1593.2, bsz=1593.2, num_updates=3250, lr=0.000354, gnorm=5.678, clip=0, train_wall=14, gb_free=61.9, wall=993 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:03:08]    INFO >> epoch 005:    240 / 770 loss=3.937, wps=5343.3, ups=3.58, wpb=1494.1, bsz=1494.1, num_updates=3300, lr=0.000354, gnorm=6.452, clip=0, train_wall=13, gb_free=57.9, wall=1007 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:03:22]    INFO >> epoch 005:    290 / 770 loss=3.97, wps=5025.9, ups=3.92, wpb=1282, bsz=1282, num_updates=3350, lr=0.000354, gnorm=5.399, clip=0, train_wall=12, gb_free=68.8, wall=1019 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:03:36]    INFO >> epoch 005:    340 / 770 loss=3.865, wps=5112.1, ups=3.51, wpb=1458.3, bsz=1458.3, num_updates=3400, lr=0.000354, gnorm=6.559, clip=0, train_wall=13, gb_free=65.9, wall=1034 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:03:50]    INFO >> epoch 005:    390 / 770 loss=3.904, wps=5255.9, ups=4.12, wpb=1274.2, bsz=1274.2, num_updates=3450, lr=0.000354, gnorm=5.975, clip=0, train_wall=11, gb_free=50.3, wall=1046 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:03:51] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.63 GiB is free. Including non-PyTorch memory, this process has 77.49 GiB memory in use. Of the allocated memory 72.83 GiB is allocated by PyTorch, and 4.15 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:03:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 21           |        cudaMalloc retries: 25        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73960 MiB |  75096 MiB | 223276 GiB | 223204 GiB |
|       from large pool |  73946 MiB |  75082 MiB | 222222 GiB | 222150 GiB |
|       from small pool |     14 MiB |     15 MiB |   1053 GiB |   1053 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73960 MiB |  75096 MiB | 223276 GiB | 223204 GiB |
|       from large pool |  73946 MiB |  75082 MiB | 222222 GiB | 222150 GiB |
|       from small pool |     14 MiB |     15 MiB |   1053 GiB |   1053 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73942 MiB |  75078 MiB | 223013 GiB | 222940 GiB |
|       from large pool |  73928 MiB |  75064 MiB | 221960 GiB | 221888 GiB |
|       from small pool |     14 MiB |     15 MiB |   1052 GiB |   1052 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78836 MiB |  79572 MiB | 162978 MiB |  84142 MiB |
|       from large pool |  78816 MiB |  79416 MiB | 160074 MiB |  81258 MiB |
|       from small pool |     20 MiB |    156 MiB |   2904 MiB |   2884 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4875 MiB |  10095 MiB | 216676 GiB | 216672 GiB |
|       from large pool |   4869 MiB |  10088 MiB | 215456 GiB | 215452 GiB |
|       from small pool |      5 MiB |     25 MiB |   1219 GiB |   1219 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   10577 K  |   10576 K  |
|       from large pool |     276    |     284    |    4193 K  |    4193 K  |
|       from small pool |     290    |     341    |    6383 K  |    6383 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   10577 K  |   10576 K  |
|       from large pool |     276    |     284    |    4193 K  |    4193 K  |
|       from small pool |     290    |     341    |    6383 K  |    6383 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     113    |     191    |    2154    |    2041    |
|       from large pool |     103    |     113    |     702    |     599    |
|       from small pool |      10    |      78    |    1452    |    1442    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     108    |     108    |    6368 K  |    6368 K  |
|       from large pool |      86    |      86    |    2650 K  |    2650 K  |
|       from small pool |      22    |      47    |    3718 K  |    3718 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:03:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:03:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:03:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:03:51] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:04:04]    INFO >> epoch 005:    441 / 770 loss=3.699, wps=5226.5, ups=3.42, wpb=1526.1, bsz=1526.1, num_updates=3500, lr=0.000354, gnorm=6.557, clip=0, train_wall=13, gb_free=65.2, wall=1060 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:04:10] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 79.14 GiB of which 329.25 MiB is free. Including non-PyTorch memory, this process has 78.79 GiB memory in use. Of the allocated memory 76.83 GiB is allocated by PyTorch, and 1.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:04:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 22           |        cudaMalloc retries: 26        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78201 MiB |  79037 MiB | 227492 GiB | 227416 GiB |
|       from large pool |  78189 MiB |  79025 MiB | 226418 GiB | 226342 GiB |
|       from small pool |     12 MiB |     13 MiB |   1073 GiB |   1073 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78201 MiB |  79037 MiB | 227492 GiB | 227416 GiB |
|       from large pool |  78189 MiB |  79025 MiB | 226418 GiB | 226342 GiB |
|       from small pool |     12 MiB |     13 MiB |   1073 GiB |   1073 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78178 MiB |  79013 MiB | 227224 GiB | 227147 GiB |
|       from large pool |  78165 MiB |  79000 MiB | 226152 GiB | 226075 GiB |
|       from small pool |     12 MiB |     13 MiB |   1072 GiB |   1072 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80176 MiB |  80448 MiB | 164590 MiB |  84414 MiB |
|       from large pool |  80160 MiB |  80160 MiB | 161418 MiB |  81258 MiB |
|       from small pool |     16 MiB |    288 MiB |   3172 MiB |   3156 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1974 MiB |   7114 MiB | 221108 GiB | 221106 GiB |
|       from large pool |   1970 MiB |   7110 MiB | 219864 GiB | 219862 GiB |
|       from small pool |      3 MiB |     15 MiB |   1243 GiB |   1243 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   10779 K  |   10779 K  |
|       from large pool |     314    |     322    |    4276 K  |    4276 K  |
|       from small pool |     291    |     336    |    6503 K  |    6503 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   10779 K  |   10779 K  |
|       from large pool |     314    |     322    |    4276 K  |    4276 K  |
|       from small pool |     291    |     336    |    6503 K  |    6503 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     114    |     250    |    2291    |    2177    |
|       from large pool |     106    |     106    |     705    |     599    |
|       from small pool |       8    |     144    |    1586    |    1578    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      89    |      93    |    6491 K  |    6490 K  |
|       from large pool |      70    |      74    |    2700 K  |    2700 K  |
|       from small pool |      19    |      39    |    3790 K  |    3790 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:04:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:04:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:04:10] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:04:10] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:04:18]    INFO >> epoch 005:    492 / 770 loss=3.907, wps=5054.4, ups=3.56, wpb=1418.2, bsz=1418.2, num_updates=3550, lr=0.000354, gnorm=6.257, clip=0, train_wall=13, gb_free=66.2, wall=1074 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:04:33]    INFO >> epoch 005:    542 / 770 loss=4.133, wps=5130.7, ups=3.75, wpb=1366.6, bsz=1366.6, num_updates=3600, lr=0.000354, gnorm=5.979, clip=0, train_wall=13, gb_free=68.1, wall=1088 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:04:36] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 327.25 MiB is free. Including non-PyTorch memory, this process has 78.80 GiB memory in use. Of the allocated memory 73.73 GiB is allocated by PyTorch, and 4.57 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:04:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 23           |        cudaMalloc retries: 27        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75062 MiB |  75501 MiB | 232776 GiB | 232703 GiB |
|       from large pool |  75050 MiB |  75489 MiB | 231679 GiB | 231606 GiB |
|       from small pool |     11 MiB |     16 MiB |   1097 GiB |   1097 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75062 MiB |  75501 MiB | 232776 GiB | 232703 GiB |
|       from large pool |  75050 MiB |  75489 MiB | 231679 GiB | 231606 GiB |
|       from small pool |     11 MiB |     16 MiB |   1097 GiB |   1097 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75041 MiB |  75480 MiB | 232502 GiB | 232429 GiB |
|       from large pool |  75030 MiB |  75469 MiB | 231407 GiB | 231333 GiB |
|       from small pool |     11 MiB |     16 MiB |   1095 GiB |   1095 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80178 MiB |  80464 MiB | 164878 MiB |  84700 MiB |
|       from large pool |  80160 MiB |  80160 MiB | 161418 MiB |  81258 MiB |
|       from small pool |     18 MiB |    304 MiB |   3460 MiB |   3442 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5115 MiB |   9885 MiB | 226608 GiB | 226603 GiB |
|       from large pool |   5109 MiB |   9878 MiB | 225336 GiB | 225331 GiB |
|       from small pool |      6 MiB |     17 MiB |   1271 GiB |   1271 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     647    |   11025 K  |   11024 K  |
|       from large pool |     349    |     355    |    4379 K  |    4379 K  |
|       from small pool |     292    |     336    |    6645 K  |    6645 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     647    |   11025 K  |   11024 K  |
|       from large pool |     349    |     355    |    4379 K  |    4379 K  |
|       from small pool |     292    |     336    |    6645 K  |    6645 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     115    |     258    |    2435    |    2320    |
|       from large pool |     106    |     106    |     705    |     599    |
|       from small pool |       9    |     152    |    1730    |    1721    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     102    |     107    |    6637 K  |    6636 K  |
|       from large pool |      80    |      85    |    2764 K  |    2764 K  |
|       from small pool |      22    |      38    |    3872 K  |    3872 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:04:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:04:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:04:36] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:04:36] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-19 05:04:37] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.41 GiB is allocated by PyTorch, and 1.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:04:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 24           |        cudaMalloc retries: 28        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79206 MiB |  79264 MiB | 233062 GiB | 232984 GiB |
|       from large pool |  78936 MiB |  78994 MiB | 231961 GiB | 231884 GiB |
|       from small pool |    269 MiB |    271 MiB |   1100 GiB |   1100 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79206 MiB |  79264 MiB | 233062 GiB | 232984 GiB |
|       from large pool |  78936 MiB |  78994 MiB | 231961 GiB | 231884 GiB |
|       from small pool |    269 MiB |    271 MiB |   1100 GiB |   1100 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79180 MiB |  79238 MiB | 232788 GiB | 232710 GiB |
|       from large pool |  78912 MiB |  78970 MiB | 231689 GiB | 231611 GiB |
|       from small pool |    268 MiB |    269 MiB |   1098 GiB |   1098 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80464 MiB | 165164 MiB |  84702 MiB |
|       from large pool |  80160 MiB |  80160 MiB | 161418 MiB |  81258 MiB |
|       from small pool |    302 MiB |    304 MiB |   3746 MiB |   3444 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1255 MiB |   7377 MiB | 226945 GiB | 226943 GiB |
|       from large pool |   1223 MiB |   7370 MiB | 225669 GiB | 225668 GiB |
|       from small pool |     32 MiB |     34 MiB |   1275 GiB |   1275 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    5206    |    5209    |   11052 K  |   11047 K  |
|       from large pool |     751    |     752    |    4385 K  |    4384 K  |
|       from small pool |    4455    |    4458    |    6667 K  |    6662 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    5206    |    5209    |   11052 K  |   11047 K  |
|       from large pool |     751    |     752    |    4385 K  |    4384 K  |
|       from small pool |    4455    |    4458    |    6667 K  |    6662 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     257    |     258    |    2578    |    2321    |
|       from large pool |     106    |     106    |     705    |     599    |
|       from small pool |     151    |     152    |    1873    |    1722    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     360    |     361    |    6653 K  |    6653 K  |
|       from large pool |      87    |      87    |    2767 K  |    2767 K  |
|       from small pool |     273    |     274    |    3886 K  |    3885 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:04:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:04:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:04:37] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:04:37] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:04:47]    INFO >> epoch 005:    594 / 770 loss=3.88, wps=4937.1, ups=3.48, wpb=1419.7, bsz=1419.7, num_updates=3650, lr=0.000354, gnorm=6.374, clip=0, train_wall=12, gb_free=60.4, wall=1102 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:05:02]    INFO >> epoch 005:    644 / 770 loss=3.951, wps=5536, ups=3.73, wpb=1484.2, bsz=1484.2, num_updates=3700, lr=0.000354, gnorm=6.231, clip=0, train_wall=13, gb_free=67.4, wall=1116 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:05:15]    INFO >> epoch 005:    694 / 770 loss=3.956, wps=5641.9, ups=3.82, wpb=1476.4, bsz=1476.4, num_updates=3750, lr=0.000354, gnorm=5.993, clip=0, train_wall=12, gb_free=64.1, wall=1129 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:05:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.58 GiB. GPU 0 has a total capacity of 79.14 GiB of which 317.25 MiB is free. Including non-PyTorch memory, this process has 78.81 GiB memory in use. Of the allocated memory 72.89 GiB is allocated by PyTorch, and 5.41 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:05:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 25           |        cudaMalloc retries: 29        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71008 MiB |  74643 MiB | 242101 GiB | 242032 GiB |
|       from large pool |  70986 MiB |  74620 MiB | 240960 GiB | 240891 GiB |
|       from small pool |     22 MiB |     22 MiB |   1141 GiB |   1141 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71008 MiB |  74643 MiB | 242101 GiB | 242032 GiB |
|       from large pool |  70986 MiB |  74620 MiB | 240960 GiB | 240891 GiB |
|       from small pool |     22 MiB |     22 MiB |   1141 GiB |   1141 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  70995 MiB |  74629 MiB | 241817 GiB | 241747 GiB |
|       from large pool |  70972 MiB |  74606 MiB | 240677 GiB | 240608 GiB |
|       from small pool |     22 MiB |     22 MiB |   1139 GiB |   1139 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80188 MiB |  80462 MiB | 165164 MiB |  84976 MiB |
|       from large pool |  80160 MiB |  80160 MiB | 161418 MiB |  81258 MiB |
|       from small pool |     28 MiB |    302 MiB |   3746 MiB |   3718 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6447 MiB |   9064 MiB | 236307 GiB | 236301 GiB |
|       from large pool |   6441 MiB |   9058 MiB | 234984 GiB | 234978 GiB |
|       from small pool |      5 MiB |     15 MiB |   1323 GiB |   1323 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   11470 K  |   11469 K  |
|       from large pool |     335    |     343    |    4562 K  |    4562 K  |
|       from small pool |     306    |     336    |    6907 K  |    6907 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   11470 K  |   11469 K  |
|       from large pool |     335    |     343    |    4562 K  |    4562 K  |
|       from small pool |     306    |     336    |    6907 K  |    6907 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     120    |     257    |    2578    |    2458    |
|       from large pool |     106    |     106    |     705    |     599    |
|       from small pool |      14    |     151    |    1873    |    1859    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     113    |     116    |    6902 K  |    6902 K  |
|       from large pool |      91    |      95    |    2876 K  |    2876 K  |
|       from small pool |      22    |      39    |    4026 K  |    4026 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:05:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:05:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:05:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:05:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:05:34]    INFO >> epoch 005:    745 / 770 loss=3.961, wps=3905.4, ups=2.8, wpb=1394.4, bsz=1394.4, num_updates=3800, lr=0.000354, gnorm=5.728, clip=0, train_wall=16, gb_free=65.5, wall=1147 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:05:41]    INFO >> epoch 005 | loss 3.928 | wps 4823.7 | ups 3.4 | wpb 1419.5 | bsz 1419.5 | num_updates 3825 | lr 0.000354 | gnorm 6.062 | clip 0 | train_wall 197 | gb_free 66.3 | wall 1153 (progress_bar.py:267, print())[0m
[33m[2025-11-19 05:05:41] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 05:05:54]    INFO >> epoch 005 | valid on 'valid' subset | loss 3.886 | wps 11620.3 | wpb 5412.5 | bsz 5412.5 | num_updates 3825 | best_loss 5.742 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 05:05:55]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 05:05:55]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/checkpoints/checkpoint_last.pt (epoch 5 @ 3825 updates, score 3.886) (writing took 0.012576 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 05:05:55] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 05:06:03]    INFO >> epoch 006:     25 / 770 loss=3.716, wps=2890, ups=1.84, wpb=1574.6, bsz=1574.6, num_updates=3850, lr=0.000327, gnorm=6.71, clip=0, train_wall=13, gb_free=63.9, wall=1174 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:06:17]    INFO >> epoch 006:     75 / 770 loss=3.873, wps=5612.3, ups=3.66, wpb=1534.1, bsz=1534.1, num_updates=3900, lr=0.000327, gnorm=5.816, clip=0, train_wall=13, gb_free=61, wall=1187 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:06:31]    INFO >> epoch 006:    125 / 770 loss=3.838, wps=5474.1, ups=3.54, wpb=1545.6, bsz=1545.6, num_updates=3950, lr=0.000327, gnorm=5.92, clip=0, train_wall=13, gb_free=65.2, wall=1202 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:06:46]    INFO >> epoch 006:    175 / 770 loss=3.776, wps=5205.7, ups=3.73, wpb=1397.1, bsz=1397.1, num_updates=4000, lr=0.000327, gnorm=6.434, clip=2, train_wall=13, gb_free=64.3, wall=1215 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:06:58] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.74 GiB is free. Including non-PyTorch memory, this process has 77.38 GiB memory in use. Of the allocated memory 72.83 GiB is allocated by PyTorch, and 4.05 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:06:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 26           |        cudaMalloc retries: 30        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73958 MiB |  75095 MiB | 263554 GiB | 263482 GiB |
|       from large pool |  73944 MiB |  75080 MiB | 262295 GiB | 262223 GiB |
|       from small pool |     14 MiB |     15 MiB |   1259 GiB |   1259 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73958 MiB |  75095 MiB | 263554 GiB | 263482 GiB |
|       from large pool |  73944 MiB |  75080 MiB | 262295 GiB | 262223 GiB |
|       from small pool |     14 MiB |     15 MiB |   1259 GiB |   1259 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73942 MiB |  75078 MiB | 263246 GiB | 263174 GiB |
|       from large pool |  73928 MiB |  75064 MiB | 261989 GiB | 261916 GiB |
|       from small pool |     14 MiB |     15 MiB |   1257 GiB |   1257 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78728 MiB |  79444 MiB | 167152 MiB |  88424 MiB |
|       from large pool |  78708 MiB |  79156 MiB | 163146 MiB |  84438 MiB |
|       from small pool |     20 MiB |    288 MiB |   4006 MiB |   3986 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4229 MiB |   9079 MiB | 256433 GiB | 256429 GiB |
|       from large pool |   4223 MiB |   9072 MiB | 254975 GiB | 254971 GiB |
|       from small pool |      5 MiB |     17 MiB |   1457 GiB |   1457 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     566    |     574    |   12580 K  |   12580 K  |
|       from large pool |     276    |     284    |    4942 K  |    4942 K  |
|       from small pool |     290    |     336    |    7638 K  |    7638 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     566    |     574    |   12580 K  |   12580 K  |
|       from large pool |     276    |     284    |    4942 K  |    4942 K  |
|       from small pool |     290    |     336    |    7638 K  |    7638 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     117    |     252    |    2711    |    2594    |
|       from large pool |     107    |     108    |     708    |     601    |
|       from small pool |      10    |     144    |    2003    |    1993    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     114    |     116    |    7581 K  |    7581 K  |
|       from large pool |      92    |      94    |    3116 K  |    3116 K  |
|       from small pool |      22    |      45    |    4465 K  |    4465 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:06:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:06:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:06:58] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:06:58] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:07:00]    INFO >> epoch 006:    226 / 770 loss=3.954, wps=4921.3, ups=3.57, wpb=1376.8, bsz=1376.8, num_updates=4050, lr=0.000327, gnorm=5.698, clip=0, train_wall=13, gb_free=69.4, wall=1229 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:07:14]    INFO >> epoch 006:    276 / 770 loss=3.841, wps=5130.8, ups=3.82, wpb=1344.2, bsz=1344.2, num_updates=4100, lr=0.000327, gnorm=5.702, clip=0, train_wall=12, gb_free=64.4, wall=1242 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:07:18] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 425.25 MiB is free. Including non-PyTorch memory, this process has 78.70 GiB memory in use. Of the allocated memory 71.88 GiB is allocated by PyTorch, and 6.32 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:07:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 27           |        cudaMalloc retries: 31        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  73168 MiB |  73750 MiB | 267615 GiB | 267544 GiB |
|       from large pool |  73156 MiB |  73739 MiB | 266338 GiB | 266267 GiB |
|       from small pool |     11 MiB |     15 MiB |   1276 GiB |   1276 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  73168 MiB |  73750 MiB | 267615 GiB | 267544 GiB |
|       from large pool |  73156 MiB |  73739 MiB | 266338 GiB | 266267 GiB |
|       from small pool |     11 MiB |     15 MiB |   1276 GiB |   1276 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  73149 MiB |  73732 MiB | 267303 GiB | 267231 GiB |
|       from large pool |  73138 MiB |  73720 MiB | 266028 GiB | 265957 GiB |
|       from small pool |     11 MiB |     15 MiB |   1274 GiB |   1274 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80080 MiB |  80202 MiB | 169166 MiB |  89086 MiB |
|       from large pool |  80062 MiB |  80062 MiB | 165040 MiB |  84978 MiB |
|       from small pool |     18 MiB |    140 MiB |   4126 MiB |   4108 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6911 MiB |  11037 MiB | 260578 GiB | 260571 GiB |
|       from large pool |   6905 MiB |  11030 MiB | 259100 GiB | 259093 GiB |
|       from small pool |      6 MiB |     23 MiB |   1477 GiB |   1477 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     640    |     646    |   12756 K  |   12756 K  |
|       from large pool |     348    |     354    |    5017 K  |    5017 K  |
|       from small pool |     292    |     342    |    7739 K  |    7738 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     640    |     646    |   12756 K  |   12756 K  |
|       from large pool |     348    |     354    |    5017 K  |    5017 K  |
|       from small pool |     292    |     342    |    7739 K  |    7738 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     116    |     177    |    2772    |    2656    |
|       from large pool |     107    |     107    |     709    |     602    |
|       from small pool |       9    |      70    |    2063    |    2054    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     103    |     110    |    7684 K  |    7684 K  |
|       from large pool |      85    |      92    |    3162 K  |    3162 K  |
|       from small pool |      18    |      48    |    4522 K  |    4522 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:07:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:07:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:07:18] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:07:18] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:07:29]    INFO >> epoch 006:    327 / 770 loss=3.806, wps=5374.8, ups=3.43, wpb=1567.6, bsz=1567.6, num_updates=4150, lr=0.000327, gnorm=5.448, clip=0, train_wall=13, gb_free=60.7, wall=1257 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:07:41]    INFO >> epoch 006:    377 / 770 loss=3.865, wps=5214.3, ups=3.93, wpb=1325.2, bsz=1325.2, num_updates=4200, lr=0.000327, gnorm=5.612, clip=0, train_wall=12, gb_free=62.4, wall=1269 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:07:56]    INFO >> epoch 006:    427 / 770 loss=3.779, wps=5299.8, ups=3.9, wpb=1358.9, bsz=1358.9, num_updates=4250, lr=0.000327, gnorm=6.074, clip=0, train_wall=12, gb_free=68.4, wall=1282 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:08:09]    INFO >> epoch 006:    477 / 770 loss=3.835, wps=5000.3, ups=3.63, wpb=1375.9, bsz=1375.9, num_updates=4300, lr=0.000327, gnorm=6.057, clip=0, train_wall=13, gb_free=62.1, wall=1296 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:08:24]    INFO >> epoch 006:    527 / 770 loss=3.929, wps=5588.9, ups=3.72, wpb=1502.3, bsz=1502.3, num_updates=4350, lr=0.000327, gnorm=6.074, clip=0, train_wall=13, gb_free=59.7, wall=1309 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:08:39] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 448.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 423.25 MiB is free. Including non-PyTorch memory, this process has 78.70 GiB memory in use. Of the allocated memory 76.47 GiB is allocated by PyTorch, and 1.73 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:08:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 28           |        cudaMalloc retries: 32        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75124 MiB |  78504 MiB | 284714 GiB | 284640 GiB |
|       from large pool |  75112 MiB |  78491 MiB | 283362 GiB | 283288 GiB |
|       from small pool |     12 MiB |     13 MiB |   1352 GiB |   1352 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75124 MiB |  78504 MiB | 284714 GiB | 284640 GiB |
|       from large pool |  75112 MiB |  78491 MiB | 283362 GiB | 283288 GiB |
|       from small pool |     12 MiB |     13 MiB |   1352 GiB |   1352 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75104 MiB |  78482 MiB | 284382 GiB | 284309 GiB |
|       from large pool |  75091 MiB |  78469 MiB | 283032 GiB | 282959 GiB |
|       from small pool |     12 MiB |     13 MiB |   1350 GiB |   1350 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80082 MiB |  80366 MiB | 169452 MiB |  89370 MiB |
|       from large pool |  80062 MiB |  80062 MiB | 165040 MiB |  84978 MiB |
|       from small pool |     20 MiB |    304 MiB |   4412 MiB |   4392 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2225 MiB |   6304 MiB | 278326 GiB | 278324 GiB |
|       from large pool |   2217 MiB |   6297 MiB | 276759 GiB | 276757 GiB |
|       from small pool |      7 MiB |     17 MiB |   1566 GiB |   1566 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     603    |     612    |   13545 K  |   13544 K  |
|       from large pool |     312    |     320    |    5354 K  |    5354 K  |
|       from small pool |     291    |     336    |    8190 K  |    8190 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     603    |     612    |   13545 K  |   13544 K  |
|       from large pool |     312    |     320    |    5354 K  |    5354 K  |
|       from small pool |     291    |     336    |    8190 K  |    8190 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     117    |     259    |    2915    |    2798    |
|       from large pool |     107    |     107    |     709    |     602    |
|       from small pool |      10    |     152    |    2206    |    2196    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      92    |      99    |    8151 K  |    8151 K  |
|       from large pool |      72    |      79    |    3369 K  |    3369 K  |
|       from small pool |      20    |      41    |    4781 K  |    4781 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:08:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:08:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:08:39] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:08:39] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:08:39]    INFO >> epoch 006:    578 / 770 loss=3.866, wps=5029.5, ups=3.36, wpb=1496.6, bsz=1496.6, num_updates=4400, lr=0.000327, gnorm=5.652, clip=0, train_wall=13, gb_free=3.3, wall=1324 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:08:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.58 GiB. GPU 0 has a total capacity of 79.14 GiB of which 509.25 MiB is free. Including non-PyTorch memory, this process has 78.62 GiB memory in use. Of the allocated memory 72.89 GiB is allocated by PyTorch, and 5.23 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:08:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 29           |        cudaMalloc retries: 33        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71009 MiB |  74643 MiB | 287784 GiB | 287714 GiB |
|       from large pool |  70986 MiB |  74621 MiB | 286420 GiB | 286350 GiB |
|       from small pool |     22 MiB |     22 MiB |   1363 GiB |   1363 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71009 MiB |  74643 MiB | 287784 GiB | 287714 GiB |
|       from large pool |  70986 MiB |  74621 MiB | 286420 GiB | 286350 GiB |
|       from small pool |     22 MiB |     22 MiB |   1363 GiB |   1363 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  70995 MiB |  74629 MiB | 287448 GiB | 287379 GiB |
|       from large pool |  70972 MiB |  74606 MiB | 286086 GiB | 286017 GiB |
|       from small pool |     22 MiB |     22 MiB |   1361 GiB |   1361 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79996 MiB |  80044 MiB | 172146 MiB |  92150 MiB |
|       from large pool |  79968 MiB |  79968 MiB | 167678 MiB |  87710 MiB |
|       from small pool |     28 MiB |     76 MiB |   4468 MiB |   4440 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6348 MiB |  10210 MiB | 281458 GiB | 281452 GiB |
|       from large pool |   6343 MiB |  10206 MiB | 279878 GiB | 279871 GiB |
|       from small pool |      5 MiB |     11 MiB |   1580 GiB |   1580 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   13673 K  |   13673 K  |
|       from large pool |     335    |     343    |    5412 K  |    5412 K  |
|       from small pool |     306    |     336    |    8261 K  |    8260 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   13673 K  |   13673 K  |
|       from large pool |     335    |     343    |    5412 K  |    5412 K  |
|       from small pool |     306    |     336    |    8261 K  |    8260 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     121    |     145    |    2944    |    2823    |
|       from large pool |     107    |     107    |     710    |     603    |
|       from small pool |      14    |      38    |    2234    |    2220    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     113    |     115    |    8226 K  |    8226 K  |
|       from large pool |      89    |      92    |    3404 K  |    3404 K  |
|       from small pool |      24    |      39    |    4821 K  |    4821 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:08:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:08:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:08:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:08:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:08:54]    INFO >> epoch 006:    629 / 770 loss=3.948, wps=4998.1, ups=3.61, wpb=1382.9, bsz=1382.9, num_updates=4450, lr=0.000327, gnorm=5.178, clip=0, train_wall=13, gb_free=7.1, wall=1338 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:09:07]    INFO >> epoch 006:    679 / 770 loss=3.832, wps=5101.8, ups=4.03, wpb=1265.9, bsz=1265.9, num_updates=4500, lr=0.000327, gnorm=5.347, clip=0, train_wall=12, gb_free=68, wall=1350 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:09:19]    INFO >> epoch 006:    729 / 770 loss=3.854, wps=5285.3, ups=3.87, wpb=1365.2, bsz=1365.2, num_updates=4550, lr=0.000327, gnorm=5.499, clip=0, train_wall=12, gb_free=61.1, wall=1363 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:09:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 51.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.46 GiB is allocated by PyTorch, and 1.10 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:09:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 30           |        cudaMalloc retries: 34        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79265 MiB |  79323 MiB | 295146 GiB | 295068 GiB |
|       from large pool |  78994 MiB |  79052 MiB | 293750 GiB | 293673 GiB |
|       from small pool |    270 MiB |    271 MiB |   1395 GiB |   1395 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79265 MiB |  79323 MiB | 295146 GiB | 295068 GiB |
|       from large pool |  78994 MiB |  79052 MiB | 293750 GiB | 293673 GiB |
|       from small pool |    270 MiB |    271 MiB |   1395 GiB |   1395 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79239 MiB |  79297 MiB | 294802 GiB | 294725 GiB |
|       from large pool |  78970 MiB |  79028 MiB | 293408 GiB | 293331 GiB |
|       from small pool |    268 MiB |    270 MiB |   1393 GiB |   1393 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80454 MiB |  80456 MiB | 175244 MiB |  94790 MiB |
|       from large pool |  80150 MiB |  80150 MiB | 170498 MiB |  90348 MiB |
|       from small pool |    304 MiB |    306 MiB |   4746 MiB |   4442 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1128 MiB |   7455 MiB | 289159 GiB | 289158 GiB |
|       from large pool |   1095 MiB |   7449 MiB | 287541 GiB | 287539 GiB |
|       from small pool |     33 MiB |     34 MiB |   1618 GiB |   1618 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    5217    |    5220    |   14014 K  |   14009 K  |
|       from large pool |     752    |     753    |    5561 K  |    5560 K  |
|       from small pool |    4465    |    4468    |    8453 K  |    8449 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    5217    |    5220    |   14014 K  |   14009 K  |
|       from large pool |     752    |     753    |    5561 K  |    5560 K  |
|       from small pool |    4465    |    4468    |    8453 K  |    8449 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     305    |     306    |    3130    |    2825    |
|       from large pool |     153    |     153    |     757    |     604    |
|       from small pool |     152    |     153    |    2373    |    2221    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     412    |     413    |    8424 K  |    8423 K  |
|       from large pool |     135    |     136    |    3496 K  |    3495 K  |
|       from small pool |     277    |     277    |    4927 K  |    4927 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:09:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:09:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:09:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:09:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:09:32]    INFO >> epoch 006 | loss 3.851 | wps 4910 | ups 3.46 | wpb 1419.5 | bsz 1419.5 | num_updates 4590 | lr 0.000327 | gnorm 5.776 | clip 0.1 | train_wall 194 | gb_free 69.1 | wall 1374 (progress_bar.py:267, print())[0m
[33m[2025-11-19 05:09:32] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 05:09:45]    INFO >> epoch 006 | valid on 'valid' subset | loss 3.873 | wps 11365.4 | wpb 5412.5 | bsz 5412.5 | num_updates 4590 | best_loss 5.742 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 05:09:46]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 05:09:46]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/checkpoints/checkpoint_last.pt (epoch 6 @ 4590 updates, score 3.873) (writing took 0.013547 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 05:09:46] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 05:09:48]    INFO >> epoch 007:     10 / 770 loss=3.773, wps=2517.2, ups=1.81, wpb=1387.3, bsz=1387.3, num_updates=4600, lr=0.000295, gnorm=5.761, clip=0, train_wall=13, gb_free=59, wall=1391 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:10:02]    INFO >> epoch 007:     60 / 770 loss=3.829, wps=5495.2, ups=4.08, wpb=1345.8, bsz=1345.8, num_updates=4650, lr=0.000295, gnorm=5.407, clip=0, train_wall=12, gb_free=63.1, wall=1403 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:10:16]    INFO >> epoch 007:    110 / 770 loss=3.825, wps=5178.1, ups=3.6, wpb=1438.2, bsz=1438.2, num_updates=4700, lr=0.000295, gnorm=5.587, clip=0, train_wall=13, gb_free=62.1, wall=1417 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:10:29]    INFO >> epoch 007:    160 / 770 loss=3.813, wps=5263, ups=3.85, wpb=1367.2, bsz=1367.2, num_updates=4750, lr=0.000295, gnorm=6.467, clip=2, train_wall=12, gb_free=64.3, wall=1430 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:10:42] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 3.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 76.42 GiB is allocated by PyTorch, and 2.19 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:10:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 31           |        cudaMalloc retries: 36        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78198 MiB |  78256 MiB | 312052 GiB | 311975 GiB |
|       from large pool |  77938 MiB |  77996 MiB | 310563 GiB | 310486 GiB |
|       from small pool |    259 MiB |    260 MiB |   1489 GiB |   1488 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78198 MiB |  78256 MiB | 312052 GiB | 311975 GiB |
|       from large pool |  77938 MiB |  77996 MiB | 310563 GiB | 310486 GiB |
|       from small pool |    259 MiB |    260 MiB |   1489 GiB |   1488 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78183 MiB |  78241 MiB | 311687 GiB | 311611 GiB |
|       from large pool |  77924 MiB |  77983 MiB | 310200 GiB | 310124 GiB |
|       from small pool |    258 MiB |    259 MiB |   1486 GiB |   1486 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80502 MiB |  80502 MiB | 175366 MiB |  94864 MiB |
|       from large pool |  80210 MiB |  80210 MiB | 170618 MiB |  90408 MiB |
|       from small pool |    292 MiB |    304 MiB |   4748 MiB |   4456 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2243 MiB |   9560 MiB | 302861 GiB | 302859 GiB |
|       from large pool |   2211 MiB |   9553 MiB | 301139 GiB | 301137 GiB |
|       from small pool |     32 MiB |     32 MiB |   1721 GiB |   1721 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    5019    |    5022    |   14881 K  |   14876 K  |
|       from large pool |     734    |     735    |    5849 K  |    5848 K  |
|       from small pool |    4285    |    4288    |    9032 K  |    9027 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    5019    |    5022    |   14881 K  |   14876 K  |
|       from large pool |     734    |     735    |    5849 K  |    5848 K  |
|       from small pool |    4285    |    4288    |    9032 K  |    9027 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     300    |     305    |    3133    |    2833    |
|       from large pool |     154    |     154    |     759    |     605    |
|       from small pool |     146    |     152    |    2374    |    2228    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     408    |     409    |    8961 K  |    8960 K  |
|       from large pool |     146    |     147    |    3686 K  |    3686 K  |
|       from small pool |     262    |     262    |    5274 K  |    5274 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:10:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:10:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:10:42] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:10:42] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:10:44]    INFO >> epoch 007:    211 / 770 loss=3.779, wps=5629, ups=3.58, wpb=1572.9, bsz=1572.9, num_updates=4800, lr=0.000295, gnorm=5.534, clip=0, train_wall=13, gb_free=65.2, wall=1444 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:10:54] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 79.14 GiB of which 337.25 MiB is free. Including non-PyTorch memory, this process has 78.79 GiB memory in use. Of the allocated memory 71.65 GiB is allocated by PyTorch, and 6.64 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:10:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 32           |        cudaMalloc retries: 37        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  69479 MiB |  73368 MiB | 314627 GiB | 314559 GiB |
|       from large pool |  69467 MiB |  73356 MiB | 313125 GiB | 313057 GiB |
|       from small pool |     12 MiB |     12 MiB |   1501 GiB |   1501 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  69479 MiB |  73368 MiB | 314627 GiB | 314559 GiB |
|       from large pool |  69467 MiB |  73356 MiB | 313125 GiB | 313057 GiB |
|       from small pool |     12 MiB |     12 MiB |   1501 GiB |   1501 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  69464 MiB |  73350 MiB | 314259 GiB | 314191 GiB |
|       from large pool |  69451 MiB |  73338 MiB | 312759 GiB | 312692 GiB |
|       from small pool |     12 MiB |     12 MiB |   1499 GiB |   1499 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80168 MiB |  80442 MiB | 175366 MiB |  95198 MiB |
|       from large pool |  80150 MiB |  80150 MiB | 170618 MiB |  90468 MiB |
|       from small pool |     18 MiB |    292 MiB |   4748 MiB |   4730 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5224 MiB |   7781 MiB | 305228 GiB | 305223 GiB |
|       from large pool |   5218 MiB |   7775 MiB | 303491 GiB | 303486 GiB |
|       from small pool |      5 MiB |     13 MiB |   1736 GiB |   1736 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     576    |   15007 K  |   15006 K  |
|       from large pool |     277    |     286    |    5900 K  |    5899 K  |
|       from small pool |     290    |     336    |    9107 K  |    9107 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     576    |   15007 K  |   15006 K  |
|       from large pool |     277    |     286    |    5900 K  |    5899 K  |
|       from small pool |     290    |     336    |    9107 K  |    9107 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     162    |     299    |    3133    |    2971    |
|       from large pool |     153    |     153    |     759    |     606    |
|       from small pool |       9    |     146    |    2374    |    2365    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     148    |     148    |    9040 K  |    9040 K  |
|       from large pool |     126    |     126    |    3719 K  |    3719 K  |
|       from small pool |      22    |      35    |    5320 K  |    5320 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:10:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:10:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:10:54] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:10:54] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:10:59]    INFO >> epoch 007:    262 / 770 loss=3.679, wps=5345.6, ups=3.46, wpb=1543.1, bsz=1543.1, num_updates=4850, lr=0.000295, gnorm=6.826, clip=0, train_wall=13, gb_free=63.4, wall=1458 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:11:13]    INFO >> epoch 007:    312 / 770 loss=3.867, wps=4999.6, ups=3.96, wpb=1263.3, bsz=1263.3, num_updates=4900, lr=0.000295, gnorm=4.982, clip=0, train_wall=12, gb_free=67.6, wall=1471 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:11:26] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 0 has a total capacity of 79.14 GiB of which 205.25 MiB is free. Including non-PyTorch memory, this process has 78.92 GiB memory in use. Of the allocated memory 74.72 GiB is allocated by PyTorch, and 3.70 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:11:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 33           |        cudaMalloc retries: 39        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75886 MiB |  76508 MiB | 321389 GiB | 321315 GiB |
|       from large pool |  75872 MiB |  76494 MiB | 319861 GiB | 319787 GiB |
|       from small pool |     14 MiB |     15 MiB |   1528 GiB |   1528 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75886 MiB |  76508 MiB | 321389 GiB | 321315 GiB |
|       from large pool |  75872 MiB |  76494 MiB | 319861 GiB | 319787 GiB |
|       from small pool |     14 MiB |     15 MiB |   1528 GiB |   1528 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75865 MiB |  76486 MiB | 321013 GiB | 320939 GiB |
|       from large pool |  75851 MiB |  76472 MiB | 319486 GiB | 319412 GiB |
|       from small pool |     14 MiB |     15 MiB |   1526 GiB |   1526 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80300 MiB |  80300 MiB | 184188 MiB | 103888 MiB |
|       from large pool |  80280 MiB |  80280 MiB | 179360 MiB |  99080 MiB |
|       from small pool |     20 MiB |     98 MiB |   4828 MiB |   4808 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4353 MiB |   8891 MiB | 311211 GiB | 311207 GiB |
|       from large pool |   4347 MiB |   8885 MiB | 309443 GiB | 309439 GiB |
|       from small pool |      5 MiB |     15 MiB |   1768 GiB |   1768 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   15296 K  |   15296 K  |
|       from large pool |     277    |     284    |    6030 K  |    6030 K  |
|       from small pool |     290    |     324    |    9265 K  |    9265 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   15296 K  |   15296 K  |
|       from large pool |     277    |     284    |    6030 K  |    6030 K  |
|       from small pool |     290    |     324    |    9265 K  |    9265 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     125    |     206    |    3183    |    3058    |
|       from large pool |     115    |     157    |     769    |     654    |
|       from small pool |      10    |      49    |    2414    |    2404    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     117    |     117    |    9214 K  |    9213 K  |
|       from large pool |      93    |      93    |    3805 K  |    3804 K  |
|       from small pool |      24    |      40    |    5409 K  |    5409 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:11:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:11:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:11:26] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:11:26] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:11:26]    INFO >> epoch 007:    363 / 770 loss=3.812, wps=4917.4, ups=3.6, wpb=1365.6, bsz=1365.6, num_updates=4950, lr=0.000295, gnorm=5.518, clip=0, train_wall=12, gb_free=5.3, wall=1485 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:11:41]    INFO >> epoch 007:    413 / 770 loss=3.909, wps=5090.4, ups=3.71, wpb=1370.5, bsz=1370.5, num_updates=5000, lr=0.000295, gnorm=4.709, clip=0, train_wall=13, gb_free=66.5, wall=1498 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:11:54]    INFO >> epoch 007:    463 / 770 loss=3.742, wps=5241.6, ups=3.88, wpb=1349.9, bsz=1349.9, num_updates=5050, lr=0.000295, gnorm=4.833, clip=0, train_wall=12, gb_free=67.7, wall=1511 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:12:07]    INFO >> epoch 007:    513 / 770 loss=3.842, wps=5212.3, ups=3.97, wpb=1314.4, bsz=1314.4, num_updates=5100, lr=0.000295, gnorm=4.586, clip=0, train_wall=12, gb_free=63.6, wall=1524 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:12:22]    INFO >> epoch 007:    563 / 770 loss=3.543, wps=5239.1, ups=3.62, wpb=1445.4, bsz=1445.4, num_updates=5150, lr=0.000295, gnorm=5.298, clip=0, train_wall=13, gb_free=65.6, wall=1538 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:12:24] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.30 GiB is free. Including non-PyTorch memory, this process has 77.82 GiB memory in use. Of the allocated memory 73.74 GiB is allocated by PyTorch, and 3.58 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:12:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 34           |        cudaMalloc retries: 41        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75066 MiB |  75505 MiB | 333393 GiB | 333320 GiB |
|       from large pool |  75055 MiB |  75494 MiB | 331811 GiB | 331738 GiB |
|       from small pool |     11 MiB |     15 MiB |   1581 GiB |   1581 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75066 MiB |  75505 MiB | 333393 GiB | 333320 GiB |
|       from large pool |  75055 MiB |  75494 MiB | 331811 GiB | 331738 GiB |
|       from small pool |     11 MiB |     15 MiB |   1581 GiB |   1581 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75041 MiB |  75480 MiB | 333003 GiB | 332929 GiB |
|       from large pool |  75030 MiB |  75469 MiB | 331423 GiB | 331350 GiB |
|       from small pool |     11 MiB |     15 MiB |   1579 GiB |   1579 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79176 MiB |  79446 MiB | 202518 MiB | 123342 MiB |
|       from large pool |  79158 MiB |  79158 MiB | 197422 MiB | 118264 MiB |
|       from small pool |     18 MiB |    288 MiB |   5096 MiB |   5078 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4109 MiB |   7856 MiB | 323662 GiB | 323658 GiB |
|       from large pool |   4102 MiB |   7850 MiB | 321830 GiB | 321826 GiB |
|       from small pool |      6 MiB |     17 MiB |   1831 GiB |   1831 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     647    |   15856 K  |   15855 K  |
|       from large pool |     349    |     355    |    6271 K  |    6271 K  |
|       from small pool |     292    |     342    |    9584 K  |    9584 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     647    |   15856 K  |   15855 K  |
|       from large pool |     349    |     355    |    6271 K  |    6271 K  |
|       from small pool |     292    |     342    |    9584 K  |    9584 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     131    |     266    |    3332    |    3201    |
|       from large pool |     122    |     122    |     784    |     662    |
|       from small pool |       9    |     144    |    2548    |    2539    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     111    |     117    |    9545 K  |    9545 K  |
|       from large pool |      90    |      96    |    3953 K  |    3953 K  |
|       from small pool |      21    |      41    |    5592 K  |    5592 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:12:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:12:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:12:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:12:24] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:12:36]    INFO >> epoch 007:    614 / 770 loss=3.733, wps=5057, ups=3.43, wpb=1472.9, bsz=1472.9, num_updates=5200, lr=0.000295, gnorm=5.665, clip=0, train_wall=13, gb_free=56.1, wall=1552 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:12:52]    INFO >> epoch 007:    664 / 770 loss=3.707, wps=5330.1, ups=3.51, wpb=1518.5, bsz=1518.5, num_updates=5250, lr=0.000295, gnorm=6.011, clip=0, train_wall=13, gb_free=62.8, wall=1567 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:12:57] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 59.25 MiB is free. Including non-PyTorch memory, this process has 79.06 GiB memory in use. Of the allocated memory 77.56 GiB is allocated by PyTorch, and 1021.75 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:12:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 35           |        cudaMalloc retries: 43        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79364 MiB |  79424 MiB | 339968 GiB | 339890 GiB |
|       from large pool |  79285 MiB |  79345 MiB | 338356 GiB | 338279 GiB |
|       from small pool |     78 MiB |     79 MiB |   1611 GiB |   1611 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79364 MiB |  79424 MiB | 339968 GiB | 339890 GiB |
|       from large pool |  79285 MiB |  79345 MiB | 338356 GiB | 338279 GiB |
|       from small pool |     78 MiB |     79 MiB |   1611 GiB |   1611 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79320 MiB |  79379 MiB | 339570 GiB | 339493 GiB |
|       from large pool |  79242 MiB |  79301 MiB | 337961 GiB | 337883 GiB |
|       from small pool |     78 MiB |     79 MiB |   1609 GiB |   1609 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80446 MiB |  80448 MiB | 270640 MiB | 190194 MiB |
|       from large pool |  80364 MiB |  80364 MiB | 265230 MiB | 184866 MiB |
|       from small pool |     82 MiB |    268 MiB |   5410 MiB |   5328 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1021 MiB |   4620 MiB | 330306 GiB | 330305 GiB |
|       from large pool |   1018 MiB |   4613 MiB | 328438 GiB | 328437 GiB |
|       from small pool |      3 MiB |     13 MiB |   1867 GiB |   1867 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    1653    |    1656    |   16161 K  |   16160 K  |
|       from large pool |     434    |     435    |    6396 K  |    6396 K  |
|       from small pool |    1219    |    1222    |    9765 K  |    9763 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    1653    |    1656    |   16161 K  |   16160 K  |
|       from large pool |     434    |     435    |    6396 K  |    6396 K  |
|       from small pool |    1219    |    1222    |    9765 K  |    9763 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     150    |     256    |    3561    |    3411    |
|       from large pool |     109    |     122    |     856    |     747    |
|       from small pool |      41    |     134    |    2705    |    2664    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     119    |     121    |    9730 K  |    9730 K  |
|       from large pool |      72    |      74    |    4030 K  |    4030 K  |
|       from small pool |      47    |      49    |    5700 K  |    5700 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:12:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:12:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:12:57] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:12:57] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:13:09]    INFO >> epoch 007:    715 / 770 loss=3.795, wps=4680, ups=2.93, wpb=1596.5, bsz=1596.5, num_updates=5300, lr=0.000295, gnorm=5.256, clip=0, train_wall=14, gb_free=61.3, wall=1584 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:13:24]    INFO >> epoch 007:    765 / 770 loss=3.81, wps=5132.4, ups=3.76, wpb=1364.9, bsz=1364.9, num_updates=5350, lr=0.000295, gnorm=5.257, clip=0, train_wall=13, gb_free=63.6, wall=1597 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:13:25]    INFO >> epoch 007 | loss 3.777 | wps 4857.1 | ups 3.42 | wpb 1419.5 | bsz 1419.5 | num_updates 5355 | lr 0.000295 | gnorm 5.462 | clip 0.1 | train_wall 194 | gb_free 63.9 | wall 1598 (progress_bar.py:267, print())[0m
[33m[2025-11-19 05:13:25] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 05:13:38]    INFO >> epoch 007 | valid on 'valid' subset | loss 3.822 | wps 11073.5 | wpb 5412.5 | bsz 5412.5 | num_updates 5355 | best_loss 5.742 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 05:13:39]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 05:13:39]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/checkpoints/checkpoint_last.pt (epoch 7 @ 5355 updates, score 3.822) (writing took 0.013662 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 05:13:39] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 05:13:51]    INFO >> epoch 008:     45 / 770 loss=3.769, wps=2508.4, ups=1.84, wpb=1364.6, bsz=1364.6, num_updates=5400, lr=0.000262, gnorm=5.23, clip=0, train_wall=12, gb_free=63, wall=1624 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:14:06]    INFO >> epoch 008:     95 / 770 loss=3.701, wps=5035.2, ups=3.75, wpb=1342, bsz=1342, num_updates=5450, lr=0.000262, gnorm=4.953, clip=0, train_wall=13, gb_free=67.5, wall=1637 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:14:11] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 43.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 75.80 GiB is allocated by PyTorch, and 2.77 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:14:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 36           |        cudaMalloc retries: 45        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  77562 MiB |  77620 MiB | 356863 GiB | 356787 GiB |
|       from large pool |  77309 MiB |  77367 MiB | 355159 GiB | 355083 GiB |
|       from small pool |    253 MiB |    254 MiB |   1704 GiB |   1703 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  77562 MiB |  77620 MiB | 356863 GiB | 356787 GiB |
|       from large pool |  77309 MiB |  77367 MiB | 355159 GiB | 355083 GiB |
|       from small pool |    253 MiB |    254 MiB |   1704 GiB |   1703 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  77538 MiB |  77596 MiB | 356444 GiB | 356368 GiB |
|       from large pool |  77286 MiB |  77344 MiB | 354742 GiB | 354667 GiB |
|       from small pool |    251 MiB |    253 MiB |   1701 GiB |   1701 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80462 MiB |  80482 MiB | 286752 MiB | 206290 MiB |
|       from large pool |  80178 MiB |  80178 MiB | 281120 MiB | 200942 MiB |
|       from small pool |    284 MiB |    304 MiB |   5632 MiB |   5348 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2839 MiB |   6987 MiB | 343663 GiB | 343660 GiB |
|       from large pool |   2808 MiB |   6978 MiB | 341693 GiB | 341690 GiB |
|       from small pool |     30 MiB |     32 MiB |   1970 GiB |   1969 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    4898    |    4901    |   17018 K  |   17013 K  |
|       from large pool |     723    |     724    |    6680 K  |    6679 K  |
|       from small pool |    4175    |    4178    |   10338 K  |   10334 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    4898    |    4901    |   17018 K  |   17013 K  |
|       from large pool |     723    |     724    |    6680 K  |    6679 K  |
|       from small pool |    4175    |    4178    |   10338 K  |   10334 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     505    |     515    |    3933    |    3428    |
|       from large pool |     363    |     363    |    1117    |     754    |
|       from small pool |     142    |     152    |    2816    |    2674    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     582    |     583    |   10270 K  |   10270 K  |
|       from large pool |     329    |     330    |    4229 K  |    4228 K  |
|       from small pool |     253    |     253    |    6041 K  |    6041 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:14:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:14:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:14:11] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:14:11] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-19 05:14:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.88 GiB is free. Including non-PyTorch memory, this process has 77.24 GiB memory in use. Of the allocated memory 74.71 GiB is allocated by PyTorch, and 2.03 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:14:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 37           |        cudaMalloc retries: 47        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75879 MiB |  76501 MiB | 357573 GiB | 357499 GiB |
|       from large pool |  75865 MiB |  76486 MiB | 355866 GiB | 355792 GiB |
|       from small pool |     14 MiB |     17 MiB |   1706 GiB |   1706 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75879 MiB |  76501 MiB | 357573 GiB | 357499 GiB |
|       from large pool |  75865 MiB |  76486 MiB | 355866 GiB | 355792 GiB |
|       from small pool |     14 MiB |     17 MiB |   1706 GiB |   1706 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75865 MiB |  76486 MiB | 357153 GiB | 357079 GiB |
|       from large pool |  75851 MiB |  76472 MiB | 355449 GiB | 355375 GiB |
|       from small pool |     14 MiB |     17 MiB |   1703 GiB |   1703 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78584 MiB |  80402 MiB | 302506 MiB | 223922 MiB |
|       from large pool |  78564 MiB |  80118 MiB | 296874 MiB | 218310 MiB |
|       from small pool |     20 MiB |    284 MiB |   5632 MiB |   5612 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2704 MiB |   7843 MiB | 344226 GiB | 344223 GiB |
|       from large pool |   2698 MiB |   7836 MiB | 342252 GiB | 342250 GiB |
|       from small pool |      5 MiB |     15 MiB |   1973 GiB |   1973 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   17045 K  |   17044 K  |
|       from large pool |     277    |     284    |    6692 K  |    6692 K  |
|       from small pool |     290    |     342    |   10352 K  |   10352 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   17045 K  |   17044 K  |
|       from large pool |     277    |     284    |    6692 K  |    6692 K  |
|       from small pool |     290    |     342    |   10352 K  |   10352 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     103    |     504    |    3949    |    3846    |
|       from large pool |      93    |     362    |    1133    |    1040    |
|       from small pool |      10    |     142    |    2816    |    2806    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     110    |     110    |   10288 K  |   10288 K  |
|       from large pool |      88    |      88    |    4237 K  |    4237 K  |
|       from small pool |      22    |      37    |    6050 K  |    6050 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:14:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:14:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:14:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:14:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:14:20]    INFO >> epoch 008:    147 / 770 loss=3.746, wps=4623.1, ups=3.52, wpb=1315.1, bsz=1315.1, num_updates=5500, lr=0.000262, gnorm=5.151, clip=0, train_wall=12, gb_free=68.3, wall=1652 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:14:22] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 78.01 GiB memory in use. Of the allocated memory 76.84 GiB is allocated by PyTorch, and 693.07 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:14:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 38           |        cudaMalloc retries: 49        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78209 MiB |  79045 MiB | 359029 GiB | 358952 GiB |
|       from large pool |  78196 MiB |  79032 MiB | 357317 GiB | 357240 GiB |
|       from small pool |     12 MiB |     13 MiB |   1711 GiB |   1711 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78209 MiB |  79045 MiB | 359029 GiB | 358952 GiB |
|       from large pool |  78196 MiB |  79032 MiB | 357317 GiB | 357240 GiB |
|       from small pool |     12 MiB |     13 MiB |   1711 GiB |   1711 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78178 MiB |  79013 MiB | 358607 GiB | 358531 GiB |
|       from large pool |  78165 MiB |  79000 MiB | 356898 GiB | 356821 GiB |
|       from small pool |     12 MiB |     13 MiB |   1709 GiB |   1709 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79376 MiB |  79826 MiB | 374434 MiB | 295058 MiB |
|       from large pool |  79360 MiB |  79808 MiB | 368766 MiB | 289406 MiB |
|       from small pool |     16 MiB |     54 MiB |   5668 MiB |   5652 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1166 MiB |   4891 MiB | 345641 GiB | 345640 GiB |
|       from large pool |   1163 MiB |   4888 MiB | 343662 GiB | 343661 GiB |
|       from small pool |      3 MiB |     13 MiB |   1979 GiB |   1979 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   17102 K  |   17101 K  |
|       from large pool |     314    |     322    |    6718 K  |    6718 K  |
|       from small pool |     291    |     335    |   10383 K  |   10383 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   17102 K  |   17101 K  |
|       from large pool |     314    |     322    |    6718 K  |    6718 K  |
|       from small pool |     291    |     335    |   10383 K  |   10383 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     115    |     120    |    4037    |    3922    |
|       from large pool |     107    |     109    |    1203    |    1096    |
|       from small pool |       8    |      27    |    2834    |    2826    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      90    |      91    |   10320 K  |   10320 K  |
|       from large pool |      70    |      71    |    4253 K  |    4253 K  |
|       from small pool |      20    |      36    |    6067 K  |    6067 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:14:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:14:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:14:22] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:14:22] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:14:37]    INFO >> epoch 008:    198 / 770 loss=3.635, wps=4885.5, ups=3.09, wpb=1581.3, bsz=1581.3, num_updates=5550, lr=0.000262, gnorm=4.89, clip=0, train_wall=13, gb_free=64.1, wall=1668 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:14:50]    INFO >> epoch 008:    248 / 770 loss=3.79, wps=5298.5, ups=3.99, wpb=1326.3, bsz=1326.3, num_updates=5600, lr=0.000262, gnorm=5.157, clip=0, train_wall=12, gb_free=66.8, wall=1680 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:15:04]    INFO >> epoch 008:    298 / 770 loss=3.8, wps=5344.1, ups=3.92, wpb=1364.4, bsz=1364.4, num_updates=5650, lr=0.000262, gnorm=5.046, clip=0, train_wall=12, gb_free=51.9, wall=1693 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:15:18]    INFO >> epoch 008:    348 / 770 loss=3.847, wps=5204, ups=3.64, wpb=1428.3, bsz=1428.3, num_updates=5700, lr=0.000262, gnorm=4.753, clip=0, train_wall=13, gb_free=61.9, wall=1707 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:15:31]    INFO >> epoch 008:    398 / 770 loss=3.626, wps=5050.4, ups=3.79, wpb=1332.2, bsz=1332.2, num_updates=5750, lr=0.000262, gnorm=6.073, clip=0, train_wall=12, gb_free=62.2, wall=1720 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:15:46]    INFO >> epoch 008:    448 / 770 loss=3.613, wps=5367.9, ups=3.63, wpb=1479.5, bsz=1479.5, num_updates=5800, lr=0.000262, gnorm=4.901, clip=0, train_wall=13, gb_free=64, wall=1734 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:16:00]    INFO >> epoch 008:    498 / 770 loss=3.687, wps=5152.7, ups=3.58, wpb=1438.6, bsz=1438.6, num_updates=5850, lr=0.000262, gnorm=5.38, clip=0, train_wall=13, gb_free=62, wall=1748 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:16:14]    INFO >> epoch 008:    548 / 770 loss=3.8, wps=5300.7, ups=3.8, wpb=1395.1, bsz=1395.1, num_updates=5900, lr=0.000262, gnorm=5.298, clip=0, train_wall=12, gb_free=64.3, wall=1761 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:16:20] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.10 GiB is free. Including non-PyTorch memory, this process has 78.02 GiB memory in use. Of the allocated memory 73.73 GiB is allocated by PyTorch, and 3.79 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:16:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 39           |        cudaMalloc retries: 50        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75057 MiB |  75496 MiB | 383538 GiB | 383465 GiB |
|       from large pool |  75045 MiB |  75484 MiB | 381718 GiB | 381645 GiB |
|       from small pool |     11 MiB |     15 MiB |   1820 GiB |   1820 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75057 MiB |  75496 MiB | 383538 GiB | 383465 GiB |
|       from large pool |  75045 MiB |  75484 MiB | 381718 GiB | 381645 GiB |
|       from small pool |     11 MiB |     15 MiB |   1820 GiB |   1820 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75041 MiB |  75480 MiB | 383088 GiB | 383015 GiB |
|       from large pool |  75030 MiB |  75469 MiB | 381271 GiB | 381198 GiB |
|       from small pool |     11 MiB |     15 MiB |   1817 GiB |   1817 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79380 MiB |  79662 MiB | 374720 MiB | 295340 MiB |
|       from large pool |  79360 MiB |  79360 MiB | 368766 MiB | 289406 MiB |
|       from small pool |     20 MiB |    302 MiB |   5954 MiB |   5934 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4322 MiB |   9786 MiB | 369616 GiB | 369612 GiB |
|       from large pool |   4314 MiB |   9777 MiB | 367508 GiB | 367504 GiB |
|       from small pool |      8 MiB |     19 MiB |   2107 GiB |   2107 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     647    |   18236 K  |   18235 K  |
|       from large pool |     349    |     355    |    7205 K  |    7205 K  |
|       from small pool |     292    |     342    |   11030 K  |   11030 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     647    |   18236 K  |   18235 K  |
|       from large pool |     349    |     355    |    7205 K  |    7205 K  |
|       from small pool |     292    |     342    |   11030 K  |   11030 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     117    |     258    |    4180    |    4063    |
|       from large pool |     107    |     107    |    1203    |    1096    |
|       from small pool |      10    |     151    |    2977    |    2967    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     130    |     130    |   10997 K  |   10997 K  |
|       from large pool |     110    |     110    |    4556 K  |    4556 K  |
|       from small pool |      20    |      45    |    6441 K  |    6441 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:16:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:16:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:16:20] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:16:20] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:16:28]    INFO >> epoch 008:    599 / 770 loss=3.801, wps=4843.7, ups=3.61, wpb=1340.5, bsz=1340.5, num_updates=5950, lr=0.000262, gnorm=4.901, clip=0, train_wall=12, gb_free=62.2, wall=1775 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:16:42]    INFO >> epoch 008:    649 / 770 loss=3.736, wps=5577.4, ups=3.92, wpb=1421.5, bsz=1421.5, num_updates=6000, lr=0.000262, gnorm=4.633, clip=0, train_wall=12, gb_free=67.8, wall=1788 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:16:46] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.58 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.09 GiB is free. Including non-PyTorch memory, this process has 78.03 GiB memory in use. Of the allocated memory 72.90 GiB is allocated by PyTorch, and 4.63 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:16:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 40           |        cudaMalloc retries: 51        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71014 MiB |  74648 MiB | 388924 GiB | 388855 GiB |
|       from large pool |  70991 MiB |  74626 MiB | 387081 GiB | 387012 GiB |
|       from small pool |     22 MiB |     22 MiB |   1843 GiB |   1843 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71014 MiB |  74648 MiB | 388924 GiB | 388855 GiB |
|       from large pool |  70991 MiB |  74626 MiB | 387081 GiB | 387012 GiB |
|       from small pool |     22 MiB |     22 MiB |   1843 GiB |   1843 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  70995 MiB |  74629 MiB | 388468 GiB | 388399 GiB |
|       from large pool |  70972 MiB |  74606 MiB | 386628 GiB | 386559 GiB |
|       from small pool |     22 MiB |     22 MiB |   1840 GiB |   1840 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79388 MiB |  79488 MiB | 374828 MiB | 295440 MiB |
|       from large pool |  79360 MiB |  79360 MiB | 368766 MiB | 289406 MiB |
|       from small pool |     28 MiB |    128 MiB |   6062 MiB |   6034 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5641 MiB |   9600 MiB | 374903 GiB | 374898 GiB |
|       from large pool |   5636 MiB |   9595 MiB | 372769 GiB | 372763 GiB |
|       from small pool |      5 MiB |     19 MiB |   2134 GiB |   2134 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   18478 K  |   18478 K  |
|       from large pool |     335    |     343    |    7311 K  |    7311 K  |
|       from small pool |     306    |     336    |   11166 K  |   11166 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   18478 K  |   18478 K  |
|       from large pool |     335    |     343    |    7311 K  |    7311 K  |
|       from small pool |     306    |     336    |   11166 K  |   11166 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     121    |     171    |    4234    |    4113    |
|       from large pool |     107    |     107    |    1203    |    1096    |
|       from small pool |      14    |      64    |    3031    |    3017    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     113    |     117    |   11141 K  |   11141 K  |
|       from large pool |      88    |      93    |    4622 K  |    4622 K  |
|       from small pool |      25    |      39    |    6518 K  |    6518 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:16:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:16:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:16:46] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:16:46] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:16:57]    INFO >> epoch 008:    700 / 770 loss=3.767, wps=4965.2, ups=3.46, wpb=1436.7, bsz=1436.7, num_updates=6050, lr=0.000262, gnorm=5.451, clip=0, train_wall=13, gb_free=62.8, wall=1802 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:17:11]    INFO >> epoch 008:    750 / 770 loss=3.728, wps=5368, ups=3.53, wpb=1521, bsz=1521, num_updates=6100, lr=0.000262, gnorm=4.771, clip=0, train_wall=13, gb_free=65.9, wall=1816 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:17:18]    INFO >> epoch 008 | loss 3.731 | wps 4845.3 | ups 3.41 | wpb 1419.5 | bsz 1419.5 | num_updates 6120 | lr 0.000262 | gnorm 5.134 | clip 0 | train_wall 194 | gb_free 58.6 | wall 1822 (progress_bar.py:267, print())[0m
[33m[2025-11-19 05:17:18] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 05:17:32]    INFO >> epoch 008 | valid on 'valid' subset | loss 3.826 | wps 11406.5 | wpb 5412.5 | bsz 5412.5 | num_updates 6120 | best_loss 5.742 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 05:17:32]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 05:17:32]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/checkpoints/checkpoint_last.pt (epoch 8 @ 6120 updates, score 3.826) (writing took 0.012586 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 05:17:32] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 05:17:40]    INFO >> epoch 009:     30 / 770 loss=3.617, wps=2870.1, ups=1.84, wpb=1561.3, bsz=1561.3, num_updates=6150, lr=0.000227, gnorm=5.265, clip=0, train_wall=13, gb_free=66.6, wall=1843 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:17:54]    INFO >> epoch 009:     80 / 770 loss=3.746, wps=5297.4, ups=3.73, wpb=1420.6, bsz=1420.6, num_updates=6200, lr=0.000227, gnorm=4.498, clip=0, train_wall=13, gb_free=66.4, wall=1857 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:17:55] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 37.25 MiB is free. Including non-PyTorch memory, this process has 79.08 GiB memory in use. Of the allocated memory 77.23 GiB is allocated by PyTorch, and 1.35 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:17:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 41           |        cudaMalloc retries: 52        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79026 MiB |  79084 MiB | 405031 GiB | 404954 GiB |
|       from large pool |  78758 MiB |  78816 MiB | 403093 GiB | 403016 GiB |
|       from small pool |    268 MiB |    269 MiB |   1937 GiB |   1937 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79026 MiB |  79084 MiB | 405031 GiB | 404954 GiB |
|       from large pool |  78758 MiB |  78816 MiB | 403093 GiB | 403016 GiB |
|       from small pool |    268 MiB |    269 MiB |   1937 GiB |   1937 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79004 MiB |  79062 MiB | 404556 GiB | 404479 GiB |
|       from large pool |  78737 MiB |  78796 MiB | 402621 GiB | 402545 GiB |
|       from small pool |    266 MiB |    267 MiB |   1934 GiB |   1934 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80468 MiB |  80470 MiB | 378642 MiB | 298174 MiB |
|       from large pool |  80168 MiB |  80168 MiB | 372306 MiB | 292138 MiB |
|       from small pool |    300 MiB |    302 MiB |   6336 MiB |   6036 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1381 MiB |   6830 MiB | 388501 GiB | 388500 GiB |
|       from large pool |   1349 MiB |   6822 MiB | 386260 GiB | 386259 GiB |
|       from small pool |     31 MiB |     33 MiB |   2241 GiB |   2241 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    5173    |    5176    |   19337 K  |   19332 K  |
|       from large pool |     748    |     749    |    7581 K  |    7580 K  |
|       from small pool |    4425    |    4428    |   11755 K  |   11751 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    5173    |    5176    |   19337 K  |   19332 K  |
|       from large pool |     748    |     749    |    7581 K  |    7580 K  |
|       from small pool |    4425    |    4428    |   11755 K  |   11751 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     315    |     316    |    4430    |    4115    |
|       from large pool |     165    |     165    |    1262    |    1097    |
|       from small pool |     150    |     151    |    3168    |    3018    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     412    |     413    |   11664 K  |   11663 K  |
|       from large pool |     143    |     144    |    4795 K  |    4795 K  |
|       from small pool |     269    |     270    |    6868 K  |    6868 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:17:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:17:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:17:55] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:17:55] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:18:07]    INFO >> epoch 009:    131 / 770 loss=3.706, wps=5066.1, ups=3.85, wpb=1315.3, bsz=1315.3, num_updates=6250, lr=0.000227, gnorm=5.504, clip=0, train_wall=12, gb_free=65, wall=1870 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:18:23]    INFO >> epoch 009:    181 / 770 loss=3.785, wps=5516.8, ups=3.48, wpb=1585.2, bsz=1585.2, num_updates=6300, lr=0.000227, gnorm=5.416, clip=0, train_wall=14, gb_free=64, wall=1884 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:18:27] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 703.25 MiB is free. Including non-PyTorch memory, this process has 78.43 GiB memory in use. Of the allocated memory 73.73 GiB is allocated by PyTorch, and 4.20 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:18:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 42           |        cudaMalloc retries: 53        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75062 MiB |  75501 MiB | 411750 GiB | 411677 GiB |
|       from large pool |  75050 MiB |  75490 MiB | 409782 GiB | 409708 GiB |
|       from small pool |     11 MiB |     12 MiB |   1968 GiB |   1968 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75062 MiB |  75501 MiB | 411750 GiB | 411677 GiB |
|       from large pool |  75050 MiB |  75490 MiB | 409782 GiB | 409708 GiB |
|       from small pool |     11 MiB |     12 MiB |   1968 GiB |   1968 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75041 MiB |  75480 MiB | 411268 GiB | 411194 GiB |
|       from large pool |  75030 MiB |  75469 MiB | 409302 GiB | 409229 GiB |
|       from small pool |     11 MiB |     12 MiB |   1965 GiB |   1965 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79802 MiB |  80412 MiB | 378646 MiB | 298844 MiB |
|       from large pool |  79784 MiB |  80108 MiB | 372306 MiB | 292522 MiB |
|       from small pool |     18 MiB |    304 MiB |   6340 MiB |   6322 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4739 MiB |  10579 MiB | 394275 GiB | 394270 GiB |
|       from large pool |   4733 MiB |  10571 MiB | 391997 GiB | 391993 GiB |
|       from small pool |      6 MiB |     15 MiB |   2277 GiB |   2277 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     647    |   19652 K  |   19652 K  |
|       from large pool |     349    |     355    |    7713 K  |    7713 K  |
|       from small pool |     292    |     336    |   11939 K  |   11938 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     647    |   19652 K  |   19652 K  |
|       from large pool |     349    |     355    |    7713 K  |    7713 K  |
|       from small pool |     292    |     336    |   11939 K  |   11938 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     172    |     316    |    4432    |    4260    |
|       from large pool |     163    |     164    |    1262    |    1099    |
|       from small pool |       9    |     152    |    3170    |    3161    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     158    |     159    |   11860 K  |   11860 K  |
|       from large pool |     138    |     139    |    4882 K  |    4882 K  |
|       from small pool |      20    |      40    |    6977 K  |    6977 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:18:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:18:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:18:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:18:27] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[33m[2025-11-19 05:18:38] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 79.14 GiB of which 255.25 MiB is free. Including non-PyTorch memory, this process has 78.87 GiB memory in use. Of the allocated memory 76.84 GiB is allocated by PyTorch, and 1.53 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:18:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 43           |        cudaMalloc retries: 54        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78207 MiB |  79042 MiB | 414035 GiB | 413959 GiB |
|       from large pool |  78194 MiB |  79029 MiB | 412058 GiB | 411981 GiB |
|       from small pool |     12 MiB |     17 MiB |   1977 GiB |   1977 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78207 MiB |  79042 MiB | 414035 GiB | 413959 GiB |
|       from large pool |  78194 MiB |  79029 MiB | 412058 GiB | 411981 GiB |
|       from small pool |     12 MiB |     17 MiB |   1977 GiB |   1977 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78178 MiB |  79013 MiB | 413550 GiB | 413474 GiB |
|       from large pool |  78165 MiB |  79000 MiB | 411575 GiB | 411499 GiB |
|       from small pool |     12 MiB |     17 MiB |   1974 GiB |   1974 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80250 MiB |  80330 MiB | 379174 MiB | 298924 MiB |
|       from large pool |  80232 MiB |  80232 MiB | 372754 MiB | 292522 MiB |
|       from small pool |     18 MiB |     98 MiB |   6420 MiB |   6402 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2042 MiB |   4602 MiB | 396144 GiB | 396142 GiB |
|       from large pool |   2037 MiB |   4597 MiB | 393855 GiB | 393853 GiB |
|       from small pool |      5 MiB |     17 MiB |   2288 GiB |   2288 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   19747 K  |   19747 K  |
|       from large pool |     314    |     322    |    7755 K  |    7754 K  |
|       from small pool |     291    |     342    |   11992 K  |   11992 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   19747 K  |   19747 K  |
|       from large pool |     314    |     322    |    7755 K  |    7754 K  |
|       from small pool |     291    |     342    |   11992 K  |   11992 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     173    |     213    |    4473    |    4300    |
|       from large pool |     164    |     164    |    1263    |    1099    |
|       from small pool |       9    |      49    |    3210    |    3201    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     125    |     125    |   11918 K  |   11918 K  |
|       from large pool |     104    |     104    |    4910 K  |    4910 K  |
|       from small pool |      21    |      41    |    7008 K  |    7008 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:18:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:18:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:18:38] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:18:38] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:18:38]    INFO >> epoch 009:    233 / 770 loss=3.787, wps=4807.3, ups=3.35, wpb=1433.2, bsz=1433.2, num_updates=6350, lr=0.000227, gnorm=5.416, clip=0, train_wall=13, gb_free=2.8, wall=1899 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:18:51]    INFO >> epoch 009:    283 / 770 loss=3.814, wps=5254.3, ups=3.93, wpb=1338.6, bsz=1338.6, num_updates=6400, lr=0.000227, gnorm=4.733, clip=0, train_wall=12, gb_free=64.1, wall=1912 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:19:05]    INFO >> epoch 009:    333 / 770 loss=3.663, wps=5487.9, ups=3.87, wpb=1419.2, bsz=1419.2, num_updates=6450, lr=0.000227, gnorm=4.649, clip=0, train_wall=12, gb_free=58.3, wall=1925 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:19:18]    INFO >> epoch 009:    383 / 770 loss=3.703, wps=5157.6, ups=3.89, wpb=1327.1, bsz=1327.1, num_updates=6500, lr=0.000227, gnorm=4.854, clip=0, train_wall=12, gb_free=57.4, wall=1938 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:19:32]    INFO >> epoch 009:    433 / 770 loss=3.746, wps=5348.9, ups=3.74, wpb=1428.8, bsz=1428.8, num_updates=6550, lr=0.000227, gnorm=4.873, clip=0, train_wall=13, gb_free=58.3, wall=1951 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:19:46]    INFO >> epoch 009:    483 / 770 loss=3.578, wps=5325.6, ups=3.81, wpb=1398.1, bsz=1398.1, num_updates=6600, lr=0.000227, gnorm=4.879, clip=0, train_wall=12, gb_free=56.1, wall=1964 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:19:56] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.58 GiB. GPU 0 has a total capacity of 79.14 GiB of which 533.25 MiB is free. Including non-PyTorch memory, this process has 78.60 GiB memory in use. Of the allocated memory 69.68 GiB is allocated by PyTorch, and 8.42 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:19:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 44           |        cudaMalloc retries: 55        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71013 MiB |  71352 MiB | 430532 GiB | 430462 GiB |
|       from large pool |  70990 MiB |  71329 MiB | 428482 GiB | 428413 GiB |
|       from small pool |     22 MiB |     22 MiB |   2049 GiB |   2049 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71013 MiB |  71352 MiB | 430532 GiB | 430462 GiB |
|       from large pool |  70990 MiB |  71329 MiB | 428482 GiB | 428413 GiB |
|       from small pool |     22 MiB |     22 MiB |   2049 GiB |   2049 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  70995 MiB |  71333 MiB | 430025 GiB | 429956 GiB |
|       from large pool |  70972 MiB |  71310 MiB | 427979 GiB | 427909 GiB |
|       from small pool |     22 MiB |     22 MiB |   2046 GiB |   2046 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79972 MiB |  80388 MiB | 379312 MiB | 299340 MiB |
|       from large pool |  79944 MiB |  80232 MiB | 372754 MiB | 292810 MiB |
|       from small pool |     28 MiB |    156 MiB |   6558 MiB |   6530 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   8958 MiB |  10223 MiB | 410315 GiB | 410307 GiB |
|       from large pool |   8953 MiB |  10218 MiB | 407942 GiB | 407934 GiB |
|       from small pool |      5 MiB |     13 MiB |   2372 GiB |   2372 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     647    |   20502 K  |   20501 K  |
|       from large pool |     335    |     341    |    8085 K  |    8084 K  |
|       from small pool |     306    |     336    |   12417 K  |   12416 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     647    |   20502 K  |   20501 K  |
|       from large pool |     335    |     341    |    8085 K  |    8084 K  |
|       from small pool |     306    |     336    |   12417 K  |   12416 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     177    |     242    |    4542    |    4365    |
|       from large pool |     163    |     164    |    1263    |    1100    |
|       from small pool |      14    |      78    |    3279    |    3265    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     161    |     163    |   12377 K  |   12377 K  |
|       from large pool |     136    |     139    |    5130 K  |    5130 K  |
|       from small pool |      25    |      32    |    7247 K  |    7247 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:19:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:19:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:19:56] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:19:56] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:20:00]    INFO >> epoch 009:    534 / 770 loss=3.696, wps=4988.7, ups=3.55, wpb=1407, bsz=1407, num_updates=6650, lr=0.000227, gnorm=4.508, clip=0, train_wall=13, gb_free=67.1, wall=1978 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:20:14] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.75 GiB is free. Including non-PyTorch memory, this process has 77.37 GiB memory in use. Of the allocated memory 74.71 GiB is allocated by PyTorch, and 2.16 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:20:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 45           |        cudaMalloc retries: 57        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75881 MiB |  76503 MiB | 434019 GiB | 433945 GiB |
|       from large pool |  75867 MiB |  76489 MiB | 431952 GiB | 431878 GiB |
|       from small pool |     14 MiB |     16 MiB |   2067 GiB |   2067 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75881 MiB |  76503 MiB | 434019 GiB | 433945 GiB |
|       from large pool |  75867 MiB |  76489 MiB | 431952 GiB | 431878 GiB |
|       from small pool |     14 MiB |     16 MiB |   2067 GiB |   2067 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75865 MiB |  76486 MiB | 433508 GiB | 433434 GiB |
|       from large pool |  75851 MiB |  76472 MiB | 431444 GiB | 431369 GiB |
|       from small pool |     14 MiB |     16 MiB |   2064 GiB |   2064 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  78714 MiB |  80232 MiB | 394656 MiB | 315942 MiB |
|       from large pool |  78692 MiB |  79944 MiB | 387838 MiB | 309146 MiB |
|       from small pool |     22 MiB |    288 MiB |   6818 MiB |   6796 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2832 MiB |   7052 MiB | 413390 GiB | 413387 GiB |
|       from large pool |   2824 MiB |   7043 MiB | 410995 GiB | 410992 GiB |
|       from small pool |      7 MiB |     19 MiB |   2395 GiB |   2395 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   20685 K  |   20685 K  |
|       from large pool |     277    |     284    |    8153 K  |    8153 K  |
|       from small pool |     290    |     342    |   12531 K  |   12531 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   20685 K  |   20685 K  |
|       from large pool |     277    |     284    |    8153 K  |    8153 K  |
|       from small pool |     290    |     342    |   12531 K  |   12531 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      98    |     307    |    4686    |    4588    |
|       from large pool |      87    |     163    |    1277    |    1190    |
|       from small pool |      11    |     144    |    3409    |    3398    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     100    |     101    |   12494 K  |   12494 K  |
|       from large pool |      78    |      79    |    5175 K  |    5175 K  |
|       from small pool |      22    |      40    |    7318 K  |    7318 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:20:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:20:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:20:14] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:20:14] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:20:16]    INFO >> epoch 009:    585 / 770 loss=3.489, wps=5228, ups=3.33, wpb=1571.7, bsz=1571.7, num_updates=6700, lr=0.000227, gnorm=5.672, clip=0, train_wall=13, gb_free=62.7, wall=1993 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:20:29]    INFO >> epoch 009:    635 / 770 loss=3.703, wps=5036.6, ups=3.75, wpb=1343.3, bsz=1343.3, num_updates=6750, lr=0.000227, gnorm=4.47, clip=0, train_wall=13, gb_free=54.7, wall=2007 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:20:45]    INFO >> epoch 009:    685 / 770 loss=3.642, wps=4961.5, ups=3.53, wpb=1404.9, bsz=1404.9, num_updates=6800, lr=0.000227, gnorm=4.827, clip=0, train_wall=13, gb_free=64.3, wall=2021 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:20:59]    INFO >> epoch 009:    735 / 770 loss=3.768, wps=4996.1, ups=3.64, wpb=1372.4, bsz=1372.4, num_updates=6850, lr=0.000227, gnorm=4.976, clip=0, train_wall=13, gb_free=58.6, wall=2034 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:21:10]    INFO >> epoch 009 | loss 3.706 | wps 4880.7 | ups 3.44 | wpb 1419.5 | bsz 1419.5 | num_updates 6885 | lr 0.000227 | gnorm 4.962 | clip 0 | train_wall 194 | gb_free 49.4 | wall 2045 (progress_bar.py:267, print())[0m
[33m[2025-11-19 05:21:10] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 05:21:23]    INFO >> epoch 009 | valid on 'valid' subset | loss 3.812 | wps 11663.2 | wpb 5412.5 | bsz 5412.5 | num_updates 6885 | best_loss 5.742 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 05:21:24]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 05:21:24]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/checkpoints/checkpoint_last.pt (epoch 9 @ 6885 updates, score 3.812) (writing took 0.013573 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 05:21:24] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[33m[2025-11-19 05:21:24] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 7.25 MiB is free. Including non-PyTorch memory, this process has 79.11 GiB memory in use. Of the allocated memory 77.75 GiB is allocated by PyTorch, and 886.94 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:21:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 46           |        cudaMalloc retries: 58        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79552 MiB |  79611 MiB | 450343 GiB | 450265 GiB |
|       from large pool |  79279 MiB |  79337 MiB | 448187 GiB | 448109 GiB |
|       from small pool |    273 MiB |    274 MiB |   2156 GiB |   2156 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79552 MiB |  79611 MiB | 450343 GiB | 450265 GiB |
|       from large pool |  79279 MiB |  79337 MiB | 448187 GiB | 448109 GiB |
|       from small pool |    273 MiB |    274 MiB |   2156 GiB |   2156 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79532 MiB |  79590 MiB | 449813 GiB | 449735 GiB |
|       from large pool |  79260 MiB |  79318 MiB | 447660 GiB | 447582 GiB |
|       from small pool |    271 MiB |    272 MiB |   2153 GiB |   2152 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80498 MiB |  80500 MiB | 396442 MiB | 315944 MiB |
|       from large pool |  80192 MiB |  80192 MiB | 389338 MiB | 309146 MiB |
|       from small pool |    306 MiB |    308 MiB |   7104 MiB |   6798 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |    885 MiB |   5016 MiB | 428061 GiB | 428060 GiB |
|       from large pool |    852 MiB |   5007 MiB | 425568 GiB | 425567 GiB |
|       from small pool |     32 MiB |     34 MiB |   2493 GiB |   2493 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    5272    |    5275    |   21508 K  |   21503 K  |
|       from large pool |     757    |     758    |    8423 K  |    8422 K  |
|       from small pool |    4515    |    4518    |   13085 K  |   13080 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    5272    |    5275    |   21508 K  |   21503 K  |
|       from large pool |     757    |     758    |    8423 K  |    8422 K  |
|       from small pool |    4515    |    4518    |   13085 K  |   13080 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     265    |     266    |    4854    |    4589    |
|       from large pool |     112    |     112    |    1302    |    1190    |
|       from small pool |     153    |     154    |    3552    |    3399    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     373    |     374    |   12991 K  |   12991 K  |
|       from large pool |      98    |      99    |    5345 K  |    5345 K  |
|       from small pool |     275    |     276    |    7646 K  |    7645 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:21:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:21:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:21:24] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:21:24] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:21:28]    INFO >> epoch 010:     16 / 770 loss=3.81, wps=2639.6, ups=1.81, wpb=1461.2, bsz=1461.2, num_updates=6900, lr=0.000193, gnorm=5.26, clip=0, train_wall=13, gb_free=67.6, wall=2062 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:21:41]    INFO >> epoch 010:     66 / 770 loss=3.643, wps=5822.2, ups=3.77, wpb=1545.6, bsz=1545.6, num_updates=6950, lr=0.000193, gnorm=4.971, clip=0, train_wall=13, gb_free=63.6, wall=2075 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:21:55]    INFO >> epoch 010:    116 / 770 loss=3.791, wps=5379.4, ups=3.89, wpb=1382.8, bsz=1382.8, num_updates=7000, lr=0.000193, gnorm=5.146, clip=0, train_wall=12, gb_free=64.3, wall=2088 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:22:08] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 810.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 345.25 MiB is free. Including non-PyTorch memory, this process has 78.78 GiB memory in use. Of the allocated memory 75.43 GiB is allocated by PyTorch, and 2.85 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:22:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 47           |        cudaMalloc retries: 59        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71998 MiB |  77400 MiB | 459654 GiB | 459584 GiB |
|       from large pool |  71975 MiB |  77377 MiB | 457455 GiB | 457385 GiB |
|       from small pool |     22 MiB |     22 MiB |   2198 GiB |   2198 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71998 MiB |  77400 MiB | 459654 GiB | 459584 GiB |
|       from large pool |  71975 MiB |  77377 MiB | 457455 GiB | 457385 GiB |
|       from small pool |     22 MiB |     22 MiB |   2198 GiB |   2198 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  71978 MiB |  77379 MiB | 459112 GiB | 459042 GiB |
|       from large pool |  71955 MiB |  77356 MiB | 456917 GiB | 456847 GiB |
|       from small pool |     22 MiB |     22 MiB |   2195 GiB |   2195 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80160 MiB |  80438 MiB | 396442 MiB | 316282 MiB |
|       from large pool |  80132 MiB |  80132 MiB | 389338 MiB | 309206 MiB |
|       from small pool |     28 MiB |    306 MiB |   7104 MiB |   7076 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5429 MiB |   7615 MiB | 437255 GiB | 437250 GiB |
|       from large pool |   5424 MiB |   7611 MiB | 434711 GiB | 434706 GiB |
|       from small pool |      5 MiB |     23 MiB |   2543 GiB |   2543 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     643    |     661    |   21947 K  |   21946 K  |
|       from large pool |     336    |     354    |    8608 K  |    8608 K  |
|       from small pool |     307    |     348    |   13338 K  |   13338 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     643    |     661    |   21947 K  |   21946 K  |
|       from large pool |     336    |     354    |    8608 K  |    8608 K  |
|       from small pool |     307    |     348    |   13338 K  |   13338 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     125    |     264    |    4854    |    4729    |
|       from large pool |     111    |     111    |    1302    |    1191    |
|       from small pool |      14    |     153    |    3552    |    3538    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     118    |     118    |   13259 K  |   13259 K  |
|       from large pool |      91    |      92    |    5465 K  |    5465 K  |
|       from small pool |      27    |      50    |    7794 K  |    7794 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:22:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:22:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:22:08] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:22:08] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:22:10]    INFO >> epoch 010:    167 / 770 loss=3.631, wps=4993.3, ups=3.34, wpb=1495, bsz=1495, num_updates=7050, lr=0.000193, gnorm=4.867, clip=0, train_wall=14, gb_free=61.3, wall=2103 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:22:24]    INFO >> epoch 010:    217 / 770 loss=3.686, wps=5157.2, ups=3.9, wpb=1324, bsz=1324, num_updates=7100, lr=0.000193, gnorm=4.476, clip=0, train_wall=12, gb_free=70.8, wall=2116 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:22:37]    INFO >> epoch 010:    267 / 770 loss=3.785, wps=5262.3, ups=3.86, wpb=1364.5, bsz=1364.5, num_updates=7150, lr=0.000193, gnorm=4.38, clip=0, train_wall=12, gb_free=61.3, wall=2129 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:22:51] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 1.17 GiB is free. Including non-PyTorch memory, this process has 77.95 GiB memory in use. Of the allocated memory 73.73 GiB is allocated by PyTorch, and 3.72 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:22:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 48           |        cudaMalloc retries: 60        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75060 MiB |  75499 MiB | 468550 GiB | 468477 GiB |
|       from large pool |  75049 MiB |  75487 MiB | 466314 GiB | 466241 GiB |
|       from small pool |     11 MiB |     13 MiB |   2236 GiB |   2236 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75060 MiB |  75499 MiB | 468550 GiB | 468477 GiB |
|       from large pool |  75049 MiB |  75487 MiB | 466314 GiB | 466241 GiB |
|       from small pool |     11 MiB |     13 MiB |   2236 GiB |   2236 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75041 MiB |  75480 MiB | 467998 GiB | 467924 GiB |
|       from large pool |  75030 MiB |  75469 MiB | 465765 GiB | 465692 GiB |
|       from small pool |     11 MiB |     13 MiB |   2232 GiB |   2232 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79310 MiB |  79434 MiB | 398448 MiB | 319138 MiB |
|       from large pool |  79294 MiB |  79294 MiB | 391232 MiB | 311938 MiB |
|       from small pool |     16 MiB |    140 MiB |   7216 MiB |   7200 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   4249 MiB |   8809 MiB | 445924 GiB | 445920 GiB |
|       from large pool |   4244 MiB |   8804 MiB | 443336 GiB | 443332 GiB |
|       from small pool |      4 MiB |     13 MiB |   2588 GiB |   2588 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     647    |   22347 K  |   22346 K  |
|       from large pool |     349    |     355    |    8785 K  |    8785 K  |
|       from small pool |     292    |     336    |   13561 K  |   13561 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     647    |   22347 K  |   22346 K  |
|       from large pool |     349    |     355    |    8785 K  |    8785 K  |
|       from small pool |     292    |     336    |   13561 K  |   13561 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     119    |     181    |    4911    |    4792    |
|       from large pool |     111    |     111    |    1303    |    1192    |
|       from small pool |       8    |      70    |    3608    |    3600    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     115    |     122    |   13499 K  |   13499 K  |
|       from large pool |      95    |     102    |    5578 K  |    5578 K  |
|       from small pool |      20    |      37    |    7920 K  |    7920 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:22:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:22:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:22:51] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:22:51] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:22:52]    INFO >> epoch 010:    318 / 770 loss=3.669, wps=5037.3, ups=3.81, wpb=1321.4, bsz=1321.4, num_updates=7200, lr=0.000193, gnorm=4.686, clip=0, train_wall=12, gb_free=58.8, wall=2142 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:23:05]    INFO >> epoch 010:    368 / 770 loss=3.714, wps=5240.6, ups=3.7, wpb=1414.7, bsz=1414.7, num_updates=7250, lr=0.000193, gnorm=4.616, clip=0, train_wall=13, gb_free=63.2, wall=2156 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:23:19]    INFO >> epoch 010:    418 / 770 loss=3.634, wps=5397.2, ups=3.73, wpb=1445.8, bsz=1445.8, num_updates=7300, lr=0.000193, gnorm=4.44, clip=0, train_wall=13, gb_free=64.5, wall=2169 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:23:27] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 0 has a total capacity of 79.14 GiB of which 649.25 MiB is free. Including non-PyTorch memory, this process has 78.48 GiB memory in use. Of the allocated memory 74.71 GiB is allocated by PyTorch, and 3.27 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:23:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 49           |        cudaMalloc retries: 62        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75883 MiB |  76504 MiB | 476017 GiB | 475943 GiB |
|       from large pool |  75869 MiB |  76490 MiB | 473749 GiB | 473675 GiB |
|       from small pool |     14 MiB |     15 MiB |   2267 GiB |   2267 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75883 MiB |  76504 MiB | 476017 GiB | 475943 GiB |
|       from large pool |  75869 MiB |  76490 MiB | 473749 GiB | 473675 GiB |
|       from small pool |     14 MiB |     15 MiB |   2267 GiB |   2267 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75865 MiB |  76486 MiB | 475455 GiB | 475381 GiB |
|       from large pool |  75851 MiB |  76472 MiB | 473191 GiB | 473117 GiB |
|       from small pool |     14 MiB |     15 MiB |   2264 GiB |   2264 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79856 MiB |  79856 MiB | 400444 MiB | 320588 MiB |
|       from large pool |  79836 MiB |  79836 MiB | 393156 MiB | 313320 MiB |
|       from small pool |     20 MiB |     88 MiB |   7288 MiB |   7268 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3912 MiB |   8275 MiB | 453323 GiB | 453319 GiB |
|       from large pool |   3906 MiB |   8269 MiB | 450697 GiB | 450694 GiB |
|       from small pool |      5 MiB |     17 MiB |   2625 GiB |   2625 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   22677 K  |   22677 K  |
|       from large pool |     277    |     284    |    8929 K  |    8929 K  |
|       from small pool |     290    |     330    |   13748 K  |   13747 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   22677 K  |   22677 K  |
|       from large pool |     277    |     284    |    8929 K  |    8929 K  |
|       from small pool |     290    |     330    |   13748 K  |   13747 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      99    |     155    |    4948    |    4849    |
|       from large pool |      89    |     111    |    1304    |    1215    |
|       from small pool |      10    |      44    |    3644    |    3634    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     102    |     103    |   13697 K  |   13697 K  |
|       from large pool |      77    |      78    |    5671 K  |    5671 K  |
|       from small pool |      25    |      39    |    8026 K  |    8026 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:23:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:23:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:23:27] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:23:27] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:23:34]    INFO >> epoch 010:    469 / 770 loss=3.741, wps=4895.6, ups=3.55, wpb=1380.9, bsz=1380.9, num_updates=7350, lr=0.000193, gnorm=3.846, clip=0, train_wall=13, gb_free=66.7, wall=2183 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:23:48]    INFO >> epoch 010:    519 / 770 loss=3.652, wps=5659.3, ups=3.64, wpb=1556.8, bsz=1556.8, num_updates=7400, lr=0.000193, gnorm=5.051, clip=0, train_wall=13, gb_free=62, wall=2197 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:24:03]    INFO >> epoch 010:    569 / 770 loss=3.692, wps=5133.1, ups=3.67, wpb=1398.6, bsz=1398.6, num_updates=7450, lr=0.000193, gnorm=4.802, clip=0, train_wall=13, gb_free=64.4, wall=2210 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:24:07] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 79.14 GiB of which 263.25 MiB is free. Including non-PyTorch memory, this process has 78.86 GiB memory in use. Of the allocated memory 76.83 GiB is allocated by PyTorch, and 1.54 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:24:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 50           |        cudaMalloc retries: 64        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78195 MiB |  79031 MiB | 484713 GiB | 484637 GiB |
|       from large pool |  78182 MiB |  79018 MiB | 482407 GiB | 482330 GiB |
|       from small pool |     12 MiB |     13 MiB |   2306 GiB |   2306 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78195 MiB |  79031 MiB | 484713 GiB | 484637 GiB |
|       from large pool |  78182 MiB |  79018 MiB | 482407 GiB | 482330 GiB |
|       from small pool |     12 MiB |     13 MiB |   2306 GiB |   2306 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78178 MiB |  79013 MiB | 484142 GiB | 484066 GiB |
|       from large pool |  78165 MiB |  79000 MiB | 481839 GiB | 481762 GiB |
|       from small pool |     12 MiB |     13 MiB |   2303 GiB |   2303 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80242 MiB |  80242 MiB | 401160 MiB | 320918 MiB |
|       from large pool |  80224 MiB |  80224 MiB | 393604 MiB | 313380 MiB |
|       from small pool |     18 MiB |    288 MiB |   7556 MiB |   7538 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   2046 MiB |   5347 MiB | 462623 GiB | 462621 GiB |
|       from large pool |   2041 MiB |   5341 MiB | 459951 GiB | 459949 GiB |
|       from small pool |      5 MiB |     13 MiB |   2672 GiB |   2672 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   23082 K  |   23081 K  |
|       from large pool |     314    |     322    |    9098 K  |    9098 K  |
|       from small pool |     291    |     336    |   13983 K  |   13983 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   23082 K  |   23081 K  |
|       from large pool |     314    |     322    |    9098 K  |    9098 K  |
|       from small pool |     291    |     336    |   13983 K  |   13983 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |      98    |     232    |    5083    |    4985    |
|       from large pool |      89    |      89    |    1305    |    1216    |
|       from small pool |       9    |     144    |    3778    |    3769    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |      96    |      96    |   13938 K  |   13938 K  |
|       from large pool |      75    |      75    |    5774 K  |    5774 K  |
|       from small pool |      21    |      36    |    8164 K  |    8164 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:24:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:24:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:24:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:24:07] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:24:17]    INFO >> epoch 010:    620 / 770 loss=3.8, wps=4811.4, ups=3.48, wpb=1381.3, bsz=1381.3, num_updates=7500, lr=0.000193, gnorm=4.867, clip=0, train_wall=13, gb_free=64.3, wall=2225 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:24:33]    INFO >> epoch 010:    670 / 770 loss=3.557, wps=4981.9, ups=3.54, wpb=1406.9, bsz=1406.9, num_updates=7550, lr=0.000193, gnorm=4.8, clip=0, train_wall=13, gb_free=65, wall=2239 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:24:47]    INFO >> epoch 010:    720 / 770 loss=3.692, wps=5562.5, ups=3.59, wpb=1548, bsz=1548, num_updates=7600, lr=0.000193, gnorm=4.787, clip=0, train_wall=13, gb_free=61.6, wall=2253 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:24:59]    INFO >> epoch 010:    770 / 770 loss=3.748, wps=5527.9, ups=3.93, wpb=1405.6, bsz=1405.6, num_updates=7650, lr=0.000193, gnorm=4.36, clip=0, train_wall=12, gb_free=62.8, wall=2266 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:24:59]    INFO >> epoch 010 | loss 3.693 | wps 4914.6 | ups 3.46 | wpb 1419.5 | bsz 1419.5 | num_updates 7650 | lr 0.000193 | gnorm 4.683 | clip 0 | train_wall 193 | gb_free 62.8 | wall 2266 (progress_bar.py:267, print())[0m
[33m[2025-11-19 05:24:59] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 05:25:14]    INFO >> epoch 010 | valid on 'valid' subset | loss 3.799 | wps 11088.6 | wpb 5412.5 | bsz 5412.5 | num_updates 7650 | best_loss 5.742 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 05:25:15]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 05:25:15]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/checkpoints/checkpoint_last.pt (epoch 10 @ 7650 updates, score 3.799) (writing took 0.013479 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[33m[2025-11-19 05:25:15] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[33m[2025-11-19 05:25:29] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.67 GiB. GPU 0 has a total capacity of 79.14 GiB of which 425.25 MiB is free. Including non-PyTorch memory, this process has 78.70 GiB memory in use. Of the allocated memory 76.83 GiB is allocated by PyTorch, and 1.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:25:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 51           |        cudaMalloc retries: 66        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  78200 MiB |  79035 MiB | 503531 GiB | 503455 GiB |
|       from large pool |  78187 MiB |  79023 MiB | 501120 GiB | 501043 GiB |
|       from small pool |     12 MiB |     13 MiB |   2411 GiB |   2411 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  78200 MiB |  79035 MiB | 503531 GiB | 503455 GiB |
|       from large pool |  78187 MiB |  79023 MiB | 501120 GiB | 501043 GiB |
|       from small pool |     12 MiB |     13 MiB |   2411 GiB |   2411 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  78178 MiB |  79013 MiB | 502937 GiB | 502861 GiB |
|       from large pool |  78165 MiB |  79000 MiB | 500529 GiB | 500453 GiB |
|       from small pool |     12 MiB |     13 MiB |   2407 GiB |   2407 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80080 MiB |  80366 MiB | 412212 MiB | 332132 MiB |
|       from large pool |  80062 MiB |  80062 MiB | 404370 MiB | 324308 MiB |
|       from small pool |     18 MiB |    304 MiB |   7842 MiB |   7824 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1879 MiB |   5005 MiB | 479802 GiB | 479800 GiB |
|       from large pool |   1874 MiB |   5000 MiB | 477012 GiB | 477011 GiB |
|       from small pool |      5 MiB |     13 MiB |   2789 GiB |   2789 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     605    |     613    |   24052 K  |   24051 K  |
|       from large pool |     314    |     322    |    9419 K  |    9419 K  |
|       from small pool |     291    |     336    |   14632 K  |   14631 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     605    |     613    |   24052 K  |   24051 K  |
|       from large pool |     314    |     322    |    9419 K  |    9419 K  |
|       from small pool |     291    |     336    |   14632 K  |   14631 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     126    |     269    |    5258    |    5132    |
|       from large pool |     117    |     117    |    1337    |    1220    |
|       from small pool |       9    |     152    |    3921    |    3912    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     104    |     105    |   14539 K  |   14539 K  |
|       from large pool |      86    |      87    |    5979 K  |    5978 K  |
|       from small pool |      18    |      41    |    8560 K  |    8560 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:25:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:25:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:25:29] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:25:29] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:25:30]    INFO >> epoch 011:     51 / 770 loss=3.494, wps=2783.7, ups=1.72, wpb=1615.3, bsz=1615.3, num_updates=7700, lr=0.000161, gnorm=5.031, clip=0, train_wall=14, gb_free=2.8, wall=2295 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:25:44]    INFO >> epoch 011:    101 / 770 loss=3.684, wps=5265.2, ups=3.91, wpb=1345.6, bsz=1345.6, num_updates=7750, lr=0.000161, gnorm=4.73, clip=0, train_wall=12, gb_free=65.8, wall=2307 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:25:58]    INFO >> epoch 011:    151 / 770 loss=3.547, wps=4849.8, ups=3.49, wpb=1388.2, bsz=1388.2, num_updates=7800, lr=0.000161, gnorm=4.37, clip=0, train_wall=13, gb_free=62.7, wall=2322 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:26:13]    INFO >> epoch 011:    201 / 770 loss=3.639, wps=5354.6, ups=3.67, wpb=1459.2, bsz=1459.2, num_updates=7850, lr=0.000161, gnorm=4.15, clip=0, train_wall=13, gb_free=58.1, wall=2335 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:26:16] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.85 GiB. GPU 0 has a total capacity of 79.14 GiB of which 427.25 MiB is free. Including non-PyTorch memory, this process has 78.70 GiB memory in use. Of the allocated memory 73.73 GiB is allocated by PyTorch, and 4.47 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:26:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 52           |        cudaMalloc retries: 67        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75060 MiB |  75499 MiB | 513078 GiB | 513005 GiB |
|       from large pool |  75048 MiB |  75487 MiB | 510625 GiB | 510552 GiB |
|       from small pool |     11 MiB |     14 MiB |   2452 GiB |   2452 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75060 MiB |  75499 MiB | 513078 GiB | 513005 GiB |
|       from large pool |  75048 MiB |  75487 MiB | 510625 GiB | 510552 GiB |
|       from small pool |     11 MiB |     14 MiB |   2452 GiB |   2452 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75041 MiB |  75480 MiB | 512473 GiB | 512400 GiB |
|       from large pool |  75030 MiB |  75469 MiB | 510024 GiB | 509951 GiB |
|       from small pool |     11 MiB |     14 MiB |   2449 GiB |   2449 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80078 MiB |  80350 MiB | 412482 MiB | 332404 MiB |
|       from large pool |  80062 MiB |  80062 MiB | 404370 MiB | 324308 MiB |
|       from small pool |     16 MiB |    288 MiB |   8112 MiB |   8096 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   5017 MiB |   8926 MiB | 488849 GiB | 488844 GiB |
|       from large pool |   5013 MiB |   8921 MiB | 486010 GiB | 486005 GiB |
|       from small pool |      4 MiB |     21 MiB |   2838 GiB |   2838 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     647    |   24483 K  |   24482 K  |
|       from large pool |     349    |     355    |    9605 K  |    9604 K  |
|       from small pool |     292    |     342    |   14877 K  |   14877 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     647    |   24483 K  |   24482 K  |
|       from large pool |     349    |     355    |    9605 K  |    9604 K  |
|       from small pool |     292    |     342    |   14877 K  |   14877 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     125    |     261    |    5393    |    5268    |
|       from large pool |     117    |     117    |    1337    |    1220    |
|       from small pool |       8    |     144    |    4056    |    4048    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     111    |     118    |   14802 K  |   14802 K  |
|       from large pool |      92    |      99    |    6099 K  |    6099 K  |
|       from small pool |      19    |      47    |    8703 K  |    8702 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:26:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:26:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:26:16] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:26:16] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:26:27]    INFO >> epoch 011:    252 / 770 loss=3.579, wps=5224.6, ups=3.47, wpb=1506, bsz=1506, num_updates=7900, lr=0.000161, gnorm=4.687, clip=0, train_wall=13, gb_free=62.6, wall=2350 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:26:40]    INFO >> epoch 011:    302 / 770 loss=3.693, wps=4981.3, ups=3.91, wpb=1272.7, bsz=1272.7, num_updates=7950, lr=0.000161, gnorm=4.273, clip=0, train_wall=12, gb_free=65.9, wall=2363 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:26:54]    INFO >> epoch 011:    352 / 770 loss=3.608, wps=5207.1, ups=4.05, wpb=1286.5, bsz=1286.5, num_updates=8000, lr=0.000161, gnorm=4.337, clip=0, train_wall=12, gb_free=64.9, wall=2375 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:27:07] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 2.58 GiB. GPU 0 has a total capacity of 79.14 GiB of which 415.25 MiB is free. Including non-PyTorch memory, this process has 78.71 GiB memory in use. Of the allocated memory 72.90 GiB is allocated by PyTorch, and 5.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:27:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 53           |        cudaMalloc retries: 68        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  71012 MiB |  74647 MiB | 523848 GiB | 523779 GiB |
|       from large pool |  70990 MiB |  74625 MiB | 521350 GiB | 521281 GiB |
|       from small pool |     22 MiB |     22 MiB |   2497 GiB |   2497 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  71012 MiB |  74647 MiB | 523848 GiB | 523779 GiB |
|       from large pool |  70990 MiB |  74625 MiB | 521350 GiB | 521281 GiB |
|       from small pool |     22 MiB |     22 MiB |   2497 GiB |   2497 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  70995 MiB |  74629 MiB | 523229 GiB | 523160 GiB |
|       from large pool |  70972 MiB |  74606 MiB | 520735 GiB | 520666 GiB |
|       from small pool |     22 MiB |     22 MiB |   2493 GiB |   2493 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80090 MiB |  80160 MiB | 412564 MiB | 332474 MiB |
|       from large pool |  80062 MiB |  80062 MiB | 404370 MiB | 324308 MiB |
|       from small pool |     28 MiB |     98 MiB |   8194 MiB |   8166 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   6345 MiB |   8790 MiB | 499020 GiB | 499014 GiB |
|       from large pool |   6339 MiB |   8785 MiB | 496129 GiB | 496122 GiB |
|       from small pool |      5 MiB |     15 MiB |   2891 GiB |   2891 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     641    |     649    |   24962 K  |   24961 K  |
|       from large pool |     335    |     343    |    9819 K  |    9819 K  |
|       from small pool |     306    |     336    |   15142 K  |   15142 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     641    |     649    |   24962 K  |   24961 K  |
|       from large pool |     335    |     343    |    9819 K  |    9819 K  |
|       from small pool |     306    |     336    |   15142 K  |   15142 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     131    |     166    |    5434    |    5303    |
|       from large pool |     117    |     117    |    1337    |    1220    |
|       from small pool |      14    |      49    |    4097    |    4083    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     117    |     125    |   15089 K  |   15089 K  |
|       from large pool |      92    |     101    |    6238 K  |    6238 K  |
|       from small pool |      25    |      39    |    8851 K  |    8851 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:27:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:27:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:27:07] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:27:07] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:27:08]    INFO >> epoch 011:    403 / 770 loss=3.725, wps=5057.7, ups=3.61, wpb=1401, bsz=1401, num_updates=8050, lr=0.000161, gnorm=4.651, clip=0, train_wall=12, gb_free=64.8, wall=2389 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:27:22]    INFO >> epoch 011:    453 / 770 loss=3.645, wps=5616.7, ups=3.79, wpb=1482.2, bsz=1482.2, num_updates=8100, lr=0.000161, gnorm=5.47, clip=0, train_wall=12, gb_free=47.3, wall=2402 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:27:35]    INFO >> epoch 011:    503 / 770 loss=3.809, wps=5334.7, ups=4, wpb=1333.9, bsz=1333.9, num_updates=8150, lr=0.000161, gnorm=4.082, clip=0, train_wall=12, gb_free=69.4, wall=2414 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:27:44] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 1.88 GiB. GPU 0 has a total capacity of 79.14 GiB of which 867.25 MiB is free. Including non-PyTorch memory, this process has 78.27 GiB memory in use. Of the allocated memory 74.71 GiB is allocated by PyTorch, and 3.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:27:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 54           |        cudaMalloc retries: 70        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  75882 MiB |  76503 MiB | 531700 GiB | 531626 GiB |
|       from large pool |  75868 MiB |  76489 MiB | 529168 GiB | 529094 GiB |
|       from small pool |     14 MiB |     15 MiB |   2531 GiB |   2531 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  75882 MiB |  76503 MiB | 531700 GiB | 531626 GiB |
|       from large pool |  75868 MiB |  76489 MiB | 529168 GiB | 529094 GiB |
|       from small pool |     14 MiB |     15 MiB |   2531 GiB |   2531 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  75865 MiB |  76486 MiB | 531071 GiB | 530997 GiB |
|       from large pool |  75851 MiB |  76472 MiB | 528544 GiB | 528470 GiB |
|       from small pool |     14 MiB |     15 MiB |   2527 GiB |   2527 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  79638 MiB |  79638 MiB | 416320 MiB | 336682 MiB |
|       from large pool |  79618 MiB |  79618 MiB | 408022 MiB | 328404 MiB |
|       from small pool |     20 MiB |    132 MiB |   8298 MiB |   8278 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   3693 MiB |   8941 MiB | 506305 GiB | 506302 GiB |
|       from large pool |   3687 MiB |   8935 MiB | 503374 GiB | 503370 GiB |
|       from small pool |      5 MiB |     15 MiB |   2931 GiB |   2931 GiB |
|---------------------------------------------------------------------------|
| Allocations           |     567    |     574    |   25320 K  |   25320 K  |
|       from large pool |     277    |     284    |    9976 K  |    9976 K  |
|       from small pool |     290    |     336    |   15344 K  |   15343 K  |
|---------------------------------------------------------------------------|
| Active allocs         |     567    |     574    |   25320 K  |   25320 K  |
|       from large pool |     277    |     284    |    9976 K  |    9976 K  |
|       from small pool |     290    |     336    |   15344 K  |   15343 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     108    |     185    |    5490    |    5382    |
|       from large pool |      98    |     119    |    1341    |    1243    |
|       from small pool |      10    |      66    |    4149    |    4139    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     104    |     105    |   15308 K  |   15308 K  |
|       from large pool |      84    |      85    |    6342 K  |    6342 K  |
|       from small pool |      20    |      38    |    8965 K  |    8965 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:27:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:27:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:27:44] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:27:44] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:27:49]    INFO >> epoch 011:    554 / 770 loss=3.77, wps=4997.9, ups=3.52, wpb=1421.6, bsz=1421.6, num_updates=8200, lr=0.000161, gnorm=4.318, clip=0, train_wall=13, gb_free=63, wall=2429 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:28:04]    INFO >> epoch 011:    604 / 770 loss=3.673, wps=4740.3, ups=3.72, wpb=1275.8, bsz=1275.8, num_updates=8250, lr=0.000161, gnorm=4.378, clip=0, train_wall=13, gb_free=64.6, wall=2442 (progress_bar.py:258, log())[0m
[33m[2025-11-19 05:28:06] WARNING >> OOM: Ran out of memory with exception: CUDA out of memory. Tried to allocate 60.00 MiB. GPU 0 has a total capacity of 79.14 GiB of which 47.25 MiB is free. Including non-PyTorch memory, this process has 79.07 GiB memory in use. Of the allocated memory 77.40 GiB is allocated by PyTorch, and 1.17 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables) (ncc_trainers.py:760, _log_oom())[0m
[33m[2025-11-19 05:28:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 0                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 55           |        cudaMalloc retries: 71        |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |  79200 MiB |  79258 MiB | 536222 GiB | 536144 GiB |
|       from large pool |  78930 MiB |  78988 MiB | 533670 GiB | 533593 GiB |
|       from small pool |    269 MiB |    271 MiB |   2551 GiB |   2551 GiB |
|---------------------------------------------------------------------------|
| Active memory         |  79200 MiB |  79258 MiB | 536222 GiB | 536144 GiB |
|       from large pool |  78930 MiB |  78988 MiB | 533670 GiB | 533593 GiB |
|       from small pool |    269 MiB |    271 MiB |   2551 GiB |   2551 GiB |
|---------------------------------------------------------------------------|
| Requested memory      |  79180 MiB |  79238 MiB | 535588 GiB | 535511 GiB |
|       from large pool |  78912 MiB |  78970 MiB | 533041 GiB | 532964 GiB |
|       from small pool |    268 MiB |    269 MiB |   2547 GiB |   2547 GiB |
|---------------------------------------------------------------------------|
| GPU reserved memory   |  80458 MiB |  80460 MiB | 417204 MiB | 336746 MiB |
|       from large pool |  80156 MiB |  80156 MiB | 408622 MiB | 328466 MiB |
|       from small pool |    302 MiB |    304 MiB |   8582 MiB |   8280 MiB |
|---------------------------------------------------------------------------|
| Non-releasable memory |   1197 MiB |   5502 MiB | 510940 GiB | 510939 GiB |
|       from large pool |   1165 MiB |   5497 MiB | 507984 GiB | 507983 GiB |
|       from small pool |     32 MiB |     34 MiB |   2955 GiB |   2955 GiB |
|---------------------------------------------------------------------------|
| Allocations           |    5206    |    5209    |   25531 K  |   25526 K  |
|       from large pool |     751    |     752    |   10064 K  |   10063 K  |
|       from small pool |    4455    |    4458    |   15466 K  |   15462 K  |
|---------------------------------------------------------------------------|
| Active allocs         |    5206    |    5209    |   25531 K  |   25526 K  |
|       from large pool |     751    |     752    |   10064 K  |   10063 K  |
|       from small pool |    4455    |    4458    |   15466 K  |   15462 K  |
|---------------------------------------------------------------------------|
| GPU reserved segments |     258    |     259    |    5642    |    5384    |
|       from large pool |     107    |     107    |    1351    |    1244    |
|       from small pool |     151    |     152    |    4291    |    4140    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |     370    |     371    |   15432 K  |   15432 K  |
|       from large pool |      97    |      98    |    6397 K  |    6397 K  |
|       from small pool |     273    |     274    |    9035 K  |    9035 K  |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:28:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 1                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:28:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 2                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:28:06] WARNING >> |===========================================================================|
|                  PyTorch CUDA memory summary, device ID 3                 |
|---------------------------------------------------------------------------|
|            CUDA OOMs: 0            |        cudaMalloc retries: 0         |
|===========================================================================|
|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |
|---------------------------------------------------------------------------|
| Allocated memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Active memory         |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Requested memory      |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| GPU reserved memory   |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Non-releasable memory |      0 B   |      0 B   |      0 B   |      0 B   |
|       from large pool |      0 B   |      0 B   |      0 B   |      0 B   |
|       from small pool |      0 B   |      0 B   |      0 B   |      0 B   |
|---------------------------------------------------------------------------|
| Allocations           |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Active allocs         |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| GPU reserved segments |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Non-releasable allocs |       0    |       0    |       0    |       0    |
|       from large pool |       0    |       0    |       0    |       0    |
|       from small pool |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize allocations  |       0    |       0    |       0    |       0    |
|---------------------------------------------------------------------------|
| Oversize GPU segments |       0    |       0    |       0    |       0    |
|===========================================================================|
 (ncc_trainers.py:763, _log_oom())[0m
[33m[2025-11-19 05:28:06] WARNING >> attempting to recover from OOM in forward/backward pass (ncc_trainers.py:399, train_step())[0m
[32m[2025-11-19 05:28:18]    INFO >> epoch 011:    655 / 770 loss=3.632, wps=4989.2, ups=3.42, wpb=1458, bsz=1458, num_updates=8300, lr=0.000161, gnorm=5.752, clip=0, train_wall=13, gb_free=61.6, wall=2457 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:28:34]    INFO >> epoch 011:    705 / 770 loss=3.69, wps=5619, ups=3.6, wpb=1558.7, bsz=1558.7, num_updates=8350, lr=0.000161, gnorm=5.342, clip=0, train_wall=13, gb_free=63.6, wall=2471 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:28:47]    INFO >> epoch 011:    755 / 770 loss=3.739, wps=5601, ups=3.72, wpb=1505.4, bsz=1505.4, num_updates=8400, lr=0.000161, gnorm=4.455, clip=0, train_wall=13, gb_free=67.9, wall=2484 (progress_bar.py:258, log())[0m
[32m[2025-11-19 05:28:51]    INFO >> epoch 011 | loss 3.659 | wps 4881.3 | ups 3.44 | wpb 1419.5 | bsz 1419.5 | num_updates 8415 | lr 0.000161 | gnorm 4.669 | clip 0 | train_wall 194 | gb_free 58.6 | wall 2488 (progress_bar.py:267, print())[0m
[33m[2025-11-19 05:28:51] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
[32m[2025-11-19 05:29:06]    INFO >> epoch 011 | valid on 'valid' subset | loss 3.798 | wps 11246.6 | wpb 5412.5 | bsz 5412.5 | num_updates 8415 | best_loss 5.742 (progress_bar.py:267, print())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 05:29:06]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 05:29:06]    INFO >> saved checkpoint /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/checkpoints/checkpoint_last.pt (epoch 11 @ 8415 updates, score 3.798) (writing took 0.012898 seconds) (checkpoint_utils.py:78, save_checkpoint())[0m
[32m[2025-11-19 05:29:06]    INFO >> æ—©åœ: éªŒè¯æ€§èƒ½å·²10è½®æœªæå‡ (train_enhanced.py:345, single_main())[0m
[32m[2025-11-19 05:29:06]    INFO >> è®­ç»ƒå®Œæˆï¼Œç”¨æ—¶ 2445.4 ç§’ (train_enhanced.py:355, single_main())[0m
No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.
[32m[2025-11-19 05:29:07]    INFO >> è®­ç»ƒæ›²çº¿å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/logs/plots/training.png (train_enhanced.py:118, plot())[0m
[32m[2025-11-19 05:29:07]    INFO >> æ‰€æœ‰æ—¥å¿—å·²ä¿å­˜åˆ°: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_batch_64/logs (train_enhanced.py:359, single_main())[0m

âœ“ exp_batch_64 æˆåŠŸ

ç­‰å¾…3ç§’...

è¿›åº¦: 5/6

============================================================
å®éªŒ: exp_hidden_128 - éšè—å±‚128
æ—¶é—´: 2025-11-19 05:29:48
============================================================

[32m[2025-11-19 05:29:50]    INFO >> åŠ è½½é…ç½®: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_hidden_128/config.yml (train_enhanced.py:382, cli_main())[0m
[32m[2025-11-19 05:29:50]    INFO >> å•GPUè®­ç»ƒ... (train_enhanced.py:410, cli_main())[0m
[32m[2025-11-19 05:29:50]    INFO >> è®­ç»ƒæ—¥å¿—å°†ä¿å­˜åˆ°: /home/zhaojunzhang/naturalcc/typilus/experiments/exp_hidden_128/logs (train_enhanced.py:296, single_main())[0m
[32m[2025-11-19 05:29:50]    INFO >> [nodes] dictionary: 9999 types (typilus.py:102, setup_task())[0m
[32m[2025-11-19 05:29:50]    INFO >> [edges] dictionary: 0 types (typilus.py:102, setup_task())[0m
[32m[2025-11-19 05:29:50]    INFO >> [supernodes.annotation] dictionary: 99 types (typilus.py:106, setup_task())[0m
[32m[2025-11-19 05:29:58]    INFO >> Typilus(
  (encoder): GGNNEncoder(
    (node_embedding): Embedding(9999, 128, padding_idx=0)
    (node_layer): Sequential(
      (0): Dropout(p=0.1, inplace=False)
      (1): Linear(in_features=128, out_features=64, bias=False)
      (2): Dropout(p=0.1, inplace=False)
    )
    (ggnns): ModuleList(
      (0): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(64, 128)
      )
      (1): GatedGNN(
        (edge_weights): ModuleDict(
          (CHILD): Linear(in_features=64, out_features=64, bias=False)
          (OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (NEXT): Linear(in_features=64, out_features=64, bias=False)
          (SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
          (_CHILD): Linear(in_features=64, out_features=64, bias=False)
          (_OCCURRENCE_OF): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT): Linear(in_features=64, out_features=64, bias=False)
          (_SUBTOKEN_OF): Linear(in_features=64, out_features=64, bias=False)
          (_COMPUTED_FROM): Linear(in_features=64, out_features=64, bias=False)
          (_LAST_LEXICAL_USE): Linear(in_features=64, out_features=64, bias=False)
          (_NEXT_USE): Linear(in_features=64, out_features=64, bias=False)
          (_RETURNS_TO): Linear(in_features=64, out_features=64, bias=False)
        )
        (rnn_cell): GRUCell(192, 128)
      )
    )
  )
  (decoder): DenseDecoder(
    (cls_layers): Sequential(
      (0): Linear(in_features=64, out_features=64, bias=False)
      (1): Dropout(p=0.1, inplace=False)
      (2): Linear(in_features=64, out_features=99, bias=True)
    )
  )
) (train_enhanced.py:303, single_main())[0m
[32m[2025-11-19 05:29:58]    INFO >> æ¨¡å‹: typilus, æŸå¤±å‡½æ•°: TypilusCriterion (train_enhanced.py:304, single_main())[0m
[32m[2025-11-19 05:29:58]    INFO >> æ¨¡å‹å‚æ•°: 1627811 (å¯è®­ç»ƒ: 1627811) (train_enhanced.py:305, single_main())[0m
[32m[2025-11-19 05:29:58]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:530, pretty_print_cuda_env_list())[0m
[32m[2025-11-19 05:29:58]    INFO >> rank   0: capabilities =  8.0  ; total memory = 81920 MB ; free memory = 80579 MB ; used memory = 1340 MB ; name = NVIDIA A800 80GB PCIe                    (utils.py:532, pretty_print_cuda_env_list())[0m
[32m[2025-11-19 05:29:58]    INFO >> ***********************CUDA enviroments for all 1 workers*********************** (utils.py:540, pretty_print_cuda_env_list())[0m
[32m[2025-11-19 05:29:58]    INFO >> ä½¿ç”¨ 1 ä¸ªGPUè®­ç»ƒ (train_enhanced.py:311, single_main())[0m
[32m[2025-11-19 05:29:58]    INFO >> no existing checkpoint found /home/zhaojunzhang/naturalcc/typilus/experiments/exp_hidden_128/checkpoints/checkpoint_last.pt (ncc_trainers.py:299, load_checkpoint())[0m
[32m[2025-11-19 05:29:58]    INFO >> loading train data for epoch 1 (ncc_trainers.py:314, get_train_iterator())[0m
[32m[2025-11-19 05:30:57]    INFO >> NOTE: your device may support faster training with fp16 or --amp (ncc_trainers.py:183, _setup_optimizer())[0m
[33m[2025-11-19 05:30:57] WARNING >> tensorboard or required dependencies not found, please see README for using tensorboard. (e.g. pip install tensorboardX) (progress_bar.py:314, __init__())[0m
/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py:348: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.
  with torch.cuda.amp.autocast(enabled=(isinstance(optimizer, AMPOptimizer))):
Traceback (most recent call last):
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py", line 415, in <module>
    cli_main()
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py", line 411, in cli_main
    single_main(args)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py", line 326, in single_main
    train_stats = train(args, trainer, task, epoch_itr)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools/train_enhanced.py", line 162, in train
    log_output = trainer.train_step(samples)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/contextlib.py", line 75, in inner
    return func(*args, **kwds)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 409, in train_step
    raise e
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/trainers/ncc_trainers.py", line 377, in train_step
    loss, sample_size_i, logging_output = self.task.train_step(
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/tasks/ncc_task.py", line 349, in train_step
    loss, sample_size, logging_output = criterion(model, sample)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/criterions/type_prediction/typilus.py", line 23, in forward
    net_output = model(**sample['net_input'], tgt_ids=sample['target'])
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/models/type_prediction/typilus.py", line 165, in forward
    encoder_out = self.encoder(src_graphs, **kwargs)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/models/type_prediction/typilus.py", line 111, in forward
    graphs.ndata[idx] = ggnn(graphs, prev_key=idx - 1, residual_state=residual_state)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/zhaojunzhang/workspace/type_pred/naturalcc/ncc/models/type_prediction/typilus.py", line 54, in forward
    prev_state = self.rnn_cell(graph_feature, prev_state)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
    return forward_call(*args, **kwargs)
  File "/mnt/data1/zhaojunzhang/packages/anaconda3/envs/naturalcc/lib/python3.8/site-packages/torch/nn/modules/rnn.py", line 1477, in forward
    ret = _VF.gru_cell(
RuntimeError: hidden0 has inconsistent hidden_size: got 64, expected 128

âœ— exp_hidden_128 å¤±è´¥(1)

å®éªŒå¤±è´¥ï¼Œç»§ç»­? (y/n): n

============================================================
å®éªŒå®Œæˆ
============================================================
âœ“ baseline
âœ“ exp_lr_1e-3
âœ“ exp_lr_1e-4
âœ“ exp_batch_64
âœ— exp_hidden_128
============================================================

æ­£åœ¨åˆ†æç»“æœ...

================================================================================
å®éªŒç»“æœå¯¹æ¯”
================================================================================
å®éªŒåç§°                      è½®æ•°       æœ€ç»ˆLoss       æœ€ä½³éªŒè¯Loss       
--------------------------------------------------------------------------------
exp_lr_1e-3               11       3.4130       3.5840         
baseline                  11       3.4400       3.6090         
exp_batch_64              11       3.6590       3.7980         
exp_lr_1e-4               11       3.8870       3.9420         
================================================================================
run_experiments.py:504: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.tight_layout()
run_experiments.py:504: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.tight_layout()
run_experiments.py:504: UserWarning: Glyph 23545 (\N{CJK UNIFIED IDEOGRAPH-5BF9}) missing from current font.
  plt.tight_layout()
run_experiments.py:504: UserWarning: Glyph 27604 (\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from current font.
  plt.tight_layout()
run_experiments.py:504: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.tight_layout()
run_experiments.py:504: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.tight_layout()
run_experiments.py:505: UserWarning: Glyph 39564 (\N{CJK UNIFIED IDEOGRAPH-9A8C}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')
run_experiments.py:505: UserWarning: Glyph 35777 (\N{CJK UNIFIED IDEOGRAPH-8BC1}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')
run_experiments.py:505: UserWarning: Glyph 23545 (\N{CJK UNIFIED IDEOGRAPH-5BF9}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')
run_experiments.py:505: UserWarning: Glyph 27604 (\N{CJK UNIFIED IDEOGRAPH-6BD4}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')
run_experiments.py:505: UserWarning: Glyph 26368 (\N{CJK UNIFIED IDEOGRAPH-6700}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')
run_experiments.py:505: UserWarning: Glyph 20339 (\N{CJK UNIFIED IDEOGRAPH-4F73}) missing from current font.
  plt.savefig(exp_base_dir / 'comparison.png', dpi=120, bbox_inches='tight')

å¯¹æ¯”å›¾å·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/comparison.png
æŠ¥å‘Šå·²ä¿å­˜: /home/zhaojunzhang/naturalcc/typilus/experiments/report.md

(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ 
(naturalcc) [01;32mzhaojunzhang@dlserver6-Super-Server[00m:[01;34m~/workspace/type_pred/naturalcc/run/type_prediction/typilus/experiment_tools[00m$ exit [A[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[C[Kt
exit
